<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="RLHF人类反馈的强化学习(RLHF)是一种结合了人类反馈与强化学习技术的机器学习方法，旨在提高人工智能模型的表现，尤其是在生成式人工智能（如LLM）中的应用。人类反馈的强化学习(RLHF)的核心思想是利用人类提供的反馈来优化机器学习模型，使其能够更好地满足用户需求和期望。传统的强化学习依赖于预定义的奖励函数来指导学习，而RLHF则将人类的主观反馈纳入其中，以便更灵活地捕捉复杂任务中的细微差别和主">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(ML)(二十二) — 强化学习探析">
<meta property="og:url" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="RLHF人类反馈的强化学习(RLHF)是一种结合了人类反馈与强化学习技术的机器学习方法，旨在提高人工智能模型的表现，尤其是在生成式人工智能（如LLM）中的应用。人类反馈的强化学习(RLHF)的核心思想是利用人类提供的反馈来优化机器学习模型，使其能够更好地满足用户需求和期望。传统的强化学习依赖于预定义的奖励函数来指导学习，而RLHF则将人类的主观反馈纳入其中，以便更灵活地捕捉复杂任务中的细微差别和主">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/ml_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/ml_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/ml_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/ml_4.png">
<meta property="article:published_time" content="2024-12-26T02:00:11.000Z">
<meta property="article:modified_time" content="2024-12-26T02:00:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/ml_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/","path":"2024/12/26/artificial-intelligence/ML22_theory_study/","title":"机器学习(ML)(二十二) — 强化学习探析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习(ML)(二十二) — 强化学习探析 | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#RLHF"><span class="nav-number">1.</span> <span class="nav-text">RLHF</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">预训练语言模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">训练奖励模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BE%AE%E8%B0%83"><span class="nav-number">1.3.</span> <span class="nav-text">强化学习微调</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NLPO"><span class="nav-number">2.</span> <span class="nav-text">NLPO</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">250</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习(ML)(二十二) — 强化学习探析 | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习(ML)(二十二) — 强化学习探析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-26 10:00:11" itemprop="dateCreated datePublished" datetime="2024-12-26T10:00:11+08:00">2024-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h4><p><strong>人类反馈的强化学习</strong>(<code>RLHF</code>)是一种结合了<strong>人类反馈</strong>与<strong>强化学习</strong>技术的<strong>机器学习</strong>方法，旨在提高人工智能模型的表现，尤其是在生成式人工智能（如<code>LLM</code>）中的应用。<strong>人类反馈的强化学习</strong>(<code>RLHF</code>)的核心思想是利用人类提供的反馈来优化<strong>机器学习</strong>模型，使其能够更好地满足用户需求和期望。传统的<strong>强化学习</strong>依赖于预定义的<strong>奖励函数</strong>来指导学习，而<code>RLHF</code>则将人类的主观反馈纳入其中，以便更灵活地捕捉复杂任务中的细微差别和主观性。</p>
<span id="more"></span>
<p><code>RLHF</code>通常包括以下几个步骤：</p>
<ul>
<li><strong>预训练语言模型</strong>：首先，使用大量标注数据对语言模型进行预训练。这一步骤通常通过<strong>监督学习</strong>完成，以确保模型能够生成合理的初步输出。</li>
<li><strong>训练奖励模型</strong>：在此阶段，生成多个可能的问答，并由人类评估这些问答的质量。人类反馈被用于训练一个<strong>奖励模型</strong>，该模型能够评估生成内容的好坏。</li>
<li><strong>强化学习微调</strong>：最后，使用训练好的<strong>奖励模型</strong>对<strong>语言模型</strong>进行<strong>微调</strong>，通过<strong>强化学习算法</strong>（如<strong>近端策略优化</strong>：<code>PPO</code>）进一步优化其表现，以便更好地符合人类反馈和偏好。</li>
</ul>
<p><strong>人类反馈的强化学习</strong>(<code>RLHF</code>)在多个领域展现了其重要性，尤其是在<strong>自然语言处理</strong>(<code>NLP</code>)和<strong>生成式</strong><code>AI</code>中。通过引入<strong>人类反馈</strong>，<code>RLHF</code>能够：<strong>提高生成内容的人性化程度</strong>，使得<code>AI</code>生成的文本更符合人类的沟通习惯和情感表达；<strong>增强适应性</strong>，<code>AI</code>系统能够根据实时反馈调整其行为，<strong>解决复杂任务</strong>，在一些难以明确量化成功标准的任务中，<code>RLHF</code>提供了一种有效的方法来利用人类直观判断作为反馈。适应不断变化的用户需求和偏好。<strong>人类反馈的强化学习</strong>(<code>RLHF</code>)是一种前沿技术，通过将<strong>人类直观反馈</strong>与<strong>强化学习</strong>结合起来，为生成式<code>AI</code>的发展提供了新的方向。它不仅提高了<code>AI</code>系统与用户之间的互动质量，也为复杂任务提供了新的解决方案。</p>
<h5 id="预训练语言模型"><a href="#预训练语言模型" class="headerlink" title="预训练语言模型"></a>预训练语言模型</h5><p>首先，使用经典的预训练目标训练一个语言模型，对这一步模型，<code>OpenAI</code>在其第一个<code>RLHF</code>模型的<code>InstructGPT</code>中使用了较小版本的<code>GPT-3</code>；<code>Anthropic</code>使用了<code>100</code>0万 <code>～</code> <code>520</code>亿参数的<code>Transformer</code>模型进行训练；<code>DeepMind</code>使用了自家的<code>2800</code>亿参数模型<code>Gopher</code>。这里可以用额外的文本或者条件对这个<code>LM</code>进行微调，例如<code>OpenAI</code>采用 “<strong>更可取</strong>”(<code>preferable</code>)的人工生成文本进行了微调；而<code>Anthropic</code>采用了“<strong>有用、诚实和无害</strong>” 的标准在上下文线索上蒸馏了原始的<code>LM</code>。这里或许使用了昂贵的增强数据，但并不是<code>RLHF</code>必要的一步。由于<code>RLHF</code>还是一个尚待探索的领域，对于” 哪种模型” 适合作为<code>RLHF</code>的起点并没有明确的答案。</p>
<img data-src="/2024/12/26/artificial-intelligence/ML22_theory_study/ml_1.png" class="">

<h5 id="训练奖励模型"><a href="#训练奖励模型" class="headerlink" title="训练奖励模型"></a>训练奖励模型</h5><p><strong>奖励模型</strong> (<code>RM</code>，也叫<strong>偏好模型</strong>)的训练是<code>RLHF</code>区别于旧范式的开端。这一模型接收一系列文本并返回一个标量奖励，数值上对应人的偏好。我们可以用端到端的方式用<code>LM</code>建模，或者用模块化的系统建模 (比如对输出进行排名，再将排名转换为奖励) 。这一奖励数值将对后续无缝接入现有的<code>RL</code>算法至关重要。关于模型选择方面，<strong>奖励模型</strong>可以是另一个经过微调的<code>LM</code>，也可以根据偏好数据从头开始训练的<code>LM</code>。例如<code>Anthropic</code>提出了一种特殊的预训练方式，即用<strong>偏好模型预训练</strong>(<code>Preference Model Pretraining，PMP</code>)来替换一般预训练后的微调过程。因为前者被认为对样本数据的利用率更高。但对于哪种<strong>奖励模型</strong>更好尚无定论。</p>
<p>关于训练文本方面，<strong>奖励模型</strong> 的提示-生成对文本是从预定义数据集中采样生成的，并用初始的<code>LM</code>给这些提示生成文本。<code>Anthropic</code>的数据主要是通过<code>Amazon Mechanical Turk</code>上的聊天工具生成的，并在<code>Hub</code>上可用，而 <code>OpenAI</code>使用了用户提交给<code>GPT API</code>的<code>prompt</code>。</p>
<p>关于训练奖励数值方面，这里需要人工对<code>LM</code>生成的回答进行排名，起初可能会认为应该直接对文本标注分数来训练<strong>奖励模型</strong>，但是由于标注者的价值观不同导致这些分数未经过校准并且充满噪声。通过排名可以比较多个模型的输出并构建更好的规范数据集。对具体的排名方式，是对不同的<code>LM</code>在相同提示下的输出进行比较，然后使用<code>Elo</code>(评分系统，是一种用于计算棋手和其他竞技游戏玩家相对技能水平的方法)系统建立一个完整的排名。这些不同的排名结果将被归一化为用于训练的<strong>标量奖励值</strong>。</p>
<img data-src="/2024/12/26/artificial-intelligence/ML22_theory_study/ml_2.png" class="">

<h5 id="强化学习微调"><a href="#强化学习微调" class="headerlink" title="强化学习微调"></a>强化学习微调</h5><p>长期以来出于工程和算法原因，人们认为用<strong>强化学习</strong>训练LM是不可能的。而目前多个组织找到的可行方案是使用<strong>策略梯度强化学习</strong>(<code>Policy Gradient RL</code>)<strong>算法</strong>、<strong>近端策略优化</strong>(<code>Proximal Policy Optimization，PPO</code>)微调初始<code>LM</code>的部分或全部参数。因为微调整个<code>10B～100B+</code>参数的成本过高。首先将微调任务表述为<strong>强化学习</strong>问题。该策略是一个接受提示并返回一系列文本(或文本的概率分布)的<code>LM</code>。这个策略的<strong>动作空间</strong>(<code>action space</code>)是<code>LM</code>的词表对应的所有<strong>词元</strong> (一般在<code>50k</code>数量级)，<strong>观察空间</strong>(<code>observation space</code>)是输入<strong>词元序列</strong>，也比较大(词汇量 <code>x</code> 输入标记的数量)。<strong>奖励函数</strong>是奖<strong>励模型</strong>和<strong>策略转变约束</strong>(<code>Policy shift constraint</code>)的结合。<code>PPO</code>算法的奖励函数计算如下：将提示(<code>prompt</code>)<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-37-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-37-TEX-I-1D465"></use></g></g></g></svg></mjx-container>输入初始LM和当前微调的LM，分别得到了输出文本<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.199ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 2297.8 647" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-36-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-36-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-36-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-36-TEX-I-1D466"></use></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-36-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(926.6,0)"><use data-c="2C" xlink:href="#MJX-36-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(1371.2,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-36-TEX-I-1D466"></use></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-36-TEX-N-32"></use></g></g></g></g></svg></mjx-container>将来自当前策略的文本传递给<strong>奖励模型</strong>得到一个标量的奖励<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="1.958ex" height="1.355ex" role="img" focusable="false" viewBox="0 -442 865.6 599.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-36-TEX-I-1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D45F" xlink:href="#MJX-36-TEX-I-1D45F"></use></g><g data-mml-node="TeXAtom" transform="translate(484,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D703" xlink:href="#MJX-36-TEX-I-1D703"></use></g></g></g></g></g></svg></mjx-container>。将两个模型的生成文本进行比较计算差异的<strong>惩罚项</strong>，在来自<code>OpenAI、Anthropic</code>和<code>DeepMind</code>的多篇论文中设计为输出词分布序列之间的<code>Kullback–Leibler (KL) divergence</code>散度的缩放，即<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="13.8ex" height="1.926ex" role="img" focusable="false" viewBox="0 -694 6099.8 851.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-36-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-36-TEX-I-1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path><path id="MJX-36-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-36-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-36-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-36-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45F" xlink:href="#MJX-36-TEX-I-1D45F"></use></g><g data-mml-node="mo" transform="translate(728.8,0)"><use data-c="3D" xlink:href="#MJX-36-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(1784.6,0)"><g data-mml-node="mi"><use data-c="1D45F" xlink:href="#MJX-36-TEX-I-1D45F"></use></g><g data-mml-node="TeXAtom" transform="translate(484,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D703" xlink:href="#MJX-36-TEX-I-1D703"></use></g></g></g><g data-mml-node="mo" transform="translate(2872.4,0)"><use data-c="2212" xlink:href="#MJX-36-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(3872.6,0)"><use data-c="1D706" xlink:href="#MJX-36-TEX-I-1D706"></use></g><g data-mml-node="msub" transform="translate(4455.6,0)"><g data-mml-node="mi"><use data-c="1D45F" xlink:href="#MJX-36-TEX-I-1D45F"></use></g><g data-mml-node="TeXAtom" transform="translate(484,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D43E" xlink:href="#MJX-36-TEX-I-1D43E"></use></g><g data-mml-node="mi" transform="translate(889,0)"><use data-c="1D43F" xlink:href="#MJX-36-TEX-I-1D43F"></use></g></g></g></g></g></svg></mjx-container>。这一项被用于惩罚<strong>强化学习</strong>策略在每个训练批次中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。此外，<code>OpenAI</code>在<code>InstructGPT</code>上实验了在<code>PPO</code>添加新的<strong>预训练梯度</strong>，可以预见到<strong>奖励函数</strong>的公式会随着<code>RLHF</code>研究的进展而继续进化。最后根据<code>PPO</code>算法，按当前批次数据的<strong>奖励指标</strong>进行优化(来自<code>PPO</code>算法<code>on-policy</code>的特性)。<code>PPO</code>算法是一种<strong>信赖域优化</strong>(<code>Trust Region Optimization，TRO</code>)算法，它使用<strong>梯度约束</strong>确保更新步骤不会破坏学习过程的稳定性。<code>DeepMind</code>对<code>Gopher</code>使用了类似的奖励设置，但是使用<code>A2C</code>(<code>synchronous advantage actor-critic</code>)算法来优化<strong>梯度</strong>。</p>
<img data-src="/2024/12/26/artificial-intelligence/ML22_theory_study/ml_3.png" class="">

<p>作为一个可选项，<code>RLHF</code>可以通过迭代<strong>奖励模型</strong>和策略共同优化。随着策略模型更新，用户可以继续将输出和早期的输出进行合并排名。<code>Anthropic</code>在他们的论文中讨论了迭代在线<code>RLHF</code>，其中策略的迭代包含在跨模型的<code>Elo</code><strong>排名系统</strong>中。这样引入策略和<strong>奖励模型</strong>演变的复杂动态，代表了一个复杂和开放的研究问题。收集人类偏好数据的质量和数量决定了<code>RLHF</code>系统性能的上限。<code>RLHF</code>系统需要两种人类偏好数据：<strong>人工生成的文本</strong>和<strong>对模型输出的偏好标签</strong>。除开数据方面的限制，一些有待开发的设计选项可以让<code>RLHF</code>取得长足进步。例如对<code>RL</code><strong>优化器</strong>的改进方面，<code>PPO</code>是一种较旧的算法，但目前没有什么结构性原因让其他算法可以在现<code>有RLHF</code>工作中更具有优势。另外，微调<code>LM</code>策略的成本是策略生成的文本都需要在<code>RM</code>上进行评估，通过离线<code>RL</code>优化策略可以节约这些大模型<code>RM</code>的预测成本。最近，出现了新的<code>RL</code>算法如<strong>隐式语言</strong><code>Q-Learning</code>(<code>Implicit Language Q-Learning，ILQL</code>) 也适用于当前<code>RL</code>的优化。在<code>RL</code>训练过程的其他核心权衡，例如探索和开发(<code>exploration-exploitation</code>) 的平衡也有待尝试和记录。</p>
<h4 id="NLPO"><a href="#NLPO" class="headerlink" title="NLPO"></a>NLPO</h4><p>大多数<strong>语言模型</strong>在训练时并没有直接的<strong>人类偏好信号</strong>，监督目标字符串仅作为代理。一个整合用户反馈的选项是采用人机协作，即用户在模型训练过程中需要为每个样本提供反馈，但这种密集监督的程度往往是不可行且低效的。自动化指标提供了一个有前景的折衷方案：如<strong>成对学习偏好模型</strong>、<code>BERTScore</code>、<code>BLEURT</code>等人类偏好的模型，与早期指标（如<code>BLEU</code>、<code>METEOR</code>等）相比，显著提高了与人类判断的相关性，并且评估成本较低。然而，这些函数通常不是<strong>逐词可微分</strong>的：与人类一样，这些指标只能对完整生成结果提供质量估计。<strong>强化学习</strong>(<code>RL</code>)为优化不可微分的标量目标提供了一条自然路径。最近的研究表明，通过约束基于偏好的奖励来结合流畅性概念，<strong>强化学习</strong>(<code>RL</code>)在将<code>LM</code>与人类偏好对齐方面取得了很好的结果，但这一研究方向的进展受到缺乏开源基准和算法实现的严重阻碍——导致人们认为<strong>强化学习</strong>(<code>RL</code>)是<code>NLP</code>的一个具有挑战性的范式。为了促进构建<strong>强化学习</strong>(<code>RL</code>)<strong>算法</strong>以更好地对齐语言模型(<code>LM</code>)。首先，发布了<code>RL4LMs</code>库，使得生成<code>HuggingFace</code>模型（如<code>GPT-2</code>或<code>T5</code>）能够使用多种现有的<strong>强化学习</strong>(<code>RL</code>)方法进行训练，例如<code>PPO</code>、<code>A2C</code>等。接下来，使用<code>RL4LMs</code>训练的模型应用于新的<code>GRUE</code>（<strong>通用强化语言理解评估</strong>）基准：<code>GRUE</code>是一个包含<code>7</code>个<code>NLP</code>任务的集合；与其他基准不同的是，每个任务配对<strong>奖励函数</strong>，而不是进行监督训练。<code>GRUE</code>保证模型在保持流畅的语言生成能力的同时，优化这些<strong>奖励函数</strong>。通过<strong>强化学习</strong>(<code>RL</code>)训练语言模型——无论是否进行任务监督预训练——以优化奖励。最后，除了现有的<strong>强化学习</strong>(<code>RL</code>)方法，还引入了一种新颖的<strong>在线强化学习</strong>(<code>RL</code>)<strong>算法——自然语言策略优化</strong>(<code>NLPO</code>)，该算法能够在逐词级别动态学习任务特定的约束。实验结果和人类评估表明，与其他方法相比，<strong>自然语言策略优化</strong>(<code>NLPO</code>)在学习偏好奖励的同时，更好地保持了语言流畅性，包含了<code>PPO</code>的能力。在使用<strong>标量奖励反馈</strong>进行学习时，发现<strong>强化学习</strong>(<code>RL</code>)可以更具：<strong>数据效率</strong>，优于通过<strong>监督学习</strong>使用额外专家示范（尽管两者结合是最佳选择）——当作为<strong>自然语言策略优化</strong>(<code>NLPO</code>)方法的信号时，学习到的<strong>奖励函数</strong>在性能上优于使用<code>5</code>倍数据训练的监督方法；<strong>参数效率</strong>——使得一个结合<strong>监督</strong>和<strong>自然语言策略优化</strong>(<code>NLPO</code>)训练的<code>2.2</code>亿参数模型超越一个<code>30</code>亿参数的<strong>监督模型</strong>。</p>
<p>在情感引导的续写任务中，<strong>自然语言策略优化</strong>(<code>NLPO</code>)旨在使<strong>语言模型</strong>（即<strong>策略</strong>）根据评论提示生成积极的情感续写。这里需要平衡两个目标：<code>1</code>、作为奖励的自动化人类偏好<strong>智能体</strong>（此处为<strong>情感分类器</strong>）；<code>2</code>、通过与未经过<strong>显式人类反馈训练</strong>的语言模型之间的<code>KL</code><strong>散度</strong>来衡量“<strong>自然性</strong>”。如下图所示，<strong>自然语言策略优化</strong>(<code>NLPO</code>)与流行的<strong>策略梯度</strong>(<code>PPO</code>)的验证学习曲线比较。如果去掉<strong>自然</strong><code>KL</code><strong>惩罚</strong>(<code>naturall KL penalty</code>)，<strong>强化学习</strong>(<code>RL</code>)方法可以轻松获得高奖励，但代价是更高的<strong>困惑度</strong>。建议方法：<code>NLPO + KL</code>成功地在<strong>奖励</strong>和<strong>自然性</strong>之间取得了比以往研究更有效的<strong>平衡</strong>。</p>
<img data-src="/2024/12/26/artificial-intelligence/ML22_theory_study/ml_4.png" class="">

<p><strong>模仿学习</strong>(<code>Imitation Learning, IL</code>)是一种<strong>强化学习范式</strong>，旨在通过从专家示范中进行<strong>监督学习</strong>来执行任务。许多与<strong>自然语言处理</strong>(<code>NLP</code>)相关的算法，如<strong>调度采样</strong>(<code>Schedule Sampling, SS</code>)、<strong>并行调度采样</strong>(<code>Parallel SS</code>)、<code>Transformer</code><strong>调度采样</strong>、<strong>差分调度采样</strong>(<code>Differential SS</code>)、<code>LOL</code>(<code>Learning to Optimize Language Sequences</code>)、<code>TextGAIL</code>和<code>SEARNN</code>，都受到<strong>DAGGER</strong>和<strong>SEARN</strong>的启发。然而，这些算法在生成过程中普遍存在<strong>偏差</strong>和<strong>马尔可夫决策过程</strong>(<code>MDP</code>)问题。在<strong>大动作空间强化学习</strong>中，<code>MIXER</code>结合了<strong>调度采样</strong>和<strong>REINFORCE</strong>的思想。<code>actor-critic</code>算法解决了<code>REINFORCE</code>进行语言生成时的<strong>方差</strong>和<strong>大动作空间问题</strong>；<code>KG-A2C、TrufLL、AE-DQN</code>和<code>GALAD</code>通过消除和减少探索过程中的动作空间来解决类似问题。</p>
<p><code>RL4LMs</code>是一个开源库，提供了用于<strong>微调</strong>和<strong>评估</strong>基于<strong>语言模型</strong>(<code>LM</code>)的<strong>强化学习</strong>(<code>RL</code>)算法的构建模块。该库是基于<code>HuggingFace</code>和<code>stable-baselines-3</code>构建。<code>RL4LMs</code>可以用于训练<code>HuggingFace</code>中的任何<strong>解码器</strong>或<strong>编码器-解码器</strong><code>Transformer</code>模型，并支持来自<code>stable-baselines-3</code>的任何在线<strong>强化学习算法</strong>。此外，还提供了针对<code>LM</code>微调的在<strong>线强化学习算法</strong>的实现，例如<code>PPO、TRPO、A2C</code>和<code>NLPO</code>。该库是<strong>模块化</strong>的，用户可以插入自定义<strong>环境</strong>、<strong>奖励函数</strong>、<strong>指标</strong>和<strong>算法</strong>。在初始版本中，支持<code>6</code>种不同的<code>NLP</code>任务、<code>16</code>种评估<strong>指标</strong>和<strong>奖励</strong>，以及<code>4</code>种<strong>强化学习算法</strong>。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/" title="机器学习(ML)(二十二) — 强化学习探析">https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/12/19/artificial-intelligence/ML21_theory_study/" rel="prev" title="机器学习(ML)(二十一) — 强化学习探析">
                  <i class="fa fa-chevron-left"></i> 机器学习(ML)(二十一) — 强化学习探析
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.3m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">73:43</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/12/26/artificial-intelligence/ML22_theory_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
