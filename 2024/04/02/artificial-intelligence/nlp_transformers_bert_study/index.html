<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="内容在本文中，我将从RNN的基础知识开始，一直到构建最新的深度学习架构来解决NLP问题。它将涵盖以下内容：  简单的RNN(循环神经网络)。 词嵌入(Word Embeddings)：定义以及如何获取。 长短期记忆网络(LSTM)。 门控循环单元(GRU)。 双向RNN。 编码器-解码器模型（Seq2Seq模型）。 注意力模型(Attention Models)。 Transformer-你所需要">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP（Transformers &amp; BERT）">
<meta property="og:url" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="内容在本文中，我将从RNN的基础知识开始，一直到构建最新的深度学习架构来解决NLP问题。它将涵盖以下内容：  简单的RNN(循环神经网络)。 词嵌入(Word Embeddings)：定义以及如何获取。 长短期记忆网络(LSTM)。 门控循环单元(GRU)。 双向RNN。 编码器-解码器模型（Seq2Seq模型）。 注意力模型(Attention Models)。 Transformer-你所需要">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_8.png">
<meta property="article:published_time" content="2024-04-02T03:20:32.000Z">
<meta property="article:modified_time" content="2024-04-02T03:20:32.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/","path":"2024/04/02/artificial-intelligence/nlp_transformers_bert_study/","title":"NLP（Transformers & BERT）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>NLP（Transformers & BERT） | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AE%B9"><span class="nav-number">1.</span> <span class="nav-text">内容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AETPU"><span class="nav-number">2.</span> <span class="nav-text">配置TPU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">简单循环神经网络（RNN）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">基本概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3"><span class="nav-number">3.2.</span> <span class="nav-text">深入理解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.3.</span> <span class="nav-text">代码实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E"><span class="nav-number">3.4.</span> <span class="nav-text">代码说明</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.4.1.</span> <span class="nav-text">构建神经网络</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="nav-number">3.4.2.</span> <span class="nav-text">对模型的评估</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5-Word-Embeddings"><span class="nav-number">4.</span> <span class="nav-text">词嵌入(Word Embeddings)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C-LSTM"><span class="nav-number">5.</span> <span class="nav-text">长短期记忆网络(LSTM)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E8%BF%B0-1"><span class="nav-number">5.1.</span> <span class="nav-text">基本概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3"><span class="nav-number">5.2.</span> <span class="nav-text">深度理解</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E8%AE%B0%E5%BF%86%E5%85%83"><span class="nav-number">5.2.1.</span> <span class="nav-text">门控记忆元</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">5.3.</span> <span class="nav-text">代码实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E-1"><span class="nav-number">5.4.</span> <span class="nav-text">代码说明</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.1.</span> <span class="nav-text">创建模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">5.4.2.</span> <span class="nav-text">对模型评估</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%88GRU%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">门控循环单元（GRU）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E8%BF%B0-2"><span class="nav-number">6.1.</span> <span class="nav-text">基本概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E8%A7%A3%E9%87%8A"><span class="nav-number">6.2.</span> <span class="nav-text">深入解释</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="nav-number">6.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%90%91RNN"><span class="nav-number">7.</span> <span class="nav-text">双向RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3-1"><span class="nav-number">7.1.</span> <span class="nav-text">深度理解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="nav-number">7.2.</span> <span class="nav-text">代码实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%AF%B4%E6%98%8E-2"><span class="nav-number">7.3.</span> <span class="nav-text">代码说明</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Seq2Seq%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">8.</span> <span class="nav-text">Seq2Seq模型架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E8%BF%B0-3"><span class="nav-number">8.1.</span> <span class="nav-text">基本概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-1"><span class="nav-number">8.2.</span> <span class="nav-text">深入理解</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%88Attention-Models%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">注意力模型（Attention Models）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformer-Attention-is-all-you-need"><span class="nav-number">10.</span> <span class="nav-text">Transformer - Attention is all you need</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-4"><span class="nav-number">10.1.</span> <span class="nav-text">代码实现</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">10.1.1.</span> <span class="nav-text">模型架构</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%A0%88"><span class="nav-number">10.1.2.</span> <span class="nav-text">编码器&amp;解码器栈</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT"><span class="nav-number">11.</span> <span class="nav-text">BERT</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">247</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="NLP（Transformers & BERT） | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP（Transformers & BERT）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-02 11:20:32" itemprop="dateCreated datePublished" datetime="2024-04-02T11:20:32+08:00">2024-04-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h4><p>在本文中，我将从<code>RNN</code>的基础知识开始，一直到构建最新的深度学习架构来解决<code>NLP</code>问题。它将涵盖以下内容：</p>
<ul>
<li>简单的<code>RNN</code>(循环神经网络)。</li>
<li>词嵌入(<code>Word Embeddings</code>)：定义以及如何获取。</li>
<li>长短期记忆网络(<code>LSTM</code>)。</li>
<li>门控循环单元(<code>GRU</code>)。</li>
<li>双向<code>RNN</code>。</li>
<li>编码器-解码器模型（<code>Seq2Seq</code>模型）。</li>
<li>注意力模型(<code>Attention Models</code>)。</li>
<li><code>Transformer</code>-你所需要的就是注意力。</li>
<li><code>BERT</code>。<span id="more"></span></li>
</ul>
<p>我将每个主题分为四个小节：</p>
<ul>
<li>基本概况。</li>
<li>深入理解。</li>
<li>代码实现。</li>
<li>代码说明。</li>
</ul>
<h4 id="配置TPU"><a href="#配置TPU" class="headerlink" title="配置TPU"></a>配置TPU</h4><p>我们将使用TPU，因为我们需要构建<code>BERT</code>模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> LSTM, GRU,SimpleRNN</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.layers.normalization <span class="keyword">import</span> BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing, decomposition, model_selection, metrics, pipeline</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence, text</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> plotly <span class="keyword">import</span> graph_objs <span class="keyword">as</span> go</span><br><span class="line"><span class="keyword">import</span> plotly.express <span class="keyword">as</span> px</span><br><span class="line"><span class="keyword">import</span> plotly.figure_factory <span class="keyword">as</span> ff</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detect hardware, return appropriate distribution strategy</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># TPU detection. No parameters necessary if TPU_NAME environment variable is</span></span><br><span class="line">    <span class="comment"># set: this is always the case on Kaggle.</span></span><br><span class="line">    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Running on TPU &#x27;</span>, tpu.master())</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    tpu = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> tpu:</span><br><span class="line">    tf.config.experimental_connect_to_cluster(tpu)</span><br><span class="line">    tf.tpu.experimental.initialize_tpu_system(tpu)</span><br><span class="line">    strategy = tf.distribute.experimental.TPUStrategy(tpu)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Default distribution strategy in Tensorflow. Works on CPU and single GPU.</span></span><br><span class="line">    strategy = tf.distribute.get_strategy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;REPLICAS: &quot;</span>, strategy.num_replicas_in_sync)</span><br><span class="line"></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&#x27;</span>)</span><br><span class="line">validation = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我们将删除其他列并将此问题作为二元分类问题来处理，并且我们将在数据集的较小部分（仅<code>12000</code>个数据点）上完成练习，以便更轻松地训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">train.drop([<span class="string">&#x27;severe_toxic&#x27;</span>,<span class="string">&#x27;obscene&#x27;</span>,<span class="string">&#x27;threat&#x27;</span>,<span class="string">&#x27;insult&#x27;</span>,<span class="string">&#x27;identity_hate&#x27;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">train = train.loc[:<span class="number">12000</span>,:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将检查评论中出现的最大字数，这将有助于我们稍后进行填充</span></span><br><span class="line">train[<span class="string">&#x27;comment_text&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(<span class="built_in">str</span>(x).split())).<span class="built_in">max</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编写一个函数来获取 auc 分数以进行验证</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">roc_auc</span>(<span class="params">predictions,target</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    This methods returns the AUC Score when given the Predictions</span></span><br><span class="line"><span class="string">    and Labels</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)</span><br><span class="line">    roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line">    <span class="keyword">return</span> roc_auc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备（Data Preparation）</span></span><br><span class="line">xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, </span><br><span class="line">                                                  stratify=train.toxic.values, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="简单循环神经网络（RNN）"><a href="#简单循环神经网络（RNN）" class="headerlink" title="简单循环神经网络（RNN）"></a>简单循环神经网络（RNN）</h4><h5 id="基本概述"><a href="#基本概述" class="headerlink" title="基本概述"></a>基本概述</h5><p>什么是<code>RNN</code>？<strong>循环神经网络</strong>（<code>RNN</code>）是一种神经网络，其中上一步的输出作为当前步骤的输入。在传统的神经网络中，所有的输入和输出都是相互独立的，但是当需要预测句子的下一个单词时，需要前面的单词，因此需要记住前面的单词。于是<code>RNN</code>应运而生，它借助<strong>隐藏层</strong>解决了这个问题。</p>
<h5 id="深入理解"><a href="#深入理解" class="headerlink" title="深入理解"></a>深入理解</h5><p>根据维基百科，循环神经网络(<code>RNN</code>)是一类人工神经网络，其中单元之间的连接沿着序列形成有向图。这使得它能够表现出时间序列的动态时间行为。与前馈神经网络不同，<code>RNN</code>可以使用其内部状态（内存）来处理输入序列。这使得它们适用于诸如未分段、连接的手写识别或语音识别等任务。让我们通过一个类比来理解这一点。假设你正在看一部电影，你始终都在看这部电影，你有上下文，因为你已经看过这部电影直到那一点，然后只有你能够正确地将所有内容联系起来。意味着您记住了您看过的所有内容。同样，<code>RNN</code>会记住一切。在其他神经网络中，所有输入都是相互独立的。但在<code>RNN</code>中，所有输入都是相互关联的。假设您必须预测给定句子中的下一个单词，在这种情况下，所有先前单词之间的关系有助于预测更好的输出。<code>RNN</code>在训练自身时会记住所有这些关系。为了实现这一目标，<code>RNN</code>创建了带有循环的网络，这使得它能够保存信息。</p>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_1.png" class="">

<p>这种循环结构允许神经网络获取输入序列。如果你看到展开的版本，你会更好地理解它。<a target="_blank" rel="noopener" href="https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html">循环神经网络详解(<code>Recurrent Neural Networks</code>)</a></p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using keras tokenizer here</span></span><br><span class="line">token = text.Tokenizer(num_words=<span class="literal">None</span>)</span><br><span class="line">max_len = <span class="number">1500</span></span><br><span class="line"></span><br><span class="line">token.fit_on_texts(<span class="built_in">list</span>(xtrain) + <span class="built_in">list</span>(xvalid))</span><br><span class="line">xtrain_seq = token.texts_to_sequences(xtrain)</span><br><span class="line">xvalid_seq = token.texts_to_sequences(xvalid)</span><br><span class="line"></span><br><span class="line"><span class="comment">#zero pad the sequences</span></span><br><span class="line">xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)</span><br><span class="line">xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)</span><br><span class="line"></span><br><span class="line">word_index = token.word_index</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    <span class="comment"># A simpleRNN without any pretrained embeddings and one dense layer</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Embedding(<span class="built_in">len</span>(word_index) + <span class="number">1</span>,</span><br><span class="line">                     <span class="number">300</span>,</span><br><span class="line">                     input_length=max_len))</span><br><span class="line">    model.add(SimpleRNN(<span class="number">100</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">model.summary()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential_1&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding_1 (Embedding)      (None, 1500, 300)         13049100  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">simple_rnn_1 (SimpleRNN)     (None, 100)               40100     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 1)                 101       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 13,089,301</span><br><span class="line">Trainable params: 13,089,301</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">CPU <span class="built_in">times</span>: user 620 ms, sys: 370 ms, total: 990 ms</span><br><span class="line">Wall time: 1.18 s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.fit(xtrain_pad, ytrain, nb_epoch=<span class="number">5</span>, batch_size=<span class="number">64</span>*strategy.num_replicas_in_sync) <span class="comment">#Multiplying by Strategy to run on TPU&#x27;s</span></span><br><span class="line">scores = model.predict(xvalid_pad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Auc: %.2f%%&quot;</span> % (roc_auc(scores,yvalid)))</span><br><span class="line">scores_model = []</span><br><span class="line">scores_model.append(&#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;SimpleRNN&#x27;</span>,<span class="string">&#x27;AUC_Score&#x27;</span>: roc_auc(scores,yvalid)&#125;)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/5</span><br><span class="line">9600/9600 [==============================] - 39s 4ms/step - loss: 0.3714 - accuracy: 0.8805</span><br><span class="line">Epoch 2/5</span><br><span class="line">9600/9600 [==============================] - 39s 4ms/step - loss: 0.2858 - accuracy: 0.9055</span><br><span class="line">Epoch 3/5</span><br><span class="line">9600/9600 [==============================] - 40s 4ms/step - loss: 0.2748 - accuracy: 0.8945</span><br><span class="line">Epoch 4/5</span><br><span class="line">9600/9600 [==============================] - 40s 4ms/step - loss: 0.2416 - accuracy: 0.9053</span><br><span class="line">Epoch 5/5</span><br><span class="line">9600/9600 [==============================] - 39s 4ms/step - loss: 0.2109 - accuracy: 0.9079</span><br><span class="line"></span><br><span class="line">Auc: 0.69%</span><br></pre></td></tr></table></figure>
<h5 id="代码说明"><a href="#代码说明" class="headerlink" title="代码说明"></a>代码说明</h5><p>我们将每个单词表示为一个维度的热向量：<code>Vocab</code>中的单词数<code>+1</code>。<code>keras Tokenizer</code>的作用是，获取语料库中所有唯一的单词，以单词为键、出现次数为值形成一个字典，然后按计数降序对字典进行排序。然后分配第一个值<code>1</code>，第二个值<code>2</code>，依此类推。假设单词“<code>the</code>”在语料库中出现次数最多，那么它将分配索引<code>1</code>，表示“<code>the</code>”的向量将是一个热向量，位置<code>1</code>处值为<code>1</code>，其余为零。打印<code>xtrain_seq</code>的前<code>2</code>个元素，您将看到每个单词现在都表示为数字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xtrain_seq[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[664,65,7,19,2262,14102,5,2262,20439,6071,4,71,32,20440,6620,39,6,664,65,11,8,20441,1502,38,6072]]</span></span><br></pre></td></tr></table></figure>
<h6 id="构建神经网络"><a href="#构建神经网络" class="headerlink" title="构建神经网络"></a>构建神经网络</h6><p>第一行<code>model.Sequential()</code>告诉<code>keras</code>将按顺序构建网络。然后我们首先添加<code>Embedding</code>层。<strong>嵌入层</strong>也是一层神经元，它将每个单词的<code>n</code>维一个热向量作为输入，并将其转换为<code>300</code>维向量，它为我们提供了类似于<code>word2vec</code>的单词嵌入。我们可以使用<code>word2vec</code>，但嵌入层在训练过程中进行学习以增强嵌入。接下来我们添加<code>100</code>个<code>LSTM</code>单元，没有任何<code>dropout</code>或正则化，最后我们添加一个具有<code>sigmoid</code>函数的神经元，该神经元从<code>100</code>个<code>LSTM</code>单元（请注意，我们有<code>100</code>个<code>LSTM</code>单元而不是层）获取输出来预测结果，然后我们编译模型，使用<code>Adam</code>优化器。</p>
<h6 id="对模型的评估"><a href="#对模型的评估" class="headerlink" title="对模型的评估"></a>对模型的评估</h6><p>我们可以看到模型达到了<code>1</code>的准确率，我知道显然<strong>过度拟合</strong>，但这是所有模型中最简单的，我们可以调整很多超参数，例如<code>RNN</code>单元，我们可以进行批量归一化、<code>dropout</code>等 以获得更好的结果。关键是我们不费吹灰之力就得到了<code>0.82</code>的<code>AUC</code>分数，并且我们已经了解了<code>RNN</code>。</p>
<h4 id="词嵌入-Word-Embeddings"><a href="#词嵌入-Word-Embeddings" class="headerlink" title="词嵌入(Word Embeddings)"></a>词嵌入(<code>Word Embeddings</code>)</h4><p>在构建简单的<code>RNN</code>模型时，我们使用了词嵌入，那么什么是词嵌入，以及我们如何获得词嵌入？<strong>词嵌入是一种学习到的文本表示，其中具有相同含义的单词具有相似的表示</strong>。这种表示单词和文档的方法可能被认为是深度学习在挑战自然语言处理问题方面的关键突破之一。使用密集和低维向量的好处之一是<strong>计算</strong>：大多数神经网络工具包不能很好地处理非常高维的稀疏向量。<strong>密集表示</strong>的主要好处是<strong>泛化能力</strong>，如果我们相信某些特征可能提供相似的线索，那么提供能够捕获这些相似性的表示是值得的。获取词嵌入的最新方法是使用预保留的<code>GLoVe</code>或<code>Fasttext</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the GloVe vectors in a dictionary:</span></span><br><span class="line"></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/glove840b300dtxt/glove.840B.300d.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">    values = line.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = np.asarray([<span class="built_in">float</span>(val) <span class="keyword">for</span> val <span class="keyword">in</span> values[<span class="number">1</span>:]])</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Found %s word vectors.&#x27;</span> % <span class="built_in">len</span>(embeddings_index))</span><br></pre></td></tr></table></figure>
<h4 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络(LSTM)"></a>长短期记忆网络(LSTM)</h4><h5 id="基本概述-1"><a href="#基本概述-1" class="headerlink" title="基本概述"></a>基本概述</h5><p>简单的<code>RNN</code>比经典的<code>ML</code>算法更好，并且给出了最领先的结果，但它无法捕获句子中存在的长期依赖关系。因此在<code>1998-99</code>年引入了<code>LSTM</code>来克服这些缺点。</p>
<h5 id="深度理解"><a href="#深度理解" class="headerlink" title="深度理解"></a>深度理解</h5><p>长期以来，隐变量模型存在着<strong>长期信息保存</strong>和<strong>短期输入缺失</strong>的问题。解决这一问题的最早方法之一是<strong>长短期存储器</strong>（<code>long short-term memory</code>，<code>LSTM</code>）(<code>Hochreiter and Schmidhuber, 1997</code>)。它有许多与<strong>门控循环单元</strong>一样的属性。有趣的是，<strong>长短期记忆网络</strong>的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。</p>
<h6 id="门控记忆元"><a href="#门控记忆元" class="headerlink" title="门控记忆元"></a>门控记忆元</h6><p>可以说，<strong>长短期记忆网络</strong>的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了记忆元（<code>memory cell</code>），或简称为单元（<code>cell</code>）。有些文献认为记忆元是<strong>隐状态</strong>的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，我们将其称为<strong>输出门</strong>（<code>output gate</code>）。另外一个门用来决定何时将数据读入单元，我们将其称为<strong>输入门</strong>（<code>input gate</code>）。我们还需要一种机制来重置单元的内容，由<strong>遗忘门</strong>（<code>forget gate</code>）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定<strong>什么时候记忆或忽略隐状态中的输入</strong>。 让我们看看这在实践中是如何运作的。</p>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_2.png" class="">

<h5 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create an embedding matrix for the words we have in the dataset</span></span><br><span class="line">embedding_matrix = np.zeros((<span class="built_in">len</span>(word_index) + <span class="number">1</span>, <span class="number">300</span>))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> tqdm(word_index.items()):</span><br><span class="line">    embedding_vector = embeddings_index.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        embedding_matrix[i] = embedding_vector</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># A simple LSTM with glove embeddings and one dense layer</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Embedding(<span class="built_in">len</span>(word_index) + <span class="number">1</span>,<span class="number">300</span>,weights=[embedding_matrix],input_length=max_len,trainable=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">    model.add(LSTM(<span class="number">100</span>, dropout=<span class="number">0.3</span>, recurrent_dropout=<span class="number">0.3</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">model.summary()</span><br><span class="line">model.fit(xtrain_pad, ytrain, nb_epoch=<span class="number">5</span>, batch_size=<span class="number">64</span>*strategy.num_replicas_in_sync)</span><br><span class="line"></span><br><span class="line">scores = model.predict(xvalid_pad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Auc: %.2f%%&quot;</span> % (roc_auc(scores,yvalid)))</span><br><span class="line"></span><br><span class="line">scores_model.append(&#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;LSTM&#x27;</span>,<span class="string">&#x27;AUC_Score&#x27;</span>: roc_auc(scores,yvalid)&#125;)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential_2&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding_2 (Embedding)      (None, 1500, 300)         13049100  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_1 (LSTM)                (None, 100)               160400    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (None, 1)                 101       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 13,209,601</span><br><span class="line">Trainable params: 160,501</span><br><span class="line">Non-trainable params: 13,049,100</span><br><span class="line">_________________________________________________________________</span><br><span class="line">CPU <span class="built_in">times</span>: user 1.33 s, sys: 1.46 s, total: 2.79 s</span><br><span class="line">Wall time: 3.09 s</span><br><span class="line"></span><br><span class="line">Epoch 1/5</span><br><span class="line">9600/9600 [==============================] - 117s 12ms/step - loss: 0.3525 - accuracy: 0.8852</span><br><span class="line">Epoch 2/5</span><br><span class="line">9600/9600 [==============================] - 114s 12ms/step - loss: 0.2397 - accuracy: 0.9192</span><br><span class="line">Epoch 3/5</span><br><span class="line">9600/9600 [==============================] - 114s 12ms/step - loss: 0.1904 - accuracy: 0.9333</span><br><span class="line">Epoch 4/5</span><br><span class="line">9600/9600 [==============================] - 114s 12ms/step - loss: 0.1659 - accuracy: 0.9394</span><br><span class="line">Epoch 5/5</span><br><span class="line">9600/9600 [==============================] - 114s 12ms/step - loss: 0.1553 - accuracy: 0.9470</span><br><span class="line"></span><br><span class="line">Auc: 0.96%</span><br></pre></td></tr></table></figure>
<h5 id="代码说明-1"><a href="#代码说明-1" class="headerlink" title="代码说明"></a>代码说明</h5><h6 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h6><p>第一步，我们根据预训练的<code>GLoVe</code>向量计算词汇表的<strong>嵌入矩阵</strong>。然后，在构建嵌入层时，我们将嵌入矩阵作为权重传递给该层，而不是通过词汇对其进行训练，因此我们传递<code>trainable = False</code>。模型的其余部分与之前相同，只是我们用<code>LSTM</code>单元替换了<code>SimpleRNN</code>。</p>
<h6 id="对模型评估"><a href="#对模型评估" class="headerlink" title="对模型评估"></a>对模型评估</h6><p>现在我们看到该模型没有过度拟合，并且达到了<code>0.96</code>的<code>auc</code>分数，这是非常值得称赞的，而且我们也缩小了准确率和<code>auc</code>之间的差距。我们看到，在这种情况下，我们使用了<code>dropout</code>来防止数据过度拟合。</p>
<h4 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h4><h5 id="基本概述-2"><a href="#基本概述-2" class="headerlink" title="基本概述"></a>基本概述</h5><p><code>2014</code>年，<strong>门控循环单元</strong>（<code>GRU</code>）旨在解决标准循环神经网络带来的<strong>梯度消失</strong>问题。<code>GRU</code>是<code>LSTM</code>的变体，因为两者设计相似，并且在某些情况下产生同样出色的结果。<code>GRU</code>的设计比<code>LSTM</code>更简单、更快，并且在大多数情况下产生同样好的结果。</p>
<h5 id="深入解释"><a href="#深入解释" class="headerlink" title="深入解释"></a>深入解释</h5><p>门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持<strong>隐状态的门控</strong>。这意味着模型有专门的机制来确定应该何时更新<strong>隐状态</strong>，以及应该何时重置隐状态。这些机制是可学习的，并且能够解决了上面列出的问题。例如，如果第一个<strong>词元</strong>非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。最后，模型还将学会在需要的时候重置隐状态。</p>
<h5 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    <span class="comment"># GRU with glove embeddings and two dense layers</span></span><br><span class="line">     model = Sequential()</span><br><span class="line">     model.add(Embedding(<span class="built_in">len</span>(word_index) + <span class="number">1</span>,</span><br><span class="line">                     <span class="number">300</span>,</span><br><span class="line">                     weights=[embedding_matrix],</span><br><span class="line">                     input_length=max_len,</span><br><span class="line">                     trainable=<span class="literal">False</span>))</span><br><span class="line">     model.add(SpatialDropout1D(<span class="number">0.3</span>))</span><br><span class="line">     model.add(GRU(<span class="number">300</span>))</span><br><span class="line">     model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">     model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])   </span><br><span class="line">    </span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.fit(xtrain_pad, ytrain, nb_epoch=<span class="number">5</span>, batch_size=<span class="number">64</span>*strategy.num_replicas_in_sync)</span><br><span class="line"></span><br><span class="line">scores = model.predict(xvalid_pad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Auc: %.2f%%&quot;</span> % (roc_auc(scores,yvalid)))</span><br><span class="line">scores_model.append(&#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;GRU&#x27;</span>,<span class="string">&#x27;AUC_Score&#x27;</span>: roc_auc(scores,yvalid)&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential_3&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding_3 (Embedding)      (None, 1500, 300)         13049100  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">spatial_dropout1d_1 (Spatial (None, 1500, 300)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">gru_1 (GRU)                  (None, 300)               540900    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 1)                 301       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 13,590,301</span><br><span class="line">Trainable params: 541,201</span><br><span class="line">Non-trainable params: 13,049,100</span><br><span class="line">_________________________________________________________________</span><br><span class="line">CPU <span class="built_in">times</span>: user 1.3 s, sys: 1.29 s, total: 2.59 s</span><br><span class="line">Wall time: 2.79 s</span><br><span class="line"></span><br><span class="line">Epoch 1/5</span><br><span class="line">9600/9600 [==============================] - 191s 20ms/step - loss: 0.3272 - accuracy: 0.8933</span><br><span class="line">Epoch 2/5</span><br><span class="line">9600/9600 [==============================] - 189s 20ms/step - loss: 0.2015 - accuracy: 0.9334</span><br><span class="line">Epoch 3/5</span><br><span class="line">9600/9600 [==============================] - 189s 20ms/step - loss: 0.1540 - accuracy: 0.9483</span><br><span class="line">Epoch 4/5</span><br><span class="line">9600/9600 [==============================] - 189s 20ms/step - loss: 0.1287 - accuracy: 0.9548</span><br><span class="line">Epoch 5/5</span><br><span class="line">9600/9600 [==============================] - 188s 20ms/step - loss: 0.1238 - accuracy: 0.9551</span><br><span class="line"></span><br><span class="line">Auc: 0.97%</span><br><span class="line"></span><br><span class="line">[&#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;SimpleRNN&#x27;</span>, <span class="string">&#x27;AUC_Score&#x27;</span>: 0.6949714081921305&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;LSTM&#x27;</span>, <span class="string">&#x27;AUC_Score&#x27;</span>: 0.9598235453841757&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;GRU&#x27;</span>, <span class="string">&#x27;AUC_Score&#x27;</span>: 0.9716554069114769&#125;]</span><br></pre></td></tr></table></figure>

<h4 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h4><h5 id="深度理解-1"><a href="#深度理解-1" class="headerlink" title="深度理解"></a>深度理解</h5><p>如果我们希望在循环神经网络中拥有一种机制，使之能够提供与<strong>隐马尔可夫模型</strong>类似的前瞻能力，我们就需要修改循环神经网络的设计。幸运的是，这在概念上很容易，只需要增加一个“<strong>从最后一个词元开始从后向前运行</strong>”的循环神经网络，而不是只有一个在前向模式下“<strong>从第一个词元开始运行</strong>”的循环神经网络。<strong>双向循环神经网络</strong>（<code>bidirectional RNNs</code>）添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络的架构。</p>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_3.png" class="">

<p>事实上，这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。 其主要区别是，在隐马尔可夫模型中的方程具有特定的统计意义。 双向循环神经网络没有这样容易理解的解释， 我们只能把它们当作通用的、可学习的函数。 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。<a target="_blank" rel="noopener" href="https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html">双向循环神经网络详解(<code>Bi-Directional RNN</code>)</a></p>
<h5 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Bi-Directional RNN</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    <span class="comment"># A simple bidirectional LSTM with glove embeddings and one dense layer</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Embedding(<span class="built_in">len</span>(word_index) + <span class="number">1</span>,</span><br><span class="line">                     <span class="number">300</span>,</span><br><span class="line">                     weights=[embedding_matrix],</span><br><span class="line">                     input_length=max_len,</span><br><span class="line">                     trainable=<span class="literal">False</span>))</span><br><span class="line">    model.add(Bidirectional(LSTM(<span class="number">300</span>, dropout=<span class="number">0.3</span>, recurrent_dropout=<span class="number">0.3</span>)))</span><br><span class="line"></span><br><span class="line">    model.add(Dense(<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.fit(xtrain_pad, ytrain, nb_epoch=<span class="number">5</span>, batch_size=<span class="number">64</span>*strategy.num_replicas_in_sync)</span><br><span class="line"></span><br><span class="line">scores = model.predict(xvalid_pad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Auc: %.2f%%&quot;</span> % (roc_auc(scores,yvalid)))</span><br><span class="line"></span><br><span class="line">scores_model.append(&#123;<span class="string">&#x27;Model&#x27;</span>: <span class="string">&#x27;Bi-directional LSTM&#x27;</span>,<span class="string">&#x27;AUC_Score&#x27;</span>: roc_auc(scores,yvalid)&#125;)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential_4&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding_4 (Embedding)      (None, 1500, 300)         13049100  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">bidirectional_1 (Bidirection (None, 600)               1442400   </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_4 (Dense)              (None, 1)                 601       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 14,492,101</span><br><span class="line">Trainable params: 1,443,001</span><br><span class="line">Non-trainable params: 13,049,100</span><br><span class="line">_________________________________________________________________</span><br><span class="line">CPU <span class="built_in">times</span>: user 2.39 s, sys: 1.62 s, total: 4 s</span><br><span class="line">Wall time: 3.41 s</span><br><span class="line"></span><br><span class="line">Epoch 1/5</span><br><span class="line">9600/9600 [==============================] - 322s 34ms/step - loss: 0.3171 - accuracy: 0.9009</span><br><span class="line">Epoch 2/5</span><br><span class="line">9600/9600 [==============================] - 318s 33ms/step - loss: 0.1988 - accuracy: 0.9305</span><br><span class="line">Epoch 3/5</span><br><span class="line">9600/9600 [==============================] - 318s 33ms/step - loss: 0.1650 - accuracy: 0.9424</span><br><span class="line">Epoch 4/5</span><br><span class="line">9600/9600 [==============================] - 318s 33ms/step - loss: 0.1577 - accuracy: 0.9414</span><br><span class="line">Epoch 5/5</span><br><span class="line">9600/9600 [==============================] - 319s 33ms/step - loss: 0.1540 - accuracy: 0.9459</span><br><span class="line"></span><br><span class="line">Auc: 0.97%</span><br></pre></td></tr></table></figure>
<h5 id="代码说明-2"><a href="#代码说明-2" class="headerlink" title="代码说明"></a>代码说明</h5><p>代码与以前相同，只是我们为之前使用的<code>LSTM</code>单元添加了双向性质。我们已经实现了与之前类似的准确率和<code>auc</code>分数，现在我们已经学习了所有类型的典型<code>RNN</code>架构。</p>
<h4 id="Seq2Seq模型架构"><a href="#Seq2Seq模型架构" class="headerlink" title="Seq2Seq模型架构"></a>Seq2Seq模型架构</h4><h5 id="基本概述-3"><a href="#基本概述-3" class="headerlink" title="基本概述"></a>基本概述</h5><p><code>RNN</code>有多种类型，不同的架构用于不同的目的。这里有一个视频解释了不同类型的<a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models/lecture/BO8PS/different-types-of-rnns">模型架构</a>。<code>Seq2Seq</code>是一个多对多的<code>RNN</code>架构，其中输入是一个序列，输出也是一个序列。该架构用于许多应用，如机器翻译、文本摘要、问答等。</p>
<h5 id="深入理解-1"><a href="#深入理解-1" class="headerlink" title="深入理解"></a>深入理解</h5><p>在一般的<strong>序列到序列</strong>（<code>Seq2Seq</code>）问题中，输入和输出的长度不同且未对齐。处理此类数据的标准方法是设计一个<strong>编码器-解码器架构</strong>，它由两个主要组件组成：<strong>一个以可变长度序列作为输入的编码器，另一个充当条件的解码器</strong>。语言模型，接收编码输入和目标序列的左侧上下文，并预测目标序列中的后续标记。</p>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_4.png" class="">

<p>让我们以从英语到法语的机器翻译为例。给定一个英文输入序列：“<code>These</code>”、“<code>are</code>”、“<code>watching</code>”、“<code>.</code>”，这种编码器-解码器架构首先将可变长度输入编码为状态，然后解码该状态以生成翻译序列、标记，作为输出：“<code>Ils</code>”、“<code>regardent</code>”、“<code>.</code>”。<strong>编码器-解码器架构</strong>可以处理由可变长度序列组成的输入和输出，因此适用于序列到序列的问题，例如机器翻译。编码器将可变长度序列作为输入，并将其转换为具有固定形状的状态。解码器将固定形状的编码状态映射到可变长度序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualization of Results obtained from various Deep learning models</span></span><br><span class="line">results = pd.DataFrame(scores_model).sort_values(by=<span class="string">&#x27;AUC_Score&#x27;</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">results.style.background_gradient(cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig = go.Figure(go.Funnelarea(</span><br><span class="line">    text =results.Model,values = results.AUC_Score,title = &#123;<span class="string">&quot;position&quot;</span>: <span class="string">&quot;top center&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;Funnel-Chart of Sentiment Distribution&quot;</span>&#125;</span><br><span class="line">    ))</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_5.png" class="">
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_6.png" class="">

<h4 id="注意力模型（Attention-Models）"><a href="#注意力模型（Attention-Models）" class="headerlink" title="注意力模型（Attention Models）"></a>注意力模型（Attention Models）</h4><p>如果你能够理解注意力模块的工作原理，那么理解<code>Transformer</code>和基于<code>Transformer</code>架构（如<code>BERT</code>）将是小菜一碟。</p>
<h4 id="Transformer-Attention-is-all-you-need"><a href="#Transformer-Attention-is-all-you-need" class="headerlink" title="Transformer - Attention is all you need"></a>Transformer - Attention is all you need</h4><p>最后我们到达了学习曲线的终点，即将开始学习彻底改变<code>NLP</code>的技术，这也是最先进的<code>NLP</code>技术的原因。<code>Google</code>在论文《<code>Attention is all you need</code>》中介绍了<code>Transformer</code>。<code>Transformer</code>是由一个编码器、解码器组件以及他们之间的连接构成。编码组件有一堆编码器组成，解码器组件也是由相同数量的解码器组成。编码器分为两层：<strong>自注意力层、前馈神经网络</strong>。编码器与编码器的结构相同，但彼此不共享权重。编码器的输入首先流入自注意力层，该层帮助编码器在对特定单词进行编码时查看输入句子中的其他单词，自注意力层的输出被馈送到前馈神经网络，完全相同的前馈网络是相互独立的。解码器分为三层：<strong>自注意力层、<code>Encoder-Decoder</code>注意力层、前馈神经网络</strong>。<code>Encoder-Decoder</code>注意力层的作用是帮助解码器专注于输入的相关部分（类似于<code>seq2seq</code>模型中注意力的作用）。</p>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_7.png" class="">

<h5 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h5><h6 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">&quot;talk&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/ntb_8.png" class="" title="Transformer 架构">

<h6 id="编码器-解码器栈"><a href="#编码器-解码器栈" class="headerlink" title="编码器&amp;解码器栈"></a>编码器&amp;解码器栈</h6><ul>
<li>编码器<br>编码器由6个独立相同的层组成的堆栈。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个子层周围 采用残差连接, 然后进行层归一化。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了促进这些残差连接，模型中的所有子层以及嵌入层都会产生维度的输出。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每层有两个子层。第一个是多头自注意力机制，第二个是全连接前馈网络。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></li>
<li>解码器<br>解码器由6个独立相同的层组成的堆栈。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除了每个编码器层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还修改了解码器堆栈中的自注意力子层，以防止位置关注后续位置。这种掩蔽与输出嵌入偏移一个位置的事实相结合，确保了位置的预测只能依赖于小于位置的已知输出。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure></li>
<li>注意力<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重是由查询与相应键的兼容性函数计算的。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
两种最常用的注意力函数是加性注意力和点积注意力。附加注意力使用具有单个隐藏层的前馈网络来计算兼容性函数。虽然两者在理论复杂性上相似，但点积注意力在实践中更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。对于单一注意力头，平均会抑制这种情况。</li>
</ul>
<p><code>Transformer</code>以三种不同的方式使用多头注意力：</p>
<ul>
<li>在“编码器-解码器注意力”层中，查询来自前一个解码器层，内存键和值来自编码器的输出。这允许解码器中的每个位置都参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制。</li>
<li>编码器包含自注意力层。在自注意力层中，所有键、值和查询都来自同一位置，在本例中是编码器中前一层的输出。编码器中的每个位置可以关注编码器上一层中的所有位置。</li>
<li>解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流以保留自回归属性。我们通过屏蔽（设置为-无穷大）<code>softmax</code>输入中对应于非法连接的所有值。</li>
</ul>
<ul>
<li>位置前馈网络<br>除了注意力子层之外，我们的编码器和解码器中的每个层都包含一个完全连接的前馈网络，该网络单独且相同地应用于每个位置。这由两个线性变换组成，中间有一个<code>ReLU</code>激活。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure></li>
<li><code>Embeddings &amp; Softmax</code><br>与其他序列转导模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度向量。我们还使用通常学习的线性变换和<code>softmax</code>函数将解码器输出转换为预测的下一个令牌概率。在我们的模型中，我们在两个嵌入层和<code>pre-softmax</code>线性变换之间共享相同的权重矩阵。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure></li>
<li>位置编码<br>由于我们的模型不包含递归和卷积，为了使模型能够利用序列的顺序，我们必须注入一些有关序列中标记的相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码具有相同的维度作为嵌入，以便将两者相加。位置编码有多种选择，有学习的和固定的。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还尝试使用学习的位置嵌入来代替，发现这两个版本产生几乎相同的结果。我们选择正弦版本，因为它可以允许模型推断出比训练期间遇到的序列长度更长的序列长度。</span></span><br></pre></td></tr></table></figure></li>
<li>完整模型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Small example model.</span></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-bert/"><code>BERT</code>详解</a></p>
<p>我们将使用<code>Hugging Face</code>和<code>KERAS</code>实现<code>BERT</code>模型。涉及步骤：</p>
<ul>
<li>数据准备：数据的标记化和编码。</li>
<li>配置<code>TPU</code>。</li>
<li>构建一个函数用于模型训练并且添加分类输出层。</li>
<li>训练模型并得到结果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading Dependencies</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> kaggle_datasets <span class="keyword">import</span> KaggleDatasets</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># LOADING THE DATA</span></span><br><span class="line">train1 = pd.read_csv(<span class="string">&quot;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv&quot;</span>)</span><br><span class="line">valid = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv&#x27;</span>)</span><br><span class="line">sub = pd.read_csv(<span class="string">&#x27;/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fast_encode</span>(<span class="params">texts, tokenizer, chunk_size=<span class="number">256</span>, maxlen=<span class="number">512</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encoder for encoding the text into sequence of integers for BERT Input</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tokenizer.enable_truncation(max_length=maxlen)</span><br><span class="line">    tokenizer.enable_padding(max_length=maxlen)</span><br><span class="line">    all_ids = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(texts), chunk_size)):</span><br><span class="line">        text_chunk = texts[i:i+chunk_size].tolist()</span><br><span class="line">        encs = tokenizer.encode_batch(text_chunk)</span><br><span class="line">        all_ids.extend([enc.ids <span class="keyword">for</span> enc <span class="keyword">in</span> encs])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.array(all_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment">#IMP DATA FOR CONFIG</span></span><br><span class="line">AUTO = tf.data.experimental.AUTOTUNE</span><br><span class="line"><span class="comment"># Configuration</span></span><br><span class="line">EPOCHS = <span class="number">3</span></span><br><span class="line">BATCH_SIZE = <span class="number">16</span> * strategy.num_replicas_in_sync</span><br><span class="line">MAX_LEN = <span class="number">192</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenization</span></span><br><span class="line"><span class="comment"># First load the real tokenizer</span></span><br><span class="line">tokenizer = transformers.DistilBertTokenizer.from_pretrained(<span class="string">&#x27;distilbert-base-multilingual-cased&#x27;</span>)</span><br><span class="line"><span class="comment"># Save the loaded tokenizer locally</span></span><br><span class="line">tokenizer.save_pretrained(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"><span class="comment"># Reload it with the huggingface tokenizers library</span></span><br><span class="line">fast_tokenizer = BertWordPieceTokenizer(<span class="string">&#x27;vocab.txt&#x27;</span>, lowercase=<span class="literal">False</span>)</span><br><span class="line">fast_tokenizer</span><br><span class="line"></span><br><span class="line">x_train = fast_encode(train1.comment_text.astype(<span class="built_in">str</span>), fast_tokenizer, maxlen=MAX_LEN)</span><br><span class="line">x_valid = fast_encode(valid.comment_text.astype(<span class="built_in">str</span>), fast_tokenizer, maxlen=MAX_LEN)</span><br><span class="line">x_test = fast_encode(test.content.astype(<span class="built_in">str</span>), fast_tokenizer, maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">y_train = train1.toxic.values</span><br><span class="line">y_valid = valid.toxic.values</span><br><span class="line"></span><br><span class="line">train_dataset = (</span><br><span class="line">    tf.data.Dataset</span><br><span class="line">    .from_tensor_slices((x_train, y_train))</span><br><span class="line">    .repeat()</span><br><span class="line">    .shuffle(<span class="number">2048</span>)</span><br><span class="line">    .batch(BATCH_SIZE)</span><br><span class="line">    .prefetch(AUTO)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">valid_dataset = (</span><br><span class="line">    tf.data.Dataset</span><br><span class="line">    .from_tensor_slices((x_valid, y_valid))</span><br><span class="line">    .batch(BATCH_SIZE)</span><br><span class="line">    .cache()</span><br><span class="line">    .prefetch(AUTO)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataset = (</span><br><span class="line">    tf.data.Dataset</span><br><span class="line">    .from_tensor_slices(x_test)</span><br><span class="line">    .batch(BATCH_SIZE)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model</span>(<span class="params">transformer, max_len=<span class="number">512</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    function for training the BERT model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=<span class="string">&quot;input_word_ids&quot;</span>)</span><br><span class="line">    sequence_output = transformer(input_word_ids)[<span class="number">0</span>]</span><br><span class="line">    cls_token = sequence_output[:, <span class="number">0</span>, :]</span><br><span class="line">    out = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(cls_token)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs=input_word_ids, outputs=out)</span><br><span class="line">    model.<span class="built_in">compile</span>(Adam(lr=<span class="number">1e-5</span>), loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    transformer_layer = (</span><br><span class="line">        transformers.TFDistilBertModel</span><br><span class="line">        .from_pretrained(<span class="string">&#x27;distilbert-base-multilingual-cased&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    model = build_model(transformer_layer, max_len=MAX_LEN)</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">n_steps = x_train.shape[<span class="number">0</span>] // BATCH_SIZE</span><br><span class="line">train_history = model.fit(</span><br><span class="line">    train_dataset,</span><br><span class="line">    steps_per_epoch=n_steps,</span><br><span class="line">    validation_data=valid_dataset,</span><br><span class="line">    epochs=EPOCHS</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sub[<span class="string">&#x27;toxic&#x27;</span>] = model.predict(test_dataset, verbose=<span class="number">1</span>)</span><br><span class="line">sub.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;model&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">input_word_ids (InputLayer)  [(None, 192)]             0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">tf_distil_bert_model (TFDist ((None, <span class="number">192</span>, <span class="number">768</span>),)       <span class="number">134734080</span> </span><br><span class="line">_________________________________________________________________</span><br><span class="line">tf_op_layer_strided_slice (T [(None, <span class="number">768</span>)]             <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, <span class="number">1</span>)                 <span class="number">769</span>       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">134</span>,<span class="number">734</span>,<span class="number">849</span></span><br><span class="line">Trainable params: <span class="number">134</span>,<span class="number">734</span>,<span class="number">849</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">CPU times: user <span class="number">34.4</span> s, sys: <span class="number">13.3</span> s, total: <span class="number">47.7</span> s</span><br><span class="line">Wall time: <span class="number">50.8</span> s</span><br><span class="line"></span><br><span class="line">Train for <span class="number">1746</span> steps, validate for <span class="number">63</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">3</span></span><br><span class="line"><span class="number">1746</span>/<span class="number">1746</span> [==============================] - <span class="number">255</span>s <span class="number">146</span>ms/step - loss: <span class="number">0.1221</span> - accuracy: <span class="number">0.9517</span> - val_loss: <span class="number">0.4484</span> - val_accuracy: <span class="number">0.8479</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">3</span></span><br><span class="line"><span class="number">1746</span>/<span class="number">1746</span> [==============================] - <span class="number">198</span>s <span class="number">114</span>ms/step - loss: <span class="number">0.0908</span> - accuracy: <span class="number">0.9634</span> - val_loss: <span class="number">0.4769</span> - val_accuracy: <span class="number">0.8491</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">3</span></span><br><span class="line"><span class="number">1746</span>/<span class="number">1746</span> [==============================] - <span class="number">198</span>s <span class="number">113</span>ms/step - loss: <span class="number">0.0775</span> - accuracy: <span class="number">0.9680</span> - val_loss: <span class="number">0.5522</span> - val_accuracy: <span class="number">0.8500</span></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/" title="NLP（Transformers &amp; BERT）">https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/01/artificial-intelligence/nlp_begineer_study/" rel="prev" title="NLP（初级）">
                  <i class="fa fa-chevron-left"></i> NLP（初级）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/03/artificial-intelligence/nlp_advance_study/" rel="next" title="NLP（GloVe & BERT & TF-IDF & LSTM）">
                  NLP（GloVe & BERT & TF-IDF & LSTM） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.3m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">71:33</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
