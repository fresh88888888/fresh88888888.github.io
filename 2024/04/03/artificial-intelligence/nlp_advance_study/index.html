<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="自然语言处理（NLP）是人工智能的一个分支，它负责连接机器以自然语言理解人类。自然语言可以是文本或声音的形式。NLP可以用人类的方式与机器进行交流。文本分类是情感分析中涉及的内容。它是将人类的意见或表达分类为不同的情绪。情绪包括正面、中立和负面、评论评级以及快乐、悲伤。 情绪分析可以针对不同的以消费者为中心的行业进行，分析人们对特定产品或主题的看法。自然语言处理起源于20世纪50年代。早在1950">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP（GloVe &amp; BERT &amp; TF-IDF &amp; LSTM）">
<meta property="og:url" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="自然语言处理（NLP）是人工智能的一个分支，它负责连接机器以自然语言理解人类。自然语言可以是文本或声音的形式。NLP可以用人类的方式与机器进行交流。文本分类是情感分析中涉及的内容。它是将人类的意见或表达分类为不同的情绪。情绪包括正面、中立和负面、评论评级以及快乐、悲伤。 情绪分析可以针对不同的以消费者为中心的行业进行，分析人们对特定产品或主题的看法。自然语言处理起源于20世纪50年代。早在1950">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_8.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_9.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_10.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_11.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_12.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_13.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_14.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_15.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_16.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_17.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_18.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_19.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_20.png">
<meta property="article:published_time" content="2024-04-03T06:09:11.000Z">
<meta property="article:modified_time" content="2024-04-03T06:09:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/na_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/","path":"2024/04/03/artificial-intelligence/nlp_advance_study/","title":"NLP（GloVe & BERT & TF-IDF & LSTM）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>NLP（GloVe & BERT & TF-IDF & LSTM） | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">1.</span> <span class="nav-text">加载数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EDA"><span class="nav-number">2.</span> <span class="nav-text">EDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B8%85%E7%90%86%E8%AF%AD%E6%96%99%E5%BA%93"><span class="nav-number">3.1.</span> <span class="nav-text">清理语料库</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Stopwords"><span class="nav-number">3.2.</span> <span class="nav-text">Stopwords</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96"><span class="nav-number">3.3.</span> <span class="nav-text">词干提取</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96-%E7%89%B9%E5%BE%81%E5%8C%96"><span class="nav-number">3.3.1.</span> <span class="nav-text">词干提取&#x2F;特征化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.2.</span> <span class="nav-text">词干提取算法</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%A8%E9%83%A8"><span class="nav-number">3.4.</span> <span class="nav-text">全部</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E7%BC%96%E7%A0%81"><span class="nav-number">3.5.</span> <span class="nav-text">目标编码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tokens-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">Tokens 可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">矢量化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B0%83%E6%95%B4CountVectorizer"><span class="nav-number">5.1.</span> <span class="nav-text">调整CountVectorizer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TF-IDF"><span class="nav-number">5.2.</span> <span class="nav-text">TF-IDF</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%EF%BC%9AGloVe"><span class="nav-number">5.3.</span> <span class="nav-text">词嵌入：GloVe</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-number">6.</span> <span class="nav-text">建模</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-DTM"><span class="nav-number">6.1.</span> <span class="nav-text">朴素贝叶斯 DTM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">6.2.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#XGBoost"><span class="nav-number">6.3.</span> <span class="nav-text">XGBoost</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C-LSTM"><span class="nav-number">7.</span> <span class="nav-text">长短期记忆网络(LSTM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT"><span class="nav-number">8.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NLP-Disaster-Tweets"><span class="nav-number">9.</span> <span class="nav-text">NLP: Disaster Tweets</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="nav-number">9.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%8D%E4%BA%91"><span class="nav-number">9.2.</span> <span class="nav-text">词云</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1-1"><span class="nav-number">9.3.</span> <span class="nav-text">建模</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GloVe-LSTM"><span class="nav-number">9.4.</span> <span class="nav-text">GloVe - LSTM</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">171</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="NLP（GloVe & BERT & TF-IDF & LSTM） | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP（GloVe & BERT & TF-IDF & LSTM）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-03 14:09:11" itemprop="dateCreated datePublished" datetime="2024-04-03T14:09:11+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><strong>自然语言处理</strong>（<code>NLP</code>）是人工智能的一个分支，它负责连接机器以自然语言理解人类。自然语言可以是文本或声音的形式。<code>NLP</code>可以用人类的方式与机器进行交流。<strong>文本分类</strong>是情感分析中涉及的内容。它是将人类的意见或表达分类为不同的情绪。<strong>情绪</strong>包括正面、中立和负面、评论评级以及快乐、悲伤。 情绪分析可以针对不同的以消费者为中心的行业进行，分析人们对特定产品或主题的看法。自然语言处理起源于<code>20</code>世纪<code>50</code>年代。早在<code>1950</code>年，艾伦·图灵就发表了一篇题为《计算机器与智能》的文章，提出了图灵测试作为智能的标准，这项任务涉及自然语言的自动解释和生成，但当时尚未明确阐述。在此内核中，我们将重点关注文本分类和情感分析部分。</p>
<span id="more"></span>

<h4 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h4><p>只需加载数据集和颜色等全局变量即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> plotly <span class="keyword">import</span> graph_objs <span class="keyword">as</span> go</span><br><span class="line"><span class="keyword">import</span> plotly.express <span class="keyword">as</span> px</span><br><span class="line"><span class="keyword">import</span> plotly.figure_factory <span class="keyword">as</span> ff</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS, ImageColorGenerator</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> spacy.util <span class="keyword">import</span> compounding</span><br><span class="line"><span class="keyword">from</span> spacy.util <span class="keyword">import</span> minibatch</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> Constant</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> (LSTM, </span><br><span class="line">                          Embedding, </span><br><span class="line">                          BatchNormalization,</span><br><span class="line">                          Dense, </span><br><span class="line">                          TimeDistributed, </span><br><span class="line">                          Dropout, </span><br><span class="line">                          Bidirectional,</span><br><span class="line">                          Flatten, </span><br><span class="line">                          GlobalMaxPool1D)</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint, ReduceLROnPlateau</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> (</span><br><span class="line">    precision_score, </span><br><span class="line">    recall_score, </span><br><span class="line">    f1_score, </span><br><span class="line">    classification_report,</span><br><span class="line">    accuracy_score</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining all our palette colours.</span></span><br><span class="line">primary_blue = <span class="string">&quot;#496595&quot;</span></span><br><span class="line">primary_blue2 = <span class="string">&quot;#85a1c1&quot;</span></span><br><span class="line">primary_blue3 = <span class="string">&quot;#3f4d63&quot;</span></span><br><span class="line">primary_grey = <span class="string">&quot;#c6ccd8&quot;</span></span><br><span class="line">primary_black = <span class="string">&quot;#202022&quot;</span></span><br><span class="line">primary_bgcolor = <span class="string">&quot;#f4f0ea&quot;</span></span><br><span class="line">primary_green = px.colors.qualitative.Plotly[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;/kaggle/input/sms-spam-collection-dataset/spam.csv&quot;</span>, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br><span class="line">df = df.dropna(how=<span class="string">&quot;any&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">df.columns = [<span class="string">&#x27;target&#x27;</span>, <span class="string">&#x27;message&#x27;</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	target	message</span><br><span class="line">0	ham	Go <span class="keyword">until</span> jurong point, crazy.. Available only ...</span><br><span class="line">1	ham	Ok lar... Joking wif u oni...</span><br><span class="line">2	spam	Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...</span><br><span class="line">3	ham	U dun say so early hor... U c already <span class="keyword">then</span> say...</span><br><span class="line">4	ham	Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;message_len&#x27;</span>] = df[<span class="string">&#x27;message&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">df.head()</span><br><span class="line"><span class="built_in">max</span>(df[<span class="string">&#x27;message_len&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	target	message	message_len</span><br><span class="line">0	ham	Go <span class="keyword">until</span> jurong point, crazy.. Available only ...	20</span><br><span class="line">1	ham	Ok lar... Joking wif u oni...	6</span><br><span class="line">2	spam	Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...	28</span><br><span class="line">3	ham	U dun say so early hor... U c already <span class="keyword">then</span> say...	11</span><br><span class="line">4	ham	Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...	13</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">171</span></span><br></pre></td></tr></table></figure>
<h4 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h4><p>现在我们来看看目标分布和消息长度。<strong>平衡数据集</strong>：- 让我们举一个简单的例子，如果在我们的数据集中我们有正值与负值大致相同。然后我们可以说我们的数据集处于<strong>平衡状态</strong>。将橙色视为正值，将蓝色视为负值。可以说正值和负值的数量大致相同。<strong>不平衡数据集</strong>：— 如果正值和负值之间存在很大差异。然后我们可以说我们的数据集是不平衡数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">balance_counts = df.groupby(<span class="string">&#x27;target&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].agg(<span class="string">&#x27;count&#x27;</span>).values</span><br><span class="line">balance_counts</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([4825,  747])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fig = go.Figure()</span><br><span class="line">fig.add_trace(go.Bar(</span><br><span class="line">    x=[<span class="string">&#x27;ham&#x27;</span>],</span><br><span class="line">    y=[balance_counts[<span class="number">0</span>]],</span><br><span class="line">    name=<span class="string">&#x27;ham&#x27;</span>,</span><br><span class="line">    text=[balance_counts[<span class="number">0</span>]],</span><br><span class="line">    textposition=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    marker_color=primary_blue</span><br><span class="line">))</span><br><span class="line">fig.add_trace(go.Bar(</span><br><span class="line">    x=[<span class="string">&#x27;spam&#x27;</span>],</span><br><span class="line">    y=[balance_counts[<span class="number">1</span>]],</span><br><span class="line">    name=<span class="string">&#x27;spam&#x27;</span>,</span><br><span class="line">    text=[balance_counts[<span class="number">1</span>]],</span><br><span class="line">    textposition=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    marker_color=primary_grey</span><br><span class="line">))</span><br><span class="line">fig.update_layout(title=<span class="string">&#x27;&lt;span style=&quot;font-size:32px; font-family:Times New Roman&quot;&gt;Dataset distribution by target&lt;/span&gt;&#x27;</span>)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_1.png" class="">

<p>正如我们所看到的，类别是不平衡的，因此我们可以考虑使用某种重采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">ham_df = df[df[<span class="string">&#x27;target&#x27;</span>] == <span class="string">&#x27;ham&#x27;</span>][<span class="string">&#x27;message_len&#x27;</span>].value_counts().sort_index()</span><br><span class="line">spam_df = df[df[<span class="string">&#x27;target&#x27;</span>] == <span class="string">&#x27;spam&#x27;</span>][<span class="string">&#x27;message_len&#x27;</span>].value_counts().sort_index()</span><br><span class="line"></span><br><span class="line">fig = go.Figure()</span><br><span class="line">fig.add_trace(go.Scatter(</span><br><span class="line">    x=ham_df.index,</span><br><span class="line">    y=ham_df.values,</span><br><span class="line">    name=<span class="string">&#x27;ham&#x27;</span>,</span><br><span class="line">    fill=<span class="string">&#x27;tozeroy&#x27;</span>,</span><br><span class="line">    marker_color=primary_blue,</span><br><span class="line">))</span><br><span class="line">fig.add_trace(go.Scatter(</span><br><span class="line">    x=spam_df.index,</span><br><span class="line">    y=spam_df.values,</span><br><span class="line">    name=<span class="string">&#x27;spam&#x27;</span>,</span><br><span class="line">    fill=<span class="string">&#x27;tozeroy&#x27;</span>,</span><br><span class="line">    marker_color=primary_grey,</span><br><span class="line">))</span><br><span class="line">fig.update_layout(</span><br><span class="line">    title=<span class="string">&#x27;&lt;span style=&quot;font-size:32px; font-family:Times New Roman&quot;&gt;Data Roles in Different Fields&lt;/span&gt;&#x27;</span></span><br><span class="line">)</span><br><span class="line">fig.update_xaxes(<span class="built_in">range</span>=[<span class="number">0</span>, <span class="number">70</span>])</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_2.png" class="">

<p>正如我们所看到的，正常邮件的长度往往低于垃圾邮件的长度。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>现在我们将对数据进行工程设计，以使模型更容易分类。这一部分对于缩小问题的维度非常重要。</p>
<h5 id="清理语料库"><a href="#清理语料库" class="headerlink" title="清理语料库"></a>清理语料库</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clean_text</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Make text lowercase, remove text in square brackets,remove links,remove punctuation</span></span><br><span class="line"><span class="string">    and remove words containing numbers.&#x27;&#x27;&#x27;</span></span><br><span class="line">    text = <span class="built_in">str</span>(text).lower()</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\[.*?\]&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;https?://\S+|www\.\S+&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;&lt;.*?&gt;+&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;[%s]&#x27;</span> % re.escape(string.punctuation), <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\w*\d\w*&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;message_clean&#x27;</span>] = df[<span class="string">&#x27;message&#x27;</span>].apply(clean_text)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  target                                            message  message_len                                      message_clean</span><br><span class="line">0    ham  Go <span class="keyword">until</span> jurong point, crazy.. Available only ...           20  go <span class="keyword">until</span> jurong point crazy available only <span class="keyword">in</span> ...</span><br><span class="line">1    ham                      Ok lar... Joking wif u oni...            6                            ok lar joking wif u oni</span><br><span class="line">2   spam  Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...           28  free entry <span class="keyword">in</span>  a wkly comp to win fa cup final...</span><br><span class="line">3    ham  U dun say so early hor... U c already <span class="keyword">then</span> say...           11        u dun say so early hor u c already <span class="keyword">then</span> say</span><br><span class="line">4    ham  Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...           13  nah i dont think he goes to usf he lives aroun...</span></span><br></pre></td></tr></table></figure>
<h5 id="Stopwords"><a href="#Stopwords" class="headerlink" title="Stopwords"></a>Stopwords</h5><p><code>Stopwords</code>是英语中常用的单词，在句子中没有上下文含义。因此，我们在分类之前将它们删除。删除<code>Stopwords</code>的一些示例是：</p>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_3.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">stop_words = stopwords.words(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">more_stopwords = [<span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;im&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">stop_words = stop_words + more_stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_stopwords</span>(<span class="params">text</span>):</span><br><span class="line">    text = <span class="string">&#x27; &#x27;</span>.join(word <span class="keyword">for</span> word <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;message_clean&#x27;</span>] = df[<span class="string">&#x27;message_clean&#x27;</span>].apply(remove_stopwords)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  target                                            message  message_len                                      message_clean</span><br><span class="line">0    ham  Go <span class="keyword">until</span> jurong point, crazy.. Available only ...           20  go jurong point crazy available bugis n great ...</span><br><span class="line">1    ham                      Ok lar... Joking wif u oni...            6                              ok lar joking wif oni</span><br><span class="line">2   spam  Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...           28  free entry  wkly comp win fa cup final tkts  m...</span><br><span class="line">3    ham  U dun say so early hor... U c already <span class="keyword">then</span> say...           11                      dun say early hor already say</span><br><span class="line">4    ham  Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...           13        nah dont think goes usf lives around though</span></span><br></pre></td></tr></table></figure>
<h5 id="词干提取"><a href="#词干提取" class="headerlink" title="词干提取"></a>词干提取</h5><h6 id="词干提取-特征化"><a href="#词干提取-特征化" class="headerlink" title="词干提取&#x2F;特征化"></a>词干提取&#x2F;特征化</h6><p>出于语法原因，文档将使用单词的不同形式，例如<code>write、writing</code>和<code>writes</code>。此外，还有一些具有相似含义的派生相关词族。<strong>词干提取</strong>和<strong>词形还原</strong>的目标都是将单词的屈折形式和有时派生相关的形式减少为共同的基本形式。</p>
<ul>
<li><strong>词干提取</strong>通常是指为了在大多数情况下正确实现目标而砍掉单词末尾的过程，并且通常包括删除派生词缀。</li>
<li><strong>词形还原</strong>通常是指使用词汇和单词的形态分析来正确地进行操作，通常旨在仅删除屈折词尾并返回单词的基本形式和字典形式。</li>
</ul>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_4.png" class="">

<h6 id="词干提取算法"><a href="#词干提取算法" class="headerlink" title="词干提取算法"></a>词干提取算法</h6><p><code>NLTK Python</code>库中实现了多种词干提取算法：</p>
<ul>
<li><code>PorterStemmer</code>使用后缀剥离来生成词干。<code>PorterStemmer</code>以其简单和速度而闻名。请注意<code>PorterStemmer</code>如何通过简单地删除<code>cat</code>后面的’<code>s</code>‘来给出单词“<code>cats</code>”的词根（词干）。这是添加到<code>cat</code>上的后缀，使其成为复数。但是，如果你看一下“<code>trouble</code>”、“<code>trouble</code>”和“<code>troubled</code>”，它们就会被归为“<code>trouble</code>”，因为<code>PorterStemmer</code>算法不遵循语言学，而是遵循一组适用于不同情况的<code>05</code>条规则，这些规则分阶段（逐步）应用于生成词干。这就是为什么<code>PorterStemmer</code>不经常生成实际英语单词的词干的原因。它不保留单词实际词干的查找表，而是应用算法规则来生成词干。它使用规则来决定删除后缀是否明智。</li>
<li>人们可以为任何语言生成自己的一组规则，这就是为什么<code>Python nltk</code>引入了<code>SnowballStemmers</code>，用于创建非英语<code>Stemmers</code>！</li>
<li><code>LancasterStemmer</code>（<code>Paice-Husk</code>词干分析器）是一种迭代算法，规则保存在外部。一个表包含约<code>120</code>条规则，按后缀的最后一个字母进行索引。在每次迭代中，它都会尝试通过单词的最后一个字符找到适用的规则。每条规则指定删除或替换结尾部分。如果没有这样的规则，则终止。如果一个单词以元音开头并且只剩下两个单词，或者如果一个单词以辅音开头并且只剩下三个字符，它也会终止。否则，应用该规则并重复该过程。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stemmer = nltk.SnowballStemmer(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stemm_text</span>(<span class="params">text</span>):</span><br><span class="line">    text = <span class="string">&#x27; &#x27;</span>.join(stemmer.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;message_clean&#x27;</span>] = df[<span class="string">&#x27;message_clean&#x27;</span>].apply(stemm_text)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
输出结果为：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  target                                            message  message_len                                      message_clean</span><br><span class="line">0    ham  Go <span class="keyword">until</span> jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...</span><br><span class="line">1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni</span><br><span class="line">2   spam  Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...</span><br><span class="line">3    ham  U dun say so early hor... U c already <span class="keyword">then</span> say...           11                      dun say earli hor alreadi say</span><br><span class="line">4    ham  Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...           13          nah dont think goe usf live around though</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="全部"><a href="#全部" class="headerlink" title="全部"></a>全部</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># Clean puntuation, urls, and so on</span></span><br><span class="line">    text = clean_text(text)</span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    text = <span class="string">&#x27; &#x27;</span>.join(word <span class="keyword">for</span> word <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words)</span><br><span class="line">    <span class="comment"># Stemm all the words in the sentence</span></span><br><span class="line">    text = <span class="string">&#x27; &#x27;</span>.join(stemmer.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;message_clean&#x27;</span>] = df[<span class="string">&#x27;message_clean&#x27;</span>].apply(preprocess_data)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  target                                            message  message_len                                      message_clean</span><br><span class="line">0    ham  Go <span class="keyword">until</span> jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...</span><br><span class="line">1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni</span><br><span class="line">2   spam  Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...</span><br><span class="line">3    ham  U dun say so early hor... U c already <span class="keyword">then</span> say...           11                        dun say ear hor alreadi say</span><br><span class="line">4    ham  Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...           13          nah dont think goe usf live around though</span></span><br></pre></td></tr></table></figure>

<h5 id="目标编码"><a href="#目标编码" class="headerlink" title="目标编码"></a>目标编码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">le.fit(df[<span class="string">&#x27;target&#x27;</span>])</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;target_encoded&#x27;</span>] = le.transform(df[<span class="string">&#x27;target&#x27;</span>])</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  target                                            message  message_len                                      message_clean  target_encoded</span><br><span class="line">0    ham  Go <span class="keyword">until</span> jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...               0</span><br><span class="line">1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni               0</span><br><span class="line">2   spam  Free entry <span class="keyword">in</span> 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...               1</span><br><span class="line">3    ham  U dun say so early hor... U c already <span class="keyword">then</span> say...           11                        dun say ear hor alreadi say               0</span><br><span class="line">4    ham  Nah I don<span class="string">&#x27;t think he goes to usf, he lives aro...           13          nah dont think goe usf live around though               0</span></span><br></pre></td></tr></table></figure>
<h4 id="Tokens-可视化"><a href="#Tokens-可视化" class="headerlink" title="Tokens 可视化"></a>Tokens 可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">twitter_mask = np.array(Image.<span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/masksforwordclouds/twitter_mask3.jpg&#x27;</span>))</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">    background_color=<span class="string">&#x27;white&#x27;</span>, </span><br><span class="line">    max_words=<span class="number">200</span>, </span><br><span class="line">    mask=twitter_mask,</span><br><span class="line">)</span><br><span class="line">wc.generate(<span class="string">&#x27; &#x27;</span>.join(text <span class="keyword">for</span> text <span class="keyword">in</span> df.loc[df[<span class="string">&#x27;target&#x27;</span>] == <span class="string">&#x27;ham&#x27;</span>, <span class="string">&#x27;message_clean&#x27;</span>]))</span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">10</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Top words for HAM messages&#x27;</span>, </span><br><span class="line">          fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">22</span>,  <span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;bottom&#x27;</span>&#125;)</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_5.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">twitter_mask = np.array(Image.<span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/masksforwordclouds/twitter_mask3.jpg&#x27;</span>))</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">    background_color=<span class="string">&#x27;white&#x27;</span>, </span><br><span class="line">    max_words=<span class="number">200</span>, </span><br><span class="line">    mask=twitter_mask,</span><br><span class="line">)</span><br><span class="line">wc.generate(<span class="string">&#x27; &#x27;</span>.join(text <span class="keyword">for</span> text <span class="keyword">in</span> df.loc[df[<span class="string">&#x27;target&#x27;</span>] == <span class="string">&#x27;spam&#x27;</span>, <span class="string">&#x27;message_clean&#x27;</span>]))</span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">10</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Top words for HAM messages&#x27;</span>, </span><br><span class="line">          fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">22</span>,  <span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;bottom&#x27;</span>&#125;)</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_6.png" class="">

<h4 id="矢量化"><a href="#矢量化" class="headerlink" title="矢量化"></a>矢量化</h4><p>目前，我们将消息作为标记列表，现在我们需要将每条消息转换为<code>SciKit Learn</code>算法模型可以使用的<strong>向量</strong>。我们将使用<code>bag-of-words</code>模型分三个步骤来完成此操作：</p>
<ul>
<li>计算某个单词在每条消息中出现的次数（称为频率）。</li>
<li>权衡计数，使频繁的标记获得较低的权重（逆文档频率）。</li>
<li>将向量标准化为单位长度，从原始文本长度中抽象出来（<code>L2</code>范数）。</li>
</ul>
<p>让我们开始第一步：<br>每个向量的维度与<code>SMS</code>语料库中唯一单词的维度一样多。我们将首先使用<code>SciKit Learn</code>的<code>CountVectorizer</code>。该模型会将文本文档集合转换为标记计数矩阵。我们可以将其想象为一个二维矩阵。其中一维是整个词汇表（每个单词<code>1</code>行），另一个维度是实际文档，在本例中每条文本消息一列。</p>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_7.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># how to define X and y (from the SMS data) for use with COUNTVECTORIZER</span></span><br><span class="line">x = df[<span class="string">&#x27;message_clean&#x27;</span>]</span><br><span class="line">y = df[<span class="string">&#x27;target_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x), <span class="built_in">len</span>(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split into train and test sets</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train), <span class="built_in">len</span>(y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_test), <span class="built_in">len</span>(y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate the vectorizer</span></span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">vect.fit(x_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the trained to create a document-term matrix from train and test sets</span></span><br><span class="line">x_train_dtm = vect.transform(x_train)</span><br><span class="line">x_test_dtm = vect.transform(x_test)</span><br></pre></td></tr></table></figure>
<h5 id="调整CountVectorizer"><a href="#调整CountVectorizer" class="headerlink" title="调整CountVectorizer"></a>调整CountVectorizer</h5><p><code>CountVectorizer</code>有一些应该知道的参数。</p>
<ul>
<li><code>stop_words</code>：由于<code>CountVectorizer</code>只是计算词汇表中每个单词的出现次数，因此像“<code>the</code>”、“<code>and</code>”等极其常见的单词将成为非常重要的特征，但它们对文本的意义不大。如果您不考虑这些因素，您的模型通常可以改进。停用词只是您不想用作特征的单词列表。您可以设置参数<code>stop_words=&#39;english&#39;</code>以使用内置列表。或者，您可以将<code>stop_words</code>设置为等于某个自定义列表。该参数默认为无。</li>
<li><code>ngram_range</code>：<code>n-gram</code>就是一串连续的<code>n</code>个单词。例如。句子“<code>I am Groot</code>”包含<code>2-grams</code>“<code>I am”和“am Groot</code>”。该句子本身就是一个<code>3</code>元语法。设置参数<code>ngram_range=(a,b)</code>，其中<code>a</code>是要包含在特征中的<code>ngram</code>的最小值，<code>b</code>是最大值。默认<code>ngram_range为(1,1)</code>。在最近的一个项目中，我对在线招聘信息进行了建模，我发现将<code>2-gram</code>包含在内可以显著提高模型的预测能力。这很直观。许多职位名称，例如“数据科学家”、“数据工程师”和“数据分析师”都是两个词长。</li>
<li><code>min_df</code>、<code>max_df</code>：这些是单词<code>/n-gram</code>必须用作特征的最小和最大文档频率。如果这些参数中的任何一个设置为整数，它们将用作每个特征必须位于的文档数量的界限才能被视为特征。如果其中一个设置为浮点数，则该数字将被解释为频率而不是数值限制。<code>min_df</code>默认为<code>1(int)</code>，<code>max_df</code>默认为<code>1.0(float)</code>。</li>
<li><code>max_features</code>：这个参数不言自明。<code>CountVectorizer</code>将选择最常出现在其词汇表中的单词&#x2F;特征，并丢弃其他所有内容。</li>
</ul>
<p>您可以在初始化<code>CountVectorizer</code>对象时设置这些参数，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vect_tunned = CountVectorizer(stop_words=<span class="string">&#x27;english&#x27;</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>), min_df=<span class="number">0.1</span>, max_df=<span class="number">0.7</span>, max_features=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>在信息检索中，<code>tf–idf</code>、<code>TF-IDF</code>或<code>TFIDF</code>是<strong>术语频率–逆文档频率</strong>的缩写，是一种数值统计量，旨在反映一个单词对于集合或语料库中的文档的重要性，经常使作为信息检索、文本挖掘和用户建模搜索中的权重因子。<code>tf-idf</code>值与单词在文档中出现的次数成比例增加，并由语料库中包含该单词的文档数量抵消，这有助于调整某些单词通常出现频率更高的事实。<code>tf–idf</code>是当今最流行的术语加权方案之一。<code>2015</code>年进行的一项调查显示，数字图书馆中<code>83%</code>的基于文本的推荐系统使用<code>tf-idf</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"></span><br><span class="line">tfidf_transformer = TfidfTransformer()</span><br><span class="line">tfidf_transformer.fit(x_train_dtm)</span><br><span class="line">x_train_tfidf = tfidf_transformer.transform(x_train_dtm)</span><br><span class="line"></span><br><span class="line">x_train_tfidf</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;4179x5684 sparse matrix of type &#x27;&lt;class &#x27;numpy.float64&#x27;&gt;&#x27; with 32201 stored elements in Compressed Sparse Row format&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="词嵌入：GloVe"><a href="#词嵌入：GloVe" class="headerlink" title="词嵌入：GloVe"></a>词嵌入：GloVe</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">texts = df[<span class="string">&#x27;message_clean&#x27;</span>]</span><br><span class="line">target = df[<span class="string">&#x27;target_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the length of our vocabulary</span></span><br><span class="line">word_tokenizer = Tokenizer()</span><br><span class="line">word_tokenizer.fit_on_texts(texts)</span><br><span class="line"></span><br><span class="line">vocab_length = <span class="built_in">len</span>(word_tokenizer.word_index) + <span class="number">1</span></span><br><span class="line">vocab_length</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">embed</span>(<span class="params">corpus</span>): </span><br><span class="line">    <span class="keyword">return</span> word_tokenizer.texts_to_sequences(corpus)</span><br><span class="line"></span><br><span class="line">longest_train = <span class="built_in">max</span>(texts, key=<span class="keyword">lambda</span> sentence: <span class="built_in">len</span>(word_tokenize(sentence)))</span><br><span class="line">length_long_sentence = <span class="built_in">len</span>(word_tokenize(longest_train))</span><br><span class="line"></span><br><span class="line">train_padded_sentences = pad_sequences(</span><br><span class="line">    embed(texts), </span><br><span class="line">    length_long_sentence, </span><br><span class="line">    padding=<span class="string">&#x27;post&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_padded_sentences</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[   2, 3179,  274, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [   8,  236,  527, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [   9,  356,  588, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        ...,</span></span><br><span class="line"><span class="comment">#        [6724, 1002, 6725, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [ 138, 1251, 1603, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [1986,  378,  170, ...,    0,    0,    0]], dtype=int32)</span></span><br></pre></td></tr></table></figure>
<p><code>GloVe</code>方法建立在一个重要的想法之上，您可以从<code>co-occurrence</code>矩阵导出单词之间的语义关系。为了获得单词的向量表示，我们可以使用一种名为<code>GloVe</code>（单词表示的全局向量）的无监督学习算法，该算法专注于整个语料库中单词的<code>co-occurrence</code>。它的嵌入与两个单词一起出现的概率有关。<strong>词嵌入</strong>是一种词表示形式，它将人类对语言的理解与机器对语言理解联系起来。他们已经学习了<code>n</code>维空间中文本的表示，其中具有相同含义的单词具有相似的表示。这意味着两个相似的单词由向量空间中非常接近的向量表示。因此，当使用词嵌入时，所有单个词都被表示为预定义向量空间中的<strong>实值向量</strong>。每个单词都映射到一个向量，并且类似于神经网络的方式学习向量值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">embeddings_dictionary = <span class="built_in">dict</span>()</span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load GloVe 100D embeddings</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/kaggle/input/glove6b100dtxt/glove.6B.100d.txt&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fp.readlines():</span><br><span class="line">        records = line.split()</span><br><span class="line">        word = records[<span class="number">0</span>]</span><br><span class="line">        vector_dimensions = np.asarray(records[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        embeddings_dictionary [word] = vector_dimensions</span><br><span class="line"></span><br><span class="line"><span class="comment"># embeddings_dictionary</span></span><br><span class="line"><span class="comment"># Now we will load embedding vectors of those words that appear in the</span></span><br><span class="line"><span class="comment"># Glove dictionary. Others will be initialized to 0.</span></span><br><span class="line"></span><br><span class="line">embedding_matrix = np.zeros((vocab_length, embedding_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, index <span class="keyword">in</span> word_tokenizer.word_index.items():</span><br><span class="line">    embedding_vector = embeddings_dictionary.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        embedding_matrix[index] = embedding_vector</span><br><span class="line">        </span><br><span class="line">embedding_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,</span></span><br><span class="line"><span class="comment">#          0.        ,  0.        ],</span></span><br><span class="line"><span class="comment">#        [-0.57832998, -0.0036551 ,  0.34658   , ...,  0.070204  ,</span></span><br><span class="line"><span class="comment">#          0.44509   ,  0.24147999],</span></span><br><span class="line"><span class="comment">#        [-0.078894  ,  0.46160001,  0.57779002, ...,  0.26352   ,</span></span><br><span class="line"><span class="comment">#          0.59397   ,  0.26741001],</span></span><br><span class="line"><span class="comment">#        ...,</span></span><br><span class="line"><span class="comment">#        [ 0.63009   , -0.036992  ,  0.24052   , ...,  0.10029   ,</span></span><br><span class="line"><span class="comment">#          0.056822  ,  0.25018999],</span></span><br><span class="line"><span class="comment">#        [-0.12002   , -1.23870003, -0.23303001, ...,  0.13658001,</span></span><br><span class="line"><span class="comment">#         -0.61848003,  0.049843  ],</span></span><br><span class="line"><span class="comment">#        [ 0.        ,  0.        ,  0.        , ...,  0.        ,</span></span><br><span class="line"><span class="comment">#          0.        ,  0.        ]])</span></span><br></pre></td></tr></table></figure>
<h4 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h4><p>创建多项式朴素贝叶斯模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> plotly.figure_factory <span class="keyword">as</span> ff</span><br><span class="line"></span><br><span class="line">x_axes = [<span class="string">&#x27;Ham&#x27;</span>, <span class="string">&#x27;Spam&#x27;</span>]</span><br><span class="line">y_axes =  [<span class="string">&#x27;Spam&#x27;</span>, <span class="string">&#x27;Ham&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conf_matrix</span>(<span class="params">z, x=x_axes, y=y_axes</span>):</span><br><span class="line">    </span><br><span class="line">    z = np.flip(z, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># change each element of z to type string for annotations</span></span><br><span class="line">    z_text = [[<span class="built_in">str</span>(y) <span class="keyword">for</span> y <span class="keyword">in</span> x] <span class="keyword">for</span> x <span class="keyword">in</span> z]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up figure </span></span><br><span class="line">    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale=<span class="string">&#x27;Viridis&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add title</span></span><br><span class="line">    fig.update_layout(title_text=<span class="string">&#x27;&lt;b&gt;Confusion matrix&lt;/b&gt;&#x27;</span>,</span><br><span class="line">                      xaxis = <span class="built_in">dict</span>(title=<span class="string">&#x27;Predicted value&#x27;</span>),</span><br><span class="line">                      yaxis = <span class="built_in">dict</span>(title=<span class="string">&#x27;Real value&#x27;</span>)</span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add colorbar</span></span><br><span class="line">    fig[<span class="string">&#x27;data&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;showscale&#x27;</span>] = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Multinomial Naive Bayes model</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">nb = MultinomialNB()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">nb.fit(x_train_dtm, y_train)</span><br></pre></td></tr></table></figure>
<h5 id="朴素贝叶斯-DTM"><a href="#朴素贝叶斯-DTM" class="headerlink" title="朴素贝叶斯 DTM"></a>朴素贝叶斯 DTM</h5><p>在统计学中，<strong>朴素贝叶斯分类器</strong>是简单的“<strong>概率分类器</strong>”，基于应用贝叶斯定理以及特征之间的强（朴素）独立性假设。它们是最简单的贝叶斯网络模型之一，但与核密度估计相结合，它们可以达到更高的精度水平。<strong>朴素贝叶斯分类器</strong>具有高度可扩展性，需要学习问题中的许多参数与变量（特征&#x2F;预测变量）的数量呈线性关系。<code>Maximum-likelihood</code>训练可以通过评估封闭表达式来完成，这需要线性时间，而不是像许多其他类型的分类器那样通过昂贵的迭代近似来完成。</p>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_8.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make class anf probability predictions</span></span><br><span class="line">y_pred_class = nb.predict(x_test_dtm)</span><br><span class="line">y_pred_prob = nb.predict_proba(x_test_dtm)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate accuracy of class predictions</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test, y_pred_class))</span><br><span class="line"><span class="comment"># Calculate AUC</span></span><br><span class="line">metrics.roc_auc_score(y_test, y_pred_prob)</span><br><span class="line"></span><br><span class="line">conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9784637473079684</span></span><br><span class="line"><span class="comment"># 0.974296765425861</span></span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_9.png" class="">

<h5 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">pipe = Pipeline([(<span class="string">&#x27;bow&#x27;</span>, CountVectorizer()), (<span class="string">&#x27;tfid&#x27;</span>, TfidfTransformer()), (<span class="string">&#x27;model&#x27;</span>, MultinomialNB())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the pipeline with the data</span></span><br><span class="line">pipe.fit(x_train, y_train)</span><br><span class="line">y_pred_class = pipe.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test, y_pred_class))</span><br><span class="line"></span><br><span class="line">conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9597989949748744</span></span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_10.png" class="">

<h5 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;bow&#x27;</span>, CountVectorizer()), </span><br><span class="line">    (<span class="string">&#x27;tfid&#x27;</span>, TfidfTransformer()),  </span><br><span class="line">    (<span class="string">&#x27;model&#x27;</span>, xgb.XGBClassifier(</span><br><span class="line">        learning_rate=<span class="number">0.1</span>,</span><br><span class="line">        max_depth=<span class="number">7</span>,</span><br><span class="line">        n_estimators=<span class="number">80</span>,</span><br><span class="line">        use_label_encoder=<span class="literal">False</span>,</span><br><span class="line">        eval_metric=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="comment"># colsample_bytree=0.8,</span></span><br><span class="line">        <span class="comment"># subsample=0.7,</span></span><br><span class="line">        <span class="comment"># min_child_weight=5,</span></span><br><span class="line">    ))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the pipeline with the data</span></span><br><span class="line">pipe.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_class = pipe.predict(x_test)</span><br><span class="line">y_pred_train = pipe.predict(x_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(metrics.accuracy_score(y_train, y_pred_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(metrics.accuracy_score(y_test, y_pred_class)))</span><br><span class="line"></span><br><span class="line">conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train: 0.9834888729361091</span></span><br><span class="line"><span class="comment"># Test: 0.9662598707824839</span></span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_11.png" class="">

<h4 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络(LSTM)"></a>长短期记忆网络(LSTM)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data into train and test sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    train_padded_sentences, </span><br><span class="line">    target, </span><br><span class="line">    test_size=<span class="number">0.25</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">glove_lstm</span>():</span><br><span class="line">    model = Sequential()</span><br><span class="line">    </span><br><span class="line">    model.add(Embedding(</span><br><span class="line">        input_dim=embedding_matrix.shape[<span class="number">0</span>], </span><br><span class="line">        output_dim=embedding_matrix.shape[<span class="number">1</span>], </span><br><span class="line">        weights = [embedding_matrix], </span><br><span class="line">        input_length=length_long_sentence</span><br><span class="line">    ))</span><br><span class="line">    </span><br><span class="line">    model.add(Bidirectional(LSTM(</span><br><span class="line">        length_long_sentence, </span><br><span class="line">        return_sequences = <span class="literal">True</span>, </span><br><span class="line">        recurrent_dropout=<span class="number">0.2</span></span><br><span class="line">    )))</span><br><span class="line">    </span><br><span class="line">    model.add(GlobalMaxPool1D())</span><br><span class="line">    model.add(BatchNormalization())</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(length_long_sentence, activation = <span class="string">&quot;relu&quot;</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(length_long_sentence, activation = <span class="string">&quot;relu&quot;</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">model = glove_lstm()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 80, 100)           672600    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">bidirectional (Bidirectional (None, 80, 160)           115840    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">global_max_pooling1d (Global (None, 160)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">batch_normalization (BatchNo (None, 160)               640       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout (Dropout)            (None, 160)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 80)                12880     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 80)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 80)                6480      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 80)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (None, 1)                 81        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 808,521</span><br><span class="line">Trainable params: 808,201</span><br><span class="line">Non-trainable params: 320</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the model and train!!</span></span><br><span class="line">model = glove_lstm()</span><br><span class="line"></span><br><span class="line">checkpoint = ModelCheckpoint(</span><br><span class="line">    <span class="string">&#x27;model.h5&#x27;</span>, </span><br><span class="line">    monitor = <span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">    verbose = <span class="number">1</span>, </span><br><span class="line">    save_best_only = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">reduce_lr = ReduceLROnPlateau(</span><br><span class="line">    monitor = <span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">    factor = <span class="number">0.2</span>, </span><br><span class="line">    verbose = <span class="number">1</span>, </span><br><span class="line">    patience = <span class="number">5</span>,                        </span><br><span class="line">    min_lr = <span class="number">0.001</span></span><br><span class="line">)</span><br><span class="line">history = model.fit(</span><br><span class="line">    X_train, </span><br><span class="line">    y_train, </span><br><span class="line">    epochs = <span class="number">7</span>,</span><br><span class="line">    batch_size = <span class="number">32</span>,</span><br><span class="line">    validation_data = (X_test, y_test),</span><br><span class="line">    verbose = <span class="number">1</span>,</span><br><span class="line">    callbacks = [reduce_lr, checkpoint]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>让我们看看结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_learning_curves</span>(<span class="params">history, arr</span>):</span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        ax[idx].plot(history.history[arr[idx][<span class="number">0</span>]])</span><br><span class="line">        ax[idx].plot(history.history[arr[idx][<span class="number">1</span>]])</span><br><span class="line">        ax[idx].legend([arr[idx][<span class="number">0</span>], arr[idx][<span class="number">1</span>]],fontsize=<span class="number">18</span>)</span><br><span class="line">        ax[idx].set_xlabel(<span class="string">&#x27;A &#x27;</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">        ax[idx].set_ylabel(<span class="string">&#x27;B&#x27;</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">        ax[idx].set_title(arr[idx][<span class="number">0</span>] + <span class="string">&#x27; X &#x27;</span> + arr[idx][<span class="number">1</span>],fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">plot_learning_curves(history, [[<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>],[<span class="string">&#x27;accuracy&#x27;</span>, <span class="string">&#x27;val_accuracy&#x27;</span>]])</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_12.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_preds = (model.predict(X_test) &gt; <span class="number">0.5</span>).astype(<span class="string">&quot;int32&quot;</span>)</span><br><span class="line">conf_matrix(metrics.confusion_matrix(y_test, y_preds))</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_13.png" class="">

<h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p><code>BERT</code>（来自<code>Transformer</code>的双向编码器表示）是<code>Google AI Language</code>的研究人员最近发表的一篇论文。它在各种<code>NLP</code>任务中展示了优秀的结果，包括问答(<code>SQuAD v1.1</code>)、自然语言推理 (<code>MNLI</code>) 等，在机器学习社区引起了轰动。<code>BERT</code>的关键技术创新是将流行的注意力模型<code>Transformer</code>的双向训练应用于语言建模。这与之前的研究形成对比，之前的研究从左到右或从右到左组合训练文本序列。论文结果显示，双向训练的语言模型比单向语言模型能够更深入地了解语言上下文和流程。在论文中，研究人员详细介绍了一种名为<code>Masked LM (MLM)</code>的新技术，该技术允许在模型中进行双向训练，而这在以前是无法想象的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TFBertModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()</span><br><span class="line">    tf.config.experimental_connect_to_cluster(tpu)</span><br><span class="line">    tf.tpu.experimental.initialize_tpu_system(tpu)</span><br><span class="line">    strategy = tf.distribute.experimental.TPUStrategy(tpu)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    strategy = tf.distribute.get_strategy()</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of replicas in sync: &#x27;</span>, strategy.num_replicas_in_sync)</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-large-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bert_encode</span>(<span class="params">data, maximum_length</span>) :</span><br><span class="line">    input_ids = []</span><br><span class="line">    attention_masks = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> data:</span><br><span class="line">        encoded = tokenizer.encode_plus(</span><br><span class="line">            text, </span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=maximum_length,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        input_ids.append(encoded[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">        attention_masks.append(encoded[<span class="string">&#x27;attention_mask&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> np.array(input_ids),np.array(attention_masks)</span><br><span class="line"></span><br><span class="line">texts = df[<span class="string">&#x27;message_clean&#x27;</span>]</span><br><span class="line">target = df[<span class="string">&#x27;target_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line">train_input_ids, train_attention_masks = bert_encode(texts,<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">bert_model</span>):</span><br><span class="line">    </span><br><span class="line">    input_ids = tf.keras.Input(shape=(<span class="number">60</span>,),dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    attention_masks = tf.keras.Input(shape=(<span class="number">60</span>,),dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    output = bert_model([input_ids,attention_masks])</span><br><span class="line">    output = output[<span class="number">1</span>]</span><br><span class="line">    output = tf.keras.layers.Dense(<span class="number">32</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(output)</span><br><span class="line">    output = tf.keras.layers.Dropout(<span class="number">0.2</span>)(output)</span><br><span class="line">    output = tf.keras.layers.Dense(<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)(output)</span><br><span class="line">    </span><br><span class="line">    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)</span><br><span class="line">    model.<span class="built_in">compile</span>(Adam(lr=<span class="number">1e-5</span>), loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">bert_model = TFBertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = create_model(bert_model)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;model&quot;</span></span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                    Output Shape         Param <span class="comment">#     Connected to                     </span></span><br><span class="line">==================================================================================================</span><br><span class="line">input_1 (InputLayer)            [(None, 60)]         0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">input_2 (InputLayer)            [(None, 60)]         0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    </span><br><span class="line">                                                                 input_2[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_6 (Dense)                 (None, 32)           24608       tf_bert_model[0][1]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_43 (Dropout)            (None, 32)           0           dense_6[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_7 (Dense)                 (None, 1)            33          dropout_43[0][0]                 </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 109,506,881</span><br><span class="line">Trainable params: 109,506,881</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">__________________________________________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">history = model.fit([train_input_ids, train_attention_masks],target,validation_split=<span class="number">0.2</span>, epochs=<span class="number">3</span>,batch_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/3</span><br><span class="line">446/446 [==============================] - 1803s 4s/step - loss: 0.2607 - accuracy: 0.9095 - val_loss: 0.0709 - val_accuracy: 0.9767</span><br><span class="line">Epoch 2/3</span><br><span class="line">446/446 [==============================] - 1769s 4s/step - loss: 0.0636 - accuracy: 0.9838 - val_loss: 0.0774 - val_accuracy: 0.9767</span><br><span class="line">Epoch 3/3</span><br><span class="line">446/446 [==============================] - 1781s 4s/step - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.0452 - val_accuracy: 0.9857</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curves(history, [[<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>],[<span class="string">&#x27;accuracy&#x27;</span>, <span class="string">&#x27;val_accuracy&#x27;</span>]])</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_14.png" class="">

<h4 id="NLP-Disaster-Tweets"><a href="#NLP-Disaster-Tweets" class="headerlink" title="NLP: Disaster Tweets"></a>NLP: Disaster Tweets</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;/kaggle/input/nlp-getting-started/train.csv&quot;</span>, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">&quot;/kaggle/input/nlp-getting-started/test.csv&quot;</span>, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br><span class="line"></span><br><span class="line">df = df.dropna(how=<span class="string">&quot;any&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">df[<span class="string">&#x27;text_len&#x27;</span>] = df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	<span class="built_in">id</span>	text	target	text_len</span><br><span class="line">0	1	Our Deeds are the Reason of this <span class="comment">#earthquake M...	1	13</span></span><br><span class="line">1	4	Forest fire near La Ronge Sask. Canada	1	7</span><br><span class="line">2	5	All residents asked to <span class="string">&#x27;shelter in place&#x27;</span> are ...	1	22</span><br><span class="line">3	6	13,000 people receive <span class="comment">#wildfires evacuation or...	1	9</span></span><br><span class="line">4	7	Just got sent this photo from Ruby <span class="comment">#Alaska as ...	1	17</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">balance_counts = df.groupby(<span class="string">&#x27;target&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].agg(<span class="string">&#x27;count&#x27;</span>).values</span><br><span class="line">balance_counts</span><br><span class="line"></span><br><span class="line">fig = go.Figure()</span><br><span class="line">fig.add_trace(go.Bar(</span><br><span class="line">    x=[<span class="string">&#x27;Fake&#x27;</span>],</span><br><span class="line">    y=[balance_counts[<span class="number">0</span>]],</span><br><span class="line">    name=<span class="string">&#x27;Fake&#x27;</span>,</span><br><span class="line">    text=[balance_counts[<span class="number">0</span>]],</span><br><span class="line">    textposition=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    marker_color=primary_blue</span><br><span class="line">))</span><br><span class="line">fig.add_trace(go.Bar(</span><br><span class="line">    x=[<span class="string">&#x27;Real disaster&#x27;</span>],</span><br><span class="line">    y=[balance_counts[<span class="number">1</span>]],</span><br><span class="line">    name=<span class="string">&#x27;Real disaster&#x27;</span>,</span><br><span class="line">    text=[balance_counts[<span class="number">1</span>]],</span><br><span class="line">    textposition=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    marker_color=primary_grey</span><br><span class="line">))</span><br><span class="line">fig.update_layout(</span><br><span class="line">    title=<span class="string">&#x27;&lt;span style=&quot;font-size:32px; font-family:Times New Roman&quot;&gt;Dataset distribution by target&lt;/span&gt;&#x27;</span></span><br><span class="line">)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_15.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">disaster_df = df[df[<span class="string">&#x27;target&#x27;</span>] == <span class="number">1</span>][<span class="string">&#x27;text_len&#x27;</span>].value_counts().sort_index()</span><br><span class="line">fake_df = df[df[<span class="string">&#x27;target&#x27;</span>] == <span class="number">0</span>][<span class="string">&#x27;text_len&#x27;</span>].value_counts().sort_index()</span><br><span class="line"></span><br><span class="line">fig = go.Figure()</span><br><span class="line">fig.add_trace(go.Scatter(</span><br><span class="line">    x=disaster_df.index,</span><br><span class="line">    y=disaster_df.values,</span><br><span class="line">    name=<span class="string">&#x27;Real disaster&#x27;</span>,</span><br><span class="line">    fill=<span class="string">&#x27;tozeroy&#x27;</span>,</span><br><span class="line">    marker_color=primary_blue,</span><br><span class="line">))</span><br><span class="line">fig.add_trace(go.Scatter(</span><br><span class="line">    x=fake_df.index,</span><br><span class="line">    y=fake_df.values,</span><br><span class="line">    name=<span class="string">&#x27;Fake&#x27;</span>,</span><br><span class="line">    fill=<span class="string">&#x27;tozeroy&#x27;</span>,</span><br><span class="line">    marker_color=primary_grey,</span><br><span class="line">))</span><br><span class="line">fig.update_layout(</span><br><span class="line">    title=<span class="string">&#x27;&lt;span style=&quot;font-size:32px; font-family:Times New Roman&quot;&gt;Data Roles in Different Fields&lt;/span&gt;&#x27;</span></span><br><span class="line">)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_16.png" class="">

<h5 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">remove_url</span>(<span class="params">text</span>):</span><br><span class="line">    url = re.<span class="built_in">compile</span>(<span class="string">r&#x27;https?://\S+|www\.\S+&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> url.sub(<span class="string">r&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_emoji</span>(<span class="params">text</span>):</span><br><span class="line">    emoji_pattern = re.<span class="built_in">compile</span>(</span><br><span class="line">        <span class="string">&#x27;[&#x27;</span></span><br><span class="line">        <span class="string">u&#x27;\U0001F600-\U0001F64F&#x27;</span>  <span class="comment"># emoticons</span></span><br><span class="line">        <span class="string">u&#x27;\U0001F300-\U0001F5FF&#x27;</span>  <span class="comment"># symbols &amp; pictographs</span></span><br><span class="line">        <span class="string">u&#x27;\U0001F680-\U0001F6FF&#x27;</span>  <span class="comment"># transport &amp; map symbols</span></span><br><span class="line">        <span class="string">u&#x27;\U0001F1E0-\U0001F1FF&#x27;</span>  <span class="comment"># flags (iOS)</span></span><br><span class="line">        <span class="string">u&#x27;\U00002702-\U000027B0&#x27;</span></span><br><span class="line">        <span class="string">u&#x27;\U000024C2-\U0001F251&#x27;</span></span><br><span class="line">        <span class="string">&#x27;]+&#x27;</span>,</span><br><span class="line">        flags=re.UNICODE)</span><br><span class="line">    <span class="keyword">return</span> emoji_pattern.sub(<span class="string">r&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_html</span>(<span class="params">text</span>):</span><br><span class="line">    html = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;.*?&gt;|&amp;([a-z0-9]+|#[0-9]&#123;1,6&#125;|#x[0-9a-f]&#123;1,6&#125;);&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> re.sub(html, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Special thanks to https://www.kaggle.com/tanulsingh077 for this function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_text</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Make text lowercase, remove text in square brackets,remove links,remove punctuation</span></span><br><span class="line"><span class="string">    and remove words containing numbers.&#x27;&#x27;&#x27;</span></span><br><span class="line">    text = <span class="built_in">str</span>(text).lower()</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\[.*?\]&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(</span><br><span class="line">        <span class="string">&#x27;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#x27;</span>, </span><br><span class="line">        <span class="string">&#x27;&#x27;</span>, </span><br><span class="line">        text</span><br><span class="line">    )</span><br><span class="line">    text = re.sub(<span class="string">&#x27;https?://\S+|www\.\S+&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;&lt;.*?&gt;+&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;[%s]&#x27;</span> % re.escape(string.punctuation), <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&#x27;\w*\d\w*&#x27;</span>, <span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">    </span><br><span class="line">    text = remove_url(text)</span><br><span class="line">    text = remove_emoji(text)</span><br><span class="line">    text = remove_html(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test emoji removal</span></span><br><span class="line">remove_emoji(<span class="string">&quot;Omg another Earthquake 😔😔&quot;</span>)</span><br><span class="line"></span><br><span class="line">stop_words = stopwords.words(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">more_stopwords = [<span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;im&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">stop_words = stop_words + more_stopwords</span><br><span class="line"></span><br><span class="line">stemmer = nltk.SnowballStemmer(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># Clean puntuation, urls, and so on</span></span><br><span class="line">    text = clean_text(text)</span><br><span class="line">    <span class="comment"># Remove stopwords and Stemm all the words in the sentence</span></span><br><span class="line">    text = <span class="string">&#x27; &#x27;</span>.join(stemmer.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">test_df[<span class="string">&#x27;text_clean&#x27;</span>] = test_df[<span class="string">&#x27;text&#x27;</span>].apply(preprocess_data)</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;text_clean&#x27;</span>] = df[<span class="string">&#x27;text&#x27;</span>].apply(preprocess_data)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   <span class="built_in">id</span>                                               text  target  text_len                                         text_clean</span><br><span class="line">0   1  Our Deeds are the Reason of this <span class="comment">#earthquake M...       1        13          deed reason earthquak may allah forgiv us</span></span><br><span class="line">1   4             Forest fire near La Ronge Sask. Canada       1         7               forest fire near la rong sask canada</span><br><span class="line">2   5  All residents asked to <span class="string">&#x27;shelter in place&#x27;</span> are ...       1        22  resid ask shelter place notifi offic evacu she...</span><br><span class="line">3   6  13,000 people receive <span class="comment">#wildfires evacuation or...       1         9       peopl receiv wildfir evacu order california </span></span><br><span class="line">4   7  Just got sent this photo from Ruby <span class="comment">#Alaska as ...       1        17  got sent photo rubi alaska smoke wildfir pour ...</span></span><br></pre></td></tr></table></figure>

<h5 id="词云"><a href="#词云" class="headerlink" title="词云"></a>词云</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_corpus_df</span>(<span class="params">tweet, target</span>):</span><br><span class="line">    corpus=[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tweet[tweet[<span class="string">&#x27;target&#x27;</span>]==target][<span class="string">&#x27;text_clean&#x27;</span>].<span class="built_in">str</span>.split():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">            corpus.append(i)</span><br><span class="line">    <span class="keyword">return</span> corpus</span><br><span class="line"></span><br><span class="line">corpus_disaster_tweets = create_corpus_df(df, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dic=defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> corpus_disaster_tweets:</span><br><span class="line">    dic[word]+=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">top=<span class="built_in">sorted</span>(dic.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:<span class="number">10</span>]</span><br><span class="line"><span class="built_in">print</span>(top)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [(&#x27;fire&#x27;, 266),(&#x27;bomb&#x27;, 179),(&#x27;kill&#x27;, 158),(&#x27;news&#x27;, 132),(&#x27;via&#x27;, 121),(&#x27;flood&#x27;, 120),(&#x27;disast&#x27;, 116),(&#x27;california&#x27;, 115),</span></span><br><span class="line"><span class="comment">#  (&#x27;crash&#x27;, 110),(&#x27;suicid&#x27;, 110)]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">twitter_mask = np.array(Image.<span class="built_in">open</span>(<span class="string">&#x27;twitter_mask3.jpg&#x27;</span>))</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">    background_color=<span class="string">&#x27;white&#x27;</span>, </span><br><span class="line">    max_words=<span class="number">200</span>, </span><br><span class="line">    mask=twitter_mask,</span><br><span class="line">)</span><br><span class="line">wc.generate(<span class="string">&#x27; &#x27;</span>.join(text <span class="keyword">for</span> text <span class="keyword">in</span> df.loc[df[<span class="string">&#x27;target&#x27;</span>] == <span class="number">1</span>, <span class="string">&#x27;text_clean&#x27;</span>]))</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Top words for Real Disaster tweets&#x27;</span>, </span><br><span class="line">          fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">22</span>,  <span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;bottom&#x27;</span>&#125;)</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_17.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">corpus_disaster_tweets = create_corpus_df(df, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">dic=defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> corpus_disaster_tweets:</span><br><span class="line">    dic[word]+=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">top=<span class="built_in">sorted</span>(dic.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:<span class="number">10</span>]</span><br><span class="line">top</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">    background_color=<span class="string">&#x27;white&#x27;</span>, </span><br><span class="line">    max_words=<span class="number">200</span>, </span><br><span class="line">    mask=twitter_mask,</span><br><span class="line">)</span><br><span class="line">wc.generate(<span class="string">&#x27; &#x27;</span>.join(text <span class="keyword">for</span> text <span class="keyword">in</span> df.loc[df[<span class="string">&#x27;target&#x27;</span>] == <span class="number">0</span>, <span class="string">&#x27;text_clean&#x27;</span>]))</span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">10</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Top words for Fake messages&#x27;</span>, </span><br><span class="line">          fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">22</span>,  <span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;bottom&#x27;</span>&#125;)</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;like&#x27;</span>, <span class="number">306</span>),(<span class="string">&#x27;get&#x27;</span>, <span class="number">222</span>),(<span class="string">&#x27;amp&#x27;</span>, <span class="number">192</span>),(<span class="string">&#x27;new&#x27;</span>, <span class="number">168</span>),(<span class="string">&#x27;go&#x27;</span>, <span class="number">142</span>),(<span class="string">&#x27;dont&#x27;</span>, <span class="number">139</span>),(<span class="string">&#x27;one&#x27;</span>, <span class="number">134</span>),(<span class="string">&#x27;bodi&#x27;</span>, <span class="number">116</span>),</span><br><span class="line"> (<span class="string">&#x27;love&#x27;</span>, <span class="number">115</span>),(<span class="string">&#x27;bag&#x27;</span>, <span class="number">108</span>)]</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_18.png" class="">

<h5 id="建模-1"><a href="#建模-1" class="headerlink" title="建模"></a>建模</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># how to define X and y (from the SMS data) for use with COUNTVECTORIZER</span></span><br><span class="line">x = df[<span class="string">&#x27;text_clean&#x27;</span>]</span><br><span class="line">y = df[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split into train and test sets</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_train), <span class="built_in">len</span>(y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_test), <span class="built_in">len</span>(y_test))</span><br><span class="line"></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;bow&#x27;</span>, CountVectorizer()), </span><br><span class="line">    (<span class="string">&#x27;tfid&#x27;</span>, TfidfTransformer()),  </span><br><span class="line">    (<span class="string">&#x27;model&#x27;</span>, xgb.XGBClassifier(</span><br><span class="line">        use_label_encoder=<span class="literal">False</span>,</span><br><span class="line">        eval_metric=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    ))</span><br><span class="line">])</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the pipeline with the data</span></span><br><span class="line">pipe.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_class = pipe.predict(x_test)</span><br><span class="line">y_pred_train = pipe.predict(x_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(metrics.accuracy_score(y_train, y_pred_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(metrics.accuracy_score(y_test, y_pred_class)))</span><br><span class="line"></span><br><span class="line">conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5709 5709</span></span><br><span class="line"><span class="comment"># 1904 1904</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train: 0.8567174636538798</span></span><br><span class="line"><span class="comment"># Test: 0.7725840336134454</span></span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_19.png" class="">

<h5 id="GloVe-LSTM"><a href="#GloVe-LSTM" class="headerlink" title="GloVe - LSTM"></a>GloVe - LSTM</h5><p>我们将使用<code>LSTM</code>（长短期记忆）模型。我们需要执行标记化——将文本分割成单词句子的处理。在这个过程中，我们也扔掉了标点符号和额外的符号。标记化的好处在于，它更容易转换为原始数字的格式，这实际上可以用于处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">train_tweets = df[<span class="string">&#x27;text_clean&#x27;</span>].values</span><br><span class="line">test_tweets = test_df[<span class="string">&#x27;text_clean&#x27;</span>].values</span><br><span class="line">train_target = df[<span class="string">&#x27;target&#x27;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the length of our vocabulary</span></span><br><span class="line">word_tokenizer = Tokenizer()</span><br><span class="line">word_tokenizer.fit_on_texts(train_tweets)</span><br><span class="line"></span><br><span class="line">vocab_length = <span class="built_in">len</span>(word_tokenizer.word_index) + <span class="number">1</span></span><br><span class="line">vocab_length</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_metrics</span>(<span class="params">pred_tag, y_test</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;F1-score: &quot;</span>, f1_score(pred_tag, y_test))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Precision: &quot;</span>, precision_score(pred_tag, y_test))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Recall: &quot;</span>, recall_score(pred_tag, y_test))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Acuracy: &quot;</span>, accuracy_score(pred_tag, y_test))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(classification_report(pred_tag, y_test))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">embed</span>(<span class="params">corpus</span>): </span><br><span class="line">    <span class="keyword">return</span> word_tokenizer.texts_to_sequences(corpus)</span><br><span class="line"></span><br><span class="line">longest_train = <span class="built_in">max</span>(train_tweets, key=<span class="keyword">lambda</span> sentence: <span class="built_in">len</span>(word_tokenize(sentence)))</span><br><span class="line">length_long_sentence = <span class="built_in">len</span>(word_tokenize(longest_train))</span><br><span class="line"></span><br><span class="line">train_padded_sentences = pad_sequences(</span><br><span class="line">    embed(train_tweets), </span><br><span class="line">    length_long_sentence, </span><br><span class="line">    padding=<span class="string">&#x27;post&#x27;</span></span><br><span class="line">)</span><br><span class="line">test_padded_sentences = pad_sequences(</span><br><span class="line">    embed(test_tweets), </span><br><span class="line">    length_long_sentence,</span><br><span class="line">    padding=<span class="string">&#x27;post&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_padded_sentences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 13704</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[3635,  467,  201, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [ 136,    2,  106, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [1338,  502, 1807, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        ...,</span></span><br><span class="line"><span class="comment">#        [ 448, 1328,    0, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [  28,  162, 2637, ...,    0,    0,    0],</span></span><br><span class="line"><span class="comment">#        [ 171,   31,  413, ...,    0,    0,    0]], dtype=int32)</span></span><br></pre></td></tr></table></figure>
<p>为了获得单词的向量表示，我们可以使用一种名为<code>GloVe</code>（单词表示的全局向量）的无监督学习算法，该算法专注于整个语料库中单词的共现。它的嵌入与两个单词一起出现的概率有关。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load GloVe 100D embeddings</span></span><br><span class="line"><span class="comment"># We are not going to do it here as they were loaded earlier.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we will load embedding vectors of those words that appear in the</span></span><br><span class="line"><span class="comment"># Glove dictionary. Others will be initialized to 0.</span></span><br><span class="line">embedding_matrix = np.zeros((vocab_length, embedding_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, index <span class="keyword">in</span> word_tokenizer.word_index.items():</span><br><span class="line">    embedding_vector = embeddings_dictionary.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        embedding_matrix[index] = embedding_vector</span><br><span class="line">        </span><br><span class="line">embedding_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train and test sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    train_padded_sentences, </span><br><span class="line">    train_target, </span><br><span class="line">    test_size=<span class="number">0.25</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the model and train!!</span></span><br><span class="line">model = glove_lstm()</span><br><span class="line"></span><br><span class="line">checkpoint = ModelCheckpoint(</span><br><span class="line">    <span class="string">&#x27;model.h5&#x27;</span>, </span><br><span class="line">    monitor = <span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">    verbose = <span class="number">1</span>, </span><br><span class="line">    save_best_only = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 模型 LSTM</span></span><br><span class="line">reduce_lr = ReduceLROnPlateau(</span><br><span class="line">    monitor = <span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">    factor = <span class="number">0.2</span>, </span><br><span class="line">    verbose = <span class="number">1</span>, </span><br><span class="line">    patience = <span class="number">5</span>,                        </span><br><span class="line">    min_lr = <span class="number">0.001</span></span><br><span class="line">)</span><br><span class="line">history = model.fit(</span><br><span class="line">    X_train, </span><br><span class="line">    y_train, </span><br><span class="line">    epochs = <span class="number">7</span>,</span><br><span class="line">    batch_size = <span class="number">32</span>,</span><br><span class="line">    validation_data = (X_test, y_test),</span><br><span class="line">    verbose = <span class="number">1</span>,</span><br><span class="line">    callbacks = [reduce_lr, checkpoint]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plot_learning_curves(history, [[<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>],[<span class="string">&#x27;accuracy&#x27;</span>, <span class="string">&#x27;val_accuracy&#x27;</span>]])</span><br></pre></td></tr></table></figure>
<img data-src="/2024/04/03/artificial-intelligence/nlp_advance_study/na_20.png" class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = model.predict_classes(X_test)</span><br><span class="line">show_metrics(preds, y_test)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">F1-score:  0.7552910052910053</span><br><span class="line">Precision:  0.6709753231492362</span><br><span class="line">Recall:  0.8638426626323752</span><br><span class="line">Acuracy:  0.805672268907563</span><br><span class="line">--------------------------------------------------</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.91      0.77      0.84      1243</span><br><span class="line">           1       0.67      0.86      0.76       661</span><br><span class="line"></span><br><span class="line">    accuracy                           0.81      1904</span><br><span class="line">   macro avg       0.79      0.82      0.80      1904</span><br><span class="line">weighted avg       0.83      0.81      0.81      1904</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/" title="NLP（GloVe &amp; BERT &amp; TF-IDF &amp; LSTM）">https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/02/artificial-intelligence/nlp_transformers_bert_study/" rel="prev" title="NLP（Transformers & BERT）">
                  <i class="fa fa-chevron-left"></i> NLP（Transformers & BERT）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/07/artificial-intelligence/nlp_embeddings_study/" rel="next" title="NLP（Preprocessing when using embeddings）">
                  NLP（Preprocessing when using embeddings） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">709k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">39:25</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/04/03/artificial-intelligence/nlp_advance_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
