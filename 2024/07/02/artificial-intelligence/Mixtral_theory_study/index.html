<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="Mixtral是由Mistral AI公司开发的一种先进的大型语言模型。Mixtral采用混合专家(Mixture of Experts, MoE)架构，总参数量为46.7B，但每次推理只使用约12.9B参数，稀疏混合专家网络架构，每层包含8个专家(前馈神经网络块)，对每个token,路由器选择2个专家处理，32K tokens的上下文窗口，支持英语、法语、意大利语、德语和西班牙语，在代码生成方面">
<meta property="og:type" content="article">
<meta property="og:title" content="Mistral &#x2F; Mixtral：滑动窗口注意力 &amp; 稀疏专家混合 &amp; 滚动缓冲区">
<meta property="og:url" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="Mixtral是由Mistral AI公司开发的一种先进的大型语言模型。Mixtral采用混合专家(Mixture of Experts, MoE)架构，总参数量为46.7B，但每次推理只使用约12.9B参数，稀疏混合专家网络架构，每层包含8个专家(前馈神经网络块)，对每个token,路由器选择2个专家处理，32K tokens的上下文窗口，支持英语、法语、意大利语、德语和西班牙语，在代码生成方面">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_8.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_9.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_10.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_11.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_11.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_13.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_14.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_15.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_16.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_17.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_18.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_19.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_20.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_21.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_22.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_23.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_24.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_25.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_26.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_27.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_28.png">
<meta property="article:published_time" content="2024-07-02T10:00:11.000Z">
<meta property="article:modified_time" content="2024-07-02T10:00:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/","path":"2024/07/02/artificial-intelligence/Mixtral_theory_study/","title":"Mistral / Mixtral：滑动窗口注意力 & 稀疏专家混合 & 滚动缓冲区"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Mistral / Mixtral：滑动窗口注意力 & 稀疏专家混合 & 滚动缓冲区 | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformer-vs-Mistral"><span class="nav-number">1.</span> <span class="nav-text">Transformer vs Mistral</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.</span> <span class="nav-text">滑动窗口注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%B5%81"><span class="nav-number">2.1.</span> <span class="nav-text">信息流</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8B%E4%B8%80%E4%B8%AAtoken%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.2.</span> <span class="nav-text">下一个token预测任务中的自注意力机制</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9B%B8%E5%85%B3%E7%9A%84KV-Cache"><span class="nav-number">3.</span> <span class="nav-text">自注意力相关的KV-Cache</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BB%9A%E5%8A%A8%E7%BC%93%E5%86%B2%E5%8C%BA%E7%BC%93%E5%AD%98"><span class="nav-number">3.1.</span> <span class="nav-text">滚动缓冲区缓存</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%A2%84%E5%A1%AB%E5%85%85%E5%92%8C%E5%88%86%E5%9D%97"><span class="nav-number">3.2.</span> <span class="nav-text">预填充和分块</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88"><span class="nav-number">4.</span> <span class="nav-text">专家混合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%93%E5%AE%B6%E5%89%8D%E9%A6%88%E5%B1%82"><span class="nav-number">4.1.</span> <span class="nav-text">专家前馈层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E5%87%BD%E6%95%B0"><span class="nav-number">4.2.</span> <span class="nav-text">门控函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87"><span class="nav-number">5.</span> <span class="nav-text">模型分片</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%A1%E9%81%93%E5%B9%B6%E8%A1%8C"><span class="nav-number">5.1.</span> <span class="nav-text">管道并行</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E6%8F%90%E7%A4%BA%E8%AF%8D%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86"><span class="nav-number">6.</span> <span class="nav-text">多提示词优化推理</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">238</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Mistral / Mixtral：滑动窗口注意力 & 稀疏专家混合 & 滚动缓冲区 | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Mistral / Mixtral：滑动窗口注意力 & 稀疏专家混合 & 滚动缓冲区
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-07-02 18:00:11" itemprop="dateCreated datePublished" datetime="2024-07-02T18:00:11+08:00">2024-07-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><code>Mixtral</code>是由<code>Mistral AI</code>公司开发的一种先进的大型语言模型。<code>Mixtral</code>采用混合专家(<code>Mixture of Experts, MoE</code>)架构，总参数量为<code>46.7B</code>，但每次推理只使用约<code>12.9B</code>参数，稀疏混合专家网络架构，每层包含<code>8</code>个专家(前馈神经网络块)，对每个<code>token</code>,路由器选择<code>2</code>个专家处理，<code>32K tokens</code>的上下文窗口，支持英语、法语、意大利语、德语和西班牙语，在代码生成方面表现出色。在多项基准测试中表现优异，超越了许多更大规模的模型，推理速度快，效率高；在多数基准测试中优于<code>Llama 2 70B</code>和<code>GPT-3.5</code>，推理速度是<code>Llama 2 70B</code>的<code>6</code>倍。</p>
<span id="more"></span>

<h4 id="Transformer-vs-Mistral"><a href="#Transformer-vs-Mistral" class="headerlink" title="Transformer vs Mistral"></a>Transformer vs Mistral</h4><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_1.png" class="">

<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_2.png" class="">

<h4 id="滑动窗口注意力机制"><a href="#滑动窗口注意力机制" class="headerlink" title="滑动窗口注意力机制"></a>滑动窗口注意力机制</h4><p>自注意力机制允许模型将单词相互关联。假设我们有以下句子：<code>“The cat is on a chair”</code>。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_3.png" class="">

<p>这里我展示了在应用<code>softmax</code>之前<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-37-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D444" xlink:href="#MJX-37-TEX-I-1D444"></use></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43E" xlink:href="#MJX-36-TEX-I-1D43E"></use></g></g></g></svg></mjx-container>矩阵的乘积。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_4.png" class="">

<p>计算因果掩码后，我们应用<code>softmax</code>，行上的剩余值使得行总和为<code>1</code>。现在，让我们看一下<strong>滑动窗口注意力</strong>。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_5.png" class="">

<p>滑动窗口大小为<code>3</code>。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_6.png" class="">

<p>减少要执行的点积数量，从而降低训练和推理期间的性能。滑动窗口注意力可能会导致模型性能下降，因为无法捕获标记之间的某些“交互”。该模型主要关注局部上下文，这取决于窗口的大小，在大多数情况下已经足够了。滑动窗口注意力仍然可以允许一个<code>token</code>观察窗口外的<code>token</code>，使用类似于卷积神经网络中的接受场的推理。</p>
<h5 id="信息流"><a href="#信息流" class="headerlink" title="信息流"></a>信息流</h5><p>应用<code>softmax</code>后，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 1778 665" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-36-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-36-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="221E" xlink:href="#MJX-36-TEX-N-221E"></use></g></g></g></svg></mjx-container>所有都变为<code>0</code>，而行中的其他值则更改为总和为<code>1</code>。<code>softmax</code>的输出可以看作概率分布。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_7.png" class="">

<p>自注意力机制(<code>Self-Attention</code>)的输出是一个与输入序列形状相同的矩阵，但现在每个<code>token</code>都会根据所应用的掩码捕获有关其他<code>token</code>的信息。在我们的例子中，输出的最后一个<code>token</code>会捕获有关其自身和前两个<code>token</code>的信息。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_8.png" class="">

<p>当滑动窗口大小为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.519ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 2881.6 765" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-36-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-36-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-36-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-36-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(1325.8,0)"><use data-c="3D" xlink:href="#MJX-36-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2381.6,0)"><use data-c="33" xlink:href="#MJX-36-TEX-N-33"></use></g></g></g></svg></mjx-container>时，每一层都会添加有关<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.176ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5382 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-35-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-35-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-35-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-35-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-35-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-35-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-35-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-35-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389,0)"><use data-c="1D44A" xlink:href="#MJX-35-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(1659.2,0)"><use data-c="2212" xlink:href="#MJX-35-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2659.4,0)"><use data-c="31" xlink:href="#MJX-35-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3159.4,0)"><use data-c="29" xlink:href="#MJX-35-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3826.2,0)"><use data-c="3D" xlink:href="#MJX-35-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4882,0)"><use data-c="32" xlink:href="#MJX-35-TEX-N-32"></use></g></g></g></svg></mjx-container>个<code>token</code>的信息。这意味着在<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-35-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-35-TEX-I-1D441"></use></g></g></g></svg></mjx-container>层之后，我们将获得按<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="7.146ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 3158.4 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-33-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-33-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-33-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-33-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(1270.2,0)"><use data-c="D7" xlink:href="#MJX-33-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(2270.4,0)"><use data-c="1D441" xlink:href="#MJX-33-TEX-I-1D441"></use></g></g></g></svg></mjx-container>顺序的信息流。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_9.png" class="">

<p>在推理的每一步，我们只对模型输出的最后一个<code>token</code>感兴趣，因为我们已经有了之前的<code>token</code>。但是，模型需要访问所有之前的<code>token</code>来决定输出哪个<code>token</code>，因为它们构成了它的上下文（或“提示”）。有没有办法让模型在推理过程中对已经见过的<code>token</code>进行更少的计算？有！解决方案是<code>KV</code>缓存！</p>
<h5 id="下一个token预测任务中的自注意力机制"><a href="#下一个token预测任务中的自注意力机制" class="headerlink" title="下一个token预测任务中的自注意力机制"></a>下一个token预测任务中的自注意力机制</h5><table>
<thead>
<tr>
<th align="left"><code>Inference</code></th>
<th align="left"><code>Prediction task</code></th>
</tr>
</thead>
<tbody><tr>
<td align="left">T &#x3D; 1</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_10.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 2</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_11.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 3</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_11.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 4</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_13.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 4</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_14.png" class=""></td>
</tr>
</tbody></table>
<h4 id="自注意力相关的KV-Cache"><a href="#自注意力相关的KV-Cache" class="headerlink" title="自注意力相关的KV-Cache"></a>自注意力相关的KV-Cache</h4><table>
<thead>
<tr>
<th align="left"><code>Inference</code></th>
<th align="left"><code>task</code></th>
</tr>
</thead>
<tbody><tr>
<td align="left">T &#x3D; 1</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_15.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 2</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_16.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 3</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_17.png" class=""></td>
</tr>
<tr>
<td align="left">T &#x3D; 4</td>
<td align="left"><img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_18.png" class=""></td>
</tr>
</tbody></table>
<h5 id="滚动缓冲区缓存"><a href="#滚动缓冲区缓存" class="headerlink" title="滚动缓冲区缓存"></a>滚动缓冲区缓存</h5><p>由于我们使用滑动窗口注意力（大小为<code>W</code>），我们不需要将所有先前的标记保留在<code>KV-Cache</code>中，但我们可以将其限制为最新的<code>W</code>个<code>token</code>。例如：我们的滑动窗口大小为<code>4</code>，我们只需要当前<code>token</code>与前<code>4</code>个<code>token</code>（包括自身）的点积。因为我们希望输出<code>token</code>仅依赖于前<code>4</code>个<code>token</code>。我们跟踪写入指针，在滚动缓冲区缓存中添加最后一个<code>token</code>的位置。让我们添加一个句子<code>“The cat is on a chair”</code>。首先我们添加一个新的<code>token</code>，并将指针向前移动我们来添加句子<code>“The cat is on a chair”</code>。我们想要“展开”缓存，因为我们想要计算传入<code>token</code>的注意力。这很容易！我们只需要使用写入指针来了解如何对项目进行排序：我们首先获取写入指针之后的所有项目，然后获取从第<code>0</code>个索引到写入指针位置的所有项目。</p>
<h5 id="预填充和分块"><a href="#预填充和分块" class="headerlink" title="预填充和分块"></a>预填充和分块</h5><p>使用语言模型生成文本时，我们使用提示词，然后使用前一个标记逐个生成<code>token</code>。处理<code>KV-Cache</code>时，我们首先需要将所有提示词的<code>token</code>添加到<code>KV-Cache</code>，以便我们可以利用它来生成下一个<code>token</code>。由于提示词是预先知道的（我们不需要生成它），我们可以使用提示词的<code>token</code>预填充<code>KV-Cache</code>。但如果提示词很大怎么办？我们可以一次添加一个<code>token</code>，但这可能很耗时，否则我们可以一次添加提示词的所有<code>token</code>，但在这种情况下，注意力矩阵（即<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="6.784ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 2998.4 683" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-33-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-33-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-33-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(1110.2,0)"><use data-c="D7" xlink:href="#MJX-33-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(2110.4,0)"><use data-c="1D441" xlink:href="#MJX-33-TEX-I-1D441"></use></g></g></g></svg></mjx-container>）可能非常大，无法放入内存中。解决方案是使用<strong>预填充和分块</strong>。基本上，我们将提示词分成固定大小的块，设置为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-31-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-31-TEX-I-1D44A"></use></g></g></g></svg></mjx-container>（最后一个除外），其中W是注意力的滑动窗口的大小。假设我们有一个大提示词，滑动窗口大小为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.519ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 2881.6 765" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-31-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-31-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-31-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-31-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(1325.8,0)"><use data-c="3D" xlink:href="#MJX-31-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2381.6,0)"><use data-c="34" xlink:href="#MJX-31-TEX-N-34"></use></g></g></g></svg></mjx-container>。为简单起见，假设每个单词都是一个<code>token</code>。提示词: <code>“Can you tell me who is the richest man in history”</code>。</p>
<p>在每一步中，我们使用<code>KV-Cache</code>的<code>token</code> + 当前块的<code>token</code>作为<code>Key</code>和<code>Value</code>来计算注意力，而仅使用传入块的<code>token</code>作为<code>Query</code>。在预填充的第一步中，<code>KV-Cache</code>最初是空的。计算完注意力后，我们将当前块的<code>token</code>添加到<code>KV-Cache</code>。这与<code>token</code>生成不同，在<code>token</code>生成中，我们首先将之前生成的<code>token</code>添加到<code>KV-Cache</code>，然后计算注意力。</p>
<p>你可能已经注意到，注意力掩码大于<code>KV-Cache</code>的大小。这是故意为之，否则新添加的<code>token</code>将不会与之前在缓存中的项计算点积。此机制仅在提示词的预填充期间使用。我们为什么要这样做？因为 <code>KV-Cache</code>的大小是固定的，但同时我们需要计算所有这些注意力。</p>
<p>只有在预填充期间，才会使用大于<code>KV-Cache</code>的注意力掩码来计算注意力。在第一个块和后续块的预填充期间，我们使用<code>KV-Cache</code>的大小和当前块中的<code>token</code>数量来生成注意力掩码。在预填充期间，注意力掩码是使用<code>KV-Cache</code> + 当前块中的<code>token</code>来计算的，因此注意力掩码可以大于<code>KV-Cache(W)</code>。在生成期间，我们首先将前一个<code>token</code>添加到<code>KV-Cache</code>，然后使用<code>KV-Cache</code>的内容生成注意力掩码。在生成期间，注意力掩码的大小是<code>KV-Cache(W)</code>的大小。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_19.png" class="">

<h4 id="专家混合"><a href="#专家混合" class="headerlink" title="专家混合"></a>专家混合</h4><p><strong>专家混合</strong>是一种集成技术，其中我们有多个“专家”模型，每个模型都针对数据的一个子集进行训练，这样每个模型都专注于它自己的那一部分，然后将专家的输出组合起来（通常是加权和或平均）以产生一个单一的输出。在<code>Mistral 8x7B</code>的情况下，我们谈论的是<strong>稀疏混合专家</strong>(<code>SMoE</code>)，因为每个<code>token</code>只使用<code>8</code>个专家中的<code>2</code>个。门控生成用于选择前<code>k</code>个专家的<code>logit</code>。然后前<code>k</code>个<code>logit</code>通过<code>softmax</code>运行来生成权重。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_20.png" class="">

<h5 id="专家前馈层"><a href="#专家前馈层" class="headerlink" title="专家前馈层"></a>专家前馈层</h5><p>对于<code>Mistral 8x7B</code>，专家是每个编码器层中的前馈层。每个编码器层由一个自注意力机制组成，后面跟着<code>8</code>个<code>FFN</code>专家的混合。门控函数为每个传入的<code>token</code>选择前<code>2</code>个专家。输出与加权和相结合。这里允许增加模型的参数，但不会影响计算时间，因为输入只会通过前<code>2</code>个专家，因此中间矩阵乘法只会在选定的专家上执行。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_21.png" class="">
<h5 id="门控函数"><a href="#门控函数" class="headerlink" title="门控函数"></a>门控函数</h5><p>门控函数只是一个线性层(<code>in_features=4096，out_feature=8，bias=False</code>)，与模型的其余部分一起训练。对于每个<code>token</code>嵌入，它都会生成<code>8</code>个<code>logit</code>，控制要选择哪些专家。如果我们将<code>softmax</code>直接应用于门控函数的输出，这将导致所有专家的“概率分布”（权重总和为<code>1</code>）。但由于我们只使用其中的前<code>k</code>个，因此我们希望只对选定的专家进行“概率分布”。这也使得在不同数量的专家上训练的模型变得更容易，因为应用于输出的权重总和将始终为<code>1</code>，与门控函数选择的专家数量无关。</p>
<h4 id="模型分片"><a href="#模型分片" class="headerlink" title="模型分片"></a>模型分片</h4><p>当我们的模型太大而无法放入单个<code>GPU</code>中时，我们可以将模型划分为“<strong>层组</strong>”，并将每组层放在一个<code>GPU</code>中。当我们进行迭代推理时：每个<code>GPU</code>的输出都会作为下一个<code>GPU</code>的输入，依此类推……<br>这项技术被称为”<strong>模型分片</strong>“。对于<code>Mistral</code>来说，由于这里有<code>32</code>个编码器层，如果我们有<code>4</code>个<code>GPU</code>，那么我们可以在每个<code>GPU</code>中存储其中<code>8</code>个。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_22.png" class="">

<p>像这样的管道虽然运行良好，但效率不高，因为任何时候都只有一个<code>GPU</code>在工作。一种更好的方法（未在<code>Mistral</code>的开源版本中实现）尤其适用于训练，即同时处理多个批次，但在时间尺度上进行移动。这种方法称为<strong>管道并行</strong>。让我们看看它是如何工作的。</p>
<h5 id="管道并行"><a href="#管道并行" class="headerlink" title="管道并行"></a>管道并行</h5><p>想象一下，我们想要在单个批次上训练我们的分片模型：这将需要<code>8</code>个时间步来完成，并且在每个时间步，只有一个<code>GPU</code>在工作，而其他<code>GPU</code>处于空闲等待状态。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_23.png" class="">

<p>我们将批次划分为更小的微批次，并在时间线上移动每个微批次的前进和后退步骤。在这种情况下，每个时间步骤花费的时间更少，因为我们处理的是小的微批次。每个微批次的梯度都会累积（梯度累积），然后我们可以运行优化器来更新权重。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_24.png" class="">

<p>我们仍然有一些时间步长，其中并非所有<code>GPU</code>都在工作（称为“<strong>气泡</strong>”）。为了避免气泡，我们可以增加批次的大小。</p>
<h4 id="多提示词优化推理"><a href="#多提示词优化推理" class="headerlink" title="多提示词优化推理"></a>多提示词优化推理</h4><p>假设您正在经营一家提供<code>LLM</code>推理服务的<code>AI</code>公司：您有许多客户发送的提示词并希望在您的模型上运行推理。每个提示词的长度可能不同。为简单起见，假设每个单词都是一个<code>token</code>。请考虑以下提示词：</p>
<ul>
<li>提示词一： <code>“Write a poem” (3 tokens)</code>。</li>
<li>提示词二： <code>“Write a historical novel” (4 tokens)</code>。</li>
<li>提示词三： <code>“Tell me a funny joke” (5 tokens)</code>。</li>
</ul>
<p>如果我们想充分利用我们的<code>GPU</code>，我们应该将所有提示词放在一个批次中，但由于它们的长度不同，我们必须用<code>[PAD]token</code>填充它们，并且当模型产生输出时，我们应该只查看对应于最后一个非填充<code>token</code>的输出，丢弃任何未来的<code>token</code>。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_25.png" class="">

<p>为了选择要生成的下一个<code>token</code>，我们将检查与最后一个非填充<code>token</code>相对应的嵌入，这些嵌入在上面的输出中突出显示。因为我们的模型是<code>LLM</code>，所以我们将使用因果掩码。此掩码适用于所有序列。我们不能对每个提示使用不同的掩码，因为所有提示的长度都相同，所以每个提示词的掩码必须是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2222.4 688" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-31-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-31-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="35" xlink:href="#MJX-31-TEX-N-35"></use></g><g data-mml-node="mo" transform="translate(722.2,0)"><use data-c="D7" xlink:href="#MJX-31-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1722.4,0)"><use data-c="35" xlink:href="#MJX-31-TEX-N-35"></use></g></g></g></svg></mjx-container>。所有无用的点积都以红色突出显示。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_26.png" class="">

<p>还有<code>KV-Cache</code>的问题：每个提示词可能具有不同的<code>KV-Cache</code>大小（想象一下一个提示词只有<code>10</code>个<code>token</code>，而另一个提示有<code>500</code>个<code>token</code>！）。解决方案是将所有提示词所有的<code>token</code>组合成一个序列，并在计算输出时跟踪每个提示词的长度。你可能会想：我们如何为这样的序列构建注意力掩码？我们可以使用<code>xformers BlockDiagonalCausalMask</code>！</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_27.png" class="">

<p>如果我们使用滑动窗口注意力，掩码可能会有所不同。</p>
<img data-src="/2024/07/02/artificial-intelligence/Mixtral_theory_study/m_28.png" class="">

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/" title="Mistral &#x2F; Mixtral：滑动窗口注意力 &amp; 稀疏专家混合 &amp; 滚动缓冲区">https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/06/29/artificial-intelligence/Mamba_S4_study/" rel="prev" title="序列化建模：Mamba / S4（深度学习）">
                  <i class="fa fa-chevron-left"></i> 序列化建模：Mamba / S4（深度学习）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/07/03/artificial-intelligence/Quantization_theory_study/" rel="next" title="量化(Quantization)（深度学习）">
                  量化(Quantization)（深度学习） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.2m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">65:55</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/07/02/artificial-intelligence/Mixtral_theory_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
