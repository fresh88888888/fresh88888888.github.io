<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="GPT代表生成式预训练Transformer(Generative Pre-trained Transformer)。这是一类基于Transformer的神经网络架构。生成式(Generative)：GPT可以生成文本；预训练(Pre-trained)：GPT基于来自于书本、互联网等来源的海量文本进行训练；Transformer：GPT是一个decoder-only的Transformer神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT模型探析（LLM）(Numpy)">
<meta property="og:url" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="GPT代表生成式预训练Transformer(Generative Pre-trained Transformer)。这是一类基于Transformer的神经网络架构。生成式(Generative)：GPT可以生成文本；预训练(Pre-trained)：GPT基于来自于书本、互联网等来源的海量文本进行训练；Transformer：GPT是一个decoder-only的Transformer神经网络">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_8.png">
<meta property="article:published_time" content="2024-06-04T03:30:11.000Z">
<meta property="article:modified_time" content="2024-06-04T03:30:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/","path":"2024/06/04/artificial-intelligence/GPT_theory_study/","title":"GPT模型探析（LLM）(Numpy)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>GPT模型探析（LLM）(Numpy) | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA"><span class="nav-number">1.</span> <span class="nav-text">输入&#x2F;输出</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5"><span class="nav-number">1.1.</span> <span class="nav-text">输入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-number">1.2.</span> <span class="nav-text">输出</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC"><span class="nav-number">2.</span> <span class="nav-text">生成文本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92"><span class="nav-number">2.1.</span> <span class="nav-text">自回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-number">2.2.</span> <span class="nav-text">采样</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E7%A4%BA%EF%BC%88prompting%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">提示（prompting）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">5.1.</span> <span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">5.2.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0"><span class="nav-number">5.3.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E5%B1%82"><span class="nav-number">5.4.</span> <span class="nav-text">基础层</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#GELU"><span class="nav-number">5.4.1.</span> <span class="nav-text">GELU</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Softmax"><span class="nav-number">5.4.2.</span> <span class="nav-text">Softmax</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.4.3.</span> <span class="nav-text">层归一化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%EF%BC%88%E5%8F%98%E6%8D%A2%EF%BC%89"><span class="nav-number">5.4.4.</span> <span class="nav-text">线性（变换）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT%E6%9E%B6%E6%9E%84"><span class="nav-number">6.</span> <span class="nav-text">GPT架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82"><span class="nav-number">6.1.</span> <span class="nav-text">嵌入层</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Token-%E5%B5%8C%E5%85%A5"><span class="nav-number">6.1.1.</span> <span class="nav-text">Token 嵌入</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%EF%BC%88Positional-Embeddings%EF%BC%89"><span class="nav-number">6.1.2.</span> <span class="nav-text">位置嵌入（Positional Embeddings）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BB%84%E5%90%88"><span class="nav-number">6.1.3.</span> <span class="nav-text">组合</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%B1%82"><span class="nav-number">6.2.</span> <span class="nav-text">解码层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8A%95%E5%BD%B1%E4%B8%BA%E8%AF%8D%E6%B1%87%E8%A1%A8-projection-to-vocab"><span class="nav-number">6.3.</span> <span class="nav-text">投影为词汇表(projection to vocab)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E6%A8%A1%E5%9D%97"><span class="nav-number">6.4.</span> <span class="nav-text">解码器模块</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%80%90%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="nav-number">6.4.1.</span> <span class="nav-text">逐位置前馈网络</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E5%9B%A0%E6%9E%9C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">6.4.2.</span> <span class="nav-text">多头因果自注意力</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="nav-number">7.</span> <span class="nav-text">完整代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU-TPU%E6%94%AF%E6%8C%81"><span class="nav-number">8.</span> <span class="nav-text">GPU&#x2F;TPU支持</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-number">9.</span> <span class="nav-text">优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="nav-number">10.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0"><span class="nav-number">11.</span> <span class="nav-text">评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2%E7%94%9F%E6%88%90"><span class="nav-number">12.</span> <span class="nav-text">停止生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%A0%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90"><span class="nav-number">13.</span> <span class="nav-text">无条件生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83"><span class="nav-number">14.</span> <span class="nav-text">微调</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E5%BE%AE%E8%B0%83"><span class="nav-number">14.1.</span> <span class="nav-text">分类微调</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E5%BE%AE%E8%B0%83"><span class="nav-number">14.2.</span> <span class="nav-text">生成式微调</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83"><span class="nav-number">14.3.</span> <span class="nav-text">指令微调</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%EF%BC%88Parameter-Efficient-Fine-tuning%EF%BC%89"><span class="nav-number">14.4.</span> <span class="nav-text">参数高效微调（Parameter Efficient Fine-tuning）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">236</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="GPT模型探析（LLM）(Numpy) | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GPT模型探析（LLM）(Numpy)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-06-04 11:30:11" itemprop="dateCreated datePublished" datetime="2024-06-04T11:30:11+08:00">2024-06-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>35k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>29 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><code>GPT</code>代表生成式预训练<code>Transformer</code>(<code>Generative Pre-trained Transformer</code>)。这是一类基于<code>Transformer</code>的神经网络架构。<strong>生成式</strong>(<code>Generative</code>)：<code>GPT</code>可以生成文本；<strong>预训练</strong>(<code>Pre-trained</code>)：<code>GPT</code>基于来自于书本、互联网等来源的海量文本进行训练；<code>Transformer</code>：<code>GPT</code>是一个<code>decoder-only</code>的<code>Transformer</code>神经网络结构。</p>
<span id="more"></span>
<h4 id="输入-输出"><a href="#输入-输出" class="headerlink" title="输入&#x2F;输出"></a>输入&#x2F;输出</h4><h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><p>输入是一些文本字符串，使用<code>tokenizer</code>（分词器）将这些文本字符串拆解为”小片段“，我们将这些片段称之为<code>token</code>。最后我们使用词汇表(<code>vocabulary</code>)将<code>token</code>映射为整数。在实际使用中，我们不仅仅使用简单的通过空格分隔去做分词，我们还会使用一些更高级的方法，比如<code>Byte-Pair Encoding</code>或者<code>WordPiece</code>，但它们的原理是一样的：</p>
<ul>
<li>有一个<code>vocab</code>即词汇表，可以将字符串<code>token</code>映射为整数索引。</li>
<li>有一个<code>encode</code>方法，即编码方法，可以实现<code>str</code> -&gt; <code>list[int]</code>的转化。</li>
<li>有一个<code>decode</code>方法，即解码方法，可以实现<code>list[int]</code> -&gt; <code>str</code>的转化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the index of a token in the vocab represents the integer id for that token</span></span><br><span class="line"><span class="comment"># the integer id for &quot;heroes&quot; would be 2, since vocab[2] = &quot;heroes&quot;</span></span><br><span class="line">vocab = [<span class="string">&quot;all&quot;</span>, <span class="string">&quot;not&quot;</span>, <span class="string">&quot;heroes&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;wear&quot;</span>, <span class="string">&quot;.&quot;</span>, <span class="string">&quot;capes&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># a pretend tokenizer that tokenizes on whitespace</span></span><br><span class="line">tokenizer = WhitespaceTokenizer(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the encode() method converts a str -&gt; list[int]</span></span><br><span class="line">ids = tokenizer.encode(<span class="string">&quot;not all heroes wear&quot;</span>) <span class="comment"># ids = [1, 0, 2, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># we can see what the actual tokens are via our vocab mapping</span></span><br><span class="line">tokens = [tokenizer.vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids] <span class="comment"># tokens = [&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the decode() method converts back a list[int] -&gt; str</span></span><br><span class="line">text = tokenizer.decode(ids) <span class="comment"># text = &quot;not all heroes wear&quot;</span></span><br></pre></td></tr></table></figure>
<h5 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h5><p>输出是一个二维数组，其中<code>output[i][j]</code>表示模型的预测概率，这个概率代表了词汇表中位于<code>vocab[j]</code>的<code>token</code>是下一个<code>tokeninputs[i+1]</code>的概率。为了针对整个序列获得下一个<code>token</code>预测，我们可以简单的选择<code>output[-1]</code>中概率最大的那个token, 比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vocab = [<span class="string">&quot;all&quot;</span>, <span class="string">&quot;not&quot;</span>, <span class="string">&quot;heroes&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;wear&quot;</span>, <span class="string">&quot;.&quot;</span>, <span class="string">&quot;capes&quot;</span>]</span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>] <span class="comment"># &quot;not&quot; &quot;all&quot; &quot;heroes&quot; &quot;wear&quot;</span></span><br><span class="line">output = gpt(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span></span><br><span class="line"><span class="comment"># output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]</span></span><br><span class="line"><span class="comment"># 在&quot;not&quot;给出的情况下，我们可以看到，(对于下一个token)模型预测&quot;all&quot;具有最高的概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span></span><br><span class="line"><span class="comment"># output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]</span></span><br><span class="line"><span class="comment"># 在序列[&quot;not&quot;, &quot;all&quot;]给出的情况下，(对于下一个token)模型预测&quot;heroes&quot;具有最高的概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span></span><br><span class="line"><span class="comment"># output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]</span></span><br><span class="line"><span class="comment"># 在整个序列[&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;]给出的情况下，(对于下一个token)模型预测&quot;capes&quot;具有最高的概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># next_token_id = 6</span></span><br><span class="line">next_token_id = np.argmax(output[-<span class="number">1</span>]) </span><br><span class="line"><span class="comment"># next_token = &quot;capes&quot;</span></span><br><span class="line">next_token = vocab[next_token_id] </span><br></pre></td></tr></table></figure>
<p>将具有最高概率的<code>token</code>作为我们的预测，称为<code>greedy decoding</code>。在一个序列中预测下一个逻辑词(<code>logical word</code>)的任务被称之为<strong>语言建模</strong>。因此我们可以称<code>GPT</code>为语言模型。</p>
<h4 id="生成文本"><a href="#生成文本" class="headerlink" title="生成文本"></a>生成文本</h4><h5 id="自回归"><a href="#自回归" class="headerlink" title="自回归"></a>自回归</h5><p>我们可以迭代地通过模型获取下一个<code>token</code>的预测，从而生成整个句子。这个过程是在预测未来的值（回归），并且将预测的值添加回输入中去（<code>auto</code>），这就是为什么你会看到GPT被描述为<strong>自回归模型</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">inputs, n_tokens_to_generate</span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_tokens_to_generate): <span class="comment"># 自回归的解码循环</span></span><br><span class="line">        output = gpt(inputs) <span class="comment"># 模型前向传递</span></span><br><span class="line">        next_id = np.argmax(output[-<span class="number">1</span>]) <span class="comment"># 贪心采样</span></span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id)) <span class="comment"># 将预测添加回输入</span></span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># 只返回生成的ids</span></span><br><span class="line"></span><br><span class="line">input_ids = [<span class="number">1</span>, <span class="number">0</span>] <span class="comment"># &quot;not&quot; &quot;all&quot;</span></span><br><span class="line">output_ids = generate(input_ids, <span class="number">3</span>) <span class="comment"># output_ids = [2, 4, 6]</span></span><br><span class="line">output_tokens = [vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> output_ids] <span class="comment"># &quot;heroes&quot; &quot;wear&quot; &quot;capes&quot;</span></span><br></pre></td></tr></table></figure>
<h5 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h5><p>我们可以通过对概率分布进行采样来替代贪心采样，从而为我们的生成引入一些随机性（<code>stochasticity</code>）。这样子，我们就可以基于同一个输入产生不同的输出句子啦。当我们结合更多的比如<code>top-k</code>，<code>top-p</code>和<code>temperature</code>这样的技巧的时候（这些技巧能够能更改采样的分布），我们输出的质量也会有很大的提高。这些技巧也引入了一些超参数，通过调整这些超参，我们可以获得不同的生成表现(<code>behaviors</code>)。比如提高<code>temperature</code>超参，我们的模型就会更加冒进，从而变得更有“创造力”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机采样</span></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>] <span class="comment"># &quot;not&quot; &quot;all&quot; &quot;heroes&quot; &quot;wear&quot;</span></span><br><span class="line">output = gpt(inputs)</span><br><span class="line">np.random.choice(np.arange(vocab_size), p=output[-<span class="number">1</span>]) <span class="comment"># capes</span></span><br><span class="line">np.random.choice(np.arange(vocab_size), p=output[-<span class="number">1</span>]) <span class="comment"># hats</span></span><br><span class="line">np.random.choice(np.arange(vocab_size), p=output[-<span class="number">1</span>]) <span class="comment"># capes</span></span><br><span class="line">np.random.choice(np.arange(vocab_size), p=output[-<span class="number">1</span>]) <span class="comment"># capes</span></span><br><span class="line">np.random.choice(np.arange(vocab_size), p=output[-<span class="number">1</span>]) <span class="comment"># pants</span></span><br></pre></td></tr></table></figure>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>与其它神经网络训练一样，针对特定的<strong>损失函数</strong>使用梯度下降训练<code>GPT</code>。对于<code>GPT</code>，我们使用语言建模任务的交叉熵损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lm_loss</span>(<span class="params">inputs: <span class="built_in">list</span>[<span class="built_in">int</span>], params</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="comment"># the labels y are just the input shifted 1 to the left</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># inputs = [not,     all,   heros,   wear,   capes]</span></span><br><span class="line">    <span class="comment">#      x = [not,     all,   heroes,  wear]</span></span><br><span class="line">    <span class="comment">#      y = [all,  heroes,     wear,  capes]</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># of course, we don&#x27;t have a label for inputs[-1], so we exclude it from x</span></span><br><span class="line">    <span class="comment"># as such, for N inputs, we have N - 1 langauge modeling example pairs</span></span><br><span class="line">    x, y = inputs[:-<span class="number">1</span>], inputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass all the predicted next token probability distributions at each position</span></span><br><span class="line">    output = gpt(x, params)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cross entropy loss we take the average over all N-1 examples</span></span><br><span class="line">    loss = np.mean(-np.log(output[y]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个极度简化的训练设置</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">texts: <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">str</span>]], params</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">        inputs = tokenizer.encode(text)</span><br><span class="line">        <span class="comment"># 计算语言建模损失</span></span><br><span class="line">        loss = lm_loss(inputs, params)</span><br><span class="line">        <span class="comment"># 损失决定了梯度，我们可以通过反向传播计算梯度</span></span><br><span class="line">        gradients = compute_gradients_via_backpropagation(loss, params)</span><br><span class="line">        <span class="comment"># 梯度来更新模型参数</span></span><br><span class="line">        params = gradient_descent_update_step(gradients, params)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<div class="note warning"><p><strong>请注意</strong>，我们在这里并未使用明确的标注数据。取而代之的是，我们可以通过原始文本自身，产生大量的输入&#x2F;标签对(<code>input/label pairs</code>)。这就是所谓的<strong>自监督学习</strong>。</p>
</div>
<p><strong>自监督学习的范式</strong>，让我们能够海量扩充训练数据。我们只需要尽可能多的搞到大量的文本数据，然后将其丢入模型即可。比如，<code>GPT-3</code>就是基于来自互联网和书籍的<code>3000</code>亿<code>token</code>进行训练的：这里你就需要一个足够大的模型有能力去从这么大量的数据中学到内容，这就是为什么<code>GPT-3</code>模型拥有<code>1750</code>亿的参数，并且大概消耗了<code>100</code>万–<code>1000</code>万美元的计算费用进行训练。</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_1.png" class="">

<p><strong>自监督训练</strong>的步骤称之为<strong>预训练</strong>，而我们可以重复使用预训练模型权重来训练下游任务上的特定模型，比如对文本进行分类（分类某条推文是有害的还是无害的）。<strong>预训练模型</strong>有时也被称为<strong>基础模型</strong>(<code>foundation models</code>)。在下游任务上训练模型被称之为<strong>微调</strong>，由于模型权重已经预训练好了，已经能够理解语言了，那么我们需要做的就是针对特定的任务去微调这些权重。这个所谓“<strong>在通用任务上预训练 + 特定任务上微调</strong>”的策略就称之为<strong>迁移学习</strong>。</p>
<h4 id="提示（prompting）"><a href="#提示（prompting）" class="headerlink" title="提示（prompting）"></a>提示（prompting）</h4><p>本质上看，原始的<code>GPT</code>论文只是提供了用来迁移学习的<code>Transformer</code>模型的预训练。文章显示，一个<code>117M</code>的<code>GPT</code>预训练模型，在针对下游任务的标注数据上微调之后，它能够在各种<code>NLP</code>(<code>natural language processing</code>)任务上达到最优性能。一个<code>GPT</code>模型只要在足够多的数据上训练，只要模型拥有足够多的参数，那么不需要微调，模型本身就有能力执行各种任务。只要你对模型进行提示，运行自回归语言模型就会得到合适的响应。这就是所谓的<strong>上下文学习</strong>(<code>in-context learning</code>)，也就是说模型仅仅根据提示的内容，就能够执行各种任务。<strong>上下文学习</strong>可以是<code>zero shot</code>,<code>one shot</code>,或者<code>few shot</code>（<code>zero shot</code>表示我们直接拿着大模型就能用于我们的任务了；<code>one shot</code>表示我们需要提供给大模型关于我们特定任务的一个列子；<code>few shot</code>表示我们需要提供给大模型关于我们特定任务的几个例子；）。</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_2.png" class="">

<p>基于提示内容生成文本也被称之为条件生成，因为我们的模型是基于特定的输入（条件）进行生成的。当然，<code>GPT</code>也不仅限于自然语言处理任务(<code>NLP</code>)。你可以将模型用于任何你想要的条件下。比如你可以将<code>GPT</code>变成一个聊天机器人(即：<code>ChatGPT</code>)，这里的条件就是你的对话历史。你也可以进一步条件化你的聊天机器人，通过提示词进行某种描述，限定其表现为某种行为（比如你可以提示：“你是个聊天机器人，请礼貌一点，请讲完整的句子，不要说有害的东西，等等”）。像这样条件化你的模型，你完全可以得到一个定制化私人助理机器人。但是这样的方式不一定很健壮，你仍然可以对你的模型进行越狱，然后让它表现失常。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>(<span class="params">inputs, wte, wpe, blocks, ln_f, n_head</span>):  <span class="comment"># [n_seq] -&gt; [n_seq, n_vocab]</span></span><br><span class="line">    <span class="comment"># token + positional embeddings</span></span><br><span class="line">    x = wte[inputs] + wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]  <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass through n_layer transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        x = transformer_block(x, **block, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># projection to vocab</span></span><br><span class="line">    x = layer_norm(x, **ln_f)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">return</span> x @ wte.T  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">inputs, params, n_head, n_tokens_to_generate</span>):</span><br><span class="line">    <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_tokens_to_generate), <span class="string">&quot;generating&quot;</span>):  <span class="comment"># auto-regressive decode loop</span></span><br><span class="line">        logits = gpt2(inputs, **params, n_head=n_head)  <span class="comment"># model forward pass</span></span><br><span class="line">        next_id = np.argmax(logits[-<span class="number">1</span>])  <span class="comment"># greedy sampling</span></span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id))  <span class="comment"># append prediction to input</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># only return generated ids</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">prompt: <span class="built_in">str</span>, n_tokens_to_generate: <span class="built_in">int</span> = <span class="number">40</span>, model_size: <span class="built_in">str</span> = <span class="string">&quot;124M&quot;</span>, models_dir: <span class="built_in">str</span> = <span class="string">&quot;models&quot;</span></span>):</span><br><span class="line">    <span class="keyword">from</span> utils <span class="keyword">import</span> load_encoder_hparams_and_params</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load encoder, hparams, and params from the released open-ai gpt-2 files</span></span><br><span class="line">    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># encode the input string using the BPE tokenizer</span></span><br><span class="line">    input_ids = encoder.encode(prompt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make sure we are not surpassing the max sequence length of our model</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(input_ids) + n_tokens_to_generate &lt; hparams[<span class="string">&quot;n_ctx&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate output ids</span></span><br><span class="line">    output_ids = generate(input_ids, params, hparams[<span class="string">&quot;n_head&quot;</span>], n_tokens_to_generate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decode the ids back into a string</span></span><br><span class="line">    output_text = encoder.decode(output_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line">    fire.Fire(main)</span><br></pre></td></tr></table></figure>
<p>我们将以上代码拆解为四部分：</p>
<ul>
<li><code>gpt2</code>函数是我们将要实现的<code>GPT</code>代码。你会注意到函数参数中除了<code>inputs</code>，还有其它的参数：<ul>
<li><code>wte, wpe, blocks, ln_f</code>这些都是模型的参数。</li>
<li><code>n_head</code>是前向计算过程中需要的超参。</li>
</ul>
</li>
<li><code>generate</code>函数是我们之前看到的自回归解码算法。为了简洁，我们使用贪心采样算法。<code>tqdm</code>是一个进度条库，它可以帮助我们随着每次生成一个<code>token</code>，可视化地观察解码过程。</li>
<li><code>main</code>函数主要处理：<ul>
<li>加载分词器(<code>encoder</code>)，模型权重（<code>params</code>），超参（<code>hparams</code>）。</li>
<li>使用分词器将输入提示词编码为<code>token ID</code>。</li>
<li>调用生成函数。</li>
<li>将输出<code>ID</code>解码为字符串。</li>
</ul>
</li>
<li><code>fire.Fire(main)</code>将我们的源文件转成一个命令行应用，然后就可以像这样运行我们的代码了：<code>python gpt2.py &quot;some prompt here&quot;</code></li>
</ul>
<h5 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h5><p>我们的<code>encoder</code>使用的是<code>GPT-2</code>中使用的<code>BPE</code>分词器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ids = encoder.encode(<span class="string">&quot;Not all heroes wear capes.&quot;</span>)</span><br><span class="line">ids</span><br><span class="line"></span><br><span class="line"><span class="comment"># [3673, 477, 10281, 5806, 1451, 274, 13]</span></span><br><span class="line"></span><br><span class="line">encoder.decode(ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;Not all heroes wear capes.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用分词器的词汇表(存储于encoder.decoder)，我们可以看看实际的token到底长啥样：</span></span><br><span class="line">[encoder.decoder[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids]</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#x27;Not&#x27;, &#x27;Ġall&#x27;, &#x27;Ġheroes&#x27;, &#x27;Ġwear&#x27;, &#x27;Ġcap&#x27;, &#x27;es&#x27;, &#x27;.&#x27;]</span></span><br></pre></td></tr></table></figure>
<div class="note warning"><p><strong>注意</strong>，有的时候我们的<code>token</code>是单词（比如：<code>Not</code>），有的时候虽然也是单词，但是可能会有一个空格在它前面（比如<code>Ġall, Ġ</code>代表一个空格），有时候是一个单词的一部分（比如：<code>capes</code>被分隔为<code>Ġcap</code>和<code>es</code>），还有可能它就是标点符号（比如：<code>.</code>）。</p>
</div>
<p><code>BPE</code>的一个好处是它可以编码任意字符串。如果遇到了某些没有在词汇表里显示的字符串，那么<code>BPE</code>就会将其分割为它能够理解的子串：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[encoder.decoder[i] <span class="keyword">for</span> i <span class="keyword">in</span> encoder.encode(<span class="string">&quot;zjqfl&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#x27;z&#x27;, &#x27;j&#x27;, &#x27;q&#x27;, &#x27;fl&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们还可以检查一下词汇表的大小</span></span><br><span class="line"><span class="built_in">len</span>(encoder.decoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 50257</span></span><br></pre></td></tr></table></figure>
<p>词汇表以及决定字符串如何分解的<strong>字节对组合</strong>(<code>byte-pair merges</code>)，是通过训练分词器获得的。当我们加载分词器，就会从一些文件加载已经训练好的词汇表和字节对组合，这些文件在我们运行<code>load_encoder_hparams_and_params</code>的时候，随着模型文件被一起下载了。你可以查看<code>models/124M/encoder.json</code>(<strong>词汇表</strong>)和<code>models/124M/vocab.bpe</code>(<strong>字节对组合</strong>)。</p>
<h5 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h5><p><code>hparams</code>是一个字典，这个字典包含着我们模型的超参：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hparams</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;n_vocab&quot;: 50257, # number of tokens in our vocabulary</span></span><br><span class="line"><span class="comment">#   &quot;n_ctx&quot;: 1024, # maximum possible sequence length of the input</span></span><br><span class="line"><span class="comment">#   &quot;n_embd&quot;: 768, # embedding dimension (determines the &quot;width&quot; of the network)</span></span><br><span class="line"><span class="comment">#   &quot;n_head&quot;: 12, # number of attention heads (n_embd must be divisible by n_head)</span></span><br><span class="line"><span class="comment">#   &quot;n_layer&quot;: 12 # number of layers (determines the &quot;depth&quot; of the network)</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<p>我们将在代码的注释中使用这些符号来表示各种的大小维度等。我们还会使用<code>n_seq</code>来表示输入序列的长度(即：<code>n_seq = len(inputs)</code>)。</p>
<h5 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h5><p><code>params</code>是一个嵌套的<code>json</code>字典，该字典具有模型训练好的权重。<code>json</code>的叶子节点是<code>NumPy</code>数组。如果我们打印<code>params</code>，用他们的形状去表示数组，我们可以得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(shape_tree(params))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#     &quot;wpe&quot;: [1024, 768],</span></span><br><span class="line"><span class="comment">#     &quot;wte&quot;: [50257, 768],</span></span><br><span class="line"><span class="comment">#     &quot;ln_f&quot;: &#123;&quot;b&quot;: [768], &quot;g&quot;: [768]&#125;,</span></span><br><span class="line"><span class="comment">#     &quot;blocks&quot;: [</span></span><br><span class="line"><span class="comment">#         &#123;</span></span><br><span class="line"><span class="comment">#             &quot;attn&quot;: &#123;</span></span><br><span class="line"><span class="comment">#                 &quot;c_attn&quot;: &#123;&quot;b&quot;: [2304], &quot;w&quot;: [768, 2304]&#125;,</span></span><br><span class="line"><span class="comment">#                 &quot;c_proj&quot;: &#123;&quot;b&quot;: [768], &quot;w&quot;: [768, 768]&#125;,</span></span><br><span class="line"><span class="comment">#             &#125;,</span></span><br><span class="line"><span class="comment">#             &quot;ln_1&quot;: &#123;&quot;b&quot;: [768], &quot;g&quot;: [768]&#125;,</span></span><br><span class="line"><span class="comment">#             &quot;ln_2&quot;: &#123;&quot;b&quot;: [768], &quot;g&quot;: [768]&#125;,</span></span><br><span class="line"><span class="comment">#             &quot;mlp&quot;: &#123;</span></span><br><span class="line"><span class="comment">#                 &quot;c_fc&quot;: &#123;&quot;b&quot;: [3072], &quot;w&quot;: [768, 3072]&#125;,</span></span><br><span class="line"><span class="comment">#                 &quot;c_proj&quot;: &#123;&quot;b&quot;: [768], &quot;w&quot;: [3072, 768]&#125;,</span></span><br><span class="line"><span class="comment">#             &#125;,</span></span><br><span class="line"><span class="comment">#         &#125;,</span></span><br><span class="line"><span class="comment">#         ... # repeat for n_layers</span></span><br><span class="line"><span class="comment">#     ]</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<p>在实现<code>GPT</code>的过程中，你可能会需要参考这个字典来确认权重的形状。为了一致性，我们将会使代码中的变量名和字典的键值保持对齐。</p>
<h5 id="基础层"><a href="#基础层" class="headerlink" title="基础层"></a>基础层</h5><h6 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h6><p><code>GPT-2</code>的非线性（激活函数）选择是<code>GELU</code>（高斯误差线性单元），这是一种类似<code>ReLU</code>的激活函数：</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_3.png" class="">

<p>它的函数函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gelu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1</span> + np.tanh(np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * x**<span class="number">3</span>)))</span><br></pre></td></tr></table></figure>
<p>和<code>ReLU</code>类似，<code>GELU</code>也对输入进行逐元素操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gelu(np.array([[<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">2</span>, <span class="number">0.5</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[ 0.84119,  1.9546 ],[-0.0454 ,  0.34571]])</span></span><br></pre></td></tr></table></figure>
<h6 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    exp_x = np.exp(x - np.<span class="built_in">max</span>(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.799ex;" xmlns="http://www.w3.org/2000/svg" width="20.809ex" height="5.856ex" role="img" focusable="false" viewBox="0 -1351.5 9197.5 2588.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-17-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-17-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-17-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-17-TEX-N-74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z"></path><path id="MJX-17-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-17-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-17-TEX-N-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path id="MJX-17-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-17-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-17-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-17-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-17-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-17-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-17-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-17-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="73" xlink:href="#MJX-17-TEX-N-73"></use><use data-c="6F" xlink:href="#MJX-17-TEX-N-6F" transform="translate(394,0)"></use><use data-c="66" xlink:href="#MJX-17-TEX-N-66" transform="translate(894,0)"></use><use data-c="74" xlink:href="#MJX-17-TEX-N-74" transform="translate(1200,0)"></use><use data-c="6D" xlink:href="#MJX-17-TEX-N-6D" transform="translate(1589,0)"></use><use data-c="61" xlink:href="#MJX-17-TEX-N-61" transform="translate(2422,0)"></use><use data-c="78" xlink:href="#MJX-17-TEX-N-78" transform="translate(2922,0)"></use></g><g data-mml-node="mo" transform="translate(3450,0)"><use data-c="28" xlink:href="#MJX-17-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(3839,0)"><use data-c="1D465" xlink:href="#MJX-17-TEX-I-1D465"></use></g><g data-mml-node="msub" transform="translate(4411,0)"><g data-mml-node="mo"><use data-c="29" xlink:href="#MJX-17-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-17-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(5404.7,0)"><use data-c="3D" xlink:href="#MJX-17-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(6460.5,0)"><g data-mml-node="msup" transform="translate(776.2,676)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-17-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-17-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-17-TEX-I-1D456"></use></g></g></g></g><g data-mml-node="munder" transform="translate(220,-710)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-17-TEX-SO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1089,-382.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-17-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(445,289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-17-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(499,621.9)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-17-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-307.4)"><use data-c="1D457" xlink:href="#MJX-17-TEX-I-1D457"></use></g></g></g></g></g></g></g></g><rect width="2497" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>
<p>这里我们使用了<code>max(x)</code>技巧来保持数值稳定性。<code>softmax</code>用来将一组实数（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 1778 665" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-16-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-16-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-16-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="221E" xlink:href="#MJX-16-TEX-N-221E"></use></g></g></g></svg></mjx-container>至<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 1000 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-16-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="221E" xlink:href="#MJX-16-TEX-N-221E"></use></g></g></g></svg></mjx-container>之间）转换为概率（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 500 688" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-16-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-16-TEX-N-30"></use></g></g></g></svg></mjx-container>至<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 500 666" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-15-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-15-TEX-N-31"></use></g></g></g></svg></mjx-container>之间，其求和为<code>1</code>）。我们将<code>softmax</code>作用于输入的最末轴上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = softmax(np.array([[<span class="number">2</span>, <span class="number">100</span>], [-<span class="number">5</span>, <span class="number">0</span>]]))</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([[0.00034, 0.99966],[0.26894, 0.73106]])</span></span><br><span class="line"></span><br><span class="line">x.<span class="built_in">sum</span>(axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># array([1., 1.])</span></span><br></pre></td></tr></table></figure>
<h6 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h6><p>层归一化将数值标准化为均值为<code>0</code>，方差为<code>1</code>的值：</p>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.308ex;" xmlns="http://www.w3.org/2000/svg" width="30.631ex" height="5.156ex" role="img" focusable="false" viewBox="0 -1259 13538.9 2279" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-15-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-15-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-15-TEX-N-79" d="M69 -66Q91 -66 104 -80T118 -116Q118 -134 109 -145T91 -160Q84 -163 97 -166Q104 -168 111 -168Q131 -168 148 -159T175 -138T197 -106T213 -75T225 -43L242 0L170 183Q150 233 125 297Q101 358 96 368T80 381Q79 382 78 382Q66 385 34 385H19V431H26L46 430Q65 430 88 429T122 428Q129 428 142 428T171 429T200 430T224 430L233 431H241V385H232Q183 385 185 366L286 112Q286 113 332 227L376 341V350Q376 365 366 373T348 383T334 385H331V431H337H344Q351 431 361 431T382 430T405 429T422 429Q477 429 503 431H508V385H497Q441 380 422 345Q420 343 378 235T289 9T227 -131Q180 -204 113 -204Q69 -204 44 -177T19 -116Q19 -89 35 -78T69 -66Z"></path><path id="MJX-15-TEX-N-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path id="MJX-15-TEX-N-72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path><path id="MJX-15-TEX-N-4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z"></path><path id="MJX-15-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-15-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-15-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-15-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-15-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-15-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-15-TEX-I-1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path><path id="MJX-15-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path id="MJX-15-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-15-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-15-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-15-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-15-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-15-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-15-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="4C" xlink:href="#MJX-15-TEX-N-4C"></use><use data-c="61" xlink:href="#MJX-15-TEX-N-61" transform="translate(625,0)"></use><use data-c="79" xlink:href="#MJX-15-TEX-N-79" transform="translate(1125,0)"></use><use data-c="65" xlink:href="#MJX-15-TEX-N-65" transform="translate(1653,0)"></use><use data-c="72" xlink:href="#MJX-15-TEX-N-72" transform="translate(2097,0)"></use><use data-c="4E" xlink:href="#MJX-15-TEX-N-4E" transform="translate(2489,0)"></use><use data-c="6F" xlink:href="#MJX-15-TEX-N-6F" transform="translate(3239,0)"></use><use data-c="72" xlink:href="#MJX-15-TEX-N-72" transform="translate(3739,0)"></use><use data-c="6D" xlink:href="#MJX-15-TEX-N-6D" transform="translate(4131,0)"></use></g><g data-mml-node="mo" transform="translate(4964,0)"><use data-c="28" xlink:href="#MJX-15-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(5353,0)"><use data-c="1D465" xlink:href="#MJX-15-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(5925,0)"><use data-c="29" xlink:href="#MJX-15-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(6591.8,0)"><use data-c="3D" xlink:href="#MJX-15-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(7647.6,0)"><use data-c="1D6FE" xlink:href="#MJX-15-TEX-I-1D6FE"></use></g><g data-mml-node="mo" transform="translate(8412.8,0)"><use data-c="22C5" xlink:href="#MJX-15-TEX-N-22C5"></use></g><g data-mml-node="mfrac" transform="translate(8913,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-15-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(794.2,0)"><use data-c="2212" xlink:href="#MJX-15-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(1794.4,0)"><use data-c="1D707" xlink:href="#MJX-15-TEX-I-1D707"></use></g></g><g data-mml-node="msqrt" transform="translate(488.4,-962)"><g transform="translate(853,0)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-15-TEX-I-1D70E"></use></g><g data-mml-node="mn" transform="translate(604,289) scale(0.707)"><use data-c="32" xlink:href="#MJX-15-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(0,142)"><use data-c="221A" xlink:href="#MJX-15-TEX-N-221A"></use></g><rect width="1007.6" height="60" x="853" y="882"></rect></g><rect width="2597.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(11972.7,0)"><use data-c="2B" xlink:href="#MJX-15-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(12972.9,0)"><use data-c="1D6FD" xlink:href="#MJX-15-TEX-I-1D6FD"></use></g></g></g></svg></mjx-container>
<p>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-14-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-14-TEX-I-1D707"></use></g></g></g></svg></mjx-container>是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-14-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-14-TEX-I-1D465"></use></g></g></g></svg></mjx-container>的均值，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.28ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1007.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-12-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-12-TEX-I-1D70E"></use></g><g data-mml-node="mn" transform="translate(604,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-12-TEX-N-32"></use></g></g></g></g></svg></mjx-container>为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-12-TEX-I-1D465"></use></g></g></g></svg></mjx-container>的方差，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6FE" xlink:href="#MJX-12-TEX-I-1D6FE"></use></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6FD" xlink:href="#MJX-12-TEX-I-1D6FD"></use></g></g></g></svg></mjx-container>为可学习的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">x, g, b, eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span>):</span><br><span class="line">    mean = np.mean(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    variance = np.var(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / np.sqrt(variance + eps)  <span class="comment"># normalize x to have mean=0 and var=1 over last axis</span></span><br><span class="line">    <span class="keyword">return</span> g * x + b  <span class="comment"># scale and offset with gamma/beta params</span></span><br></pre></td></tr></table></figure>
<p>层归一化确保每层的输入总是在一个一致的范围里，而这将为训练过程的加速和稳定提供支持。与批归一化类似，归一化之后的输出通过两个可学习参数<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6FE" xlink:href="#MJX-12-TEX-I-1D6FE"></use></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-12-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6FD" xlink:href="#MJX-12-TEX-I-1D6FD"></use></g></g></g></svg></mjx-container>进行缩放和偏移。分母中的小<code>epsilon</code>项用来避免计算中的分母为零错误。</p>
<h6 id="线性（变换）"><a href="#线性（变换）" class="headerlink" title="线性（变换）"></a>线性（变换）</h6><p>这里是标准的矩阵乘法+偏置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, w, b</span>):  <span class="comment"># [m, in], [in, out], [out] -&gt; [m, out]</span></span><br><span class="line">    <span class="keyword">return</span> x @ w + b</span><br></pre></td></tr></table></figure>
<p><strong>线性层</strong>也通常被认为是<strong>投影</strong>操作（因为它们将一个向量空间投影到另一个向量空间）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.normal(size=(<span class="number">64</span>, <span class="number">784</span>)) <span class="comment"># input dim = 784, batch/sequence dim = 64</span></span><br><span class="line">w = np.random.normal(size=(<span class="number">784</span>, <span class="number">10</span>)) <span class="comment"># output dim = 10</span></span><br><span class="line">b = np.random.normal(size=(<span class="number">10</span>,))</span><br><span class="line">x.shape <span class="comment"># shape before linear projection</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (64, 784)</span></span><br><span class="line"></span><br><span class="line">linear(x, w, b).shape <span class="comment"># shape after linear projection</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (64, 10)</span></span><br></pre></td></tr></table></figure>

<h4 id="GPT架构"><a href="#GPT架构" class="headerlink" title="GPT架构"></a>GPT架构</h4><img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_4.png" class="">

<p>但它仅仅使用了解码器层（上图中的右边部分）：</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_5.png" class="" title="GPT架构">

<p>从宏观的角度来看，<code>GPT</code>架构有三个部分组成：</p>
<ul>
<li>文本 <code>+</code> 位置嵌入(<code>positional embeddings</code>)。</li>
<li>基于<code>Transformer</code>的解码器层(<code>decoder stack</code>)。</li>
<li>投影为词汇表(<code>projection to vocab</code>)的步骤。</li>
</ul>
<p>代码层面的话，就像这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>(<span class="params">inputs, wte, wpe, blocks, ln_f, n_head</span>):  <span class="comment"># [n_seq] -&gt; [n_seq, n_vocab]</span></span><br><span class="line">    <span class="comment"># token + positional embeddings</span></span><br><span class="line">    x = wte[inputs] + wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]  <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass through n_layer transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        x = transformer_block(x, **block, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># projection to vocab</span></span><br><span class="line">    x = layer_norm(x, **ln_f)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">return</span> x @ wte.T  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span></span><br></pre></td></tr></table></figure>
<h5 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h5><h6 id="Token-嵌入"><a href="#Token-嵌入" class="headerlink" title="Token 嵌入"></a>Token 嵌入</h6><p>对于神经网络而言，<code>token ID</code>本身并不是一个好的表示。第一，<code>token ID</code>的相对大小会传递错误的信息（比如，在我们的词汇表中，如果<code>Apple = 5，Table=10</code>，那就意味着<code>2 * Table = Apple</code>？显然不对）。其二，单个的数也没有足够的维度喂给神经网络，也就是说单个的数字包含的特征信息不够丰富。为了解决这些限制，我们将利用词向量，即通过一个学习到的嵌入矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wte[inputs] <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br></pre></td></tr></table></figure>
<p><code>wte</code>是一个<code>[n_vocab, n_emdb]</code>的矩阵。这就像一个查找表，矩阵中的第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>行对应我们的词汇表中的第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>个<code>token</code>的向量表示（学出来的）。<code>wte[inputs]</code>使用了<code>integer array indexing</code>来检索我们输入中每个<code>token</code>所对应的向量。就像神经网络中的其他参数，<code>wte</code>是可学习的。也就是说，在训练开始的时候它是随机初始化的，然后随着训练的进行，通过梯度下降不断更新。</p>
<h6 id="位置嵌入（Positional-Embeddings）"><a href="#位置嵌入（Positional-Embeddings）" class="headerlink" title="位置嵌入（Positional Embeddings）"></a>位置嵌入（Positional Embeddings）</h6><p>单纯的<code>Transformer</code>架构的一个古怪地方在于它并不考虑位置。当我们随机打乱输入位置顺序的时候，输出可以保持不变（输入的顺序对输出并未产生影响）。可是词的顺序当然是语言中重要的部分啊，因此我们需要使用某些方式将位置信息编码进我们的输入。为了这个目标，我们可以使用另一个学习到的嵌入矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))] <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br></pre></td></tr></table></figure>
<p><code>wpe</code>是一个<code>[n_ctx, n_emdb]</code>矩阵。矩阵的第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>行包含一个编码输入中第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>个位置信息的向量。与<code>wte</code>类似，这个矩阵也是通过梯度下降来学习到的。需要注意的是，这将限制模型的最大序列长度为<code>n_ctx</code>。也就是说必须满足<code>len(inputs) &lt;= n_ctx</code>。</p>
<h6 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h6><p>现在我们可以将<code>token</code>嵌入与位置嵌入联合为一个组合嵌入，这个嵌入将<code>token</code>信息和位置信息都编码进来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># token + positional embeddings</span></span><br><span class="line">x = wte[inputs] + wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]  <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x[i] represents the word embedding for the ith word + the positional embedding for the ith position</span></span><br></pre></td></tr></table></figure>
<h5 id="解码层"><a href="#解码层" class="headerlink" title="解码层"></a>解码层</h5><p>我们将刚才的嵌入通过一连串的<code>n_layertransformer</code>解码器模块。一方面，堆叠更多的层让我们可以控制到底我们的网络有多“深”。以<code>GPT-3</code>为例，其高达<code>96</code>层。另一方面，选择一个更大的<code>n_embd</code>值，让我们可以控制网络有多“宽”（还是以<code>GPT-3</code>为例，它使用的嵌入大小为<code>12288</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward pass through n_layer transformer blocks</span></span><br><span class="line"><span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">    x = transformer_block(x, **block, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br></pre></td></tr></table></figure>
<h5 id="投影为词汇表-projection-to-vocab"><a href="#投影为词汇表-projection-to-vocab" class="headerlink" title="投影为词汇表(projection to vocab)"></a>投影为词汇表(projection to vocab)</h5><p>在最后的步骤中，我们将<code>Transformer</code>最后一个结构块的输入投影为字符表的一个概率分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># projection to vocab</span></span><br><span class="line">x = layer_norm(x, **ln_f)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"><span class="keyword">return</span> x @ wte.T  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在进行投影操作之前，我们先将x通过<strong>最后的层归一化层</strong>。这是<code>GPT-2</code>架构所特有的（并没有出现在<code>GPT</code>原始论文和<code>Transformer</code>论文中）。</li>
<li>我们复用了嵌入矩阵<code>wte</code>进行投影操作。其它的<code>GPT</code>实现当然可以选择使用另外学习到的权重矩阵进行投影，但是权重矩阵共享具有以下一些优势：<ul>
<li>你可以节省一些参数。</li>
<li>考虑到这个矩阵作用于转换到词与来自于词的两种转换，理论上，相对于分别使用两个矩阵来做这件事，使用同一个矩阵将学到更为丰富的表征。</li>
</ul>
</li>
<li>在最后，我们并未使用<code>softmax</code>，因此我们的输出是<code>logits</code>而不是<code>0-1</code>之间的概率。这样做的理由是：<ul>
<li><code>softmax</code>是单调的，因此对于贪心采样而言，<code>np.argmax(logits)</code>和<code>np.argmax(softmax(logits))</code>是等价的，因此使用<code>softmax</code>就变得多此一举。</li>
<li><code>softmax</code>是不可逆的，这意味着我们总是可以通过<code>softmax</code>将<code>logits</code>变为<code>probabilities</code>，但不能从<code>probabilities</code>变为<code>softmax</code>，为了让灵活性最大，我们选择直接输出<code>logits</code>。</li>
<li>数值稳定性的考量。比如计算交叉熵损失的时候，相对于<code>log_softmax(logits)，log(softmax(logits))</code>的数值稳定性就差。</li>
</ul>
</li>
</ul>
<p>投影为词汇表的过程有时候也被称之为<strong>语言建模头</strong>(<code>language modeling head</code>)。这里的“头”是什么意思呢？你的<code>GPT</code>一旦被预训练完毕，那么你可以通过更换其他投影操作的语言建模头，比如你可以将其更换为<strong>分类头</strong>，从而在一些分类任务上微调你的模型（让其完成分类任务）。因此你的模型可以拥有多种头。</p>
<h5 id="解码器模块"><a href="#解码器模块" class="headerlink" title="解码器模块"></a>解码器模块</h5><p><code>Transformer</code>解码器模块由两个子层组成：</p>
<ul>
<li>多头因果自注意力(<code>Multi-head causal self attention</code>)</li>
<li>逐位置前馈神经网络(<code>Position-wise feed forward neural network</code>)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_block</span>(<span class="params">x, mlp, attn, ln_1, ln_2, n_head</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># multi-head causal self attention</span></span><br><span class="line">    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-wise feed forward network</span></span><br><span class="line">    x = x + ffn(layer_norm(x, **ln_2), **mlp)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>每个子层都在输入上使用了层归一化，也使用了残差连接（即将子层的输入直接连接到子层的输出）。</p>
<ul>
<li><strong>多头因果自注意力机制</strong>便于输入之间的通信。在网络的其它地方，模型是不允许输入相互“看到”彼此的。嵌入层、逐位置前馈网络、层归一化以及投影到词汇表的操作，都是逐位置对我们的输入进行的。建模输入之间的关系完全由注意力机制来处理。</li>
<li><strong>逐位置前馈神经网络</strong>只是一个常规的两层全连接神经网络。它只是为我们的模型增加一些可学习的参数，以促进学习过程。</li>
<li>在原始的<code>Transformer</code>论文中，层归一化被放置在输出层<code>layer_norm(x + sublayer(x))</code>上，而我们在这里为了匹配<code>GPT-2</code>，将层归一化放置在输入<code>x + sublayer(layer_norm(x))</code>上。这被称为<strong>预归一化</strong>，并且已被证明在改善<code>Transformer</code>的性能方面尤为重要。</li>
<li><strong>残差连接</strong>这这里有几个不同的目的：1.使得深度神经网络（即层数非常多的神经网络）更容易进行优化。其思想是为梯度提供“捷径”，使得梯度更容易地回传到网络的初始的层，从而更容易进行优化；2.如果没有残差连接的话，加深模型层数会导致性能下降（可能是因为梯度很难在没有损失信息的情况下回传到整个深层网络中）。残差连接似乎可以为更深层的网络提供一些精度提升；3.可以帮助解决梯度消失&#x2F;爆炸的问题。</li>
</ul>
<h6 id="逐位置前馈网络"><a href="#逐位置前馈网络" class="headerlink" title="逐位置前馈网络"></a>逐位置前馈网络</h6><p><strong>逐位置前馈网络</strong>(<code>Position-wise Feed Forward Network</code>)是一个简单的两层的多层感知器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ffn</span>(<span class="params">x, c_fc, c_proj</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># project up</span></span><br><span class="line">    a = gelu(linear(x, **c_fc))  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 4*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># project back down</span></span><br><span class="line">    x = linear(a, **c_proj)  <span class="comment"># [n_seq, 4*n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这里没有什么特别的技巧，我们只是将<code>n_embd</code>投影到一个更高的维度<code>4*n_embd</code>，然后再将其投影回<code>n_embd</code>。回忆一下我们的<code>params</code>字典，我们的<code>mlp</code>参数如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;mlp&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;c_fc&quot;</span>:   &#123;<span class="string">&quot;b&quot;</span>: [4*n_embd], <span class="string">&quot;w&quot;</span>: [n_embd, 4*n_embd]&#125;,</span><br><span class="line">    <span class="string">&quot;c_proj&quot;</span>: &#123;<span class="string">&quot;b&quot;</span>: [n_embd], <span class="string">&quot;w&quot;</span>: [4*n_embd, n_embd]&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="多头因果自注意力"><a href="#多头因果自注意力" class="headerlink" title="多头因果自注意力"></a>多头因果自注意力</h6><p>这一层可能是理解<code>Transformer</code>最困难的部分。因此我们通过分别解释“<strong>多头因果自注意力</strong>”的每个词，一步步理解“<strong>多头因果自注意力</strong>”：</p>
<ul>
<li>注意力（<code>Attention</code>）。</li>
<li>自身(<code>Self</code>)。</li>
<li>因果(<code>Causal</code>)。</li>
<li>多头(<code>Multi-Head</code>)。</li>
</ul>
<p><strong>注意力</strong><br>我从头开始推导了原始<code>Transformer</code>论文中提出的缩放点积方程：</p>
<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.308ex;" xmlns="http://www.w3.org/2000/svg" width="38.916ex" height="5.741ex" role="img" focusable="false" viewBox="0 -1517.7 17200.7 2537.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-11-TEX-N-74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z"></path><path id="MJX-11-TEX-N-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path id="MJX-11-TEX-N-6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-11-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-11-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-11-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-11-TEX-I-1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path><path id="MJX-11-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-11-TEX-I-1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path><path id="MJX-11-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-11-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-11-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-11-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-11-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-11-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-11-TEX-N-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path id="MJX-11-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-11-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path><path id="MJX-11-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-11-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="61" xlink:href="#MJX-11-TEX-N-61"></use><use data-c="74" xlink:href="#MJX-11-TEX-N-74" transform="translate(500,0)"></use><use data-c="74" xlink:href="#MJX-11-TEX-N-74" transform="translate(889,0)"></use><use data-c="65" xlink:href="#MJX-11-TEX-N-65" transform="translate(1278,0)"></use><use data-c="6E" xlink:href="#MJX-11-TEX-N-6E" transform="translate(1722,0)"></use><use data-c="74" xlink:href="#MJX-11-TEX-N-74" transform="translate(2278,0)"></use><use data-c="69" xlink:href="#MJX-11-TEX-N-69" transform="translate(2667,0)"></use><use data-c="6F" xlink:href="#MJX-11-TEX-N-6F" transform="translate(2945,0)"></use><use data-c="6E" xlink:href="#MJX-11-TEX-N-6E" transform="translate(3445,0)"></use></g><g data-mml-node="mo" transform="translate(4001,0)"><use data-c="28" xlink:href="#MJX-11-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4390,0)"><use data-c="1D444" xlink:href="#MJX-11-TEX-I-1D444"></use></g><g data-mml-node="mo" transform="translate(5181,0)"><use data-c="2C" xlink:href="#MJX-11-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(5625.7,0)"><use data-c="1D43E" xlink:href="#MJX-11-TEX-I-1D43E"></use></g><g data-mml-node="mo" transform="translate(6514.7,0)"><use data-c="2C" xlink:href="#MJX-11-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(6959.3,0)"><use data-c="1D449" xlink:href="#MJX-11-TEX-I-1D449"></use></g><g data-mml-node="mo" transform="translate(7728.3,0)"><use data-c="29" xlink:href="#MJX-11-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(8395.1,0)"><use data-c="3D" xlink:href="#MJX-11-TEX-N-3D"></use></g><g data-mml-node="mtext" transform="translate(9450.9,0)"><use data-c="73" xlink:href="#MJX-11-TEX-N-73"></use><use data-c="6F" xlink:href="#MJX-11-TEX-N-6F" transform="translate(394,0)"></use><use data-c="66" xlink:href="#MJX-11-TEX-N-66" transform="translate(894,0)"></use><use data-c="74" xlink:href="#MJX-11-TEX-N-74" transform="translate(1200,0)"></use><use data-c="6D" xlink:href="#MJX-11-TEX-N-6D" transform="translate(1589,0)"></use><use data-c="61" xlink:href="#MJX-11-TEX-N-61" transform="translate(2422,0)"></use><use data-c="78" xlink:href="#MJX-11-TEX-N-78" transform="translate(2922,0)"></use></g><g data-mml-node="mo" transform="translate(12900.9,0)"><use data-c="28" xlink:href="#MJX-11-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(13289.9,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><use data-c="1D444" xlink:href="#MJX-11-TEX-I-1D444"></use></g><g data-mml-node="msup" transform="translate(791,0)"><g data-mml-node="mi"><use data-c="1D43E" xlink:href="#MJX-11-TEX-I-1D43E"></use></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><use data-c="1D447" xlink:href="#MJX-11-TEX-I-1D447"></use></g></g></g><g data-mml-node="msqrt" transform="translate(464.2,-855.6)"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-11-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><use data-c="1D458" xlink:href="#MJX-11-TEX-I-1D458"></use></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><use data-c="221A" xlink:href="#MJX-11-TEX-N-221A"></use></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g><rect width="2512.8" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(16042.7,0)"><use data-c="29" xlink:href="#MJX-11-TEX-N-29"></use></g><g data-mml-node="mi" transform="translate(16431.7,0)"><use data-c="1D449" xlink:href="#MJX-11-TEX-I-1D449"></use></g></g></g></svg></mjx-container>
<p>注意力实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v</span>):  <span class="comment"># [n_q, d_k], [n_k, d_k], [n_k, d_v] -&gt; [n_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> softmax(q @ k.T / np.sqrt(q.shape[-<span class="number">1</span>])) @ v</span><br></pre></td></tr></table></figure>
<p><strong>自身</strong><br>当<code>q, k</code>和<code>v</code>来自同一来源时，我们就是在执行自注意力（即让我们的输入序列自我关注）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attention</span>(<span class="params">x</span>): <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">return</span> attention(q=x, k=x, v=x)</span><br></pre></td></tr></table></figure>
<p>例如，如果我们的输入是<code>“Jay went to the store, he bought 10 apples.”</code>，我们让单词<code>“he”</code>关注所有其它单词，包括<code>“Jay”</code>，这意味着模型可以学习到<code>“he”</code>指的是<code>“Jay”</code>。</p>
<p>我们可以通过为<code>q、k、v</code>和注意力输出引入投影来增强自注意力：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attention</span>(<span class="params">x, w_k, w_q, w_v, w_proj</span>): <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projections</span></span><br><span class="line">    q = x @ w_k <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    k = x @ w_q <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    v = x @ w_v <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform self attention</span></span><br><span class="line">    x = attention(q, k, v) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = x @ w_proj <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这使得我们的模型为<code>q, k, v</code>学到一个最好的映射，以帮助注意力区分输入之间的关系。如果我们将<code>w_q、w_k</code>和<code>w_v</code>组合成一个单独的矩阵<code>w_fc</code>，执行投影操作，然后拆分结果，我们就可以将矩阵乘法的数量从<code>4</code>个减少到<code>2</code>个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attention</span>(<span class="params">x, w_fc, w_proj</span>): <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projections</span></span><br><span class="line">    x = x @ w_fc <span class="comment"># [n_seq, n_embd] @ [n_embd, 3*n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    q, k, v = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>) <span class="comment"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform self attention</span></span><br><span class="line">    x = attention(q, k, v) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = x @ w_proj <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样会更加高效，因为现代加速器（如<code>GPU</code>）可以更好地利用一个大的矩阵乘法，而不是顺序执行<code>3</code>个独立的小矩阵乘法。最后，我们添加偏置向量以匹配<code>GPT-2</code>的实现，然后使用我们的<code>linear</code>函数，并将参数重命名以匹配我们的<code>params</code>字典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attention</span>(<span class="params">x, c_attn, c_proj</span>): <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projections</span></span><br><span class="line">    x = linear(x, **c_attn) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    q, k, v = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>) <span class="comment"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform self attention</span></span><br><span class="line">    x = attention(q, k, v) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = linear(x, **c_proj) <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>回忆一下，从我们的<code>params</code>字典中可知，<code>attn</code>参数类似：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;attn&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;c_attn&quot;</span>: &#123;<span class="string">&quot;b&quot;</span>: [3*n_embd], <span class="string">&quot;w&quot;</span>: [n_embd, 3*n_embd]&#125;,</span><br><span class="line">    <span class="string">&quot;c_proj&quot;</span>: &#123;<span class="string">&quot;b&quot;</span>: [n_embd], <span class="string">&quot;w&quot;</span>: [n_embd, n_embd]&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>因果</strong><br>我们当前的自注意力设置存在一个问题，就是我们的输入能够“看到”未来的信息！比如，如果我们的输入是<code>[“not”, “all”, “heroes”, “wear”, “capes”]</code>，在自注意力中，<code>“wear”</code>可以看到<code>“capes”</code>。这意味着<code>“wear”</code>的输出概率将会受到偏差，因为模型已经知道正确的答案是<code>“capes”</code>。这是不好的，因为我们的模型会从中学习到，输入<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>的正确答案可以从输入<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.677ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2067.4 748" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-11-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-11-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(567.2,0)"><use data-c="2B" xlink:href="#MJX-11-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1567.4,0)"><use data-c="31" xlink:href="#MJX-11-TEX-N-31"></use></g></g></g></svg></mjx-container>中获取。为了防止这种情况发生，我们需要修改注意力矩阵，以隐藏或屏蔽我们的输入，使其无法看到未来的信息。例如，假设我们的注意力矩阵如下所示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">        not    all   heroes wear   capes</span><br><span class="line">   not 0.116  0.159  0.055  0.226  0.443</span><br><span class="line">   all 0.180  0.397  0.142  0.106  0.175</span><br><span class="line">heroes 0.156  0.453  0.028  0.129  0.234</span><br><span class="line">  wear 0.499  0.055  0.133  0.017  0.295</span><br><span class="line"> capes 0.089  0.290  0.240  0.228  0.153</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里每一行对应一个查询(<code>query</code>)，每一列对应一个键值(<code>key</code>)。在这个例子中，查看<code>“wear”</code>对应的行，可以看到它在最后一列以<code>0.295</code>的权重与<code>“capes”</code>相关。为了防止这种情况发生，我们要将这项设为<code>0.0</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">        not    all    heroes wear   capes</span><br><span class="line">   not 0.116  0.159  0.055  0.226  0.443</span><br><span class="line">   all 0.180  0.397  0.142  0.106  0.175</span><br><span class="line">heroes 0.156  0.453  0.028  0.129  0.234</span><br><span class="line">  wear 0.499  0.055  0.133  0.017  0.</span><br><span class="line"> capes 0.089  0.290  0.240  0.228  0.153</span><br></pre></td></tr></table></figure>
<p>通常，为了防止输入中的所有查询看到未来信息，我们将所有满足<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="4.73ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 2090.6 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-11-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-11-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(689.8,0)"><use data-c="3E" xlink:href="#MJX-11-TEX-N-3E"></use></g><g data-mml-node="mi" transform="translate(1745.6,0)"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg></mjx-container>的位置<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="2.719ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 1201.7 865" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-11-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-11-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="2C" xlink:href="#MJX-11-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(789.7,0)"><use data-c="1D457" xlink:href="#MJX-11-TEX-I-1D457"></use></g></g></g></svg></mjx-container>都设置为<code>0</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">         not    all    heroes wear   capes</span><br><span class="line">   not 0.116  0.     0.     0.     0.</span><br><span class="line">   all 0.180  0.397  0.     0.     0.</span><br><span class="line">heroes 0.156  0.453  0.028  0.     0.</span><br><span class="line">  wear 0.499  0.055  0.133  0.017  0.</span><br><span class="line"> capes 0.089  0.290  0.240  0.228  0.153</span><br></pre></td></tr></table></figure>
<p>我们将这称为<strong>掩码</strong>(<code>masking</code>)。掩码方法的一个问题是我们的行不再加起来为<code>1</code>（因为我们在使用<code>softmax</code>后才将它们设为<code>0</code>）。为了确保我们的行仍然加起来为<code>1</code>，我们需要在使用<code>softmax</code>之前先修改注意力矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, mask</span>):  <span class="comment"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> softmax(q @ k.T / np.sqrt(q.shape[-<span class="number">1</span>]) + mask) @ v</span><br></pre></td></tr></table></figure>
<p>其中<code>mask</code>表示矩阵<code>（n_seq=5）</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0 -1e10 -1e10 -1e10 -1e10</span><br><span class="line">0   0   -1e10 -1e10 -1e10</span><br><span class="line">0   0     0   -1e10 -1e10</span><br><span class="line">0   0     0     0   -1e10</span><br><span class="line">0   0     0     0     0</span><br></pre></td></tr></table></figure>
<p>我们用<code>-1e10</code>替换<code>-np.inf</code>， 因为<code>-np.inf</code>会导致<code>nans</code>错误。添加<code>mask</code>到我们的注意力矩阵中，而不是明确设置值为<code>-1e10</code>，是因为在实际操作中，任何数加上<code>-inf</code>还是<code>-inf</code>。我们可以在<code>NumPy</code>中通过<code>(1 - np.tri(n_seq)) * -1e10</code>来计算<code>mask</code>矩阵。我们得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, mask</span>):  <span class="comment"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> softmax(q @ k.T / np.sqrt(q.shape[-<span class="number">1</span>]) + mask) @ v</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">causal_self_attention</span>(<span class="params">x, c_attn, c_proj</span>): <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projections</span></span><br><span class="line">    x = linear(x, **c_attn) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    q, k, v = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>) <span class="comment"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># causal mask to hide future inputs from being attended to</span></span><br><span class="line">    causal_mask = (<span class="number">1</span> - np.tri(x.shape[<span class="number">0</span>]), dtype=x.dtype) * -<span class="number">1e10</span>  <span class="comment"># [n_seq, n_seq]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform causal self attention</span></span><br><span class="line">    x = attention(q, k, v, causal_mask) <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = linear(x, **c_proj) <span class="comment"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><strong>多头</strong><br>我们可以进一步改进我们的实现，通过进行<code>n_head</code>个独立的注意力计算，将我们的查询(<code>queries</code>)，键(<code>keys</code>)和值(<code>values</code>)拆分到多个头(<code>heads</code>)里去：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mha</span>(<span class="params">x, c_attn, c_proj, n_head</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projection</span></span><br><span class="line">    x = linear(x, **c_attn)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    qkv = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>)  <span class="comment"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into heads(拆分q， k， v到n_head个头)</span></span><br><span class="line">    qkv_heads = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: np.split(x, n_head, axis=-<span class="number">1</span>), qkv))  <span class="comment"># [3, n_seq, n_embd] -&gt; [3, n_head, n_seq, n_embd/n_head]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># causal mask to hide future inputs from being attended to</span></span><br><span class="line">    causal_mask = (<span class="number">1</span> - np.tri(x.shape[<span class="number">0</span>], dtype=x.dtype)) * -<span class="number">1e10</span>  <span class="comment"># [n_seq, n_seq]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform attention over each head(为每个头计算注意力)</span></span><br><span class="line">    out_heads = [attention(q, k, v, causal_mask) <span class="keyword">for</span> q, k, v <span class="keyword">in</span> <span class="built_in">zip</span>(*qkv_heads)]  <span class="comment"># [3, n_head, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge heads(合并每个头的输出)</span></span><br><span class="line">    x = np.hstack(out_heads)  <span class="comment"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = linear(x, **c_proj)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这里添加了三步:</p>
<ul>
<li>拆分<code>q， k， v</code>到<code>n_head</code>个头。</li>
<li>为每个头计算注意力。</li>
<li>合并每个头的输出。</li>
</ul>
<div class="note warning"><p><strong>注意</strong>，这样可以将每个注意力计算的维度从<code>n_embd</code>减少到<code>n_embd/n_head</code>。这是一个权衡。对于缩减了的维度，我们的模型在通过注意力建模关系时获得了额外的子空间。例如，也许一个注意力头负责将代词与代词所指的人联系起来；也许另一个注意力头负责通过句号将句子分组；另一个则可能只是识别哪些单词是实体，哪些不是。虽然这可能也只是另一个神经网络黑盒而已。我们编写的代码按顺序循环执行每个头的注意力计算（每次一个），当然这并不是很高效。在实践中，你会希望并行处理这些计算。当然在本文中考虑到简洁性，我们将保持这种顺序执行。</p>
</div>

<h4 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gelu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1</span> + np.tanh(np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * x**<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    exp_x = np.exp(x - np.<span class="built_in">max</span>(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> exp_x / np.<span class="built_in">sum</span>(exp_x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">x, g, b, eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span>):</span><br><span class="line">    mean = np.mean(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    variance = np.var(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / np.sqrt(variance + eps)  <span class="comment"># normalize x to have mean=0 and var=1 over last axis</span></span><br><span class="line">    <span class="keyword">return</span> g * x + b  <span class="comment"># scale and offset with gamma/beta params</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, w, b</span>):  <span class="comment"># [m, in], [in, out], [out] -&gt; [m, out]</span></span><br><span class="line">    <span class="keyword">return</span> x @ w + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ffn</span>(<span class="params">x, c_fc, c_proj</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># project up</span></span><br><span class="line">    a = gelu(linear(x, **c_fc))  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 4*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># project back down</span></span><br><span class="line">    x = linear(a, **c_proj)  <span class="comment"># [n_seq, 4*n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, mask</span>):  <span class="comment"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> softmax(q @ k.T / np.sqrt(q.shape[-<span class="number">1</span>]) + mask) @ v</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mha</span>(<span class="params">x, c_attn, c_proj, n_head</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projection</span></span><br><span class="line">    x = linear(x, **c_attn)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    qkv = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>)  <span class="comment"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># split into heads</span></span><br><span class="line">    qkv_heads = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: np.split(x, n_head, axis=-<span class="number">1</span>), qkv))  <span class="comment"># [3, n_seq, n_embd] -&gt; [3, n_head, n_seq, n_embd/n_head]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># causal mask to hide future inputs from being attended to</span></span><br><span class="line">    causal_mask = (<span class="number">1</span> - np.tri(x.shape[<span class="number">0</span>], dtype=x.dtype)) * -<span class="number">1e10</span>  <span class="comment"># [n_seq, n_seq]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform attention over each head</span></span><br><span class="line">    out_heads = [attention(q, k, v, causal_mask) <span class="keyword">for</span> q, k, v <span class="keyword">in</span> <span class="built_in">zip</span>(*qkv_heads)]  <span class="comment"># [3, n_head, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge heads</span></span><br><span class="line">    x = np.hstack(out_heads)  <span class="comment"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = linear(x, **c_proj)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_block</span>(<span class="params">x, mlp, attn, ln_1, ln_2, n_head</span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># multi-head causal self attention</span></span><br><span class="line">    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-wise feed forward network</span></span><br><span class="line">    x = x + ffn(layer_norm(x, **ln_2), **mlp)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>(<span class="params">inputs, wte, wpe, blocks, ln_f, n_head</span>):  <span class="comment"># [n_seq] -&gt; [n_seq, n_vocab]</span></span><br><span class="line">    <span class="comment"># token + positional embeddings</span></span><br><span class="line">    x = wte[inputs] + wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]  <span class="comment"># [n_seq] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass through n_layer transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        x = transformer_block(x, **block, n_head=n_head)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># projection to vocab</span></span><br><span class="line">    x = layer_norm(x, **ln_f)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">return</span> x @ wte.T  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">inputs, params, n_head, n_tokens_to_generate</span>):</span><br><span class="line">    <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_tokens_to_generate), <span class="string">&quot;generating&quot;</span>):  <span class="comment"># auto-regressive decode loop</span></span><br><span class="line">        logits = gpt2(inputs, **params, n_head=n_head)  <span class="comment"># model forward pass</span></span><br><span class="line">        next_id = np.argmax(logits[-<span class="number">1</span>])  <span class="comment"># greedy sampling</span></span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id))  <span class="comment"># append prediction to input</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># only return generated ids</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">prompt: <span class="built_in">str</span>, n_tokens_to_generate: <span class="built_in">int</span> = <span class="number">40</span>, model_size: <span class="built_in">str</span> = <span class="string">&quot;124M&quot;</span>, models_dir: <span class="built_in">str</span> = <span class="string">&quot;models&quot;</span></span>):</span><br><span class="line">    <span class="keyword">from</span> utils <span class="keyword">import</span> load_encoder_hparams_and_params</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load encoder, hparams, and params from the released open-ai gpt-2 files</span></span><br><span class="line">    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># encode the input string using the BPE tokenizer</span></span><br><span class="line">    input_ids = encoder.encode(prompt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make sure we are not surpassing the max sequence length of our model</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(input_ids) + n_tokens_to_generate &lt; hparams[<span class="string">&quot;n_ctx&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate output ids</span></span><br><span class="line">    output_ids = generate(input_ids, params, hparams[<span class="string">&quot;n_head&quot;</span>], n_tokens_to_generate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decode the ids back into a string</span></span><br><span class="line">    output_text = encoder.decode(output_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> fire</span><br><span class="line"></span><br><span class="line">    fire.Fire(main)</span><br></pre></td></tr></table></figure>
<h4 id="GPU-TPU支持"><a href="#GPU-TPU支持" class="headerlink" title="GPU&#x2F;TPU支持"></a>GPU&#x2F;TPU支持</h4><p>将<code>NumPy</code>替换为<code>JAX</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jax.numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lm_loss</span>(<span class="params">params, inputs, n_head</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    x, y = inputs[:-<span class="number">1</span>], inputs[<span class="number">1</span>:]</span><br><span class="line">    output = gpt2(x, **params, n_head=n_head)</span><br><span class="line">    loss = np.mean(-np.log(output[y]))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">grads = jax.grad(lm_loss)(params, inputs, n_head)</span><br><span class="line"></span><br><span class="line"><span class="comment"># gpt2函数批量化</span></span><br><span class="line">gpt2_batched = jax.vmap(gpt2, in_axes=[<span class="number">0</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>])</span><br><span class="line">gpt2_batched(batched_inputs) <span class="comment"># [batch, seq_len] -&gt; [batch, seq_len, vocab]</span></span><br></pre></td></tr></table></figure>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>我们的实现相当低效。除了支持<code>GPU</code>和批处理之外，最快且最有效的优化可能是实现一个键值缓存。此外，我们顺序地实现了注意力头计算，而实际上我们应该使用并行计算等。</p>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>训练<code>GPT</code>对于神经网络来说是非常标准的行为（针对损失函数进行梯度下降）。当然，在训练<code>GPT</code>时你还需要使用一堆常规的技巧（使用<code>Adam</code>优化器，找到最佳的学习率，通过<code>dropout</code>和<code>/</code>或权重衰减进行正则化，使用学习率规划器，使用正确的权重初始化，进行分批处理等等）。而训练一个好的<code>GPT</code>模型的<strong>真正秘诀在于能够扩展数据和模型，这也是真正的挑战所在</strong>。</p>
<p>为了扩展数据量，您需要拥有大规模、高质量、多样化的文本语料库。</p>
<ul>
<li><strong>大规模</strong>意味着拥有数十亿的<code>token</code>（数百万<code>GB</code>的数据）。例如可以查看<code>The Pile</code>，这是一个用于大型语言模型的开源预训练数据集。</li>
<li><strong>高质量</strong>意味着需要过滤掉重复的示例、未格式化的文本、不连贯的文本、垃圾文本等等。</li>
<li><strong>多样性</strong>意味着序列长度变化大，涵盖了许多不同的主题，来自不同的来源，具有不同的观点等等。当然，如果数据中存在任何偏见，它将反映在模型中，因此您需要谨慎处理。</li>
</ul>
<p>将模型扩展到数十亿个参数需要超级大量的工程（金钱）可以使用<code>NVIDIA</code>的<code>Megatron Framework, Cohere</code>的训练框架, <code>Google</code>的<code>PALM</code>, 开源的<code>mesh-transformer-jax</code>（用于训练<code>EleutherAI</code>的开源模型）</p>
<h4 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h4><p>老实说，这是一个非常困难的问题。<code>HELM</code>是一个相当全面且不错的起点，但你应该始终对基准测试和评估指标保持怀疑的态度。</p>
<h4 id="停止生成"><a href="#停止生成" class="headerlink" title="停止生成"></a>停止生成</h4><p>我们当前的实现需要事先指定要生成的确切<code>token</code>数量。这不是一个很好的方法，因为我们生成的文本可能会太长、太短或在句子中间截断。为了解决这个问题，我们可以引入一个特殊的句子结束（<code>EOS</code>）<code>token</code>。在预训练期间，我们在输入的末尾附加<code>EOS token</code>（比如，<code>tokens = [&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;, &quot;capes&quot;, &quot;.&quot;, &quot;&lt;|EOS|&gt;&quot;]</code>）。在生成过程中，我们只需要在遇到<code>EOS token</code>时停止（或者达到最大序列长度）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">inputs, eos_id, max_seq_len</span>):</span><br><span class="line">	prompt_len = <span class="built_in">len</span>(inputs)</span><br><span class="line">	<span class="keyword">while</span> inputs[-<span class="number">1</span>] != eos_id <span class="keyword">and</span> <span class="built_in">len</span>(inputs) &lt; max_seq_len:</span><br><span class="line">        output = gpt(inputs)</span><br><span class="line">        next_id = np.argmax(output[-<span class="number">1</span>])</span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id))</span><br><span class="line">    <span class="keyword">return</span> inputs[prompt_len:]</span><br></pre></td></tr></table></figure>
<p><code>GPT-2</code>没有使用<code>EOS token</code>进行预训练，因此我们无法在我们的代码中使用这种方法，但是现在大多数<code>LLMs</code>都已经使用<code>EOS token</code>了。</p>
<h4 id="无条件生成"><a href="#无条件生成" class="headerlink" title="无条件生成"></a>无条件生成</h4><p>使用我们的模型生成文本需要对其提供提示条件。但是我们也可以让模型执行无条件生成，即模型在没有任何输入提示的情况下生成文本。这是通过在预训练期间在输入开头加上一个特殊的句子开头（<code>BOS</code>）<code>token</code>来实现的（例如 <code>tokens = [&quot;&lt;|BOS|&gt;&quot;, &quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;, &quot;capes&quot;, &quot;.&quot;]</code>）。要进行无条件文本生成的话，我们就输入一个仅包含<code>BOS token</code>的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_unconditioned</span>(<span class="params">bos_id, n_tokens_to_generate</span>):</span><br><span class="line">	inputs = [bos_id]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_tokens_to_generate):</span><br><span class="line">        output = gpt(inputs)</span><br><span class="line">        next_id = np.argmax(output[-<span class="number">1</span>])</span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id))</span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<p><code>GPT-2</code>的预训练是带有<code>BOS token</code>的（不过它有一个令人困惑的名字<code>&lt;|endoftext|&gt;</code>），因此在我们的实现中要运行无条件生成的话，只需要简单地将这行代码更改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_ids = encoder.encode(prompt) <span class="keyword">if</span> prompt <span class="keyword">else</span> [encoder.encoder[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>]]</span><br></pre></td></tr></table></figure>
<p>由于我们使用的是贪心采样，所以输出结果不是很好（重复的内容较多），且每次运行代码的输出结果都是确定的。为了获得更高质量的、不确定性更大的生成结果，我们需要直接从概率分布中进行采样（最好在使用<code>top-p</code>之类的方法后进行采样）。无条件生成不是特别有用，但它是演示GPT能力的一种有趣方式。</p>
<h4 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h4><p>我们在训练部分简要介绍了微调。回想一下，微调是指我们复用预训练的权重，对模型在某些下游任务上进行训练。我们称这个过程为<strong>迁移学习</strong>。理论上，我们可以使用零样本或少样本提示来让模型完成我们的任务，但是如果您可以访问一个标注的数据集，对<code>GPT</code>进行微调将会产生更好的结果（这些结果可以在获得更多数据和更高质量的数据时进行扩展）。</p>
<h5 id="分类微调"><a href="#分类微调" class="headerlink" title="分类微调"></a>分类微调</h5><p>在分类微调中，我们会给模型一些文本，并要求它预测它属于哪个类别。以<code>IMDB</code>数据集为例，它包含着电影评论，将电影评为好或坏：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--- Example 1 ---</span><br><span class="line">Text: I wouldn<span class="string">&#x27;t rent this one even on dollar rental night.</span></span><br><span class="line"><span class="string">Label: Bad</span></span><br><span class="line"><span class="string">--- Example 2 ---</span></span><br><span class="line"><span class="string">Text: I don&#x27;</span>t know why I like this movie so well, but I never get tired of watching it.</span><br><span class="line">Label: Good</span><br><span class="line">--- Example 3 ---</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>为了微调我们的模型，我们需要用分类头替换语言建模头，将其应用于最后一个<code>token</code>的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>(<span class="params">inputs, wte, wpe, blocks, ln_f, cls_head, n_head</span>):</span><br><span class="line">    x = wte[inputs] + wpe[<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))]</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        x = transformer_block(x, **block, n_head=n_head)</span><br><span class="line">    x = layer_norm(x, **ln_f)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># project to n_classes [n_embd] @ [n_embd, n_classes] -&gt; [n_classes]</span></span><br><span class="line">    <span class="keyword">return</span> x[-<span class="number">1</span>] @ cls_head</span><br></pre></td></tr></table></figure>
<p>这里我们只使用最后一个<code>token</code>的输出<code>x[-1]</code>，因为我们只需要为整个输入产生一个单一的概率分布，而不是像语言模型一样产生<code>n_seq</code>个分布。我们特别选择最后一个<code>token</code>（而不是第一个<code>token</code>或所有<code>token</code>的组合），因为最后一个<code>token</code>是唯一允许关注整个序列的<code>token</code>，因此它具有关于整个输入文本的信息。同往常一样，我们根据交叉熵损失进行优化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">singe_example_loss_fn</span>(<span class="params">inputs: <span class="built_in">list</span>[<span class="built_in">int</span>], label: <span class="built_in">int</span>, params</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    logits = gpt(inputs, **params)</span><br><span class="line">    probs = softmax(logits)</span><br><span class="line">    loss = -np.log(probs[label]) <span class="comment"># cross entropy loss</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>我们还可以执行多标签分类（即一个样本可以属于多个类别，而不仅仅是一个类别），这可以通过使用<code>sigmoid</code>替代<code>softmax</code>并针对每个类别采用二分交叉熵损失。</p>
<h5 id="生成式微调"><a href="#生成式微调" class="headerlink" title="生成式微调"></a>生成式微调</h5><p>有些任务无法被简单地认为是分类，如摘要的任务。我们可以通过对输入和标签拼接进行语言建模，从而实现这类任务的微调。例如，下面就是一个摘要训练样本的示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Article ---</span><br><span class="line">This is an article I would like to summarize.</span><br><span class="line">--- Summary ---</span><br><span class="line">This is the summary.</span><br></pre></td></tr></table></figure>
<p>我们就像预训练时那样训练这个模型（根据语言建模的损失进行优化）。在预测时，我们将直到<code>&quot;--- Summary ---&quot;</code>的输入喂给模型，然后执行自回归语言建模以生成摘要。定界符”<code>--- Article ---&quot;</code>和<code>&quot;--- Summary ---&quot;</code>的选择是任意的。如何选择文本格式由您决定，只要在训练和推断中保持一致即可。请注意，其实我们也可以将分类任务表述为生成任务（以<code>IMDB</code>为例）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Text ---</span><br><span class="line">I wouldn<span class="string">&#x27;t rent this one even on dollar rental night.</span></span><br><span class="line"><span class="string">--- Label ---</span></span><br><span class="line"><span class="string">Bad</span></span><br></pre></td></tr></table></figure>
<p>然而，这种方法的表现很可能会比直接进行分类微调要差（损失函数包括对整个序列进行语言建模，而不仅仅是对最终预测的输出进行建模，因此与预测有关的损失将被稀释）。</p>
<h5 id="指令微调"><a href="#指令微调" class="headerlink" title="指令微调"></a>指令微调</h5><p>目前大多数最先进的大型语言模型在预训练后还需要经过一个额外的指令微调步骤。在这个步骤中，模型在成千上万个由人工标注的指令提示+补全对上进行微调（生成式）。指令微调也可以称为监督式微调，因为数据是人工标记的（即有监督的）。那指令微调的好处是什么呢？虽然在预测维基百科文章中的下一个词时，模型在续写句子方面表现得很好，但它并不擅长遵循说明、进行对话或对文件进行摘要（这些是我们希望<code>GPT</code>能够做到的事情）。在人类标记的指令 + 完成对中微调它们是教育模型如何变得更有用，并使它们更容易交互的一种方法。我们将其称为<code>AI</code>对齐(<code>AI alignment</code>)，因为我们需要模型以我们想要的方式做事和表现。对齐是一个活跃的研究领域，它不仅仅只包括遵循说明（还涉及偏见、安全、意图等）的问题。那么这些指令数据到底是什么样子的呢？<code>Google</code>的<code>FLAN</code>模型是在多个学术的自然语言处理数据集（这些数据集已经被人工标注）上进行训练的：</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_6.png" class="">

<p><code>OpenAI</code>的<code>InstructGPT</code>则使用了从其<code>API</code>中收集的提示进行训练。然后他们雇佣工人为这些提示编写补全。下面是这些数据的详细信息：</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_7.png" class="">

<h5 id="参数高效微调（Parameter-Efficient-Fine-tuning）"><a href="#参数高效微调（Parameter-Efficient-Fine-tuning）" class="headerlink" title="参数高效微调（Parameter Efficient Fine-tuning）"></a>参数高效微调（Parameter Efficient Fine-tuning）</h5><p>当我们在上面的部分讨论微调时，我们是在更新模型的所有参数。虽然这可以获得最佳性能，但成本非常高，无论是在计算方面（需要经过整个模型进行反向传播），还是在存储方面（对于每个微调的模型，您需要存储一份全新的参数副本）。最简单的解决方法是<strong>只更新模型头部并冻结模型的其它部分</strong>。虽然这样做可以加速训练并大大减少新参数的数量，但其表现并不好，因为某种意义上我们损失了深度学习中的深度。相反，我们可以<strong>选择性地冻结特定层</strong>（例如冻结除了最后四层外的所有层，或每隔一层进行冻结，或冻结除多头注意力参数外的所有参数），那么这将有助于恢复深度。这种方法的性能要好得多，但我们也变得不那么参数高效(<code>parameter efficient</code>)，同时也失去了一些训练速度的优势。除此之外，我们还可以利用<strong>参数高效微调</strong>(<code>Parameter Efficient Fine-tuning</code>)方法。这仍然是一个活跃的研究领域。</p>
<p>我们可以看看<code>Adapters</code>论文。在这种方法中，我们在<code>Transformer</code>模块的<code>FFN</code>和<code>MHA</code>层后添加了一个额外的<code>“adapter”</code>层。这里的<code>adapter</code>层只是一个简单的两层全连接神经网络，其中输入和输出维度是<code>n_embd</code>，而隐藏维度小于<code>n_embd</code>。适配器方法中，隐藏层的大小是一个我们可以设置的超参数，这使我们能够在参数和性能之间进行权衡。该论文表明，对于<code>BERT</code>模型，使用这种方法可以将训练参数数量降低到<code>2％</code>，而与完全微调相比仅有少量的性能下降(<code>&lt;1%</code>)。</p>
<img data-src="/2024/06/04/artificial-intelligence/GPT_theory_study/gpt_8.png" class="">

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/" title="GPT模型探析（LLM）(Numpy)">https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/30/artificial-intelligence/nlp_theory_study/" rel="prev" title="自然语言处理 (预训练)">
                  <i class="fa fa-chevron-left"></i> 自然语言处理 (预训练)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/06/05/artificial-intelligence/Speculative_Sampling_study/" rel="next" title="利用推测采样加速大型语言模型解码（LLM）">
                  利用推测采样加速大型语言模型解码（LLM） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">2.8m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">38:43</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/06/04/artificial-intelligence/GPT_theory_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
