<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="介绍 处理现实世界数据集中常见的数据类型（缺失值、分类变量）。 设计管道以提高机器学习代码的质量。 使用先进的技术进行模型验证（交叉验证）。 构建最先进的模型，广泛用于赢得Kaggle比赛(XGBoost)。 避免常见且重要的数据科学错误（泄漏）。  缺失值（Missing Values）您将学习三种处理缺失值的方法。然后，您将在现实数据集上比较这些方法的有效性。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习（中级）">
<meta property="og:url" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="介绍 处理现实世界数据集中常见的数据类型（缺失值、分类变量）。 设计管道以提高机器学习代码的质量。 使用先进的技术进行模型验证（交叉验证）。 构建最先进的模型，广泛用于赢得Kaggle比赛(XGBoost)。 避免常见且重要的数据科学错误（泄漏）。  缺失值（Missing Values）您将学习三种处理缺失值的方法。然后，您将在现实数据集上比较这些方法的有效性。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_8.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_9.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_10.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_11.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_12.png">
<meta property="article:published_time" content="2024-03-07T02:20:32.000Z">
<meta property="article:modified_time" content="2024-03-07T02:20:32.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/","path":"2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/","title":"机器学习（中级）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习（中级） | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%EF%BC%88Missing-Values%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">缺失值（Missing Values）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-1"><span class="nav-number">2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.</span> <span class="nav-text">缺失值处理的三种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E5%88%A0%E9%99%A4%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E5%88%97"><span class="nav-number">2.2.1.</span> <span class="nav-text">1.删除缺失值的列</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E6%9B%B4%E5%A5%BD%E7%9A%84%E9%80%89%E6%8B%A9%EF%BC%9A%E6%8F%92%E8%A1%A5"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.更好的选择：插补</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E6%8F%92%E8%A1%A5%E7%9A%84%E6%89%A9%E5%B1%95"><span class="nav-number">2.2.3.</span> <span class="nav-text">3.插补的扩展</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B"><span class="nav-number">2.3.</span> <span class="nav-text">举例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E8%A1%A1%E9%87%8F%E6%AF%8F%E7%A7%8D%E6%96%B9%E6%B3%95%E8%B4%A8%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.1.</span> <span class="nav-text">定义衡量每种方法质量的函数</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E5%88%A0%E9%99%A4%E5%85%B7%E6%9C%89%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E5%88%97%EF%BC%89"><span class="nav-number">2.3.2.</span> <span class="nav-text">方法一的得分（删除具有缺失值的列）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E6%8F%92%E8%A1%A5%EF%BC%89"><span class="nav-number">2.3.3.</span> <span class="nav-text">方法二的得分（插补）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%89%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E6%8F%92%E8%A1%A5%E7%9A%84%E6%89%A9%E5%B1%95%EF%BC%89"><span class="nav-number">2.3.4.</span> <span class="nav-text">方法三的得分（插补的扩展）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%82%A3%E4%B9%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8F%92%E8%A1%A5%E6%AF%94%E5%88%A0%E9%99%A4%E5%88%97%E8%A1%A8%E7%8E%B0%E6%9B%B4%E5%A5%BD%E5%91%A2%EF%BC%9F"><span class="nav-number">2.3.5.</span> <span class="nav-text">那么，为什么插补比删除列表现更好呢？</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">2.4.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%EF%BC%88Categorical-Variables%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">分类变量（Categorical Variables）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-2"><span class="nav-number">3.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">三种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%A0%E9%99%A4%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="nav-number">3.2.1.</span> <span class="nav-text">删除分类变量</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%BA%8F%E6%95%B0%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.2.</span> <span class="nav-text">序数编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%80%E6%AC%A1%E6%80%A7%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.3.</span> <span class="nav-text">一次性编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-1"><span class="nav-number">3.2.4.</span> <span class="nav-text">举例</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E8%A1%A1%E9%87%8F%E6%AF%8F%E7%A7%8D%E6%96%B9%E6%B3%95%E8%B4%A8%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0-1"><span class="nav-number">3.2.5.</span> <span class="nav-text">定义衡量每种方法质量的函数</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E5%88%A0%E9%99%A4%E7%B1%BB%E5%88%AB%E5%8F%98%E9%87%8F%EF%BC%89"><span class="nav-number">3.2.6.</span> <span class="nav-text">方法一的得分（删除类别变量）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E5%BA%8F%E6%95%B0%E7%BC%96%E7%A0%81%EF%BC%89"><span class="nav-number">3.2.7.</span> <span class="nav-text">方法二的得分（序数编码）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%89%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88One-Hot-%E7%BC%96%E7%A0%81%EF%BC%89"><span class="nav-number">3.2.8.</span> <span class="nav-text">方法三的得分（One-Hot 编码）</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%93%AA%E7%A7%8D%E6%96%B9%E6%B3%95%E6%9C%80%E5%A5%BD%EF%BC%9F"><span class="nav-number">3.3.</span> <span class="nav-text">哪种方法最好？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%A1%E9%81%93"><span class="nav-number">4.</span> <span class="nav-text">管道</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-3"><span class="nav-number">4.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-2"><span class="nav-number">4.2.</span> <span class="nav-text">举例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%AE%9A%E4%B9%89%E9%A2%84%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4"><span class="nav-number">4.2.1.</span> <span class="nav-text">第一步：定义预处理步骤</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.2.2.</span> <span class="nav-text">第二步：定义模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E5%88%9B%E5%BB%BA%E5%B9%B6%E8%AF%84%E4%BC%B0%E7%AE%A1%E9%81%93"><span class="nav-number">4.2.3.</span> <span class="nav-text">第三步：创建并评估管道</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA-1"><span class="nav-number">4.2.4.</span> <span class="nav-text">结论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%EF%BC%88Cross-Validation%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">交叉验证（Cross-Validation）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-4"><span class="nav-number">5.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%EF%BC%9F"><span class="nav-number">5.2.</span> <span class="nav-text">什么是交叉验证？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%EF%BC%9F"><span class="nav-number">5.3.</span> <span class="nav-text">什么时候应该使用交叉验证？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-3"><span class="nav-number">5.4.</span> <span class="nav-text">举例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="nav-number">5.5.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost"><span class="nav-number">6.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-5"><span class="nav-number">6.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%EF%BC%88Gradient-boosting%EF%BC%89"><span class="nav-number">6.2.</span> <span class="nav-text">梯度提升（Gradient boosting）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-4"><span class="nav-number">6.3.</span> <span class="nav-text">举例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4"><span class="nav-number">6.3.1.</span> <span class="nav-text">参数调整</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA-3"><span class="nav-number">6.4.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B3%84%E9%9C%B2%EF%BC%88Data-Leakage%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">数据泄露（Data Leakage）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-6"><span class="nav-number">7.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%B3%84%E6%BC%8F"><span class="nav-number">7.1.1.</span> <span class="nav-text">目标泄漏</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%97%E8%BD%A6%E6%B5%8B%E8%AF%95%E6%B1%A1%E6%9F%93"><span class="nav-number">7.1.2.</span> <span class="nav-text">列车测试污染</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-5"><span class="nav-number">7.2.</span> <span class="nav-text">举例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA-4"><span class="nav-number">7.3.</span> <span class="nav-text">结论</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">220</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习（中级） | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习（中级）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-07 10:20:32" itemprop="dateCreated datePublished" datetime="2024-03-07T10:20:32+08:00">2024-03-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>31 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><ul>
<li>处理现实世界数据集中常见的数据类型（缺失值、分类变量）。</li>
<li>设计管道以提高机器学习代码的质量。</li>
<li>使用先进的技术进行模型验证（交叉验证）。</li>
<li>构建最先进的模型，广泛用于赢得<code>Kaggle</code>比赛(<code>XGBoost</code>)。</li>
<li>避免常见且重要的数据科学错误（泄漏）。</li>
</ul>
<h4 id="缺失值（Missing-Values）"><a href="#缺失值（Missing-Values）" class="headerlink" title="缺失值（Missing Values）"></a>缺失值（Missing Values）</h4><p>您将学习三种处理缺失值的方法。然后，您将在现实数据集上比较这些方法的有效性。</p>
<span id="more"></span>

<h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h5><p>数据最终可能会出现缺失值的情况有很多。例如：</p>
<ul>
<li>两居室的房子不包括第三间卧室的尺寸值。</li>
<li>调查受访者可以选择不分享他的收入。</li>
</ul>
<p>如果您尝试使用缺失值的数据构建模型，大多数机器学习库（包括<code>scikit-learn</code>）都会出错。因此，您需要选择以下策略之一。</p>
<h5 id="缺失值处理的三种方法"><a href="#缺失值处理的三种方法" class="headerlink" title="缺失值处理的三种方法"></a>缺失值处理的三种方法</h5><h6 id="1-删除缺失值的列"><a href="#1-删除缺失值的列" class="headerlink" title="1.删除缺失值的列"></a>1.删除缺失值的列</h6><p>最简单的选择是删除缺失值的列。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_1.png" class="">

<p>除非删除的列中的大多丢失数值，否则模型将无法使用此方法访问大量（可能有用！）信息。作为一个极端的示例，请考虑一个包含<code>10,000</code>行的数据集，其中一个重要列缺少单个条目。这种方法会完全删除该列！</p>
<h6 id="2-更好的选择：插补"><a href="#2-更好的选择：插补" class="headerlink" title="2.更好的选择：插补"></a>2.更好的选择：插补</h6><p>插补用一些数字填充缺失值。例如，我们可以填写每列的平均值。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_2.png" class="">

<p>在大多数情况下，估算值并不完全正确，但与完全删除列相比，它通常会产生更准确的模型。</p>
<h6 id="3-插补的扩展"><a href="#3-插补的扩展" class="headerlink" title="3.插补的扩展"></a>3.插补的扩展</h6><p>插补是标准方法，通常效果很好。但是，估算值可能系统地高于或低于其实际值（未在数据集中收集）。或者，具有缺失值的行可能以其他方式是唯一的。在这种情况下，您的模型将通过考虑最初丢失的值来做出更好的预测。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_3.png" class="">

<p>在这种方法中，我们像以前一样估算缺失值。此外，对于原始数据集中缺少条目的每一列，我们添加一个新列来显示估算条目的位置。就我而言，这将显着改善结果。在其他情况下，它根本没有帮助。</p>
<h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p>在示例中，我们将使用墨尔本住房数据集。我们的模型将使用房间数量和土地面积等信息来预测房价。我们不会关注数据加载步骤。相反，您可以想象您已经在<code>X_train、X_valid、y_train 和 y_valid</code>中拥有训练和验证数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select target</span></span><br><span class="line">y = data.Price</span><br><span class="line"></span><br><span class="line"><span class="comment"># To keep things simple, we&#x27;ll use only numerical predictors</span></span><br><span class="line">melb_predictors = data.drop([<span class="string">&#x27;Price&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">X = melb_predictors.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Divide data into training and validation subsets</span></span><br><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=<span class="number">0.8</span>, test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                      random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h6 id="定义衡量每种方法质量的函数"><a href="#定义衡量每种方法质量的函数" class="headerlink" title="定义衡量每种方法质量的函数"></a>定义衡量每种方法质量的函数</h6><p>我们定义一个函数<code>Score_dataset()</code>来比较处理缺失值的不同方法。此函数报告随机森林模型的平均绝对误差(<code>MAE</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function for comparing different approaches</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">score_dataset</span>(<span class="params">X_train, X_valid, y_train, y_valid</span>):</span><br><span class="line">    model = RandomForestRegressor(n_estimators=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    preds = model.predict(X_valid)</span><br><span class="line">    <span class="keyword">return</span> mean_absolute_error(y_valid, preds)</span><br></pre></td></tr></table></figure>
<h6 id="方法一的得分（删除具有缺失值的列）"><a href="#方法一的得分（删除具有缺失值的列）" class="headerlink" title="方法一的得分（删除具有缺失值的列）"></a>方法一的得分（删除具有缺失值的列）</h6><p>由于我们同时使用训练集和验证集，因此我们会小心地在两个<code>DataFrame</code>中删除相同的列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get names of columns with missing values</span></span><br><span class="line">cols_with_missing = [col <span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns</span><br><span class="line">                     <span class="keyword">if</span> X_train[col].isnull().<span class="built_in">any</span>()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Drop columns in training and validation data</span></span><br><span class="line">reduced_X_train = X_train.drop(cols_with_missing, axis=<span class="number">1</span>)</span><br><span class="line">reduced_X_valid = X_valid.drop(cols_with_missing, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 1 (Drop columns with missing values):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 1 (Drop columns with missing values):</span><br><span class="line">183550.22137772635</span><br></pre></td></tr></table></figure>
<h6 id="方法二的得分（插补）"><a href="#方法二的得分（插补）" class="headerlink" title="方法二的得分（插补）"></a>方法二的得分（插补）</h6><p>接下来，我们使用<code>SimpleImputer</code>将缺失值替换为每列的平均值。虽然很简单，但填充平均值通常效果很好（但这因数据集而异）。虽然统计学家尝试了更复杂的方法来确定估算值（例如回归估算），但一旦将结果插入复杂的机器学习模型，复杂的策略通常不会带来额外的好处。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation</span></span><br><span class="line">my_imputer = SimpleImputer()</span><br><span class="line">imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))</span><br><span class="line">imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation removed column names; put them back</span></span><br><span class="line">imputed_X_train.columns = X_train.columns</span><br><span class="line">imputed_X_valid.columns = X_valid.columns</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 2 (Imputation):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 2 (Imputation):</span><br><span class="line">178166.46269899711</span><br></pre></td></tr></table></figure>
<p>我们看到方法<code>2</code>的<code>MAE</code>低于方法<code>1</code>，因此方法<code>2</code>在此数据集上表现更好。</p>
<h6 id="方法三的得分（插补的扩展）"><a href="#方法三的得分（插补的扩展）" class="headerlink" title="方法三的得分（插补的扩展）"></a>方法三的得分（插补的扩展）</h6><p>接下来，我们估算缺失值，同时还跟踪估算了哪些值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make copy to avoid changing original data (when imputing)</span></span><br><span class="line">X_train_plus = X_train.copy()</span><br><span class="line">X_valid_plus = X_valid.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make new columns indicating what will be imputed</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols_with_missing:</span><br><span class="line">    X_train_plus[col + <span class="string">&#x27;_was_missing&#x27;</span>] = X_train_plus[col].isnull()</span><br><span class="line">    X_valid_plus[col + <span class="string">&#x27;_was_missing&#x27;</span>] = X_valid_plus[col].isnull()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation</span></span><br><span class="line">my_imputer = SimpleImputer()</span><br><span class="line">imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))</span><br><span class="line">imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation removed column names; put them back</span></span><br><span class="line">imputed_X_train_plus.columns = X_train_plus.columns</span><br><span class="line">imputed_X_valid_plus.columns = X_valid_plus.columns</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 3 (An Extension to Imputation):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 3 (An Extension to Imputation):</span><br><span class="line">178927.503183954</span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，方法<code>3</code>的表现比方法<code>2</code>稍差。</p>
<h6 id="那么，为什么插补比删除列表现更好呢？"><a href="#那么，为什么插补比删除列表现更好呢？" class="headerlink" title="那么，为什么插补比删除列表现更好呢？"></a>那么，为什么插补比删除列表现更好呢？</h6><p>训练数据有<code>10864</code>行和<code>12</code>列，其中<code>3</code>列包含缺失数据。对于每一列，缺失的条目不到一半。因此，删除列会删除很多有用的信息，因此插补会表现得更好是有道理的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shape of training data (num_rows, num_columns)</span></span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of missing values in each column of training data</span></span><br><span class="line">missing_val_count_by_column = (X_train.isnull().<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(missing_val_count_by_column[missing_val_count_by_column &gt; <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(10864, 12)</span><br><span class="line">Car               49</span><br><span class="line">BuildingArea    5156</span><br><span class="line">YearBuilt       4307</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>通常，相对于我们简单地删除包含缺失值的列（在方法<code>1</code>中），估算缺失值（在方法<code>2</code>和方法<code>3</code>中）会产生更好的结果。</p>
<h4 id="分类变量（Categorical-Variables）"><a href="#分类变量（Categorical-Variables）" class="headerlink" title="分类变量（Categorical Variables）"></a>分类变量（Categorical Variables）</h4><h5 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h5><p>分类变量仅采用有限数量的值。</p>
<ul>
<li>考虑一项调查，询问您吃早餐的频率，并提供四个选项：“从不”、“很少”、“大多数天”或“每天”。在这种情况下，数据是分类的，因为响应属于一组固定的类别。</li>
<li>如果人们回答关于他们拥有什么品牌的汽车的调查，那么回答将分为“本田”、“丰田”和“福特”等类别。在这种情况下，数据也是分类的。</li>
</ul>
<p>如果您尝试将这些变量插入到<code>Python</code>中的大多数机器学习模型中而不首先对其进行预处理，则会出现错误。我们将比较可用于准备分类数据的三种方法。</p>
<h5 id="三种方法"><a href="#三种方法" class="headerlink" title="三种方法"></a>三种方法</h5><h6 id="删除分类变量"><a href="#删除分类变量" class="headerlink" title="删除分类变量"></a>删除分类变量</h6><p>处理分类变量的最简单方法是将它们从数据集中删除。仅当列不包含有用信息时，此方法才有效。</p>
<h6 id="序数编码"><a href="#序数编码" class="headerlink" title="序数编码"></a>序数编码</h6><p>序数编码将每个唯一值分配给不同的整数。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_4.png" class="">

<p>此方法假设类别的顺序为：“从不”<code>(0)</code>&lt;“很少”<code>(1)</code>&lt;“大多数天”<code>(2)</code>&lt;“每天”<code>(3)</code>。这个假设在这个例子中是有意义的，因为类别有无可争议的排名。并非所有类别变量的值都有明确的排序，但我们将那些具有明确排序的变量称为序数变量。对于基于树的模型（例如决策树和随机森林），您可以期望序数编码能够很好地处理序数变量。</p>
<h6 id="一次性编码"><a href="#一次性编码" class="headerlink" title="一次性编码"></a>一次性编码</h6><p><code>One-hot</code>编码创建新列，指示原始数据中每个可能值的存在（或不存在）。为了理解这一点，我们将通过一个示例来进行操作。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_5.png" class="">

<p>在原始数据集中，“颜色”是一个分类变量，具有三个类别：“红色”、“黄色”和“绿色”。相应的<code>one-hot</code>编码包含原始数据集中每个可能值的一列和每一行的一行。只要原始值为“<code>Red</code>”，我们就在“<code>Red</code>”列中输入<code>1</code>； 如果原始值为“黄色”，我们在“黄色”列中输入<code>1</code>，依此类推。与序数编码相反，<code>one-hot</code>编码不假设类别的顺序。因此，如果分类数据中没有明确的排序（例如，“红色”既不大于也不小于“黄色”），您可以预期这种方法会特别有效。我们将没有内在排名的分类变量称为名义变量。如果分类变量采用大量值（即，您通常不会将其用于采用超过<code>15</code>个不同值的变量），<code>One-hot</code>编码通常表现不佳。</p>
<h6 id="举例-1"><a href="#举例-1" class="headerlink" title="举例"></a>举例</h6><p>我们将使用墨尔本住房数据集。我们不会关注数据加载步骤。相反，您可以想象您已经在<code>X_train、X_valid、y_train</code>和<code>y_valid</code>中拥有训练和验证数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate target from predictors</span></span><br><span class="line">y = data.Price</span><br><span class="line">X = data.drop([<span class="string">&#x27;Price&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Divide data into training and validation subsets</span></span><br><span class="line">X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=<span class="number">0.8</span>, test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                                random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Drop columns with missing values (simplest approach)</span></span><br><span class="line">cols_with_missing = [col <span class="keyword">for</span> col <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> X_train_full[col].isnull().<span class="built_in">any</span>()] </span><br><span class="line">X_train_full.drop(cols_with_missing, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">X_valid_full.drop(cols_with_missing, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;Cardinality&quot; means the number of unique values in a column</span></span><br><span class="line"><span class="comment"># Select categorical columns with relatively low cardinality (convenient but arbitrary)</span></span><br><span class="line">low_cardinality_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> X_train_full[cname].nunique() &lt; <span class="number">10</span> <span class="keyword">and</span> </span><br><span class="line">                        X_train_full[cname].dtype == <span class="string">&quot;object&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select numerical columns</span></span><br><span class="line">numerical_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> X_train_full[cname].dtype <span class="keyword">in</span> [<span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep selected columns only</span></span><br><span class="line">my_cols = low_cardinality_cols + numerical_cols</span><br><span class="line">X_train = X_train_full[my_cols].copy()</span><br><span class="line">X_valid = X_valid_full[my_cols].copy()</span><br></pre></td></tr></table></figure>
<p>我们使用下面的<code>head()</code>方法查看训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.head()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_6.png" class="">

<p>接下来，我们获得训练数据中所有分类变量的列表。我们通过检查每列的数据类型（或<code>dtype</code>）来做到这一点。对象数据类型指示列有文本（理论上它还可以是其他东西，但这对我们的目的来说并不重要）。对于此数据集，带有文本的列表示分类变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get list of categorical variables</span></span><br><span class="line">s = (X_train.dtypes == <span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">object_cols = <span class="built_in">list</span>(s[s].index)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Categorical variables:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(object_cols)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Categorical variables:</span><br><span class="line">[<span class="string">&#x27;Type&#x27;</span>, <span class="string">&#x27;Method&#x27;</span>, <span class="string">&#x27;Regionname&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h6 id="定义衡量每种方法质量的函数-1"><a href="#定义衡量每种方法质量的函数-1" class="headerlink" title="定义衡量每种方法质量的函数"></a>定义衡量每种方法质量的函数</h6><p>我们定义一个函数<code>Score_dataset()</code>来比较处理<code>calcategori</code>变量的三种不同方法。此函数报告随机森林模型的平均绝对误差(<code>MAE</code>)。一般来说，我们希望<code>MAE</code>尽可能低！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function for comparing different approaches</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">score_dataset</span>(<span class="params">X_train, X_valid, y_train, y_valid</span>):</span><br><span class="line">    model = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    preds = model.predict(X_valid)</span><br><span class="line">    <span class="keyword">return</span> mean_absolute_error(y_valid, preds)</span><br></pre></td></tr></table></figure>
<h6 id="方法一的得分（删除类别变量）"><a href="#方法一的得分（删除类别变量）" class="headerlink" title="方法一的得分（删除类别变量）"></a>方法一的得分（删除类别变量）</h6><p>我们使用<code>select_dtypes()</code>方法删除对象列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drop_X_train = X_train.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>])</span><br><span class="line">drop_X_valid = X_valid.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 1 (Drop categorical variables):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 1 (Drop categorical variables):</span><br><span class="line">175703.48185157913</span><br></pre></td></tr></table></figure>
<h6 id="方法二的得分（序数编码）"><a href="#方法二的得分（序数编码）" class="headerlink" title="方法二的得分（序数编码）"></a>方法二的得分（序数编码）</h6><p><code>Scikit-learn</code>有一个<code>OrdinalEncoder</code>类，可用于获取序数编码。我们循环分类变量并将序数编码器分别应用于每一列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make copy to avoid changing original data </span></span><br><span class="line">label_X_train = X_train.copy()</span><br><span class="line">label_X_valid = X_valid.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply ordinal encoder to each column with categorical data</span></span><br><span class="line">ordinal_encoder = OrdinalEncoder()</span><br><span class="line">label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])</span><br><span class="line">label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 2 (Ordinal Encoding):&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(score_dataset(label_X_train, label_X_valid, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 2 (Ordinal Encoding):</span><br><span class="line">165936.40548390493</span><br></pre></td></tr></table></figure>
<p>在上面的代码单元中，对于每一列，我们将每个唯一值随机分配给不同的整数，这是一种常见的方法，比提供自定义标签更简单；然而，如果我们为所有序数变量提供更明智的标签，我们可以期待性能的进一步提升。</p>
<h6 id="方法三的得分（One-Hot-编码）"><a href="#方法三的得分（One-Hot-编码）" class="headerlink" title="方法三的得分（One-Hot 编码）"></a>方法三的得分（One-Hot 编码）</h6><p>我们使用<code>scikit-learn</code>中的<code>OneHotEncoder</code>类来获取<code>one-hot</code>编码。有许多参数可用于自定义其行为。</p>
<ul>
<li>我们设置<code>handle_unknown =&#39;ignore&#39;</code>以避免当验证数据包含训练数据中未表示的类时出现错误。</li>
<li>设置稀疏<code>= False</code>确保编码列作为<code>numpy</code>数组（而不是稀疏矩阵）返回。</li>
</ul>
<p>为了使用编码器，我们只提供我们想要进行<code>one-hot</code>编码的分类列。例如，为了对训练数据进行编码，我们提供<code>X_train[object_cols]</code>。（下面代码单元中的<code>object_cols</code>是包含分类数据的列名称列表，因此<code>X_train[object_cols]</code>包含训练集中的所有分类数据。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply one-hot encoder to each column with categorical data</span></span><br><span class="line">OH_encoder = OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>, sparse=<span class="literal">False</span>)</span><br><span class="line">OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))</span><br><span class="line">OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding removed index; put it back</span></span><br><span class="line">OH_cols_train.index = X_train.index</span><br><span class="line">OH_cols_valid.index = X_valid.index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove categorical columns (will replace with one-hot encoding)</span></span><br><span class="line">num_X_train = X_train.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line">num_X_valid = X_valid.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add one-hot encoded columns to numerical features</span></span><br><span class="line">OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=<span class="number">1</span>)</span><br><span class="line">OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensure all columns have string type</span></span><br><span class="line">OH_X_train.columns = OH_X_train.columns.astype(<span class="built_in">str</span>)</span><br><span class="line">OH_X_valid.columns = OH_X_valid.columns.astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE from Approach 3 (One-Hot Encoding):&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MAE from Approach 3 (One-Hot Encoding):</span><br><span class="line">166089.4893009678</span><br></pre></td></tr></table></figure>
<h5 id="哪种方法最好？"><a href="#哪种方法最好？" class="headerlink" title="哪种方法最好？"></a>哪种方法最好？</h5><p>在这种情况下，删除分类列（方法<code>1</code>）效果最差，因为它的<code>MAE</code>得分最高。至于其他两种方法，由于返回的<code>MAE</code>分数的值非常接近，因此其中一种方法似乎没有比另一种方法有任何有意义的好处。一般来说，<code>one-hot</code>编码（方法<code>3</code>）通常会表现最佳，而删除分类列（方法<code>1</code>）通常表现最差，但具体情况会有所不同。</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>世界充满了分类数据。如果您知道如何使用这种常见数据类型，您将成为一名更高效的数据科学家！</p>
<h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><h5 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h5><p>管道是保持数据预处理和建模代码井井有条的简单方法。具体来说，管道捆绑了预处理和建模步骤，因此您可以像使用单个步骤一样使用整个捆绑包。许多数据科学家在没有管道的情况下组合模型，但管道有一些重要的好处。其中包括：</p>
<ul>
<li>更清晰的代码：在预处理的每个步骤中计算数据可能会变得混乱。使用管道，您无需在每个步骤中手动跟踪训练和验证数据。</li>
<li>错误更少：误用步骤或忘记预处理步骤的机会更少。</li>
<li>更容易生产：将模型从原型转变为可大规模部署的模型可能非常困难。我们不会在这里讨论许多相关的问题，但管道可以提供帮助。</li>
<li>模型验证的更多选项</li>
</ul>
<h5 id="举例-2"><a href="#举例-2" class="headerlink" title="举例"></a>举例</h5><p>我们将使用墨尔本住房数据集。我们不会关注数据加载步骤。相反，您可以想象您已经在<code>X_train、X_valid、y_train</code>和<code>y_valid</code>中拥有训练和验证数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate target from predictors</span></span><br><span class="line">y = data.Price</span><br><span class="line">X = data.drop([<span class="string">&#x27;Price&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Divide data into training and validation subsets</span></span><br><span class="line">X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=<span class="number">0.8</span>, test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                                random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;Cardinality&quot; means the number of unique values in a column</span></span><br><span class="line"><span class="comment"># Select categorical columns with relatively low cardinality (convenient but arbitrary)</span></span><br><span class="line">categorical_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> X_train_full[cname].nunique() &lt; <span class="number">10</span> <span class="keyword">and</span> </span><br><span class="line">                        X_train_full[cname].dtype == <span class="string">&quot;object&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select numerical columns</span></span><br><span class="line">numerical_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> X_train_full[cname].dtype <span class="keyword">in</span> [<span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep selected columns only</span></span><br><span class="line">my_cols = categorical_cols + numerical_cols</span><br><span class="line">X_train = X_train_full[my_cols].copy()</span><br><span class="line">X_valid = X_valid_full[my_cols].copy()</span><br></pre></td></tr></table></figure>
<p>我们使用下面的<code>head()</code>方法查看训练数据。请注意，数据包含分类数据和具有缺失值的列。有了管道，就可以轻松处理这两件事！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.head()</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_7.png" class="">
<p>我们分三步构建完整的管道。</p>
<h6 id="第一步：定义预处理步骤"><a href="#第一步：定义预处理步骤" class="headerlink" title="第一步：定义预处理步骤"></a>第一步：定义预处理步骤</h6><p>与管道如何将预处理和建模步骤捆绑在一起类似，我们使用<code>ColumnTransformer</code>类将不同的预处理步骤捆绑在一起。代码如下：</p>
<ul>
<li>估算数值数据中的缺失值。</li>
<li>估算缺失值并对分类数据应用<code>one-hot</code>编码。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for numerical data</span></span><br><span class="line">numerical_transformer = SimpleImputer(strategy=<span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for categorical data</span></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;onehot&#x27;</span>, OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing for numerical and categorical data</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numerical_transformer, numerical_cols),</span><br><span class="line">        (<span class="string">&#x27;cat&#x27;</span>, categorical_transformer, categorical_cols)</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<h6 id="第二步：定义模型"><a href="#第二步：定义模型" class="headerlink" title="第二步：定义模型"></a>第二步：定义模型</h6><p>我们使用熟悉的<code>RandomForestRegressor</code>类定义随机森林模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line">model = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h6 id="第三步：创建并评估管道"><a href="#第三步：创建并评估管道" class="headerlink" title="第三步：创建并评估管道"></a>第三步：创建并评估管道</h6><p>最后，我们使用<code>Pipeline</code>类来定义捆绑预处理和建模步骤的管道。有一些重要的事情需要注意：</p>
<ul>
<li>通过管道，我们预处理训练数据并在一行代码中拟合模型。（相反，如果没有管道，我们必须在单独的步骤中进行插补、<code>one-hot</code>编码和模型训练。如果我们必须处理数值变量和分类变量，这会变得特别混乱！）</li>
<li>通过管道，我们将<code>X_valid</code>中未处理的特征提供给<code>Predict()</code>命令，管道在生成预测之前自动预处理这些特征。（但是，如果没有管道，我们必须记住在进行预测之前对验证数据进行预处理。）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing and modeling code in a pipeline</span></span><br><span class="line">my_pipeline = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">                              (<span class="string">&#x27;model&#x27;</span>, model)</span><br><span class="line">                             ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of training data, fit model </span></span><br><span class="line">my_pipeline.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of validation data, get predictions</span></span><br><span class="line">preds = my_pipeline.predict(X_valid)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">score = mean_absolute_error(y_valid, preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE:&#x27;</span>, score)</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAE: 160679.18917034855</span><br></pre></td></tr></table></figure>
<h6 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h6><p>管道对于清理机器学习代码和避免错误非常有价值，对于具有复杂数据预处理的工作流程尤其有用。</p>
<h4 id="交叉验证（Cross-Validation）"><a href="#交叉验证（Cross-Validation）" class="headerlink" title="交叉验证（Cross-Validation）"></a>交叉验证（Cross-Validation）</h4><h5 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h5><p>机器学习是一个迭代过程。您将面临有关使用哪些预测变量、使用什么类型的模型、为这些模型提供哪些参数等的选择。到目前为止，您已经通过验证来衡量模型质量，以数据驱动的方式做出了这些选择（ 或坚持）设置。但这种方法有一些缺点。 要看到这一点，假设您有一个包含<code>5000</code>行的数据集。您通常会保留大约<code>20%</code>的数据作为验证数据集，即<code>1000</code>行。但这在确定模型分数时留下了一些随机机会。也就是说，模型可能在一组<code>1000</code>行上表现良好，即使它在不同的<code>1000</code>行上可能不准确。在极端情况下，您可以想象验证集中只有<code>1</code>行数据。如果您比较其他模型，哪个模型对单个数据点做出最好的预测将主要取决于运气！一般来说，验证集越大，我们衡量模型质量的随机性（也称为“噪声”）就越少，并且越可靠。不幸的是，我们只能通过从训练数据中删除行来获得大的验证集，而较小的训练数据集意味着更差的模型！</p>
<h5 id="什么是交叉验证？"><a href="#什么是交叉验证？" class="headerlink" title="什么是交叉验证？"></a>什么是交叉验证？</h5><p>在交叉验证中，我们对不同的数据子集运行建模过程，以获得模型质量的多种度量。例如，我们可以首先将数据分为<code>5</code>部分，每部分占完整数据集的<code>20%</code>。在本例中，我们说我们已将数据分成<code>5</code>个“折叠”。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_8.png" class="">

<p>然后，我们为每个折叠运行一个实验：</p>
<ul>
<li>在实验<code>1</code>中，我们使用第一次折叠作为验证（或保留）集，其他所有内容作为训练数据。这为我们提供了基于<code>20%</code>保留集的模型质量衡量标准。</li>
<li>在实验<code>2</code>中，我们保留第二次折叠中的数据（并使用除第二次折叠之外的所有数据来训练模型）。然后使用保留集来获得模型质量的第二次估计。</li>
<li>我们重复这个过程，使用每个折叠一次作为保留集。 总而言之。<code>100%</code>的数据在某个时刻被用作保留，我们最终得到基于数据集中所有行的模型质量度量（即使我们不同时使用所有行）。</li>
</ul>
<h5 id="什么时候应该使用交叉验证？"><a href="#什么时候应该使用交叉验证？" class="headerlink" title="什么时候应该使用交叉验证？"></a>什么时候应该使用交叉验证？</h5><p>交叉验证可以更准确地衡量模型质量，如果您要做出大量建模决策，这一点尤其重要。但是，它可能需要更长的时间来运行，因为它估计多个模型（每个折叠一个）。那么，考虑到这些权衡，您应该何时使用每种方法？</p>
<ul>
<li>对于小型数据集，额外的计算负担并不是什么大问题，您应该运行交叉验证。</li>
<li>对于较大的数据集，单个验证集就足够了。您的代码将运行得更快，并且您可能拥有足够的数据，几乎不需要重复使用其中的一些数据来保留。</li>
</ul>
<p>对于什么构成大数据集和小数据集，没有简单的阈值。但是，如果您的模型需要几分钟或更短的时间才能运行，则可能值得切换到交叉验证。或者，您可以运行交叉验证，看看每个实验的分数是否看起来很接近。如果每个实验产生相同的结果，则单个验证集可能就足够了。</p>
<h5 id="举例-3"><a href="#举例-3" class="headerlink" title="举例"></a>举例</h5><p>我们将使用与上一个教程中相同的数据。我们将输入数据加载到<code>X</code>中，将输出数据加载到<code>y</code>中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select subset of predictors</span></span><br><span class="line">cols_to_use = [<span class="string">&#x27;Rooms&#x27;</span>, <span class="string">&#x27;Distance&#x27;</span>, <span class="string">&#x27;Landsize&#x27;</span>, <span class="string">&#x27;BuildingArea&#x27;</span>, <span class="string">&#x27;YearBuilt&#x27;</span>]</span><br><span class="line">X = data[cols_to_use]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select target</span></span><br><span class="line">y = data.Price</span><br></pre></td></tr></table></figure>
<p>然后，我们定义一个管道，使用输入器来填充缺失值，并使用随机森林模型来进行预测。虽然可以在没有管道的情况下进行交叉验证，但这非常困难！使用管道将使代码变得非常简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line">my_pipeline = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, SimpleImputer()),</span><br><span class="line">                              (<span class="string">&#x27;model&#x27;</span>, RandomForestRegressor(n_estimators=<span class="number">50</span>,</span><br><span class="line">                                                              random_state=<span class="number">0</span>))])</span><br></pre></td></tr></table></figure>
<p>我们使用<code>scikit-learn</code>中的<code>cross_val_score()</code>函数获取交叉验证分数。我们使用<code>cv</code>参数设置折叠次数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiply by -1 since sklearn calculates *negative* MAE</span></span><br><span class="line">scores = -<span class="number">1</span> * cross_val_score(my_pipeline, X, y,</span><br><span class="line">                              cv=<span class="number">5</span>,</span><br><span class="line">                              scoring=<span class="string">&#x27;neg_mean_absolute_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE scores:\n&quot;</span>, scores)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAE scores:[301628.7893587  303164.4782723  287298.331666   236061.84754543 260383.45111427]</span><br></pre></td></tr></table></figure>
<p>评分参数选择要报告的模型质量度量：在本例中，我们选择负平均绝对误差 (<code>MAE</code>)。<code>scikit-learn</code>的文档显示了选项列表。我们指定负<code>MAE</code>有点令人惊讶。<code>Scikit-learn</code>有一个约定，其中定义了所有指标，因此数字越大越好。在这里使用负数可以使它们与该约定保持一致，尽管负<code>MAE</code>在其他地方几乎闻所未闻。我们通常需要单一的模型质量度量来比较替代模型。所以我们取实验的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average MAE score (across experiments):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(scores.mean())</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Average MAE score (across experiments):</span><br><span class="line">277707.3795913405</span><br></pre></td></tr></table></figure>
<h5 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h5><p>使用交叉验证可以更好地衡量模型质量，并具有清理代码的额外好处：请注意，我们不再需要跟踪单独的训练集和验证集。因此，特别是对于小型数据集，这是一个很好的改进！</p>
<h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h4><p>您将学习如何使用<strong>梯度提升</strong>来构建和优化模型。该方法在许多<code>Kaggle</code>竞赛中占据主导地位，并在各种数据集上取得了成果。</p>
<h5 id="介绍-5"><a href="#介绍-5" class="headerlink" title="介绍"></a>介绍</h5><p>您已经使用随机森林方法进行了预测，该方法通过对许多决策树的预测进行平均来实现比单个决策树更好的性能。我们将随机森林方法称为“集成方法”。根据定义，集成方法结合了多个模型的预测（例如，在随机森林的情况下是多个树）。接下来，我们将学习另一种称为梯度提升的集成方法。</p>
<h5 id="梯度提升（Gradient-boosting）"><a href="#梯度提升（Gradient-boosting）" class="headerlink" title="梯度提升（Gradient boosting）"></a>梯度提升（Gradient boosting）</h5><p><strong>梯度提升</strong>是一种通过循环迭代将模型添加到集成中的方法。它首先使用单个模型初始化集成，该模型的预测可能非常幼稚。（即使它的预测非常不准确，随后对集合的添加也将解决这些错误。）然后，我们开始循环：</p>
<ul>
<li>首先，我们使用当前的集合来为数据集中的每个观察生成预测。为了进行预测，我们将集合中所有模型的预测相加。</li>
<li>这些预测用于计算损失函数（例如均方误差）。</li>
<li>然后，我们使用损失函数来拟合将添加到集成中的新模型。具体来说，我们确定模型参数，以便将这个新模型添加到集成中将减少损失。（旁注：“梯度提升”中的“梯度”指的是我们将在损失函数上使用梯度下降来确定这个新模型中的参数。）。</li>
<li>最后，我们将新模型添加到集成中。</li>
<li>重复迭代。<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_9.png" class=""></li>
</ul>
<h5 id="举例-4"><a href="#举例-4" class="headerlink" title="举例"></a>举例</h5><p>我们首先在 X_train、X_valid、y_train 和 y_valid 中加载训练和验证数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select subset of predictors</span></span><br><span class="line">cols_to_use = [<span class="string">&#x27;Rooms&#x27;</span>, <span class="string">&#x27;Distance&#x27;</span>, <span class="string">&#x27;Landsize&#x27;</span>, <span class="string">&#x27;BuildingArea&#x27;</span>, <span class="string">&#x27;YearBuilt&#x27;</span>]</span><br><span class="line">X = data[cols_to_use]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select target</span></span><br><span class="line">y = data.Price</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate data into training and validation sets</span></span><br><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X, y)</span><br></pre></td></tr></table></figure>
<p>在此示例中，您将使用<code>XGBoost</code>库。<code>XGBoost</code>代表极限梯度提升，它是梯度提升的一种实现，具有一些注重性能和速度的附加功能。（<code>Scikit-learn</code>有另一个版本的梯度提升，但<code>XGBoost</code>有一些技术优势。）在下一个代码单元中，我们导入<code>XGBoost</code>的<code>scikit-learn API(xgboost.XGBRegressor)</code>。这使我们能够像在<code>scikit-learn</code>中一样构建和拟合模型。正如您将在输出中看到的，<code>XGBRegressor</code>类有许多可调参数——您很快就会了解这些！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"></span><br><span class="line">my_model = XGBRegressor(random_state=<span class="number">0</span>)</span><br><span class="line">my_model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">XGBRegressor(base_score=0.5, booster=<span class="string">&#x27;gbtree&#x27;</span>, callbacks=None,</span><br><span class="line">             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,</span><br><span class="line">             early_stopping_rounds=None, enable_categorical=False,</span><br><span class="line">             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=<span class="string">&#x27;depthwise&#x27;</span>,</span><br><span class="line">             importance_type=None, interaction_constraints=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,</span><br><span class="line">             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,</span><br><span class="line">             missing=nan, monotone_constraints=<span class="string">&#x27;()&#x27;</span>, n_estimators=100, n_jobs=0,</span><br><span class="line">             num_parallel_tree=1, predictor=<span class="string">&#x27;auto&#x27;</span>, random_state=0, reg_alpha=0,</span><br><span class="line">             reg_lambda=1, ...)</span><br></pre></td></tr></table></figure>
<p>我们还进行预测并评估模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line">predictions = my_model.predict(X_valid)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Absolute Error: &quot;</span> + <span class="built_in">str</span>(mean_absolute_error(predictions, y_valid)))</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mean Absolute Error: 241041.5160392121</span><br></pre></td></tr></table></figure>
<h6 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h6><p><code>XGBoost</code>有一些参数可以显着影响准确性和训练速度。您应该了解的第一个参数是：<code>n_estimators</code>指定经历上述建模周期的次数。它等于我们包含在集成中的模型数量。</p>
<ul>
<li>值太低会导致欠拟合，从而导致训练数据和测试数据的预测不准确。</li>
<li>太高的值会导致过拟合，从而导致对训练数据的预测准确，但对测试数据的预测不准确（这是我们关心的）。</li>
</ul>
<p>典型值范围为<code>100-1000</code>，但这在很大程度上取决于下面讨论的<code>learning_rate</code>参数。以下是设置集成中模型数量的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_model = XGBRegressor(n_estimators=<span class="number">500</span>)</span><br><span class="line">my_model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">XGBRegressor(base_score=0.5, booster=<span class="string">&#x27;gbtree&#x27;</span>, callbacks=None,</span><br><span class="line">             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,</span><br><span class="line">             early_stopping_rounds=None, enable_categorical=False,</span><br><span class="line">             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=<span class="string">&#x27;depthwise&#x27;</span>,</span><br><span class="line">             importance_type=None, interaction_constraints=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,</span><br><span class="line">             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,</span><br><span class="line">             missing=nan, monotone_constraints=<span class="string">&#x27;()&#x27;</span>, n_estimators=500, n_jobs=0,</span><br><span class="line">             num_parallel_tree=1, predictor=<span class="string">&#x27;auto&#x27;</span>, random_state=0, reg_alpha=0,</span><br><span class="line">             reg_lambda=1, ...)</span><br></pre></td></tr></table></figure>
<p><code>Early_stopping_rounds</code>提供了一种自动查找<code>n_estimators</code>理想值的方法。当验证分数停止提高时，提前停止会导致模型停止迭代，即使我们没有处于<code>n_estimators</code>的硬停止状态。明智的做法是为<code>n_estimators</code>设置一个较高的值，然后使用<code>Early_stopping_rounds</code>来找到停止迭代的最佳时间。由于随机机会有时会导致单轮验证分数没有提高，因此您需要指定一个数字，表示在停止之前允许进行多少轮直接恶化。设置<code>early_stopping_rounds=5</code>是一个合理的选择。在这种情况下，我们在连续<code>5</code>轮验证分数恶化后停止。使用<code>early_stopping_rounds时</code>，您还需要留出一些数据来计算验证分数 - 这是通过设置<code>eval_set</code>参数来完成的。</p>
<p>我们可以修改上面的示例以包括提前停止：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_model = XGBRegressor(n_estimators=<span class="number">500</span>)</span><br><span class="line">my_model.fit(X_train, y_train, </span><br><span class="line">             early_stopping_rounds=<span class="number">5</span>, </span><br><span class="line">             eval_set=[(X_valid, y_valid)],</span><br><span class="line">             verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">XGBRegressor(base_score=0.5, booster=<span class="string">&#x27;gbtree&#x27;</span>, callbacks=None,</span><br><span class="line">             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,</span><br><span class="line">             early_stopping_rounds=None, enable_categorical=False,</span><br><span class="line">             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=<span class="string">&#x27;depthwise&#x27;</span>,</span><br><span class="line">             importance_type=None, interaction_constraints=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,</span><br><span class="line">             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,</span><br><span class="line">             missing=nan, monotone_constraints=<span class="string">&#x27;()&#x27;</span>, n_estimators=500, n_jobs=0,</span><br><span class="line">             num_parallel_tree=1, predictor=<span class="string">&#x27;auto&#x27;</span>, random_state=0, reg_alpha=0,</span><br><span class="line">             reg_lambda=1, ...)</span><br></pre></td></tr></table></figure>
<p>如果您稍后想要使用所有数据来拟合模型，请将<code>n_estimators</code>设置为您在早期停止运行时发现的最佳值。我们不是通过简单地将每个组件模型的预测相加来获得预测，而是可以将每个模型的预测乘以一个小数（称为<strong>学习率</strong>），然后再添加它们。这意味着我们添加到集合中的每棵树对我们的帮助都会减少。因此，我们可以为<code>n_estimators</code>设置更高的值而不会过度拟合。如果我们使用提前停止，则会自动确定适当的树木数量。</p>
<p>一般来说，较小的学习率和大量的估计器将产生更准确的<code>XGBoost</code>模型，但模型的训练时间也会更长，因为它在循环中进行了更多的迭代。默认情况下，<code>XGBoost</code>设置<code>learning_rate=0.1</code>。修改上面的示例以更改学习率会产生以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_model = XGBRegressor(n_estimators=<span class="number">1000</span>, learning_rate=<span class="number">0.05</span>)</span><br><span class="line">my_model.fit(X_train, y_train, </span><br><span class="line">             early_stopping_rounds=<span class="number">5</span>, </span><br><span class="line">             eval_set=[(X_valid, y_valid)], </span><br><span class="line">             verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,</span><br><span class="line">             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,</span><br><span class="line">             early_stopping_rounds=None, enable_categorical=False,</span><br><span class="line">             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,</span><br><span class="line">             importance_type=None, interaction_constraints=&#x27;&#x27;,</span><br><span class="line">             learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,</span><br><span class="line">             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,</span><br><span class="line">             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,</span><br><span class="line">             n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,</span><br><span class="line">             reg_alpha=0, reg_lambda=1, ...)</span><br></pre></td></tr></table></figure>
<p>在考虑运行时间的较大数据集上，您可以使用并行性来更快地构建模型。通常将参数<code>n_jobs</code>设置为等于计算机上的核心数。对于较小的数据集，这没有帮助。生成的模型不会更好，因此对拟合时间进行微观优化通常只会分散注意力。但是，它在大型数据集中非常有用，否则您将在<code>fit</code>命令期间等待很长时间。这是修改后的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_model = XGBRegressor(n_estimators=<span class="number">1000</span>, learning_rate=<span class="number">0.05</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line">my_model.fit(X_train, y_train, </span><br><span class="line">             early_stopping_rounds=<span class="number">5</span>, </span><br><span class="line">             eval_set=[(X_valid, y_valid)], </span><br><span class="line">             verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">XGBRegressor(base_score=0.5, booster=<span class="string">&#x27;gbtree&#x27;</span>, callbacks=None,</span><br><span class="line">             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,</span><br><span class="line">             early_stopping_rounds=None, enable_categorical=False,</span><br><span class="line">             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=<span class="string">&#x27;depthwise&#x27;</span>,</span><br><span class="line">             importance_type=None, interaction_constraints=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">             learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,</span><br><span class="line">             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,</span><br><span class="line">             missing=nan, monotone_constraints=<span class="string">&#x27;()&#x27;</span>, n_estimators=1000,</span><br><span class="line">             n_jobs=4, num_parallel_tree=1, predictor=<span class="string">&#x27;auto&#x27;</span>, random_state=0,</span><br><span class="line">             reg_alpha=0, reg_lambda=1, ...)</span><br></pre></td></tr></table></figure>

<h5 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h5><p><code>XGBoost</code>是一个领先的软件库，用于处理标准表格数据（存储在<code>Pandas DataFrame</code>中的数据类型，而不是图像和视频等更奇特的数据类型）。通过仔细调整参数，您可以训练高度准确的模型。</p>
<h4 id="数据泄露（Data-Leakage）"><a href="#数据泄露（Data-Leakage）" class="headerlink" title="数据泄露（Data Leakage）"></a>数据泄露（Data Leakage）</h4><p>您将了解什么是数据泄漏以及如何防止数据泄漏。如果您不知道如何预防，泄漏就会频繁发生，并且会以微妙而危险的方式毁掉您的模型。因此，这是数据科学家实践中最重要的概念之一。</p>
<h5 id="介绍-6"><a href="#介绍-6" class="headerlink" title="介绍"></a>介绍</h5><p>当你的训练数据包含有关目标的信息，但当模型用于预测时，类似的数据将不可用时，就会发生<strong>数据泄漏</strong>（或泄漏）。这会导致训练集（甚至可能是验证数据）上的高性能，但模型在生产中表现不佳。换句话说，泄漏会导致模型看起来很准确，直到您开始使用模型做出决策，然后模型就会变得非常不准确。泄漏主要有两种类型：<strong>目标泄漏</strong>和<strong>列车测试污染</strong>。</p>
<h6 id="目标泄漏"><a href="#目标泄漏" class="headerlink" title="目标泄漏"></a>目标泄漏</h6><p>当您的预测变量包含在您进行预测时不可用的数据时，就会发生<strong>目标泄漏</strong>。重要的是要根据数据可用的时间或时间顺序来考虑目标泄漏，而不仅仅是某个功能是否有助于做出良好的预测。一个例子会有所帮助。 想象一下，您想要预测谁会患上肺炎。原始数据的前几行如下所示：</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_10.png" class="">

<p>人们在患肺炎后服用抗生素药物才能康复。原始数据显示这些列之间存在很强的关系，但在确定<code>got_pneumonia</code>的值后，<code>take_antibiotic_medicine</code>经常发生更改。这就是目标泄漏。该模型会发现，任何<code>take_antibiotic_medicine</code>值为<code>False</code>的人都没有患有肺炎。由于验证数据与训练数据来自同一来源，因此该模式将在验证中重复，并且模型将具有很高的验证（或交叉验证）分数。但当随后在现实世界中部署时，该模型将非常不准确，因为当我们需要预测他们未来的健康状况时，即使是患有肺炎的患者也不会接受抗生素治疗。为了防止这种类型的数据泄漏，应排除在实现目标值后更新（或创建）的任何变量。</p>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_11.png" class="">

<h6 id="列车测试污染"><a href="#列车测试污染" class="headerlink" title="列车测试污染"></a>列车测试污染</h6><p>当您不小心区分训练数据和验证数据时，就会发生另一种类型的泄漏。回想一下，验证旨在衡量模型如何处理之前未考虑过的数据。如果验证数据影响预处理行为，您可能会以微妙的方式破坏此过程。这有时称为<strong>列车测试污染</strong>。例如，假设您在调用<code>train_test_split()</code>之前运行预处理（例如为缺失值拟合输入器）。您的模型可能会获得良好的验证分数，让您对它充满信心，但在部署它来做出决策时却表现不佳。毕竟，您将验证或测试数据中的数据合并到预测中，因此即使无法推广到新数据，也可能在该特定数据上表现良好。当您进行更复杂的特征工程时，这个问题变得更加微妙（也更危险）。如果您的验证基于简单的训练测试分割，请从任何类型的拟合中排除验证数据，包括预处理步骤的拟合。如果您使用<code>scikit-learn</code>管道，这会更容易。使用交叉验证时，在管道内进行预处理更为重要！</p>
<h5 id="举例-5"><a href="#举例-5" class="headerlink" title="举例"></a>举例</h5><p>在本示例中，您将学习一种检测和消除目标泄漏的方法。我们将使用有关信用卡申请的数据集并跳过基本数据设置代码。最终结果是有关每个信用卡申请的信息都存储在<code>DataFrame X</code>中。我们将使用它来预测系列<code>y</code>中接受了哪些申请。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;../input/aer-credit-card-data/AER_credit_card_data.csv&#x27;</span>, </span><br><span class="line">                   true_values = [<span class="string">&#x27;yes&#x27;</span>], false_values = [<span class="string">&#x27;no&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select target</span></span><br><span class="line">y = data.card</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select predictors</span></span><br><span class="line">X = data.drop([<span class="string">&#x27;card&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of rows in the dataset:&quot;</span>, X.shape[<span class="number">0</span>])</span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/iml_12.png" class="">

<p>由于这是一个小数据集，我们将使用交叉验证来确保模型质量的准确测量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since there is no preprocessing, we don&#x27;t need a pipeline (used anyway as best practice!)</span></span><br><span class="line">my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=<span class="number">100</span>))</span><br><span class="line">cv_scores = cross_val_score(my_pipeline, X, y, </span><br><span class="line">                            cv=<span class="number">5</span>,</span><br><span class="line">                            scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cross-validation accuracy: %f&quot;</span> % cv_scores.mean())</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cross-validation accuracy: 0.981052</span><br></pre></td></tr></table></figure>
<p>根据经验，您会发现很难找到准确率达到<code>98%</code>的模型。这种情况确实发生过，但这种情况并不常见，因此我们应该更仔细地检查数据是否存在目标泄漏。以下是数据摘要，您也可以在数据选项卡下找到：</p>
<ul>
<li><code>card: 1 if credit card application accepted, 0 if not</code></li>
<li><code>reports: Number of major derogatory reports</code></li>
<li><code>age: Age n years plus twelfths of a year</code></li>
<li><code>income: Yearly income (divided by 10,000)</code></li>
<li><code>share: Ratio of monthly credit card expenditure to yearly income</code></li>
<li><code>expenditure: Average monthly credit card expenditure</code></li>
<li><code>owner: 1 if owns home, 0 if rents</code></li>
<li><code>selfempl: 1 if self-employed, 0 if not</code></li>
<li><code>dependents: 1 + number of dependents</code></li>
<li><code>months: Months living at current address</code></li>
<li><code>majorcards: Number of major credit cards held</code></li>
<li><code>active: Number of active credit accounts</code></li>
</ul>
<p>一些变量看起来很可疑。例如，支出是指这张卡上的支出还是申请前使用过的卡上的支出？此时，基本数据比较会非常有帮助：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">expenditures_cardholders = X.expenditure[y]</span><br><span class="line">expenditures_noncardholders = X.expenditure[~y]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Fraction of those who did not receive a card and had no expenditures: %.2f&#x27;</span> \</span><br><span class="line">      %((expenditures_noncardholders == <span class="number">0</span>).mean()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Fraction of those who received a card and had no expenditures: %.2f&#x27;</span> \</span><br><span class="line">      %(( expenditures_cardholders == <span class="number">0</span>).mean()))</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fraction of those <span class="built_in">who</span> did not receive a card and had no expenditures: 1.00</span><br><span class="line">Fraction of those <span class="built_in">who</span> received a card and had no expenditures: 0.02</span><br></pre></td></tr></table></figure>
<p>如上所示，所有没有收到卡的人都没有支出，而只有<code>2%</code>的收到卡的人没有支出。我们的模型似乎具有很高的准确性，这并不奇怪。但这似乎也是一种目标泄漏的情况，其中支出可能意味着他们申请的卡上的支出。由于份额部分由支出决定，因此也应排除在外。 变量<code>active</code>和<code>Majorcards</code>不太清楚，但从描述来看，它们听起来令人担忧。在大多数情况下，如果您无法追踪创建数据的人以了解更多信息，那么安全总比后悔好。我们将运行一个没有目标泄漏的模型，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop leaky predictors from dataset</span></span><br><span class="line">potential_leaks = [<span class="string">&#x27;expenditure&#x27;</span>, <span class="string">&#x27;share&#x27;</span>, <span class="string">&#x27;active&#x27;</span>, <span class="string">&#x27;majorcards&#x27;</span>]</span><br><span class="line">X2 = X.drop(potential_leaks, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model with leaky predictors removed</span></span><br><span class="line">cv_scores = cross_val_score(my_pipeline, X2, y, </span><br><span class="line">                            cv=<span class="number">5</span>,</span><br><span class="line">                            scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cross-val accuracy: %f&quot;</span> % cv_scores.mean())</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cross-val accuracy: 0.830919</span><br></pre></td></tr></table></figure>
<p>这个准确度相当低，这可能会令人失望。然而，我们可以预期，当在新应用程序中使用时，它的正确率约为<code>80%</code>，而泄漏模型的表现可能会比这差得多（尽管其在交叉验证中的明显得分更高）。</p>
<h5 id="结论-4"><a href="#结论-4" class="headerlink" title="结论"></a>结论</h5><p>在许多数据科学应用中，数据泄漏可能会造成数百万美元的错误。仔细分离训练和验证数据可以防止训练测试污染，而管道可以帮助实现这种分离。同样，谨慎、常识和数据探索的结合可以帮助识别目标泄漏。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/" title="机器学习（中级）">https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/05/artificial-intelligence/Machine_Learning_Explainability_study/" rel="prev" title="机器学习可解释性">
                  <i class="fa fa-chevron-left"></i> 机器学习可解释性
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/12/artificial-intelligence/data_visual_study/" rel="next" title="数据可视化（Seaborn）">
                  数据可视化（Seaborn） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">985k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">54:44</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/03/07/artificial-intelligence/Intermediate_Machine_Learning_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
