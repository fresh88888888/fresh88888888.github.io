<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="介绍什么是深度学习？近年来，人工智能领域最令人印象深刻的一些进展出现在深度学习领域。自然语言翻译、图像识别和游戏都是深度学习模型已经接近甚至超过人类水平的表现。那么什么是深度学习呢？深度学习是一种以深度计算堆栈为特征的机器学习方法。这种计算深度使得深度学习模型能够理清最具挑战性的现实数据集中发现的各种复杂和分层模式。神经网络凭借其强大的功能和可扩展性，已成为深度学习的定义模型。神经网络由神经元组成">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习（TensorFlow &amp; Keras）">
<meta property="og:url" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="介绍什么是深度学习？近年来，人工智能领域最令人印象深刻的一些进展出现在深度学习领域。自然语言翻译、图像识别和游戏都是深度学习模型已经接近甚至超过人类水平的表现。那么什么是深度学习呢？深度学习是一种以深度计算堆栈为特征的机器学习方法。这种计算深度使得深度学习模型能够理清最具挑战性的现实数据集中发现的各种复杂和分层模式。神经网络凭借其强大的功能和可扩展性，已成为深度学习的定义模型。神经网络由神经元组成">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_2.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_3.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_4.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_5.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_6.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_7.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_8.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_9.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_10.gif">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_11.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_12.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_13.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_14.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_15.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_16.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_17.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_18.gif">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_19.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_20.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_21.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_22.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_23.png">
<meta property="og:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_24.png">
<meta property="article:published_time" content="2024-03-18T03:20:32.000Z">
<meta property="article:modified_time" content="2024-03-18T03:20:32.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/","path":"2024/03/18/artificial-intelligence/Deep_Learning_study/","title":"深度学习（TensorFlow & Keras）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习（TensorFlow & Keras） | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">什么是深度学习？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83"><span class="nav-number">1.2.</span> <span class="nav-text">线性单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E4%BD%9C%E4%B8%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">举例 - 线性单元作为模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5"><span class="nav-number">1.4.</span> <span class="nav-text">多输入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Keras-%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E5%8D%95%E4%BD%8D"><span class="nav-number">1.5.</span> <span class="nav-text">Keras 中的线性单位</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">深度神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-1"><span class="nav-number">2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B1%82"><span class="nav-number">2.2.</span> <span class="nav-text">层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A0%86%E5%8F%A0%E5%AF%86%E9%9B%86%E5%B1%82-Stacking-Dense-Layers"><span class="nav-number">2.4.</span> <span class="nav-text">堆叠密集层(Stacking Dense Layers)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.5.</span> <span class="nav-text">构建序列模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Stochastic-Gradient-Descent%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">随机梯度下降（Stochastic Gradient Descent）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-2"><span class="nav-number">3.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88The-Loss-Function%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">损失函数（The Loss Function）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">3.3.</span> <span class="nav-text">优化器 - 随机梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%92%8C%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="nav-number">3.4.</span> <span class="nav-text">学习率和批量大小</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%8D%9F%E5%A4%B1%E5%B9%B6%E4%BC%98%E5%8C%96"><span class="nav-number">3.5.</span> <span class="nav-text">添加损失并优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-%E7%BA%A2%E9%85%92%E5%93%81%E8%B4%A8"><span class="nav-number">3.6.</span> <span class="nav-text">举例 - 红酒品质</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%88Overfitting-Underfitting%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">过拟合和欠拟合（Overfitting &amp;&amp; Underfitting）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="nav-number">4.1.</span> <span class="nav-text">解释学习曲线</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%B9%E9%87%8F%EF%BC%88Capacity%EF%BC%89"><span class="nav-number">4.2.</span> <span class="nav-text">容量（Capacity）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2"><span class="nav-number">4.3.</span> <span class="nav-text">提前停止</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2"><span class="nav-number">4.4.</span> <span class="nav-text">添加提前停止</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-%E8%AE%AD%E7%BB%83%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.5.</span> <span class="nav-text">举例 - 训练提前停止的模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout-%E5%92%8C%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">Dropout 和批量归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-3"><span class="nav-number">5.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dropout"><span class="nav-number">5.2.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0Dropout"><span class="nav-number">5.3.</span> <span class="nav-text">添加Dropout</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.4.</span> <span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.5.</span> <span class="nav-text">添加批量归一化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-%E4%BD%BF%E7%94%A8-Dropout-%E5%92%8C%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.6.</span> <span class="nav-text">举例 - 使用 Dropout 和批量归一化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%EF%BC%88Binary-Classification%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">二元分类（Binary Classification）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-4"><span class="nav-number">6.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">6.2.</span> <span class="nav-text">二元分类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%87%86%E7%A1%AE%E6%80%A7%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="nav-number">6.3.</span> <span class="nav-text">准确性和交叉熵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Sigmoid%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E6%A6%82%E7%8E%87"><span class="nav-number">6.4.</span> <span class="nav-text">使用Sigmoid函数计算概率</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B-%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">6.5.</span> <span class="nav-text">举例 - 二元分类</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">245</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习（TensorFlow & Keras） | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习（TensorFlow & Keras）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-18 11:20:32" itemprop="dateCreated datePublished" datetime="2024-03-18T11:20:32+08:00">2024-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>29 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><h5 id="什么是深度学习？"><a href="#什么是深度学习？" class="headerlink" title="什么是深度学习？"></a>什么是深度学习？</h5><p>近年来，人工智能领域最令人印象深刻的一些进展出现在深度学习领域。自然语言翻译、图像识别和游戏都是深度学习模型已经接近甚至超过人类水平的表现。那么什么是深度学习呢？<strong>深度学习</strong>是一种以深度计算堆栈为特征的机器学习方法。这种计算深度使得深度学习模型能够理清最具挑战性的现实数据集中发现的各种复杂和分层模式。神经网络凭借其强大的功能和可扩展性，已成为深度学习的定义模型。神经网络由神经元组成，其中每个神经元单独执行简单的计算。神经网络的力量来自于这些神经元可以形成的连接的复杂性。</p>
<span id="more"></span>
<h5 id="线性单元"><a href="#线性单元" class="headerlink" title="线性单元"></a>线性单元</h5><p>让我们从神经网络的基本组成部分开始：单个神经元。如图所示，具有一个输入的神经元（或单元）如下所示：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_1.png" class="">

<p>输入是<code>x</code>。它与神经元的连接权重为<code>w</code>。每当一个值流经连接时，您就将该值乘以连接的权重。对于输入<code>x</code>，到达神经元的是<code>w * x</code>。神经网络通过修改其权重来“学习”。<code>b</code>是一种特殊的权重，我们称之为偏差。该偏差没有任何与之相关的输入数据；相反，我们在图中放入<code>1</code>，这样到达神经元的值就是<code>b</code>（因为<code>1 * b = b</code>）。偏差使神经元能够独立于其输入来修改输出。<code>y</code>是神经元最终输出的值。为了获得输出，神经元将通过其连接接收到的所有值相加。该神经元的激活为<code>y = w * x + b</code>，或作为公式<code>𝑦=𝑤𝑥+𝑏</code>。</p>
<h5 id="举例-线性单元作为模型"><a href="#举例-线性单元作为模型" class="headerlink" title="举例 - 线性单元作为模型"></a>举例 - 线性单元作为模型</h5><p>尽管单个神经元通常仅作为神经网络的一部分发挥作用，但从单个神经元模型作为基线开始通常很有用。单神经元模型是线性模型。让我们考虑一下这如何适用于<code>80 Cereals</code>这样的数据集。以“糖”（每份的糖克数）作为输入，以“卡路里”（每份的卡路里）作为输出来训练模型，我们可能会发现偏差为<code>b=90</code>，权重为<code>w=2.5</code>。 我们可以这样估算每份含<code>5</code>克糖的麦片的卡路里含量：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_2.png" class="">

<p>检查我们的公式，<code>𝑐𝑎𝑙𝑜𝑟𝑖𝑒𝑠=2.5×5+90=102.5</code>, 就像我们期望的那样。</p>
<h5 id="多输入"><a href="#多输入" class="headerlink" title="多输入"></a>多输入</h5><p><code>80 Cereals</code>数据集除了“糖”之外还有更多特征。如果我们想扩展我们的模型以包含纤维或蛋白质含量等内容该怎么办？这很容易。我们可以向神经元添加更多输入连接，每个附加功能对应一个输入连接。为了找到输出，我们将每个输入乘以其连接权重，然后将它们全部加在一起。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_3.png" class="">

<p>该神经元的公式为<code>𝑦=𝑤0𝑥0+𝑤1𝑥1+𝑤2𝑥2+𝑏</code>。具有两个输入的线性单元将适合一个平面，而具有更多输入的单元将适合一个超平面。</p>
<h5 id="Keras-中的线性单位"><a href="#Keras-中的线性单位" class="headerlink" title="Keras 中的线性单位"></a>Keras 中的线性单位</h5><p>在<code>Keras</code>中创建模型的最简单方法是通过<code>keras.Sequential</code>，它将神经网络创建为层堆栈。我们可以使用密集层创建如上所述的模型。我们可以定义一个线性模型，接受三个输入特征（“糖”、“纤维”和“蛋白质”）并产生单个输出（“卡路里”），如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a network with 1 linear unit</span></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(units=<span class="number">1</span>, input_shape=[<span class="number">3</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>使用第一个参数，单位，我们定义我们想要的输出数量。在本例中，我们只是预测“卡路里”，因此我们将使用<code>units=1</code>。通过第二个参数<code>input_shape</code>，我们告诉<code>Keras</code>输入的维度。设置 <code>input_shape=[3]</code>确保模型接受三个特征作为输入（“糖”、“纤维”和“蛋白质”）。</p>
<div class="note info"><p>为什么<code>input_shape</code>是一个<code>Python</code>列表？我们将为数据集中的每个特征提供一个输入。这些特征按列排列，因此我们始终有<code>input_shape=[num_columns]</code>。<code>Keras</code>在这里使用列表的原因是允许使用更复杂的数据集。例如，图像数据可能需要三个维度：<code>[height, width, channels]</code>。</p>
</div>

<h4 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a>深度神经网络</h4><h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h5><p>这里的关键思想是模块化，从更简单的功能单元构建复杂的网络。我们已经了解了线性单元如何计算线性函数——现在我们将了解如何组合和修改这些单个单元以建模更复杂的关系。</p>
<h5 id="层"><a href="#层" class="headerlink" title="层"></a>层</h5><p>神经网络通常将其神经元组织成层。当我们将具有一组公共输入的线性单元收集在一起时，我们得到了一个密集层。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_4.png" class="">

<p>您可以将神经网络中的每一层视为执行某种相对简单的转换。通过深层堆栈，神经网络可以以越来越复杂的方式转换其输入。在训练有素的神经网络中，每一层都是一次转换，让我们更接近解决方案。</p>
<div class="note info"><p><strong>多种层</strong>:<code>Keras</code>中的“层”是一种非常通用的东西。本质上，层可以是任何类型的数据转换。许多层（例如卷积层和循环层）通过使用神经元来转换数据，并且主要区别在于它们形成的连接模式。其他人则用于特征工程或只是简单的算术。</p>
</div>

<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>然而事实证明，两个中间没有任何东西的致密层并不比单个致密层本身更好。密集的层次本身永远无法让我们脱离线和面的世界。我们需要的是非线性的东西。我们需要的是激活函数。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_5.png" class="">

<p>激活函数只是我们应用于每个层的输出（其激活）的函数。最常见的是整流器函数：<code>𝑚𝑎𝑥(0,𝑥)</code></p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_6.png" class="">

<p>整流器函数有一个图形，该图形是一条线，其中负部分“整流”为零。将函数应用于神经元的输出将使数据弯曲，使我们远离简单的线条。当我们将整流器连接到线性单元时，我们得到一个整流线性单元或<code>ReLU</code>。（因此，通常将整流器函数称为“<code>ReLU</code>函数”。）将<code>ReLU</code>激活应用于线性单元意味着输出变为<code>max(0, w * x + b)</code>，我们可以将其绘制在如下图中:</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_7.png" class="">

<h5 id="堆叠密集层-Stacking-Dense-Layers"><a href="#堆叠密集层-Stacking-Dense-Layers" class="headerlink" title="堆叠密集层(Stacking Dense Layers)"></a>堆叠密集层(Stacking Dense Layers)</h5><p>现在我们已经有了一些非线性，让我们看看如何堆叠层来获得复杂的数据转换。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_8.png" class="">

<p>输出层之前的层有时被称为隐藏层，因为我们永远不会直接看到它们的输出。现在，请注意最后（输出）层是线性单元（意味着没有激活函数）。这使得这个网络适合回归任务，我们试图预测一些任意数值。其他任务（例如分类）可能需要输出上的激活函数。</p>
<h5 id="构建序列模型"><a href="#构建序列模型" class="headerlink" title="构建序列模型"></a>构建序列模型</h5><p>我们一直使用的顺序模型将按从第一个到最后一个的顺序将层列表连接在一起：第一层获取输入，最后一层产生输出。这将创建上图中的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    <span class="comment"># the hidden ReLU layers</span></span><br><span class="line">    layers.Dense(units=<span class="number">4</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=[<span class="number">2</span>]),</span><br><span class="line">    layers.Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># the linear output layer </span></span><br><span class="line">    layers.Dense(units=<span class="number">1</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>确保将所有层一起传递到一个列表中，例如<code>[layer,layer,layer, ...]</code>，而不是作为单独的参数。要将激活函数添加到层，只需在激活参数中给出其名称即可。</p>
<h4 id="随机梯度下降（Stochastic-Gradient-Descent）"><a href="#随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent）"></a>随机梯度下降（Stochastic Gradient Descent）</h4><h5 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h5><p>我们学习了如何用密集层的堆栈构建完全连接的网络。首次创建时，网络的所有权重都是随机设置的——网络还不“知道”任何事情。在本课中，我们将了解如何训练神经网络；我们将看到神经网络如何学习。与所有机器学习任务一样，我们从一组训练数据开始。训练数据中的每个示例都包含一些特征（输入）和预期目标（输出）。训练网络意味着调整其权重，使其能够将特征转化为目标。例如，在<code>80</code>种谷物数据集中，我们想要一个网络能够获取每种谷物的“糖”、“纤维”和“蛋白质”含量，并预测该谷物的“卡路里”。如果我们能够成功地训练一个网络来做到这一点，它的权重必须以某种方式表示这些特征与训练数据中表达的目标之间的关系。除了训练数据之外，我们还需要两件事：</p>
<ul>
<li>衡量网络预测效果的“损失函数”。</li>
<li>一个“优化器”，可以告诉网络如何改变其权重。</li>
</ul>
<h5 id="损失函数（The-Loss-Function）"><a href="#损失函数（The-Loss-Function）" class="headerlink" title="损失函数（The Loss Function）"></a>损失函数（The Loss Function）</h5><p>我们已经了解了如何设计网络架构，但还没有了解如何告诉网络要解决什么问题。这就是损失函数的工作。<strong>损失函数</strong>衡量目标真实值与模型预测值之间的差异。不同的问题需要不同的损失函数。我们一直在研究回归问题，其中的任务是预测一些数值——<code>80</code>种谷物中的卡路里、红酒质量的评级。其他回归任务可能是预测房屋的价格或汽车的燃油效率。回归问题的常见损失函数是平均绝对误差或 <code>MAE</code>。对于每个预测<code>y_pred，MAE</code>通过绝对差<code>abs(y_true - y_pred)</code>来测量与真实目标<code>y_true</code>的差异。数据集上的总<code>MAE</code>损失是所有这些绝对差值的平均值。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_9.png" class="">

<p>除了<code>MAE</code>之外，您可能会在回归问题中看到的其他损失函数是均方误差 (<code>MSE</code>) 或<code>Huber</code>损失（两者都在<code>Keras</code>中可用）。在训练期间，模型将使用损失函数作为找到其权重的正确值的指南（损失越低越好）。换句话说，损失函数告诉网络它的目标。</p>
<h5 id="优化器-随机梯度下降"><a href="#优化器-随机梯度下降" class="headerlink" title="优化器 - 随机梯度下降"></a>优化器 - 随机梯度下降</h5><p>我们已经描述了我们希望网络解决的问题，但现在我们需要说明如何解决它。这是优化器的工作。优化器是一种调整权重以最小化损失的算法。事实上，深度学习中使用的所有优化算法都属于随机梯度下降家族。它们是逐步训练网络的迭代算法。训练的一步是这样的：</p>
<ul>
<li>对一些训练数据进行采样并通过网络运行它以进行预测。</li>
<li>测量预测值与真实值之间的损失。</li>
<li>最后，向使损失较小的方向调整权重。</li>
</ul>
<p>然后一遍又一遍地这样做，直到损失达到你想要的程度（或者直到它不再减少）。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_10.gif" class="">

<p>每次迭代的训练数据样本称为“<strong>小批量</strong>”（或通常简称“批次”），而完整一轮的训练数据称为“<strong>纪元</strong>”。您训练的纪元数是网络将看到每个训练示例的次数。该动画显示了使用<code>SGD</code>训练第<code>1</code>课中的线性模型。淡红点描绘了整个训练集，而实心红点是小批量。每次<code>SGD</code>看到一个新的小批量时，它都会将权重（<code>w</code>是斜率、<code>b</code>是<code>y</code>的截距）移向该批次的正确值。一批又一批，这条线最终收敛到最佳拟合。您可以看到，随着权重越接近其真实值，损失就越小。</p>
<h5 id="学习率和批量大小"><a href="#学习率和批量大小" class="headerlink" title="学习率和批量大小"></a>学习率和批量大小</h5><p>请注意，该线仅在每个批次的方向上进行小幅移动（而不是一路移动）。这些变化的大小由学习率决定。较小的学习率意味着网络在其权重收敛到最佳值之前需要看到更多的小批量。学习率和小批量的大小是对<code>SGD</code>训练影响最大的两个参数。它们的相互作用通常很微妙，并且这些参数的正确选择并不总是显而易见的。幸运的是，对于大多数工作来说，无需进行广泛的超参数搜索即可获得满意的结果。 <code>Adam</code>是一种<code>SGD</code>算法，具有自适应学习率，使其适用于大多数问题，无需任何参数调整（从某种意义上来说，它是“自调整”）。<code>Adam</code>是一位出色的通用优化器。</p>
<h5 id="添加损失并优化"><a href="#添加损失并优化" class="headerlink" title="添加损失并优化"></a>添加损失并优化</h5><p>定义模型后，您可以使用模型的编译方法添加损失函数和优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&quot;adam&quot;</span>,</span><br><span class="line">    loss=<span class="string">&quot;mae&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>请注意，我们可以仅使用字符串来指定损失和优化器。您还可以直接通过<code>Keras API</code>访问这些——例如，如果您想调整参数——但对我们来说，默认值就可以正常工作。</p>
<div class="note info"><p>名字里有什么？<br>梯度是一个向量，告诉我们权重需要朝哪个方向移动。更准确地说，它告诉我们如何改变权重以使损失变化最快。我们将这个过程称为<strong>梯度下降</strong>，因为它使用梯度将损失曲线下降到最小值。随机意味着“由机会决定”。我们的训练是随机的，因为小批量是数据集中的随机样本。这就是为什么它被称为<code>SGD</code>！</p>
</div>

<h5 id="举例-红酒品质"><a href="#举例-红酒品质" class="headerlink" title="举例 - 红酒品质"></a>举例 - 红酒品质</h5><p>现在我们知道了开始训练深度学习模型所需的一切。那么让我们来看看它的实际效果吧！我们将使用红酒质量数据集。该数据集包含约<code>1600</code>种葡萄牙红酒的理化测量值。还包括盲品测试中每种葡萄酒的质量评级。我们如何通过这些测量来预测葡萄酒的感知质量？我们已将所有数据准备工作放入下一个隐藏单元中。 这对于接下来的内容并不重要，所以可以跳过它。不过，您现在可能会注意到的一件事是，我们已经重新调整了每个特征以位于区间<code>[0,1]</code>内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">red_wine = pd.read_csv(<span class="string">&#x27;../input/dl-course-data/red-wine.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create training and validation splits</span></span><br><span class="line">df_train = red_wine.sample(frac=<span class="number">0.7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">df_valid = red_wine.drop(df_train.index)</span><br><span class="line">display(df_train.head(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale to [0, 1]</span></span><br><span class="line">max_ = df_train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">min_ = df_train.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">df_train = (df_train - min_) / (max_ - min_)</span><br><span class="line">df_valid = (df_valid - min_) / (max_ - min_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split features and target</span></span><br><span class="line">X_train = df_train.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_valid = df_valid.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_train = df_train[<span class="string">&#x27;quality&#x27;</span>]</span><br><span class="line">y_valid = df_valid[<span class="string">&#x27;quality&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_11.png" class="">

<p>该网络应该有多少个输入？我们可以通过查看数据矩阵中的列数来发现这一点。请确保此处不包含目标（’<code>quality</code>‘）——仅包含输入特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line">(<span class="number">1119</span>, <span class="number">11</span>)</span><br></pre></td></tr></table></figure>
<p>十一列意味着十一个输入。我们选择了一个包含超过<code>1500</code>个神经元的三层网络。该网络应该能够学习数据中相当复杂的关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=[<span class="number">11</span>]),</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>决定模型的架构应该是一个过程的一部分。从简单开始并使用验证损失作为指导。您将在练习中了解有关模型开发的更多信息。定义模型后，我们编译优化器和损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;mae&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>现在我们准备开始训练了！我们告诉<code>Keras</code>一次向优化器提供<code>256</code>行训练数据（<code>batch_size</code>），并在整个数据集（<code>epoch</code>）中执行<code>10</code>次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    validation_data=(X_valid, y_valid),</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">5/5 [==============================] - 1s 66ms/step - loss: 0.2470 - val_loss: 0.1357</span><br><span class="line">Epoch 2/10</span><br><span class="line">5/5 [==============================] - 0s 21ms/step - loss: 0.1349 - val_loss: 0.1231</span><br><span class="line">Epoch 3/10</span><br><span class="line">5/5 [==============================] - 0s 23ms/step - loss: 0.1181 - val_loss: 0.1173</span><br><span class="line">Epoch 4/10</span><br><span class="line">5/5 [==============================] - 0s 21ms/step - loss: 0.1117 - val_loss: 0.1066</span><br><span class="line">Epoch 5/10</span><br><span class="line">5/5 [==============================] - 0s 22ms/step - loss: 0.1071 - val_loss: 0.1028</span><br><span class="line">Epoch 6/10</span><br><span class="line">5/5 [==============================] - 0s 20ms/step - loss: 0.1049 - val_loss: 0.1050</span><br><span class="line">Epoch 7/10</span><br><span class="line">5/5 [==============================] - 0s 20ms/step - loss: 0.1035 - val_loss: 0.1009</span><br><span class="line">Epoch 8/10</span><br><span class="line">5/5 [==============================] - 0s 20ms/step - loss: 0.1019 - val_loss: 0.1043</span><br><span class="line">Epoch 9/10</span><br><span class="line">5/5 [==============================] - 0s 19ms/step - loss: 0.1005 - val_loss: 0.1035</span><br><span class="line">Epoch 10/10</span><br><span class="line">5/5 [==============================] - 0s 20ms/step - loss: 0.1011 - val_loss: 0.0977</span><br></pre></td></tr></table></figure>
<p>您可以看到<code>Keras</code>会在模型训练时向您通报损失的最新情况。通常，查看损失的更好方法是将其绘制出来。<code>fit</code>方法实际上在<code>History</code>对象中保存了训练过程中产生的损失的记录。我们将数据转换为<code>Pandas</code>数据框，这使得绘图变得容易。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert the training history to a dataframe</span></span><br><span class="line">history_df = pd.DataFrame(history.history)</span><br><span class="line"><span class="comment"># use Pandas native plot method</span></span><br><span class="line">history_df[<span class="string">&#x27;loss&#x27;</span>].plot()</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_12.png" class="">

<p>请注意损失如何随着时间的流逝而趋于平稳。当损失曲线变得像这样水平时，这意味着模型已经学到了它能学到的一切，并且没有理由继续额外的执行次数。</p>
<h4 id="过拟合和欠拟合（Overfitting-Underfitting）"><a href="#过拟合和欠拟合（Overfitting-Underfitting）" class="headerlink" title="过拟合和欠拟合（Overfitting &amp;&amp; Underfitting）"></a>过拟合和欠拟合（Overfitting &amp;&amp; Underfitting）</h4><h5 id="解释学习曲线"><a href="#解释学习曲线" class="headerlink" title="解释学习曲线"></a>解释学习曲线</h5><p>您可能会认为训练数据中的信息有两种：信号和噪声。信号是概括的部分，可以帮助我们的模型根据新数据进行预测。噪声是仅适用于训练数据的部分；噪声是来自现实世界中的数据的所有随机波动，或者是所有偶然的、非信息性的模式，这些模式实际上不能帮助模型进行预测。噪音是该部件可能看起来有用但实际上没有用。我们通过选择最小化训练集损失的权重或参数来训练模型。然而，您可能知道，为了准确评估模型的性能，我们需要在一组新数据（验证数据）上对其进行评估。当我们训练模型时，我们会逐个遍历绘制训练集上的损失。为此，我们还将添加验证数据图。这些图我们称之为学习曲线。为了有效地训练深度学习模型，我们需要能够解释它们。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_13.png" class="">

<p>现在，当模型学习信号或学习噪声时，训练损失都会下降。但只有当模型学习到信号时，验证损失才会下降。（无论模型从训练集中学习到什么噪声，都不会推广到新数据。）因此，当模型学习信号时，两条曲线都会下降，但当它学习噪声时，曲线中会产生间隙。间隙的大小告诉您模型学到了多少噪声。理想情况下，我们将创建能够学习所有信号而不学习任何噪声的模型。这实际上永远不会发生。相反，我们进行交易。我们可以让模型以学习更多噪声为代价来学习更多信号。只要交易对我们有利，验证损失就会继续减少。然而，在某一点之后，交易可能会对我们不利，成本超过收益，验证损失开始上升。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_14.png" class="">

<p>这种权衡表明，训练模型时可能会出现两个问题：信号不足或噪声太多。<strong>训练集欠拟合是指由于模型没有学习到足够的信号而导致损失没有达到应有的水平。过度拟合训练集是指由于模型学习了太多噪声而导致损失没有达到应有的水平。训练深度学习模型的技巧是找到两者之间的最佳平衡</strong>。我们将研究几种从训练数据中获取更多信号同时减少噪声量的方法。</p>
<h5 id="容量（Capacity）"><a href="#容量（Capacity）" class="headerlink" title="容量（Capacity）"></a>容量（Capacity）</h5><p>模型的<strong>容量</strong>是指它能够学习的模式的大小和复杂性。对于神经网络来说，这很大程度上取决于它有多少个神经元以及它们如何连接在一起。如果您的网络似乎不适合数据，您应该尝试增加其容量。您可以通过加宽网络（向现有层添加更多单元）或使其更深（添加更多层）来增加网络的容量。更宽的网络更容易学习更多的线性关系，而更深的网络更喜欢更多的非线性关系。哪个更好只取决于数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">16</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">wider = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">deeper = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">16</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">16</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>您将在练习中探索网络容量如何影响其性能。</p>
<h5 id="提前停止"><a href="#提前停止" class="headerlink" title="提前停止"></a>提前停止</h5><p>我们提到，当模型过于急切地学习噪声时，验证损失可能会在训练期间开始增加。为了防止这种情况，只要验证损失似乎不再减少，我们就可以停止训练。以这种方式中断训练称为<strong>提前停止</strong>。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_15.png" class="">

<p>一旦我们检测到验证损失开始再次上升，我们就可以将权重重置回最小值发生的位置。这确保了模型不会继续学习噪声并过度拟合数据。提前停止训练还意味着我们在网络完成信号学习之前过早停止训练的危险较小。因此，除了防止训练时间过长而导致过拟合之外，提前停止还可以防止训练时间不够而导致欠拟合。只需将您的训练周期设置为某个较大的数字（超出您的需要），然后提前停止即可完成其余的工作。</p>
<h5 id="添加提前停止"><a href="#添加提前停止" class="headerlink" title="添加提前停止"></a>添加提前停止</h5><p>在<code>Keras</code>中，我们通过回调在训练中加入早期停止。回调只是您希望在网络训练时经常运行的函数。早期停止回调将在每个遍历后运行。（<code>Keras</code>预定义了各种有用的回调，但您也可以定义自己的回调。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(</span><br><span class="line">    min_delta=<span class="number">0.001</span>, <span class="comment"># minimium amount of change to count as an improvement</span></span><br><span class="line">    patience=<span class="number">20</span>, <span class="comment"># how many epochs to wait before stopping</span></span><br><span class="line">    restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这些参数表示：“如果在过去<code>20</code>个<code>epoch</code>中验证损失没有至少改善<code>0.001</code>，则停止训练并保留您找到的最佳模型。” 有时很难判断验证损失的增加是由于过度拟合还是仅仅由于随机批次变化。这些参数允许我们设置一些关于何时停止的容差。正如我们将在示例中看到的，我们将将此回调与损失和优化器一起传递给<code>fit</code>方法。</p>
<h5 id="举例-训练提前停止的模型"><a href="#举例-训练提前停止的模型" class="headerlink" title="举例 - 训练提前停止的模型"></a>举例 - 训练提前停止的模型</h5><p>我们将增加该网络的容量，同时添加提前停止回调以防止过度拟合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">red_wine = pd.read_csv(<span class="string">&#x27;../input/dl-course-data/red-wine.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create training and validation splits</span></span><br><span class="line">df_train = red_wine.sample(frac=<span class="number">0.7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">df_valid = red_wine.drop(df_train.index)</span><br><span class="line">display(df_train.head(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale to [0, 1]</span></span><br><span class="line">max_ = df_train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">min_ = df_train.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">df_train = (df_train - min_) / (max_ - min_)</span><br><span class="line">df_valid = (df_valid - min_) / (max_ - min_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split features and target</span></span><br><span class="line">X_train = df_train.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_valid = df_valid.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_train = df_train[<span class="string">&#x27;quality&#x27;</span>]</span><br><span class="line">y_valid = df_valid[<span class="string">&#x27;quality&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_16.png" class="">

<p>现在让我们增加网络的容量。我们将选择一个相当大的网络，但一旦验证损失显示出增加的迹象，就依靠回调来停止训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, callbacks</span><br><span class="line"></span><br><span class="line">early_stopping = callbacks.EarlyStopping(</span><br><span class="line">    min_delta=<span class="number">0.001</span>, <span class="comment"># minimium amount of change to count as an improvement</span></span><br><span class="line">    patience=<span class="number">20</span>, <span class="comment"># how many epochs to wait before stopping</span></span><br><span class="line">    restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=[<span class="number">11</span>]),</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;mae&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>定义回调后，将其添加为<code>fit</code>中的参数（可以有多个，因此将其放入列表中）。使用提前停止时<code>epoch</code>选择的大一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    validation_data=(X_valid, y_valid),</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    epochs=<span class="number">500</span>,</span><br><span class="line">    callbacks=[early_stopping], <span class="comment"># put your callbacks in a list</span></span><br><span class="line">    verbose=<span class="number">0</span>,  <span class="comment"># turn off training log</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">history_df = pd.DataFrame(history.history)</span><br><span class="line">history_df.loc[:, [<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>]].plot();</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Minimum validation loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(history_df[<span class="string">&#x27;val_loss&#x27;</span>].<span class="built_in">min</span>()))</span><br></pre></td></tr></table></figure>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_17.png" class="">

<p>果然，<code>Keras</code>在满<code>500</code>个<code>epoch</code>之前就停止了训练！</p>
<h4 id="Dropout-和批量归一化"><a href="#Dropout-和批量归一化" class="headerlink" title="Dropout 和批量归一化"></a>Dropout 和批量归一化</h4><h5 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h5><p>深度学习的世界不仅仅是密集层。您可以向模型添加数十种层。有些就像密集层并定义神经元之间的连接，而其他则可以进行其他类型的预处理或转换。我们将学习两种特殊层，它们本身不包含任何神经元，但添加了一些有时可以以多种方式使模型受益的功能。</p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>第一个是“<code>dropout</code>层”，它可以帮助纠正过度拟合。在上一课中，我们讨论了网络学习训练数据中的虚假模式是如何导致过拟合的。为了识别这些虚假模式，网络通常依赖于非常特定的权重组合，这是一种权重的“阴谋”。由于它们如此具体，因此往往很脆弱：删除其中一个，阴谋就会崩溃。这就是<code>dropout</code>背后的想法。为了打破这些阴谋，我们在训练的每一步中随机丢弃层输入单元的一部分，从而使网络更难学习训练数据中的那些虚假模式。相反，它必须寻找广泛、通用的模式，其权重模式往往更稳健。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_18.gif" class="">

<p>您还可以将<code>dropout</code>视为创建一种网络集合。预测将不再由一个大网络做出，而是由较小网络组成的委员会做出。委员会中的个人往往会犯不同类型的错误，但同时又是正确的，这使得委员会作为一个整体比任何个人都更好。（如果您熟悉随机森林作为决策树的集合，那么这是相同的想法。）</p>
<h5 id="添加Dropout"><a href="#添加Dropout" class="headerlink" title="添加Dropout"></a>添加Dropout</h5><p>在<code>Keras</code>中，退出率参数<code>rate</code>定义了要关闭的输入单元的百分比。将<code>Dropout</code>图层放在您想要应用<code>Dropout</code>的图层之前：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.Sequential([</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    layers.Dropout(rate=<span class="number">0.3</span>), <span class="comment"># apply 30% dropout to the next layer</span></span><br><span class="line">    layers.Dense(<span class="number">16</span>),</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h5 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h5><p>我们将看到的下一个特殊层执行“批量归一化”（或“<code>batchnorm</code>”），这可以帮助纠正缓慢或不稳定的训练。对于神经网络，通常最好将所有数据放在一个通用的尺度上，也许可以使用 <code>scikit-learn</code>的<code>StandardScaler</code>或<code>MinMaxScaler</code>之类的东西。原因是<code>SGD</code>会根据数据产生的激活大小按比例改变网络权重。倾向于产生不同大小激活的特征可能会导致训练行为不稳定。现在，如果在数据进入网络之前对数据进行标准化是件好事，也许在网络内部进行标准化会更好！事实上，我们有一种特殊的层可以做到这一点，即批量归一化层。批次归一化层会查看每个批次的数据，首先使用批次自身的均值和标准差对批次进行归一化，然后使用两个可训练的缩放参数将数据置于新的尺度上。实际上，<code>Batchnorm</code>对其输入执行了一种协调的重新调整。最常见的是，添加批归一化作为优化过程的辅助（尽管有时它也可以帮助预测性能）。具有批量归一化的模型往往需要更少的时期来完成训练。此外，<code>Batchnorm</code>还可以修复各种可能导致训练“卡住”的问题。考虑向您的模型添加批量归一化，特别是当您在训练期间遇到问题时。</p>
<h5 id="添加批量归一化"><a href="#添加批量归一化" class="headerlink" title="添加批量归一化"></a>添加批量归一化</h5><p>看来批量归一化几乎可以在网络中的任何点使用。你可以把它放在一层之后…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">layers.Dense(<span class="number">16</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">layers.BatchNormalization(),</span><br></pre></td></tr></table></figure>
<p>..或者在层及其激活函数之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layers.Dense(<span class="number">16</span>),</span><br><span class="line">layers.BatchNormalization(),</span><br><span class="line">layers.Activation(<span class="string">&#x27;relu&#x27;</span>),</span><br></pre></td></tr></table></figure>
<p>如果将其添加为网络的第一层，它可以充当一种自适应预处理器，替代<code>SciKit-Learn</code>的<code>StandardScaler</code>之类的东西。</p>
<h5 id="举例-使用-Dropout-和批量归一化"><a href="#举例-使用-Dropout-和批量归一化" class="headerlink" title="举例 - 使用 Dropout 和批量归一化"></a>举例 - 使用 Dropout 和批量归一化</h5><p>让我们继续开发红酒模型。现在我们将进一步增加容量，但添加<code>dropout</code>来控制过度拟合和批量归一化以加快优化速度。这次，我们也将不再对数据进行标准化，以演示批量标准化如何稳定训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup plotting</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&#x27;seaborn-whitegrid&#x27;</span>)</span><br><span class="line"><span class="comment"># Set Matplotlib defaults</span></span><br><span class="line">plt.rc(<span class="string">&#x27;figure&#x27;</span>, autolayout=<span class="literal">True</span>)</span><br><span class="line">plt.rc(<span class="string">&#x27;axes&#x27;</span>, labelweight=<span class="string">&#x27;bold&#x27;</span>, labelsize=<span class="string">&#x27;large&#x27;</span>,</span><br><span class="line">       titleweight=<span class="string">&#x27;bold&#x27;</span>, titlesize=<span class="number">18</span>, titlepad=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">red_wine = pd.read_csv(<span class="string">&#x27;../input/dl-course-data/red-wine.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create training and validation splits</span></span><br><span class="line">df_train = red_wine.sample(frac=<span class="number">0.7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">df_valid = red_wine.drop(df_train.index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split features and target</span></span><br><span class="line">X_train = df_train.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_valid = df_valid.drop(<span class="string">&#x27;quality&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_train = df_train[<span class="string">&#x27;quality&#x27;</span>]</span><br><span class="line">y_valid = df_valid[<span class="string">&#x27;quality&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>添加<code>dropout</code>时，您可能需要增加<code>Dense</code>层中的单元数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers, callbacks</span><br><span class="line"></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=[<span class="number">11</span>]),</span><br><span class="line">    layers.Dropout(<span class="number">0.3</span>),</span><br><span class="line">    layers.BatchNormalization(),</span><br><span class="line">    layers.Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dropout(<span class="number">0.3</span>),</span><br><span class="line">    layers.BatchNormalization(),</span><br><span class="line">    layers.Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dropout(<span class="number">0.3</span>),</span><br><span class="line">    layers.BatchNormalization(),</span><br><span class="line">    layers.Dense(<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;mae&#x27;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    validation_data=(X_valid, y_valid),</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    epochs=<span class="number">100</span>,</span><br><span class="line">    verbose=<span class="number">0</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the learning curves</span></span><br><span class="line">history_df = pd.DataFrame(history.history)</span><br><span class="line">history_df.loc[:, [<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>]].plot()</span><br></pre></td></tr></table></figure>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_19.png" class="">

<p>如果在使用数据进行训练之前对数据进行标准化，通常会获得更好的性能。然而，我们能够使用原始数据，这表明批量归一化在更困难的数据集上是多么有效。</p>
<h4 id="二元分类（Binary-Classification）"><a href="#二元分类（Binary-Classification）" class="headerlink" title="二元分类（Binary Classification）"></a>二元分类（Binary Classification）</h4><h5 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h5><p>到目前为止，我们已经了解了神经网络如何解决回归问题。现在我们将把神经网络应用于另一个常见的机器学习问题：分类。到目前为止我们学到的大部分内容仍然适用。主要区别在于我们使用的损失函数以及我们希望最终层产生什么样的输出。</p>
<h5 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h5><p>分类为两类之一是常见的机器学习问题。您可能想要预测客户是否有可能进行购买、信用卡交易是否存在欺诈、深空信号是否显示新行星的证据或疾病的医学测试证据。这些都是二元分类问题。在原始数据中，类可能由“是”和“否”或“狗”和“猫”等字符串表示。在使用这些数据之前，我们将分配一个类标签：一个类为<code>0</code>，另一个类为<code>1</code>。分配数字标签会将数据置于神经网络可以使用的形式中。</p>
<h5 id="准确性和交叉熵"><a href="#准确性和交叉熵" class="headerlink" title="准确性和交叉熵"></a>准确性和交叉熵</h5><p>准确性是衡量分类问题成功与否的众多指标之一。准确率是正确预测与总预测的比率：准确率&#x3D;正确数&#x2F;总计。始终正确预测的模型的准确度得分为<code>1.0</code>。在其他条件相同的情况下，只要数据集中的类以大致相同的频率出现，准确性就是一个合理的指标。准确性（以及大多数其他分类指标）的问题在于它不能用作损失函数。<code>SGD</code>需要一个平滑变化的损失函数，但准确度（作为计数的比率）会“跳跃”变化。因此，我们必须选择一个替代函数来充当损失函数。这个替代品就是<strong>交叉熵函数</strong>。现在，回想一下损失函数定义了训练期间网络的目标。通过回归，我们的目标是最小化预期结果和预测结果之间的距离。我们选择<code>MAE</code>来测量这个距离。对于分类，我们想要的是概率之间的距离，这就是交叉熵所提供的。交叉熵是一种衡量从一个概率分布到另一个概率分布的距离的度量。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_20.png" class="">

<p>我们的想法是，我们希望我们的网络以<code>1.0</code>的概率预测正确的类别。预测概率距离<code>1.0</code>越远，交叉熵损失就越大。我们使用交叉熵的技术原因有点微妙，但是本节的主要内容就是：使用交叉熵进行分类损失；您可能关心的其他指标（例如准确性）往往会随之提高。</p>
<h5 id="使用Sigmoid函数计算概率"><a href="#使用Sigmoid函数计算概率" class="headerlink" title="使用Sigmoid函数计算概率"></a>使用Sigmoid函数计算概率</h5><p><strong>交叉熵和准确度函数</strong>都需要概率作为输入，即从<code>0</code>到<code>1</code>的数字。为了将密集层产生的实值输出转换为概率，我们附加了一种新的激活函数，即<code>sigmoid</code>激活。</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_21.png" class="">

<p>为了获得最终的类别预测，我们定义<strong>阈值概率</strong>。通常情况下，该值为<code>0.5</code>，因此四舍五入将为我们提供正确的类别：低于<code>0.5</code>表示具有标签<code>0</code>的类别，<code>0.5</code>或以上表示具有标签<code>1</code>的类别。<code>0.5</code>阈值是<code>Keras</code>默认使用的准确度指标。</p>
<h5 id="举例-二元分类"><a href="#举例-二元分类" class="headerlink" title="举例 - 二元分类"></a>举例 - 二元分类</h5><p>电离层数据集包含从聚焦于地球大气层电离层的雷达信号获得的特征。任务是确定信号是否表明存在某种物体，或者只是空气。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">ion = pd.read_csv(<span class="string">&#x27;../input/dl-course-data/ion.csv&#x27;</span>, index_col=<span class="number">0</span>)</span><br><span class="line">display(ion.head())</span><br><span class="line"></span><br><span class="line">df = ion.copy()</span><br><span class="line">df[<span class="string">&#x27;Class&#x27;</span>] = df[<span class="string">&#x27;Class&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;good&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;bad&#x27;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line">df_train = df.sample(frac=<span class="number">0.7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">df_valid = df.drop(df_train.index)</span><br><span class="line"></span><br><span class="line">max_ = df_train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">min_ = df_train.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">df_train = (df_train - min_) / (max_ - min_)</span><br><span class="line">df_valid = (df_valid - min_) / (max_ - min_)</span><br><span class="line">df_train.dropna(axis=<span class="number">1</span>, inplace=<span class="literal">True</span>) <span class="comment"># drop the empty feature in column 2</span></span><br><span class="line">df_valid.dropna(axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_train = df_train.drop(<span class="string">&#x27;Class&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_valid = df_valid.drop(<span class="string">&#x27;Class&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_train = df_train[<span class="string">&#x27;Class&#x27;</span>]</span><br><span class="line">y_valid = df_valid[<span class="string">&#x27;Class&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_22.png" class="">

<p>我们将像回归任务一样定义我们的模型，但有一个例外。在最后一层中包含“<code>Sigmoid</code>”激活，以便模型产生<strong>类别概率</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers, callbacks</span><br><span class="line"></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(<span class="number">4</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=[<span class="number">33</span>]),</span><br><span class="line">    layers.Dense(<span class="number">4</span>, activation=<span class="string">&#x27;relu&#x27;</span>),    </span><br><span class="line">    layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>使用其编译方法将<strong>交叉熵损失和准确性度量</strong>添加到模型中。对于二分类问题，请务必使用“<strong>二进制</strong>”版本。<code>Adam</code>优化器也非常适合分类，因此我们将坚持使用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">    metrics=[<span class="string">&#x27;binary_accuracy&#x27;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个特定问题中的模型可能需要相当多的<code>epochs</code>才能完成训练，因此为了方便起见，我们将包含一个早期停止回调。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">    patience=<span class="number">10</span>,</span><br><span class="line">    min_delta=<span class="number">0.001</span>,</span><br><span class="line">    restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    validation_data=(X_valid, y_valid),</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    epochs=<span class="number">1000</span>,</span><br><span class="line">    callbacks=[early_stopping],</span><br><span class="line">    verbose=<span class="number">0</span>, <span class="comment"># hide the output because we have so many epochs</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们将一如既往地查看学习曲线，并检查我们在验证集上获得的损失和准确性的最佳值。（请记住，提前停止会将权重恢复到获得这些值的权重。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">history_df = pd.DataFrame(history.history)</span><br><span class="line"><span class="comment"># Start the plot at epoch 5</span></span><br><span class="line">history_df.loc[<span class="number">5</span>:, [<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;val_loss&#x27;</span>]].plot()</span><br><span class="line">history_df.loc[<span class="number">5</span>:, [<span class="string">&#x27;binary_accuracy&#x27;</span>, <span class="string">&#x27;val_binary_accuracy&#x27;</span>]].plot()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((<span class="string">&quot;Best Validation Loss: &#123;:0.4f&#125;&quot;</span> +\</span><br><span class="line">      <span class="string">&quot;\nBest Validation Accuracy: &#123;:0.4f&#125;&quot;</span>)\</span><br><span class="line">      .<span class="built_in">format</span>(history_df[<span class="string">&#x27;val_loss&#x27;</span>].<span class="built_in">min</span>(), </span><br><span class="line">              history_df[<span class="string">&#x27;val_binary_accuracy&#x27;</span>].<span class="built_in">max</span>()))</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Best Validation Loss: 0.6040 </span><br><span class="line">Best Validation Accuracy: 0.8381</span><br></pre></td></tr></table></figure>
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_23.png" class="">
<img data-src="/2024/03/18/artificial-intelligence/Deep_Learning_study/dl_24.png" class="">

<p>尽管我们可以看到训练损失继续下降，但提前停止回调可以防止任何过度拟合。此外，随着交叉熵的下降，准确率以相同的速度上升，因此，最小化交叉熵似乎是一个很好的替代方案。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/" title="深度学习（TensorFlow &amp; Keras）">https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/15/artificial-intelligence/Feature_Engineering_practice_study/" rel="prev" title="特征工程实践之—房价预测">
                  <i class="fa fa-chevron-left"></i> 特征工程实践之—房价预测
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/19/artificial-intelligence/time_series_study/" rel="next" title="时间序列（Python）">
                  时间序列（Python） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.3m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">70:13</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/03/18/artificial-intelligence/Deep_Learning_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
