<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="JAX是一个高性能机器学习库。JAX在加速器（例如GPU和TPU）上编译并运行NumPy代码。您可以使用JAX（以及为JAX构建的神经网络库FLAX）来构建和训练深度学习模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow &amp; JAX">
<meta property="og:url" content="https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="JAX是一个高性能机器学习库。JAX在加速器（例如GPU和TPU）上编译并运行NumPy代码。您可以使用JAX（以及为JAX构建的神经网络库FLAX）来构建和训练深度学习模型。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-03-27T10:10:32.000Z">
<meta property="article:modified_time" content="2024-03-27T10:10:32.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/","path":"2024/03/27/artificial-intelligence/jax_tensorflow_study/","title":"TensorFlow & JAX"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TensorFlow & JAX | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.</span> <span class="nav-text">环境设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensors%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">张量（Tensors）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#String-Tensors"><span class="nav-number">2.1.</span> <span class="nav-text">String Tensors</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Ragged-Tensors"><span class="nav-number">2.1.1.</span> <span class="nav-text">Ragged Tensors</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sparse-tensors"><span class="nav-number">2.2.</span> <span class="nav-text">Sparse tensors</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%EF%BC%88Variables%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">变量（Variables）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E5%92%8C%E6%A2%AF%E5%BA%A6%EF%BC%88Automatic-Differentiation-and-Gradients%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">自动微分和梯度（Automatic Differentiation and Gradients）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E8%A2%AB%E7%9B%91%E8%A7%86%E7%9A%84%E5%AF%B9%E8%B1%A1"><span class="nav-number">4.1.</span> <span class="nav-text">访问所有被监视的对象</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2%E6%A2%AF%E5%BA%A6"><span class="nav-number">4.2.</span> <span class="nav-text">停止梯度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%AD%E8%A6%81%E8%A7%82%E7%9C%8B%E7%9A%84%E5%86%85%E5%AE%B9"><span class="nav-number">4.3.</span> <span class="nav-text">选择上下文中要观看的内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multiple-Tapes"><span class="nav-number">4.4.</span> <span class="nav-text">Multiple Tapes</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E6%A2%AF%E5%BA%A6"><span class="nav-number">4.5.</span> <span class="nav-text">高阶梯度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%99%B7%E9%98%B1%EF%BC%88Gotchas%EF%BC%89"><span class="nav-number">4.6.</span> <span class="nav-text">陷阱（Gotchas）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E5%92%8C%E6%A2%AF%E5%BA%A6"><span class="nav-number">4.7.</span> <span class="nav-text">状态和梯度</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">218</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TensorFlow & JAX | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorFlow & JAX
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-27 18:10:32" itemprop="dateCreated datePublished" datetime="2024-03-27T18:10:32+08:00">2024-03-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><code>JAX</code>是一个<strong>高性能机器学习库</strong>。<code>JAX</code>在加速器（例如<code>GPU</code>和<code>TPU</code>）上编译并运行<code>NumPy</code>代码。您可以使用<code>JAX</code>（以及为<code>JAX</code>构建的神经网络库<code>FLAX</code>）来构建和训练深度学习模型。</p>
<span id="more"></span>
<h4 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers, callbacks</span><br><span class="line"></span><br><span class="line">seed=<span class="number">1234</span></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">tf.random.set_seed(seed)</span><br></pre></td></tr></table></figure>
<h4 id="张量（Tensors）"><a href="#张量（Tensors）" class="headerlink" title="张量（Tensors）"></a>张量（Tensors）</h4><p>什么是<strong>张量</strong>？尽管张量的含义与我们在机器学习中通常使用的含义有很大不同，但每当我们在机器学习中提到张量时，我们的意思是它是一个<strong>多维数组</strong>，其中所有值都具有统一的数据类型。创建<code>TF</code>张量的方法有很多种。我们将看看其中的一些重要的。<code>tf.constant(..)</code>：这是创建张量对象的最简单方法，但存在一些问题。首先，让我们尝试用它创建一个张量，然后我们稍后再看看其中的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A zero rank tensor. A zero rank tensor is nothing but a single value</span></span><br><span class="line">x = tf.constant(<span class="number">5.0</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(5.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>正如您在上面看到的，<strong>张量对象</strong>具有<strong>形状和数据类型</strong>。还有其他与张量对象关联的<code>attributes/properties</code>。</p>
<ul>
<li>形状(<code>Shape</code>)：张量每个轴的长度（元素数量）。</li>
<li>等级(<code>Rank</code>)：轴的数量。例如，矩阵是<code>2</code>阶张量。</li>
<li>轴或维度(<code>Axis or Dimension</code>)：张量的特定维度。</li>
<li>大小(<code>Size</code>)：张量中的项目总数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can convert any tensor object to `ndarray` by calling the `numpy()` method</span></span><br><span class="line">y = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.int8).numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;`y` is now a <span class="subst">&#123;<span class="built_in">type</span>(y)&#125;</span> object and have a value == <span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`y` is now a &lt;class <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt; object and have a value == [1 2 3]</span><br></pre></td></tr></table></figure>
<div class="note warning"><p><strong>注意</strong></p>
<ul>
<li>人们将<code>tf.constant(..)</code>与创建常量张量的操作混淆。不存在这样的关系。这与我们如何在<code>tf.Graph</code>中嵌入节点有关。</li>
<li>默认情况下，<code>TensorFlow</code>中的任何张量都是不可变的，即张量一旦创建就无法更改其值。你必须创造一个新的。这与<code>numpy</code>和<code>pytorch</code>不同，在<code>numpy</code>和<code>pytorch</code>中您可以修改。</li>
<li>与<code>tf.constant</code>最接近的成员之一是<code>tf.convert_to_tensor()</code>方法，有一些差异。</li>
<li><code>tf.constant(..)</code>只是创建张量的多种方法之一。</li>
</ul>
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Immutability check</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rank-1 tensor</span></span><br><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">2</span>], dtype=tf.int8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try to modify the values</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    x[<span class="number">1</span>] = <span class="number">3</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.constant(..) is no special. Let&#x27;s create a tensor using a diff method</span></span><br><span class="line">x = tf.ones(<span class="number">2</span>, dtype=tf.int8)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    x[<span class="number">0</span>] = <span class="number">3</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>, <span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check all the properties of a tensor object</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of x : <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Another method to obtain the shape using `tf.shape(..)`: <span class="subst">&#123;tf.shape(x)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nRank of the tensor: <span class="subst">&#123;x.ndim&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dtype of the tensor: <span class="subst">&#123;x.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total size of the tensor: <span class="subst">&#123;tf.size(x)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Values of the tensor: <span class="subst">&#123;x.numpy()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TypeError <span class="string">&#x27;tensorflow.python.framework.ops.EagerTensor&#x27;</span> object does not support item assignment</span><br><span class="line"></span><br><span class="line">tf.Tensor([1 1], shape=(2,), dtype=int8)</span><br><span class="line">TypeError <span class="string">&#x27;tensorflow.python.framework.ops.EagerTensor&#x27;</span> object does not support item assignment</span><br><span class="line"></span><br><span class="line">Shape of x : (2,)</span><br><span class="line">Another method to obtain the shape using `tf.shape(..)`: [2]</span><br><span class="line">Rank of the tensor: 1</span><br><span class="line">dtype of the tensor: &lt;dtype: <span class="string">&#x27;int8&#x27;</span>&gt;</span><br><span class="line">Total size of the tensor: 2</span><br><span class="line">Values of the tensor: [1 1]</span><br></pre></td></tr></table></figure>
<p>无法在<code>Tensor</code>对象中进行赋值有点令人沮丧。那有什么办法解决呢？我发现的一直适用于我的用例的最佳方法是创建一个掩码或使用<code>tf.tensor_scatter_nd_update</code>。让我们看一个例子。<br><code>Original tensor</code> -&gt; <code>[1, 2, 3, 4, 5]</code>和<code>Output tensor we want</code> -&gt; <code>[1, 200, 3, 400, 5]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tensor first. Here is another way</span></span><br><span class="line">x = tf.cast([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=tf.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original tensor: &quot;</span>, x)</span><br><span class="line"></span><br><span class="line">mask = x%<span class="number">2</span> == <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original mask: &quot;</span>, mask)</span><br><span class="line"></span><br><span class="line">mask = tf.cast(mask, dtype=x.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mask casted to original tensor type: &quot;</span>, mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some kind of operation on an tensor that is of same size </span></span><br><span class="line"><span class="comment"># or broadcastable to the original tensor. Here we will simply</span></span><br><span class="line"><span class="comment"># use the range object to create that tensor</span></span><br><span class="line">temp = tf.cast(tf.<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>) * <span class="number">100</span>, dtype=x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output tensor</span></span><br><span class="line"><span class="comment"># Input tensor -&gt; [1, 2, 3, 4, 5]</span></span><br><span class="line"><span class="comment"># Mask -&gt; [0, 1, 0, 1, 0]</span></span><br><span class="line">out = x * (<span class="number">1</span>-mask) + mask * temp</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output tensor: &quot;</span>, out)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original tensor:  tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)</span><br><span class="line">Original mask:  tf.Tensor([False  True False  True False], shape=(5,), dtype=bool)</span><br><span class="line">Mask casted to original tensor <span class="built_in">type</span>:  tf.Tensor([0. 1. 0. 1. 0.], shape=(5,), dtype=float32)</span><br><span class="line">Output tensor:  tf.Tensor([  1. 200.   3. 400.   5.], shape=(5,), dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Another way to achieve the same thing</span></span><br><span class="line">indices_to_update = tf.where(x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Indices to update: &quot;</span>, indices_to_update)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update the tensor values</span></span><br><span class="line">updates = [<span class="number">200.</span>, <span class="number">400.</span>]</span><br><span class="line">out = tf.tensor_scatter_nd_update(x, indices_to_update, updates)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nOutput tensor&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Indices to update:  tf.Tensor([[1][3]], shape=(2, 1), dtype=int64)</span><br><span class="line"></span><br><span class="line">Output tensor</span><br><span class="line">tf.Tensor([  1. 200.   3. 400.   5.], shape=(5,), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>现在让我们看看另一件有趣的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This works!</span></span><br><span class="line">arr = np.random.randint(<span class="number">5</span>, size=(<span class="number">5</span>,), dtype=np.int32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Numpy array: &quot;</span>, arr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accessing numpy array elements based on a  condition with irregular strides&quot;</span>, arr[[<span class="number">1</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># This doesn&#x27;t work</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accessing tensor elements based on a  condition with irregular strides&quot;</span>, x[[<span class="number">1</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Numpy array:  [3 4 4 0 1]</span><br><span class="line">Accessing numpy array elements based on a  condition with irregular strides [4 1]</span><br><span class="line">InvalidArgumentError Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice</span><br></pre></td></tr></table></figure>
<p>现在怎么办？如果您想从具有不规则步长或定义不明确的步长的张量中提取多个元素，那么<code>tf.gather</code>和<code>tf.gather_nd</code>是您的朋友。让我们再试一次！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original tensor: &quot;</span>, x.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using the indices that we used for mask</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nIndices to update: &quot;</span>, indices_to_update.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># This works!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n Accesing tensor elements using gather&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>, tf.gather(x, indices_to_update).numpy())</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original tensor:  [1. 2. 3. 4. 5.]</span><br><span class="line">Indices to update:  [[1][3]]</span><br><span class="line">Accesing tensor elements using gather</span><br><span class="line">[[2.][4.]]</span><br></pre></td></tr></table></figure>
<p>还有另一种方法<code>tf.convert_to_tensor(..)</code>来创建张量。这与<code>tf.constant(..)</code>非常相似，但有一些细微的差别：</p>
<ul>
<li>每当您将非<code>tf.Tensor</code>对象（例如<code>Python</code>列表或<code>ndarray</code>）传递给操作时，始终会自动调用<code>Convert_to_tensor(..)</code></li>
<li>它不会形成为输入参数。</li>
<li>它甚至允许传递符号张量。</li>
</ul>
<p>何时使用<code>tf.convert_to_tensor(..)</code>？ 这取决于你的思维模式！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  An example with a python list</span></span><br><span class="line">y = tf.convert_to_tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor from python list: &quot;</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  An example with a ndarray</span></span><br><span class="line">y = tf.convert_to_tensor(np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor from ndarray: &quot;</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  An example with symbolic tensors</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Graph().as_default():</span><br><span class="line">    y = tf.convert_to_tensor(tf.compat.v1.placeholder(shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>], dtype=tf.int32))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor from python list: &quot;</span>, y)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor from python list:  tf.Tensor([1 2 3], shape=(3,), dtype=int32)</span><br><span class="line">Tensor from ndarray:  tf.Tensor([1 2 3], shape=(3,), dtype=int64)</span><br><span class="line">Tensor from python list:  Tensor(<span class="string">&quot;Placeholder:0&quot;</span>, shape=(None, None, None), dtype=int32)</span><br></pre></td></tr></table></figure>
<h5 id="String-Tensors"><a href="#String-Tensors" class="headerlink" title="String Tensors"></a>String Tensors</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># String as a tensor object with dtype==tf.string</span></span><br><span class="line">string = tf.constant(<span class="string">&quot;abc&quot;</span>, dtype=tf.string)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;String tensor: &quot;</span>, string)</span><br><span class="line"></span><br><span class="line"><span class="comment"># String tensors are atomic and non-indexable. </span></span><br><span class="line"><span class="comment"># This doen&#x27;t work as expected!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAccessing second element of the string&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(string[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String tensor:  tf.Tensor(b<span class="string">&#x27;abc&#x27;</span>, shape=(), dtype=string)</span><br><span class="line">Accessing second element of the string</span><br><span class="line">InvalidArgumentError Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice</span><br></pre></td></tr></table></figure>
<h6 id="Ragged-Tensors"><a href="#Ragged-Tensors" class="headerlink" title="Ragged Tensors"></a>Ragged Tensors</h6><p>简而言之，一个沿某个轴具有可变数量元素的张量。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This works!</span></span><br><span class="line">y = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">     [<span class="number">6</span>]</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">ragged = tf.ragged.constant(y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Creating ragged tensor from python sequence: &quot;</span>, ragged)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This won&#x27;t work</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Trying to create tensor from above python sequence\n&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    z = tf.constant(y)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Creating ragged tensor from python sequence:  &lt;tf.RaggedTensor [[1, 2, 3], [4, 5], [6]]&gt;</span><br><span class="line">Trying to create tensor from above python sequence</span><br><span class="line">ValueError Can<span class="string">&#x27;t convert non-rectangular Python sequence to Tensor.</span></span><br></pre></td></tr></table></figure>
<h5 id="Sparse-tensors"><a href="#Sparse-tensors" class="headerlink" title="Sparse tensors"></a>Sparse tensors</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s say you have a an array like this one</span></span><br><span class="line"><span class="comment"># [[1 0 0]</span></span><br><span class="line"><span class="comment">#  [0 2 0]</span></span><br><span class="line"><span class="comment">#  [0 0 3]]</span></span><br><span class="line"><span class="comment"># If there are too many zeros in your `huge` tensor, then it is wise to use `sparse`</span></span><br><span class="line"><span class="comment"># tensors instead of `dense` one. Let&#x27;s say how to create this one. We need to specify:</span></span><br><span class="line"><span class="comment"># 1. Indices where our values are</span></span><br><span class="line"><span class="comment"># 2. The values </span></span><br><span class="line"><span class="comment"># 3. The actual shape</span></span><br><span class="line"></span><br><span class="line">sparse_tensor = tf.SparseTensor(indices=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">                                values=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                                dense_shape=[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">                               )</span><br><span class="line"><span class="built_in">print</span>(sparse_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can convert sparse tensors to dense as well</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>, tf.sparse.to_dense(sparse_tensor))</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SparseTensor(indices=tf.Tensor([[0 0][1 1][2 2]], shape=(3, 2), dtype=int64), values=tf.Tensor([1 2 3], shape=(3,), dtype=int32), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))</span><br><span class="line"></span><br><span class="line">tf.Tensor([[1 0 0][0 2 0][0 0 3]], shape=(3, 3), dtype=int32)</span><br></pre></td></tr></table></figure>
<h4 id="变量（Variables）"><a href="#变量（Variables）" class="headerlink" title="变量（Variables）"></a>变量（Variables）</h4><p><strong>变量</strong>是一种“特殊”的张量。它用于表示或存储可变状态。<code>tf.Variable</code>表示一个张量，其值可以通过对其运行操作来更改。想一想您会使用<code>Variable</code>对象的情况吗？<strong>神经网络的权重</strong>是变量使用的最好例子之一。我们将首先了解如何创建变量对象，然后我们将研究其属性和一些陷阱。</p>
<p>只有一种方法可以创建<code>Variable</code>对象：<code>tf.Variable(..)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Variables with an integer value of 2 as initial value</span></span><br><span class="line">x = tf.Variable(<span class="number">2</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># Nested list as initial value</span></span><br><span class="line">y = tf.Variable([[<span class="number">2</span>, <span class="number">3</span>]], dtype=tf.int32)</span><br><span class="line">y</span><br><span class="line"><span class="comment"># Tuples also work but beware it isn&#x27;t the same as a nested list.</span></span><br><span class="line"><span class="comment"># Check the difference between the current output and the previous cell output</span></span><br><span class="line">w = tf.Variable(((<span class="number">2</span>, <span class="number">3</span>)), dtype=tf.int32)</span><br><span class="line">w</span><br><span class="line"><span class="comment"># You can even pass a tensor object as an initial value</span></span><br><span class="line">t = tf.constant([<span class="number">1</span>, <span class="number">2</span>,], dtype=tf.int32)</span><br><span class="line">z = tf.Variable(t)</span><br><span class="line">z</span><br><span class="line"></span><br><span class="line"><span class="comment"># An interesting thing to note. </span></span><br><span class="line"><span class="comment"># You can&#x27;t change the values of the tensor `t` in the above example</span></span><br><span class="line"><span class="comment"># but you can change the values of the variable created using it</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This won&#x27;t work</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    t[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># This also won&#x27;t work</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    z[<span class="number">0</span>] = <span class="number">10</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># This works though</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nOriginal variable: &quot;</span>, z)</span><br><span class="line">z[<span class="number">0</span>].assign(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Updated variable: &quot;</span>, z)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=int32, numpy=2&gt;</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(1, 2) dtype=int32, numpy=array([[2, 3]], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">TypeError <span class="string">&#x27;tensorflow.python.framework.ops.EagerTensor&#x27;</span> object does not support item assignment</span><br><span class="line">TypeError <span class="string">&#x27;ResourceVariable&#x27;</span> object does not support item assignment</span><br><span class="line"></span><br><span class="line">Original variable:  &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line">Updated variable:  &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([5, 2], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<div class="note info"><p><strong>有几点需要注意</strong>：</p>
<ul>
<li>您可以通过传递初始值来创建变量，该初始值可以是<code>Tensor</code>或可转换为<code>Tensor</code>的<code>Python</code>对象。</li>
<li>您传递的张量对象是不可变的，但使用它创建的变量是可变的。</li>
<li><strong>变量是一种特殊的张量</strong>，但张量和变量的底层数据结构都是<code>tf.Tensor</code>。</li>
<li>由于数据结构相同，因此两者的大多数属性都是相同的。</li>
<li>直接赋值（如<code>z[0]=5</code>）也不适用于<code>tf.Variable</code>。要更改值，您需要调用<code>allocate(...)</code>、<code>assign_add(...)</code>或<code>allocate_sub(...)</code>等方法。</li>
<li>任何变量都与任何其他<code>Python</code>对象具有相同的生命周期。当没有对变量的引用时，它会自动释放。</li>
</ul>
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Most of the properties that we saw for tensors in part1 are the same for variables</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of variable : <span class="subst">&#123;z.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Another method to obtain the shape using `tf.shape(..)`: <span class="subst">&#123;tf.shape(z)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dtype of the variable: <span class="subst">&#123;z.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total size of the variable: <span class="subst">&#123;tf.size(z)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Values of the variable: <span class="subst">&#123;z.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Rank: <span class="subst">&#123;z.ndim&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Crap! How to find out the no of dimensions then?</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Rank: <span class="subst">&#123;tf.shape(z)&#125;</span> or like this <span class="subst">&#123;z.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Whatever operator overloading is available for a Tensor, is also available for a Variable</span></span><br><span class="line"><span class="comment"># We have a tensor `t` and a varibale `z`. </span></span><br><span class="line">t = tf.constant([<span class="number">1</span>, <span class="number">2</span>,], dtype=tf.int32)</span><br><span class="line">z = tf.Variable(t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor t: &quot;</span>, t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Variable z: &quot;</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nThis works: &quot;</span>, (t+<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;So does this: &quot;</span>, (z +<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Another example just for demonstration: <span class="subst">&#123;(t*<span class="number">5</span>).numpy()&#125;</span>, <span class="subst">&#123;(z*<span class="number">5</span>).numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gather works as well</span></span><br><span class="line">tf.gather(z, indices=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here is another interesting difference between the properties of </span></span><br><span class="line"><span class="comment"># a tensor and a variable</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Is variable z trainable? &quot;</span>, z.trainable)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Is tensor t trainable? &quot;</span>, t.trainable)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Shape of variable : (2,)</span><br><span class="line">Another method to obtain the shape using `tf.shape(..)`: [2]</span><br><span class="line">dtype of the variable: &lt;dtype: <span class="string">&#x27;int32&#x27;</span>&gt;</span><br><span class="line">Total size of the variable: 2</span><br><span class="line">Values of the variable: [5 2]</span><br><span class="line"></span><br><span class="line">AttributeError <span class="string">&#x27;ResourceVariable&#x27;</span> object has no attribute <span class="string">&#x27;ndim&#x27;</span></span><br><span class="line"></span><br><span class="line">Rank: [2] or like this (2,)</span><br><span class="line"></span><br><span class="line">Tensor t:  tf.Tensor([1 2], shape=(2,), dtype=int32)</span><br><span class="line">Variable z:  &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">This works:  tf.Tensor([6 7], shape=(2,), dtype=int32)</span><br><span class="line">So does this:  tf.Tensor([6 7], shape=(2,), dtype=int32)</span><br><span class="line">Another example just <span class="keyword">for</span> demonstration: [ 5 10], [ 5 10]</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">Is variable z trainable?  True</span><br><span class="line">AttributeError <span class="string">&#x27;tensorflow.python.framework.ops.EagerTensor&#x27;</span> object has no attribute <span class="string">&#x27;trainable&#x27;</span></span><br></pre></td></tr></table></figure>
<p>让我们来谈谈为什么对于<code>Variable</code>对象而言，<code>trainable</code>是一个有趣的属性。</p>
<ul>
<li>任何变量都会被自动跟踪（如果它在范围内），除非它不可训练。</li>
<li>在继承<code>tf.Module</code>的类范围内定义的任何变量都会被自动跟踪，并且可以通过<code>trainable_variables</code>、变量或子模块属性进行收集。</li>
<li>有时我们不想要某个变量的梯度。在这种情况下，我们可以通过设置<code>trainable=False</code>来关闭跟踪。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, trainable=<span class="literal">False</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">z = tf.Variable(<span class="number">6.0</span>, name=<span class="string">&quot;z&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    x = x + <span class="number">2</span></span><br><span class="line">    y = y + <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>([variable.name <span class="keyword">for</span> variable <span class="keyword">in</span> tape.watched_variables()])</span><br><span class="line"><span class="comment"># [&#x27;x:0&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>变量的最大优点是可以重用内存。您可以修改这些值而不创建新值，但需要记住一些事项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a variable instance</span></span><br><span class="line">z = tf.Variable([<span class="number">1</span>, <span class="number">2</span>], dtype=tf.int32, name=<span class="string">&quot;z&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Variable <span class="subst">&#123;z.name&#125;</span>: &quot;</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Can we change the dtype while changing the values?</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    z.assign([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nOh dear...what have you done!&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Can we change the shape while assigning a new value?</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    z.assign([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nAre you thinking clearly?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># A way to create variable with an arbitrary shape</span></span><br><span class="line">x = tf.Variable(<span class="number">5</span>, dtype=tf.int32, shape=tf.TensorShape(<span class="literal">None</span>), name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nOriginal Variable x: &quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign a proper value with a defined shape</span></span><br><span class="line">x.assign([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Modified Variable x: &quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try assigning a value with a diff shape now.</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    x.assign([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nThis works!!&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Variable value modified with a diff shape: &quot;</span>, x)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nDid you forget what we just learned?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Variable z:0:  &lt;tf.Variable <span class="string">&#x27;z:0&#x27;</span> shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">Oh dear...what have you <span class="keyword">done</span>!</span><br><span class="line">TypeError Cannot convert [1.0, 2.0] to EagerTensor of dtype int32</span><br><span class="line"></span><br><span class="line">Are you thinking clearly?</span><br><span class="line">ValueError Cannot assign to variable z:0 due to variable shape (2,) and value shape (3,) are incompatible</span><br><span class="line"></span><br><span class="line">Original Variable x:  &lt;tf.Variable <span class="string">&#x27;x:0&#x27;</span> shape=&lt;unknown&gt; dtype=int32, numpy=5&gt;</span><br><span class="line">Modified Variable x:  &lt;tf.Variable <span class="string">&#x27;x:0&#x27;</span> shape=&lt;unknown&gt; dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;</span><br><span class="line"></span><br><span class="line">This works!!</span><br><span class="line">Variable value modified with a diff shape:  &lt;tf.Variable <span class="string">&#x27;x:0&#x27;</span> shape=&lt;unknown&gt; dtype=int32, numpy=</span><br><span class="line">array([[1, 2, 3],[4, 5, 6]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>

<h4 id="自动微分和梯度（Automatic-Differentiation-and-Gradients）"><a href="#自动微分和梯度（Automatic-Differentiation-and-Gradients）" class="headerlink" title="自动微分和梯度（Automatic Differentiation and Gradients）"></a>自动微分和梯度（Automatic Differentiation and Gradients）</h4><p><strong>自动微分和梯度</strong>是非常重要的概念，没有必要理解它的每一点，但你越深入它，你就会越欣赏它的美丽。假设您在前向传递中对输入应用一系列操作。要自动区分，您需要某种机制来弄清楚：</p>
<ul>
<li>前向传递中应用了哪些操作？ </li>
<li>应用操作的顺序是什么？</li>
</ul>
<p>对于<code>autodiff</code>，你需要记住上面两条。不同的框架可以以不同的方式实现相同的想法，但基本原理保持不变。<code>TensorFlow</code>提供了<code>tf.GradientTape API</code>用于<strong>自动微分</strong>。在 <code>GradientTape</code>上下文中执行的任何相关操作都会被记录下来以进行<strong>梯度计算</strong>。要计算梯度，您需要执行以下操作：</p>
<ul>
<li>记录<code>tf.GradientTape</code>上下文中的操作 。</li>
<li>使用<code>GradientTape.gradient(target, sources)</code>计算梯度。</li>
</ul>
<p>让我们为此编写几个示例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We will initialize a few Variables here</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Variable x: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is x trainable?: <span class="subst">&#123;x.trainable&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nVariable y: <span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is y trainable?: <span class="subst">&#123;y.trainable&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Variable x: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=float32, numpy=3.0&gt;</span><br><span class="line">Is x trainable?: True</span><br><span class="line"></span><br><span class="line">Variable y: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=float32, numpy=4.0&gt;</span><br><span class="line">Is y trainable?: True</span><br></pre></td></tr></table></figure>
<p>我们将在这里做一个简单的操作：<code>z = x * y</code>。我们将计算<code>z</code>对于<code>x</code>和<code>y</code>的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember we need to execute the operations inside the context</span></span><br><span class="line"><span class="comment"># of GradientTape so that we can record them</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line">    </span><br><span class="line">dx, dy = tape.gradient(z, [x, y])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input Variable x: <span class="subst">&#123;x.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input Variable y: <span class="subst">&#123;y.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output z: <span class="subst">&#123;z&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dz / dx</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dx&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dz / dy</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input Variable x: 3.0</span><br><span class="line">Input Variable y: 4.0</span><br><span class="line">Output z: 12.0</span><br><span class="line"></span><br><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: 3.0</span><br></pre></td></tr></table></figure>
<p>您可以通过以嵌套方式传递该计算中涉及的所有可训练变量（例如可以是列表或字典）来计算某些变量的梯度。返回的梯度将遵循输入传递到<code>tape</code>相同的嵌套结构。如果我们分别计算上述代码中关于 <code>x</code>和<code>y</code>的梯度，会发生什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    dx = tape.gradient(z, x)</span><br><span class="line">    dy = tape.gradient(z, y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dx&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dy&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ERROR! ERROR! ERROR!\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR! ERROR! ERROR!</span><br><span class="line">RuntimeError A non-persistent GradientTape can only be used tocompute one <span class="built_in">set</span> of gradients (or jacobians)</span><br></pre></td></tr></table></figure>
<p>一旦调用<code>GradientTape.gradient(...)</code>，<code>GradientTape</code>所持有的所有资源都会被释放。因此，如果您计算了一次梯度，那么您将无法再次调用它。解决方案是将持久参数设置为<code>True</code>。这允许多次调用<code>gradient()</code>方法，因为当<code>tape</code>对象被垃圾收集时资源被释放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the persistent argument</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    dx = tape.gradient(z, x)</span><br><span class="line">    dy = tape.gradient(z, y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dx&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dy&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ERROR! ERROR! ERROR!\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(ex).__name__, ex)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: 3.0</span><br></pre></td></tr></table></figure>
<p>如果一个变量不可训练，会如何？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># What if one of the Variables is non-trainable?</span></span><br><span class="line"><span class="comment"># Let&#x27;s make y non-trainable in the above example and run</span></span><br><span class="line"><span class="comment"># the computation again</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line">    </span><br><span class="line">dx, dy = tape.gradient(z, [x, y])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Variable x: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is x trainable?: <span class="subst">&#123;x.trainable&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nVariable y: <span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is y trainable?: <span class="subst">&#123;y.trainable&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Variable x: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=float32, numpy=3.0&gt;</span><br><span class="line">Is x trainable?: True</span><br><span class="line"></span><br><span class="line">Variable y: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=float32, numpy=4.0&gt;</span><br><span class="line">Is y trainable?: False</span><br><span class="line"></span><br><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: None</span><br></pre></td></tr></table></figure>
<div class="note warning"><p><strong>注意</strong>：要记住的重要一点是，<strong>切勿混合<code>AD</code>数据类型的拓扑和计算梯度</strong>。当我说拓扑时，我的意思是不要混合使用<code>float、int、string</code>类型。事实上，您不能对数据类型为<code>int</code>或 <code>string</code>的任何操作采用梯度。</p>
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note the dtypes</span></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>, dtype=tf.float32)</span><br><span class="line">y = tf.Variable(<span class="number">4</span>, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * tf.cast(y, x.dtype)</span><br><span class="line">    </span><br><span class="line">dx, dy = tape.gradient(z, [x, y])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input Variable x: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input Variable y: <span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output z: <span class="subst">&#123;z&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input Variable x: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=float32, numpy=3.0&gt;</span><br><span class="line">Input Variable y: &lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=int32, numpy=4&gt;</span><br><span class="line">Output z: 12.0</span><br><span class="line"></span><br><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: None</span><br></pre></td></tr></table></figure>
<p>您可能会说您的代码是正确的，从某种意义上来说是正确的，但是您将得到<code>None</code>作为源和目标数据类型不同的变量的梯度值。在我们讨论一个非常重要的概念之前，让我们总结一下到目前为止我们学到的所有内容：</p>
<ul>
<li><code>tf.GradientTape</code>是在<code>TensorFlow</code>中进行<code>AD</code>的<code>API</code>。</li>
<li>为了使用<code>Tape</code>计算梯度，我们需要：<ul>
<li>在<code>Tape</code>上下文中记录相关操作</li>
<li>通过调用<code>GradientTape.gradient(...)</code>方法计算渐变</li>
</ul>
</li>
<li>如果您希望多次调用<code>gradient(...)</code>方法，请确保将持久参数设置为<code>GradientTape</code>。</li>
<li>如果计算中涉及任何不可训练的变量，则该变量的梯度将为<code>None</code>。</li>
<li>混合数据类型拓扑是一个错误。</li>
</ul>
<p><strong>精细增益控制</strong>(<code>Fine-gain control</code>)</p>
<ul>
<li>如何访问所有正在监视的对象？</li>
<li>如何停止梯度通过特定变量&#x2F;路径的流动？</li>
<li>如果您不想查看<code>GradientTape</code>上下文中的所有变量怎么办？</li>
<li>如果您想观看上下文之外的内容怎么办？</li>
</ul>
<p>我们将为上述每个案例举几个例子，以便更好地理解它。</p>
<h5 id="访问所有被监视的对象"><a href="#访问所有被监视的对象" class="headerlink" title="访问所有被监视的对象"></a>访问所有被监视的对象</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">t = tf.Variable(tf.random.normal(shape=(<span class="number">2</span>, <span class="number">2</span>)), name=<span class="string">&quot;t&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tape is watching all of these:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tape.watched_variables():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;var.name&#125;</span> and it&#x27;s value is <span class="subst">&#123;var.numpy()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tape is watching all of these:</span><br><span class="line">x:0 and it<span class="string">&#x27;s value is 3.0</span></span><br><span class="line"><span class="string">y:0 and it&#x27;</span>s value is 4.0</span><br></pre></td></tr></table></figure>
<h5 id="停止梯度"><a href="#停止梯度" class="headerlink" title="停止梯度"></a>停止梯度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The ugly way</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * y</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stop the grasdient flow</span></span><br><span class="line">    <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">        zz = x*x + y*y</span><br><span class="line"></span><br><span class="line">dz_dx, dz_dy = tape.gradient(z, [x, y])</span><br><span class="line">dzz_dx, dzz_dy = tape.gradient(zz, [x, y])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dz_dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dz_dy&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of zz wrt x: <span class="subst">&#123;dzz_dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of zz wrt y: <span class="subst">&#123;dzz_dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: 3.0</span><br><span class="line"></span><br><span class="line">Gradient of zz wrt x: None</span><br><span class="line">Gradient of zz wrt y: None</span><br></pre></td></tr></table></figure>
<p>停止梯度流的更好方法是使用<code>tf.stop_gradient(...)</code>。为什么？</p>
<ul>
<li>不需要访问<code>tape </code></li>
<li>干净且具有更好的语义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The better way!</span></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = x * tf.stop_gradient(y)</span><br><span class="line"></span><br><span class="line">dz_dx, dz_dy = tape.gradient(z, [x, y])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dz_dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dz_dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: None</span><br></pre></td></tr></table></figure>
<h5 id="选择上下文中要观看的内容"><a href="#选择上下文中要观看的内容" class="headerlink" title="选择上下文中要观看的内容"></a>选择上下文中要观看的内容</h5><p>默认情况下，<code>GradientTape将</code>自动监视在上下文中访问的任何可训练变量，但如果您只需要选定变量的梯度，则可以通过将<code>watch_accessed_variables=False</code>传递给<code>tape</code>构造函数来禁用自动跟踪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Both variables are trainable</span></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Telling the tape: Hey! I will tell you what to record.</span></span><br><span class="line"><span class="comment"># Don&#x27;t start recording automatically!</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(watch_accessed_variables=<span class="literal">False</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># Watch x but not y</span></span><br><span class="line">    tape.watch(x)</span><br><span class="line">    z = x * y</span><br><span class="line"></span><br><span class="line">dz_dx, dz_dy = tape.gradient(z, [x, y])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dz_dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dz_dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: None</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># What if something that you wanted to watch,</span></span><br><span class="line"><span class="comment"># wasn&#x27;t present in the computation done inside the context?</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">t = tf.Variable(<span class="number">5.0</span>, name=<span class="string">&quot;t&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Telling the tape: Hey! I will tell you what to record.</span></span><br><span class="line"><span class="comment"># Don&#x27;t start recording automatically!</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(watch_accessed_variables=<span class="literal">False</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># Watch x but not y</span></span><br><span class="line">    tape.watch(x)</span><br><span class="line">    z = x * y</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># `t` isn&#x27;t involved in any computation here</span></span><br><span class="line">    <span class="comment"># but what if we want to record it as well</span></span><br><span class="line">    tape.watch(t)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tape watching only these objects that you asked it to watch&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tape.watched_variables():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;var.name&#125;</span> and it&#x27;s value is <span class="subst">&#123;var.numpy()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tape watching only these objects that you asked it to watch</span><br><span class="line">x:0 and it<span class="string">&#x27;s value is 3.0</span></span><br><span class="line"><span class="string">t:0 and it&#x27;</span>s value is 5.0</span><br></pre></td></tr></table></figure>
<h5 id="Multiple-Tapes"><a href="#Multiple-Tapes" class="headerlink" title="Multiple Tapes"></a>Multiple Tapes</h5><p>您可以使用多个<code>GradientTape</code>来记录不同的对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>, name=<span class="string">&quot;y&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape_for_x, tf.GradientTape() <span class="keyword">as</span> tape_for_y:</span><br><span class="line">    <span class="comment"># Watching different variables with different tapes</span></span><br><span class="line">    tape_for_x.watch(x)</span><br><span class="line">    tape_for_y.watch(y)</span><br><span class="line">    </span><br><span class="line">    z = x * y</span><br><span class="line"></span><br><span class="line">dz_dx = tape_for_x.gradient(z, x)</span><br><span class="line">dz_dy = tape_for_y.gradient(z, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt x: <span class="subst">&#123;dz_dx&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of z wrt y: <span class="subst">&#123;dz_dy&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient of z wrt x: 4.0</span><br><span class="line">Gradient of z wrt y: 3.0</span><br></pre></td></tr></table></figure>
<h5 id="高阶梯度"><a href="#高阶梯度" class="headerlink" title="高阶梯度"></a>高阶梯度</h5><p>在<code>GradientTape</code>上下文中完成的任何计算都会被记录。如果计算涉及<strong>梯度计算</strong>，它也会被记录下来。这使得使用相同的<code>API</code>可以轻松计算<strong>高阶梯度</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">3.0</span>, name=<span class="string">&quot;x&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape0:</span><br><span class="line">        y = x * x * x</span><br><span class="line">    first_order_grad = tape0.gradient(y, x)</span><br><span class="line">second_order_grad = tape1.gradient(first_order_grad, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Variable x: <span class="subst">&#123;x.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nEquation is y = x^3&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First Order Gradient wrt x (3 * x^2): <span class="subst">&#123;first_order_grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Second Order Gradient wrt x (6^x): <span class="subst">&#123;second_order_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable x: 3.0</span><br><span class="line">Equation is y = x^3</span><br><span class="line">First Order Gradient wrt x (3 * x^2): 27.0</span><br><span class="line">Second Order Gradient wrt x (6^x): 18.0</span><br></pre></td></tr></table></figure>
<h5 id="陷阱（Gotchas）"><a href="#陷阱（Gotchas）" class="headerlink" title="陷阱（Gotchas）"></a>陷阱（Gotchas）</h5><p>我们已经知道<code>int</code>或<code>string</code>数据类型的梯度未定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># What happens when you tries to take gradient wrt a Tensor?</span></span><br><span class="line">x = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = x * x</span><br><span class="line">    </span><br><span class="line">dy_dx = tape.gradient(y, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nGradient of y wrt x: &quot;</span>, dy_dx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.Tensor(3.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="comment"># Gradient of y wrt x:  None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s modify the above code a bit</span></span><br><span class="line">x = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch(x)</span><br><span class="line">    y = x * x</span><br><span class="line">    </span><br><span class="line">dy_dx = tape.gradient(y, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nGradient of y wrt x: &quot;</span>, dy_dx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.Tensor(3.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="comment"># Gradient of y wrt x:  tf.Tensor(6.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<h5 id="状态和梯度"><a href="#状态和梯度" class="headerlink" title="状态和梯度"></a>状态和梯度</h5><p><code>GradientTape</code>只能从当前状态读取，而不能从它的历史记录中读取。状态阻止梯度计算回溯到更远的地方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># Change the state of x by making x = x + y</span></span><br><span class="line">    x.assign_add(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Let&#x27;s do some computation e.g z = x * x </span></span><br><span class="line">    <span class="comment"># This is equivalent to z = (x + y) * (x + y) because of above assign_add</span></span><br><span class="line">    z = x * x</span><br><span class="line">    </span><br><span class="line">dy = tape.gradient(z, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Gradients of z wrt y: &quot;</span>, dy)</span><br></pre></td></tr></table></figure>
<p>结果输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradients of z wrt y:  None</span><br><span class="line"></span><br></pre></td></tr></table></figure>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/" title="TensorFlow &amp; JAX">https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/27/artificial-intelligence/game_ai_study/" rel="prev" title="游戏AI & 强化学习">
                  <i class="fa fa-chevron-left"></i> 游戏AI & 强化学习
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/28/artificial-intelligence/jax_study/" rel="next" title="JAX（DeviceArray）">
                  JAX（DeviceArray） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">976k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">54:14</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2024/03/27/artificial-intelligence/jax_tensorflow_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
