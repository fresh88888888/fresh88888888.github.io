<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="AGI系统：实现AGI机制超越Transformers：尽管Transformers架构取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。混合专家(MoEs)使用多个“专家”子网络组成的条件模块替换了Transformer模型中的稠密层。使用路由机制在词元级或任务级动态决定使用哪个专家。尽管拥有多个专家，稀疏的MoEs通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(ML)(二十七) — 强化学习探析">
<meta property="og:url" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="AGI系统：实现AGI机制超越Transformers：尽管Transformers架构取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。混合专家(MoEs)使用多个“专家”子网络组成的条件模块替换了Transformer模型中的稠密层。使用路由机制在词元级或任务级动态决定使用哪个专家。尽管拥有多个专家，稀疏的MoEs通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_2.png">
<meta property="article:published_time" content="2025-03-15T08:00:11.000Z">
<meta property="article:modified_time" content="2025-03-15T08:00:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/","path":"2025/03/15/artificial-intelligence/ML27_theory_study/","title":"机器学习(ML)(二十七) — 强化学习探析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习(ML)(二十七) — 强化学习探析 | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#AGI%E7%B3%BB%E7%BB%9F%EF%BC%9A%E5%AE%9E%E7%8E%B0AGI%E6%9C%BA%E5%88%B6"><span class="nav-number">1.</span> <span class="nav-text">AGI系统：实现AGI机制</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">255</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习(ML)(二十七) — 强化学习探析 | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习(ML)(二十七) — 强化学习探析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-15 16:00:11" itemprop="dateCreated datePublished" datetime="2025-03-15T16:00:11+08:00">2025-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>31 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h5 id="AGI系统：实现AGI机制"><a href="#AGI系统：实现AGI机制" class="headerlink" title="AGI系统：实现AGI机制"></a>AGI系统：实现AGI机制</h5><p><strong>超越</strong><code>Transformers</code>：尽管<code>Transformers</code><strong>架构</strong>取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。<strong>混合专家</strong>(<code>MoEs</code>)使用多个“<strong>专家</strong>”子网络组成的条件模块替换了<code>Transformer</code><strong>模型</strong>中的稠密层。使用<strong>路由机制</strong>在<strong>词元级</strong>或<strong>任务级</strong>动态决定使用哪个<strong>专家</strong>。尽管拥有多个<strong>专家</strong>，稀疏的<code>MoEs</code>通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务中实现。然而，<code>MoEs</code>在推理过程中也带来了其他挑战，例如将所有专家加载到<code>VRAM</code>中的要求以及在多个节点上分发<strong>专家</strong>。</p>
<span id="more"></span>

<p><strong>状态空间模型</strong>(<code>SSMs</code>)最近被应用于建模<strong>序列到序列</strong>的转换，可以在各种模型架构拓扑中替代<strong>二次自注意力机制</strong>。（<strong>离散化</strong>）<code>SSM</code>通过一组可学习参数(<code>∆, A, ¯ B, C ¯</code>)在每个时间步（标记）上定义<strong>递归关系</strong>，大多数研究试图解决的主要挑战是如何以<strong>并行化</strong>的方式计算这种递归，以便有效利用<strong>现代硬件加速器</strong>（例如<code>FFTConv</code>）。在这一类别中，最简单的形式是<strong>线性注意力</strong>，它可以被视为一种退化的<code>SSM</code>。在其核心，<strong>线性注意力</strong>将<strong>自注意力表示</strong>为<strong>核特征映射</strong>的<strong>线性点积</strong>，并利用<strong>矩阵乘积</strong>的结合性质将复杂度降低到线性。<code>S4</code>通过<strong>低秩校正</strong>对<code>SSM</code>进行了表达性和高效的参数化，使其能够稳定<strong>对角化</strong>，并将<code>SSM</code>简化为<strong>柯西核</strong>的计算。在<code>S4</code>之后，还有许多后续研究尝试对<strong>转移矩阵</strong><code>A¯</code>（和其他矩阵）进行不同的参数化，以提高计算效率和建模能力。<code>H3</code>提出了一个由两个堆叠的独立<code>SSM</code>组成的<code>SSM</code>块，专门设计用于应对回忆早期标记的挑战，并支持跨序列的标记比较。<code>Hyena</code>通过用交错和隐式参数化的<strong>长卷积</strong>以及<strong>数据控制门</strong>替换<code>S4</code>层，推广了<code>H3</code>，从而将参数大小与滤波器大小解耦，增加了表达能力。后来提出了一种名为<code>Retentive Network</code>的基础架构，其中包含额外的<strong>门</strong>和<strong>多头注意力</strong>的变体，实现了<strong>恒定推理成本</strong>和<strong>线性长序列内存消耗</strong>。<code>RWKV</code>是一种新架构，结合了<code>Transformer</code>的高效并行训练和<code>RNN</code>的高效推理。本质上，主要的“<code>WKV</code>”操作涉及线性时不变(<code>LTI</code>)递归，可以解释为两个<code>SSM</code>的比率。为了克服以前<code>SSM</code>模型的主要弱点，即它们无法进行基于内容的推理，<code>Mamba</code>提出了<strong>选择性状态空间</strong>，可以使<code>SSM</code>参数成为输入的函数，从而将<code>SSM</code>从<code>LTI</code>转变为时变。尽管不再能够应用高效卷积，但他们设计了面向硬件的并行算法，用于递归计算，称为<strong>并行关联扫描</strong>，使其能够实现比<code>Transformer</code>高<code>5</code>倍的吞吐量，在多种模态上达到最先进的性能，并在实际数据上不断提高，直至百万长度的序列。重新审视<strong>循环神经网络</strong>（<code>RNN</code>）的兴趣火花也随着其在长上下文处理中的主要优势（即<strong>隐藏状态的线性时间</strong>和<strong>恒定内存</strong>）而出现。<code>RNN</code>的一个挑战是如何高效地扩展训练和推理。一种基于<code>RNN</code>的模型，具有<strong>门控线性递归</strong>，以及<code>Griffin</code>，它将<strong>门控线性递归</strong>与<strong>局部注意力</strong>相结合。他们展示了<code>Hawk</code>在下游任务上的优越性能，<code>Griffin</code>的性能与<code>Llama-2</code>相当，但训练标记数量减少了六倍。他们不仅展示了<strong>长上下文能力</strong>的潜力，还解释了如何在<strong>分布式训练</strong>和推理期间通过将 <code>Griffin</code>扩展到<code>14B</code>参数来有效利用硬件加速器。紧接着，一系列名为<code>RecurrentGemma</code>的模型以各种尺寸发布，包括预训练和指令调优版本。这些进展展示了在不依赖<code>Transformer</code>架构的情况下，训练一个数据高效、固定状态大小、长上下文和表达能力强的模型的可能性。最近的研究还探索了高层次架构混合策略，旨在结合不同变体的优势。并提出将<code>Transformer</code>与<code>Mamba</code>结合，通过<strong>交错层</strong>实现，在标准和长上下文任务上取得了令人印象深刻的结果，同时资源需求可控。除了手动设计，<code>MAD</code>将这一过程整合到一个端到端的流水线中，包含能够预测扩展规律的小规模能力单元测试。<code>MAD</code>成功找到了一种高效的架构，名为<code>Striped Hyena</code>，基于<strong>混合</strong>和<strong>稀疏</strong>，在计算最优预算和过度训练条件下，其扩展性能优于最先进的<code>Transformer</code>、<code>卷积</code>和<code>循环架构</code>（<code>Transformer++、Hyena、Mamba</code>）。这些研究很可能将继续激发对既高效又具扩展性的架构设计的进一步探索，打破当前的<code>Transformer</code>范式。</p>
<p><strong>大规模训练</strong>：扩展大模型的训练在现代硬件上面临许多挑战，例如由于内存需求增加，模型无法再容纳在单个<code>GPU</code>中，通过更多计算单元加速训练速度同时尽量减少开销（<strong>线性扩展</strong>），以及利用分散资源等。<strong>并行计算</strong>：大语言模型在具有多个计算单元的集群环境中的并行性通常可以分为四种主要模式，通常称为“<code>4D</code><strong>并行</strong>”。</p>
<ul>
<li><strong>分布式数据并行</strong>(<code>DDP</code>)：是最简单的设置，其中模型在各单元上复制，数据被切片并分别输入每个模型，通常在每次传递结束时进行同步步骤。<code>DDP</code>的更复杂版本，如<code>ZeRO</code>和<code>FSDP</code>，在现代大型训练框架（如 <code>DeepSpeed</code>、<code>FairScale</code>和<code>Megatron-LM</code>中被广泛使用。</li>
<li><strong>张量并行</strong>(<code>TP</code>)或<strong>模型并行</strong>：将模型权重分成多个块，分布在多个<code>GPU</code>上。这种水平分割允许数据在分片权重上并行处理，然后在每个步骤结束时聚合结果，通常涉及清晰融合以减少同步通信。</li>
<li><strong>流水线并行</strong>(<code>PP</code>)：将模型层垂直分配到不同的<code>GPU</code>上，数据将在不同单元的各个阶段之间移动。</li>
<li><strong>序列并行</strong>(<code>SP</code>)：主要针对长上下文任务，沿序列维度进行分割，以缓解计算和存储负载。</li>
</ul>
<p>结合不同的并行方式可能会带来高效的系统。然而，它们独特的权衡和集群配置，这并不容易实现。<code>Alpha、HexGen</code>和<code>FlexFlow</code>尝试自动化模型训练和推理的并行化过程，以最大化硬件利用率。<strong>集群配置</strong>（如内存、带宽和单个加速器的延迟、网络带宽等）通常是估计的，并使用<strong>动态规划</strong>和<strong>受限优化</strong>等搜索算法来找到最佳<strong>并行策略</strong>。通过自适应分配请求来支持<strong>非对称计算</strong>，以满足延迟要求。这些自动并行调度方法在许多涉及硬件和<strong>网络异构</strong>的情况下，已被证明可以与手动设计媲美甚至超越。</p>
<p><strong>内存管理</strong>：<strong>内存管理</strong>是训练和部署大模型的最关键方面之一，特别是在长上下文领域，<code>KV</code><strong>缓存</strong>的内存占用可能轻松超过模型权重和激活的总和。受传统操作系统设计启发，<code>vLLM</code>的<strong>分页注意力</strong>通过将<code>KV</code><strong>缓存</strong>分区为非连续的内存块来解决内存碎片问题，显著提高了内存利用率，从而提高了系统<strong>吞吐量</strong>和<strong>效率</strong>。<code>FastGen</code>引入了一种自适应的<code>KV</code>缓存压缩技术，通过结构分析动态驱逐非特殊标记，减少内存使用。<code>Scissorhands</code>和<code>H2O</code>也有类似的观察，即保留关键标记可以在最小的微调和内存节省下保留大部分性能。<code>Infinite-LLM</code>首先将注意力计算分割成可分配给不同单元的较小子程序。为了实现这些子程序的高效分配，开发了一种专用服务器，可以动态管理<code>KV</code><strong>缓存</strong>，并有效协调数据中心范围内的所有可用<code>GPU</code>和<code>CPU</code>内存。许多重要技术已被广泛采用于流行的<strong>深度学习框架</strong>中，以将更大的模型适配到固定内存的设备中。<code>CPU</code>卸载允许模型有选择地将<strong>权重</strong>（层）或<code>KV</code><strong>缓存</strong>转移到具有更多内存的<code>CPU</code>，并仅将必要的网络部分加载到<code>GPU</code>进行处理。在极端情况下，<code>FlexGen</code>可以在单个<code>16GB</code> <code>GPU</code>上实现<code>OPT-175B</code>的显著<strong>批处理吞吐量</strong>。<strong>梯度检查点</strong>通过在<strong>反向传播</strong>期间重新计算计算图的部分来减少<strong>峰值内存</strong>使用。毫无疑问，高效的内存管理将继续作为核心投资方向，使可扩展系统的部署和更大批次的并行处理成为可能。</p>
<p><strong>高效微调</strong>：预训练的大模型通常内化了大量知识，可以通过（<strong>指令</strong>）<strong>微调</strong>释放出来。然而，尽管通常只需少量示例即可成功<strong>微调</strong>，但其成本和时间仍然高昂且不经济。<strong>高效微调</strong>的主要目标是在成本（实现难度、数据需求、训练预算等）和<strong>持续预训练</strong>的性能差距之间找到平衡。为应对这一挑战，开发了一系列<strong>参数高效微调</strong>(<code>PEFT</code>)技术，这些技术只需训练少量新参数，通常比上下文学习表现更好。<code>LoRA</code>作为最受欢迎的<code>PEFT</code>方法之一，近来备受关注。<code>LoRA</code>及其许多变体（如<code>LoHA、AdaLoRA、Q-LoRA</code>和最近的<code>PiSSA</code>）通过<strong>插入低秩分解</strong>的可学习矩阵来代替<strong>增量权重矩阵</strong>。<code>LLaMA-Adapter</code>通过很少的计算预算高效地将<code>LLaMA</code>微调为指令遵循模型。一组可学习的适配提示首先被添加到上下文中，并通过零初始化的<strong>注意力机制</strong>和<strong>零门控机制</strong>，仅使用<code>52K</code>自我指导示例进行训练。来自适配器的额外<code>1.2M</code>参数可以产生高质量的输出，与完全微调的结果相当。与<code>LoRA</code>类似，<code>IA3</code>通过可学习向量而非矩阵来缩放模型激活。其他插入可学习组件的<code>PEFT</code>方法展示了强大的<strong>泛化能力</strong>，而基于提示的方法（如软提示）则在保持原始模型权重不变的情况下，向输入嵌入中添加额外的可学习参数。<strong>适配器</strong>在<strong>注意力块</strong>内添加可训练参数，而<strong>前缀调整</strong>则在<strong>注意力</strong>中的<code>KV</code>表示中附加<strong>可学习向量</strong>。与传统的<code>PEFT</code>技术不同，微调<code>Transformer</code>的<code>LayerNorm</code>层可以作为一个强大的基准，带来不错的性能。</p>
<p><strong>去中心化</strong>：许多研究致力于利用云端分散且硬件异构的计算设备进行模型训练和推理。位置上分离的集群面临的一个挑战是通信开销，这使得数据移动（如<strong>训练数据</strong>、<strong>梯度</strong>、<code>KV</code><strong>缓存</strong>等）成本高昂，从而抵消了<strong>去中心化</strong>的好处。<code>CacheGen</code>通过<strong>编码器</strong>将<code>KV</code><strong>缓存</strong>压缩成紧凑的<strong>比特流表示</strong>，减少了上下文获取和处理的延迟。<code>CocktailSGD</code>采用<strong>稀疏化</strong>和<strong>量化技术</strong>的组合，使得在慢速网络上微调高达<code>20B</code>的大语言模型成为可能，并且与数据中心的快速互连相比，仅有最小的减速。<code>DiLoCo</code>在连接不佳的设备岛上引入了一种新的<strong>联合平均算法</strong>，声称在<code>C4</code>数据集上的表现与完全同步优化相当，同时通信量减少了<code>500</code>倍。<strong>协作训练</strong>通过众包方式从个人用户那里获取普通<code>GPU</code>，其中最著名的例子是<code>Petals</code>，该系统能够使用多方的普通<code>GPU</code>以不错的性能（例如支持交互式会话）提供和<strong>微调</strong><code>BLOOM-176B</code>和<code>OPT-175B</code>。<strong>去中心化</strong>的<strong>人工智能系统</strong>开启了全球设备互联的可能性，确保了<strong>容错性</strong>和异构设备及网络的<strong>兼容性</strong>，同时优化了有限的<strong>网络带宽</strong>和<strong>数据隐私</strong>。</p>
<p><strong>训练动态与扩展</strong>：大语言模型的科学难以捉摸，理解它们可以极大地改进各种<strong>人工智能</strong>的开发。然而，大多数成功的<code>LLM</code>不仅在数据和模型权重方面不完全“<strong>开放</strong>”，还在中间检查点和工件日志等其他方面也不开放，而这些信息可以帮助<strong>推理训练动态</strong>，因为我们将模型扩展到不同的规模。<code>OPT</code>模型在各种下游任务上的中间检查点，试图强调<strong>困惑度</strong>作为模型性能的<strong>预测指标</strong>，而不是其大小，表明较大的模型出现<strong>幻觉</strong>的频率较低，并且模型在训练早期阶段表现出最小的回报。与此互补，研究了不同<strong>模型大小</strong>、<strong>数据集大小</strong>和学习率下的<strong>记忆能力</strong>，并提出了关于名词和数字作为记忆单个训练示例的<strong>唯一标识符</strong>的重要性的有趣假设。除了纯粹的分析，<code>Pythia</code>推出了一套由<code>16</code>个<code>LLM</code>组成的套件，这些模型在公共数据上训练，参数量从<code>70M</code>到<code>12B</code>不等。通过将这些中间检查点发布给更广泛的社区，研究人员可以更轻松、更高效地通过检查和基准测试单个保存的权重和损失来找到与<strong>训练动态</strong>相关的问题的答案。最后，<code>OLMo</code>发布了整个框架，包括训练数据和训练及评估代码，以便更容易研究<code>LLM</code>背后的科学。</p>
<p><strong>推理技术</strong>：<code>AGI</code><strong>推理系统</strong>需要确保用户响应性、可用性和效率，从而在<strong>训练阶段</strong>释放大模型的最大潜力，并改变用户与系统的交互方式。</p>
<p><strong>解码算法</strong>：在这里主要关注<strong>精确解码加速</strong>，目的是在不影响准确性的情况下最大化性能。对几种近似方法进行了全面的综述，包括<strong>采样策略</strong>、<strong>非自回归解码</strong>、<strong>半自回归解码</strong>、<strong>块并行解码</strong>等。大量研究探讨了推测解码的理念，通过并行计算来提高生成多个标记的机会。通常，<strong>推测解码</strong>过程从一个高效的<strong>草稿模型</strong>开始，该模型对多个步骤进行预测，生成的提案由我们想要采样的<strong>目标模型验证</strong>。然而，这其中存在许多挑战，包括：如何使草稿模型轻量化，同时仍能生成有用的猜测以实现高效进展；如何避免大量架构变化和微调以实现更快的适应；如何更有效地部署<strong>草稿模型</strong>。最简单但有效的变体称为<strong>提示查找解码</strong>，其中<strong>草稿模型</strong>被从现有数据库中进行简单前缀字符串匹配所取代，以生成<strong>候选标记</strong>。这种与模型无关的方法可以非常快速地解码，而无需任何微调或模型更改，但其性能严重依赖于字符串池的质量和多样性。为了加快对大量候选项的验证，<code>SpecInfer</code>将<strong>草稿模型</strong>的输出组织成一个标记树，每个节点都是一个<strong>候选标记</strong>，其正确性可以被基础模型并行高效地检查。类似地，<code>Medusa</code>引入了一种树<strong>注意力机制</strong>，同时检查来自<code>medusa</code>头的所有标记，这通过特殊的<strong>掩码模式</strong>实现了高效的并行计算。<strong>自推测解码</strong>提出完全放弃<strong>草稿模型</strong>的要求，通过选择性地跳过一部分中间层来生成候选序列。<strong>硬件感知算法</strong>在解码阶段特别有效和吸引人。遵循高效<strong>自注意力</strong>工作，<code>Flash-Decoding</code>沿<strong>序列维度</strong>进行分割，并与其<code>KV</code><strong>缓存</strong>和统计数据并行处理这些块，其结果将通过<strong>归约步骤聚合</strong>以获得精确输出。为了克服<code>Flash-Decoding</code>的局限性并应用更多系统级优化，<code>FlashDecoding++</code>引入了基于统一最大值的异步<code>softmax</code>（避免同步开销）、优化的平坦<code>GEMM</code>操作与<strong>双缓冲</strong>（<code>GEMM</code>的性能取决于矩阵形状）以及基于启发式的数据流与硬件资源适应，以加速解码过程，结果比<code>HuggingFace</code>快了<code>4</code>倍以上。</p>
<p><strong>请求调度</strong>：<strong>大语言模型</strong>(<code>LLM</code>)的请求调度与传统<strong>机器学习系统</strong>相比，具有几个独特的挑战。成熟的<strong>请求调度策略</strong>的重要特性包括：1、<strong>高效地预取上下文</strong>（用户信息、过去的<code>KV</code><strong>缓存</strong>和<strong>模型适配器</strong>等）以便为给定输入提供服务；2、处理具有<strong>可变序列长度</strong>的示例，以最大化<code>GPU</code>利用率；3、在时间到首个<strong>令牌</strong>(<code>TTFT</code>)、<strong>作业完成时间</strong>(<code>JCT</code>)、<strong>批量令牌吞吐量</strong>和<strong>推理延迟</strong>等请求级别指标之间进行权衡。<code>Orca</code>提出了一种迭代级别的<strong>调度机制</strong>，以满足<code>LLM</code>推理请求的<strong>自回归特性</strong>，当与<strong>选择性批处理</strong>的技术结合时，可以更好地利用硬件，从而在<strong>吞吐量</strong>和<strong>延迟</strong>方面超越之前的<strong>推理引擎</strong>，如<code>FastTransformer</code>(<code>NVIDIA, 2023a</code>)。其他动态批处理策略也被广泛探索，例如<code>vLLM</code>的<strong>连续批处理</strong>和<code>TensorRT-LLM</code>(<code>NVIDIA, 2023b</code>)的<strong>飞行批处理</strong>。与<strong>请求调度</strong>不同，<code>FastServe</code>利用<code>LLM</code>推理的<strong>自回归模式</strong>，使每个输出令牌的抢占成为可能，通过一种新的<strong>跳跃连接多级反馈队列调度器</strong>优化<code>JCT</code>，该调度器利用输入长度信息以提高效率。<strong>推理工作负载</strong>与示例的<strong>平均序列长度</strong>密切相关，因此我们希望最小化最长和最短句子之间的差距。<code>S³</code>预测批处理中每个示例的潜在响应长度，用于在相同的<strong>内存约束</strong>（例如<code>GPU</code>内存）下容纳更多示例。<code>DeepSpeed-FastGen</code>的动态<code>SplitFuse</code>利用<code>LLM</code>推理的思想（批量大小与令牌数量变化对模型性能的影响），提出了一种<strong>令牌组合策略</strong>。动态<code>SplitFuse</code>通过从提示中获取部分令牌并与生成进行组合，以一致的前向大小运行。例如，长提示被分割成几个前向迭代中的较小块，而短提示则被组合以与其他请求对齐。通过这种策略，系统不仅提供了更好的效率和响应能力，还减少了请求的差异。</p>
<p><strong>多模型服务</strong>：除了为同一模型提供多个副本外，能够高效部署大量任务专用模型成为许多应用场景（如<code>LLM</code><strong>智能体</strong>、<strong>个性化聊天机器人</strong>、<strong>隐私敏感助手</strong>等）的重要特性。然而，简单地扩展实例数量在计算上不仅成本高昂，而且浪费资源。随着<code>PEFT</code>技术的进步，使用<strong>多样化适配器</strong>为基础模型提供服务成为许多从业者青睐的范式，因为<code>PEFT</code>模型轻量且易于维护，同时具有灵活性和强大的功能。<code>多模型</code>(<code>PEFT</code>)<strong>服务</strong>的主要挑战是如何动态且高效地为每个示例加载“正确的”（以延迟或任务性能等衡量）<strong>适配器</strong>。<code>Punica</code>通过设计新的<code>CUDA</code><strong>内核</strong>，实现了在一个批次中对异构<code>LoRA</code>头的高效计算，该内核共享一个预训练模型，实现了高达<code>12</code>倍的吞吐量，同时仅增加了轻微的额外延迟。<code>S-LoRA</code>引入了<strong>统一分页</strong>，使用统一的<strong>内存池</strong>进行<strong>动态适配器管理</strong>，并使用高度优化的<code>CUDA</code><strong>内核</strong>并行化<code>LoRA</code>计算。<code>LoRAX</code>还提供<strong>适配器交换调度</strong>，在<code>GPU</code>和<code>CPU</code>内存之间<strong>异步预取</strong>和<strong>卸载适配器</strong>，并<strong>调度请求批处理</strong>以优化<strong>总吞吐量</strong>。通过这些系统，可以在单个<code>GPU</code>上为超过一千个不同的<code>LoRA头</code>提供服务，从而开启了<strong>模型协作</strong>、<strong>任务泛化</strong>和<strong>模型合并</strong>等更广泛的可能性。</p>
<p><strong>成本与效率</strong>：与模型训练和推理相关的成本往往容易被忽视，但在实际应用中，尤其是在工业环境中，这些因素常常影响许多决策，如<strong>模型架构设计</strong>、<strong>数据混合选择</strong>和<strong>服务定价</strong>。</p>
<p><strong>数据经济</strong>：数据在模型性能中起着至关重要的作用，数据价值的问题因此变得非常重要：1、我们应收集哪些数据以添加到现有数据混合中以提高性能；2、我们应如何支付给数据提供者；3、我们能否移除非必要的数据（异常值）以使我们的模型更健壮。为了回答这些问题，许多计算机科学和经济学（<strong>博弈论</strong>）的研究探讨了不同的形式来定义“<strong>数据价值</strong>”及其高效估计方法。来自<strong>经典博弈论</strong>的<strong>夏普利值</strong>(<code>Shapley value</code>)独特地满足了公平数据评估的几个自然属性。由于其丰富的理论成果，<strong>夏普利值</strong>在<strong>数据经济</strong>领域被广泛用作数据重要性的<strong>定量</strong>和<strong>替代性度量</strong>（例如，夏普利值估计可用于数据采样、清理、定价、异常检测等）。计算数据<strong>夏普利值</strong>需要指数时间，因此<strong>蒙特卡罗方法</strong>和<strong>基于梯度的方法</strong>被用来提高效率。<code>TracIn</code>采用了类似的思想，利用<strong>梯度信息</strong>追踪单个训练示例的影响。为了使这些算法实用且易于使用，<code>DataScope</code>被开发为一个端到端的系统，可以高效地计算整个管道中各种<strong>机器学习算法</strong>和<strong>数据转换的训练数据的夏普利值</strong>，使其成为数据调试的强大工具。随着数据评估的成熟，数据提供者更有动力做出贡献，从而促进了更健康和健壮的<strong>数据中心生态系统</strong>。</p>
<p><strong>模型组合</strong>(<code>Model Combination，MC</code>)旨在通过协调或合并一系列（专用）大模型来提高整个系统的性能。<strong>模型组合</strong>的主要优势在于无需<strong>显式训练</strong>，并且能带来更好的<strong>下游性能</strong>和<strong>任务泛化能力</strong>。例如，<code>FrugalGPT</code>以级联方式将请求路由到不同的<code>LLM</code>，并使用学习到的<strong>评分函数</strong>决定是否以灵活的方式返回中间结果，从而大幅降低成本并提高质量。合并多个<code>LLM</code>的权重已被广泛探索并证明其有效。流行的方法包括<strong>简单平均</strong>、<strong>任务算术</strong>、<strong>多模态（编码器）合并</strong>、<strong>基于学习路由函数的合并</strong>、<code>SLERP</code>以及<strong>加权</strong>（<strong>共轭梯度下降、随机和基于群体的优化算法</strong>）<strong>合并</strong>。<strong>模型组合</strong>对于<strong>联邦学习</strong>也非常有前景，因为只需要<strong>交换模型权重</strong>，从而更容易保证<strong>数据隐私</strong>。例如，<code>CoID Fusion</code>提出通过将基础模型的副本发送到工作者并在不传输数据的情况下合并学习到的权重来协作改进<strong>多任务学习</strong>。<strong>模型组合</strong>可以形成<strong>复合系统</strong>，这些系统由多个<code>LLM</code>通过<strong>合并</strong>、<strong>路由</strong>或<strong>知识共享</strong>方式协同工作。例如，<code>AIOS</code>设计了一种机制，将多个<code>LLM</code><strong>智能体</strong>集成到<strong>操作系统</strong>中，这些<strong>智能体</strong>的<strong>协同组合</strong>使得能够处理越来越复杂的<strong>多模态任务</strong>，这些任务需要推理、执行以及与物理世界的交互。<code>Tandom Transformer</code>通过让较小的模型关注较大模型的丰富表示（该表示可以同时处理多个令牌），形成了一个拼接的<strong>学生-教师系统</strong>，从而在下游任务中提高<strong>准确性</strong>和<strong>效率</strong>。然而，开发复杂的<strong>复合系统</strong>也面临着多个挑战：如何协同优化多个<code>LLM</code>；识别故障（不安全）组件比调试单体系统要困难得多；如何为大型系统的不同组件设计成熟的<strong>数据管道</strong>。</p>
<p><strong>自动化</strong>：随着大模型的复杂性不断增加，为了实现<strong>民主化</strong>和<strong>敏捷开发</strong>，需要一个更加成熟的<strong>自动化过程</strong>。<strong>自动机器学习</strong>(<code>AutoML</code>)在过去几年中已在许多机器学习任务中取得了显著的成功，这证明了它在大模型自动化中的前景。然而，将<code>AutoML</code>技术应用于<strong>大语言模型</strong>(<code>LLM</code>)面临诸多挑战，例如预训练成本、多个不同阶段以及性能指标的多样性，使得整体优化变得困难甚至不可行。例如，<code>PriorBand</code>尝试通过利用专家信念和廉价的代理任务来弥补<strong>传统机器学习</strong>和<strong>现代深度学习</strong>之间的超参数优化(<code>HPO</code>)成本差距。<code>AdaBERT</code>是一种基于<strong>可微分神经架构搜索</strong>(<code>NAS</code>)的<strong>自动化任务压缩算法</strong>，受任务导向的<code>知识蒸馏损失</code>和<strong>效率感知损失</strong>的指导。为了减轻提示工程的负担，自动提示工程师(<code>APE</code>)提出利用多个<code>LLM</code>之间的相互作用来实现自动提示生成和选择，其中一个<code>LLM</code>提出或修改提示，另一个<code>LLM</code>对其进行评分和选择。<code>EcoOptiGen</code>通过找到更好的超参数（如响应数量、温度和最大令牌数）来优化解码的效用和成本，展示了将<code>AutoML</code>应用于推理阶段的潜力。一种非常令人兴奋的方法是让多个<code>LLM</code>在分解的方式下合作解决大问题。一种实现方式是让<code>LLM</code>或<code>VLM</code>在管道中服务于不同的目的，这可能非常具有挑战性，需要<strong>调优、优化、模块化</strong>和<code>调试</code>。<code>DSPy</code>通过将系统的流程与每个步骤的参数（即模型提示和权重）分离，然后使用专用算法根据用户定义的指标进行调优来解决这一问题。</p>
<p><strong>计算平台</strong>：<strong>大语言模型</strong>的进步和实用性在很大程度上取决于不断演进的硬件加速器趋势。<code>GPU</code>是最常用的选择，它们通过<strong>优化并行计算</strong>和<strong>快速线程共享内存</strong>来提高性能。<code>GPU</code>非常适合现代深度学习，特别是在大量向量和矩阵乘法方面。<code>NVIDIA</code>的<code>Ampere</code>和<code>Hopper GPU</code>架构是许多最先进模型的基石，主要得益于其增强的<strong>内存容量、访问速度</strong>和<strong>计算性能</strong>（增加了张量核心）。这些<code>GPU</code>支持不同的算术精度（<code>32</code>位和<code>16</code>位浮点数）和格式（张量浮点数和脑浮点数），在数值精度和效率之间进行权衡。除了<code>NVIDIA</code>外，其他制造商也在投资专门用于深度学习应用的加速器，例如<code>TPU</code>、<code>FPGA</code>、<code>AWS Inferentia</code>和<code>Groq</code>的<code>LPU</code>，每种都有其优势。大模型需要巨大的内存容量来支持训练和推理（例如，未经额外优化的原生<code>Llama-70B</code>模型需要<code>8</code>块<code>A100 GPU</code>，总共<code>80GB</code>的<code>VRAM</code>）。然而，开发高效算法需要对底层硬件的规格有深入的理解（例如<strong>模型并行、内存层次、网络配置</strong>等）。随着我们将模型扩展到万亿甚至更大规模，需要更复杂的并行技术，这可能很难概念化、实现和维护。<code>NVIDIA DGX GH200</code>通过提供一个巨大的<strong>共享内存空间</strong>（最高<code>144TB</code>）来简化编程模型，该内存空间跨越了相互连接的<code>Grace Hopper</code> <code>Superchips</code>（一个<code>Grace CPU</code>与一个<code>Grace GPU</code>配对）。<code>Qualcomm Cloud AI 100 Ultra</code>可以在单个<code>150</code>瓦卡上支持<code>100</code>亿参数模型（与<code>LED</code>灯泡的功耗相同）。加速器的强大和效率伴随着灵活性，这得益于专门设计的编程语言，如<code>NVIDIA</code>的<code>CUDA</code>和<code>AMD</code>的<code>ROCm</code>，它们提供了对线程利用和计算逻辑的更细粒度的控制。许多工作，如<code>TVM</code>和<code>MLC-LLM</code>，试图通过<strong>编译器加速</strong>来普及<strong>机器学习</strong>和<strong>深度学习模型</strong>在各类设备上的部署，旨在最大化各种加速器的潜力。</p>
<img data-src="/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png" class="" title="AGI系统的未来形式：集中式服务模型（左）、分布式云端模型（中）、边缘设备网络模型（右）">

<p>三种常见的<code>AGI系统范式</code>：</p>
<ul>
<li><strong>集中式服务模型</strong>（左）：这种模型通常将<code>AI</code>模型部署在中央服务器上，通过强连接的客户端提供快速稳定的服务。这种架构在当前的数据中心中非常常见，能够支持高吞吐量和复杂任务的处理。</li>
<li><strong>分布式云端模型</strong>（中）：这种模型将<code>AI</code>模型（完整副本或分片）分散到云端的异构设备上，这些设备通过不同的网络连接，请求可以在不经过单一节点的情况下处理。这种架构提高了系统的容错性和灵活性，能够更好地利用分散的计算资源。</li>
<li><strong>边缘设备网络模型</strong>（右）：这种模型不仅将高性能设备，还包括<strong>物联网</strong>(<code>IoT</code>)设备连接起来，只有必要的数据通过网络传输，以减少网络负担。这种架构优化了用户数据隐私、快速适应和响应式个人助手，非常适合需要保护用户数据隐私和提供快速响应的应用场景。</li>
</ul>
<p><strong>AGI系统的未来</strong>：<code>AGI</code><strong>系统</strong>作为支持各种应用的基础设施，其目标是不断改进<strong>稳定性、资源利用率、性能和安全性</strong>。受前期工作和最近硬件趋势启发的三种<code>AGI</code><strong>系统</strong>，设想了三种主要的<code>AGI</code><strong>系统</strong>，它们针对不同应用场景，具有各自的<strong>资源可用性、核心系统指标、安全性和性能要求</strong>。以下是这些系统的关键特征及其目标应用：</p>
<ul>
<li><strong>数据中心</strong><code>SoTA</code><strong>模型</strong>：这些模型正在随着新技术的发展而演进，以支持更高的吞吐量和解决复杂任务，如科学发现和世界模拟。它们与当前的最先进模型类似，通常部署在数据中心。我们可以预期网络、加速器和推理基础设施将继续演进，以支持超高吞吐量和解决更复杂的任务。</li>
<li><strong>去中心化社区驱动模型</strong>：这些模型实现了<strong>容错、透明</strong>和<strong>民主</strong>的计算资源利用。分散的计算资源如果能协同使用，将具有重要意义。这些模型将由多个服务器以<strong>去中心化</strong>方式维护，类似于<strong>分布式账本系统</strong>，任何单个参与者都难以破坏整个系统。通过精心设计的激励机制，<strong>去中心化</strong>的大模型具有<strong>容错性、透明性</strong>，并由整个社区驱动，用户可以同时贡献和受益，从而实现<strong>大模型民主</strong>。</li>
<li><strong>本地和专用模型</strong>：这些模型优化了用户数据隐私、快速适应和响应式个人助手。它们通常部署在较便宜、性能较低且异构的边缘设备上，可以在网络中<strong>异步交换</strong>必要信息。这些模型非常适合<strong>快速任务适应、保护用户数据隐私</strong>、<strong>提供较简单的个人助手</strong>，并确保闪电般的响应时间。</li>
</ul>
<p>这些系统的发展将在未来<code>AI</code>技术的进步中发挥重要作用，尤其是在<strong>数据中心</strong>的<strong>高性能计算</strong>、<strong>去中心化AI</strong>的<strong>透明性和安全性</strong>，以及<strong>本地模型的隐私保护方面</strong>。系统支持内部和外部<code>AGI</code>模块，系统研究和工程的进步如何促进内部和外部<code>AGI</code>模块的发展，可能性是无穷的。以下是一些例子，希望它们能够激发未来的努力：</p>
<ul>
<li><strong>具有更长有效上下文长度和更强处理能力的系统</strong>：最常见的将多模态数据整合到一个共同空间（例如<code>LLM</code>中的令牌）的方式会导致数据长度爆炸，即使使用足够的压缩技术，我们仍然希望未来<code>AI</code>系统能够处理更多信息。同样的严格要求也出现在世界模型构建中，用户可能需要更频繁、大量地输入数据。其他需要长上下文理解的情况包括<strong>批量数据处理</strong>（用于金融和数据分析）、<strong>医疗史检查</strong>、<strong>人格聊天机器人</strong>等。这些应用要求模型能够处理更长的上下文输入，这需要专门设计的系统来应对高效扩展的挑战。</li>
<li><strong>与模型架构协同设计的系统以支持高效外部资源获取</strong>：能够使用多样化工具并获取外部知识是未来<code>AGI</code>系统的必备要求。我们可以设想持续投资于开发和协同设计模型友好的工具接口（例如与人类使用的<code>API</code>不同、适应模型输出模式的检索索引等），这可以大大提高模型获取外部知识的效率。<code>AI</code>系统的一个关键期望是<strong>终身学习</strong>，这需要复杂的<strong>记忆</strong>和<strong>能力存储</strong>，这是系统研究的一个有前途的方向。</li>
<li><strong>多个智能体的系统编排</strong>：协作<code>AI</code><strong>智能体</strong>之间的协同作用可以显著造福世界的各个方面。然而，达到这种<strong>多智能体系统</strong>的高效和有效并非易事，需要在支持<strong>智能体</strong>之间的<strong>通信、资源共享、调制和任务编排</strong>的基础设施上投入大量精力。此外，随着<strong>智能体</strong>数量和复杂性的增长，我们需要在系统技术上进行更多投资，例如日志记录和监控，这些技术使得调试和故障恢复变得更容易。<img data-src="/2025/03/15/artificial-intelligence/ML27_theory_study/ml_2.png" class="" title="AGI对齐：前向对齐、后向对齐、去中心化对齐"></li>
</ul>
<p><code>AGI</code>的期望包括其能力和伦理问题。<code>AGI</code>系统应该具备<strong>解决复杂问题的能力</strong>，同时也需要考虑其对社会的影响和潜在风险。<strong>伦理问题</strong>包括确保<code>AGI</code>不会对人类造成伤害，并且其行为符合人类的价值观。当前的对齐技术可以分为三个主要类别：</p>
<ul>
<li><strong>前向对齐</strong>(<code>Forward Alignment</code>)：通过训练<code>AI</code>系统从<strong>人类反馈中学习</strong>，并在数据分布变化时保持稳定。常用的方法包括<strong>强化学习</strong>和<strong>偏好建模</strong>。</li>
<li><strong>后向对齐</strong>(<code>Backward Alignment</code>)：通过评估和监管<code>AI</code>系统的行为，以确保其与人类价值观保持一致。这包括<strong>安全评估、可解释性</strong>和<strong>价值遵守</strong>等方面。</li>
<li><strong>去中心化对齐</strong>：强调<strong>开放性、包容性</strong>和<strong>与人类价值观的对齐</strong>，特别是在<strong>去中心化</strong>的<code>AGI</code>开发中。</li>
</ul>
<p><strong>基于接口的未来AGI对齐路线</strong>：未来<code>AGI</code>对齐可以通过设计和优化接口来实现。这种方法包括开发能够与人类有效交互的<code>AI</code>系统，并确保这些系统能够从<strong>人类反馈中学习</strong>。通过协同设计<code>AI</code>系统和其接口，可以提高对齐的效率和安全性。此外，<code>AGI</code><strong>对齐</strong>还需要解决诸如<strong>可靠性、可解释性、可控性</strong>和<strong>伦理性</strong>等关键原则。随着<code>AGI</code>的发展，确保其与人类价值观保持一致将变得越来越重要。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/" title="机器学习(ML)(二十七) — 强化学习探析">https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/02/23/artificial-intelligence/ML26_theory_study/" rel="prev" title="机器学习(ML)(二十六) — 强化学习探析">
                  <i class="fa fa-chevron-left"></i> 机器学习(ML)(二十六) — 强化学习探析
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.4m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">77:59</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
