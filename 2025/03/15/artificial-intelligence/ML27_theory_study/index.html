<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta name="google-site-verification" content="lk2gSYFP_NyLNFob-fFnt7fm-I_n1ZYws-WZll7mshg">
  <meta name="msvalidate.01" content="6Jdc01DjYOLguhS5">
  <meta name="baidu-site-verification" content="code-NR10G09zww">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7Ccursive:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/yellow/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"fresh88888888.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/local-search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":true}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/config.min.js"></script>

    <meta name="description" content="AGI系统：实现AGI机制超越Transformers：尽管Transformers架构取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。混合专家(MoEs)使用多个“专家”子网络组成的条件模块替换了Transformer模型中的稠密层。使用路由机制在词元级或任务级动态决定使用哪个专家。尽管拥有多个专家，稀疏的MoEs通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(ML)(二十七) — AGI探析">
<meta property="og:url" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/index.html">
<meta property="og:site_name" content="UMBRELLA">
<meta property="og:description" content="AGI系统：实现AGI机制超越Transformers：尽管Transformers架构取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。混合专家(MoEs)使用多个“专家”子网络组成的条件模块替换了Transformer模型中的稠密层。使用路由机制在词元级或任务级动态决定使用哪个专家。尽管拥有多个专家，稀疏的MoEs通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png">
<meta property="og:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_2.png">
<meta property="article:published_time" content="2025-03-15T08:00:11.000Z">
<meta property="article:modified_time" content="2025-03-15T08:00:11.000Z">
<meta property="article:author" content="umbrella">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png">


<link rel="canonical" href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/","path":"2025/03/15/artificial-intelligence/ML27_theory_study/","title":"机器学习(ML)(二十七) — AGI探析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习(ML)(二十七) — AGI探析 | UMBRELLA</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">UMBRELLA</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">未雨绸缪，举重若轻</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-算法"><a href="/Algorithm/" rel="section"><i class="fa fa-calendar fa-fw"></i>算法</a></li><li class="menu-item menu-item-c++-&nbsp;编程"><a href="/Programming-C++/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>C++ &nbsp;编程</a></li><li class="menu-item menu-item-rust-编程"><a href="/Programming-Rust/" rel="section"><i class="fa fa-cat fa-fw"></i>Rust 编程</a></li><li class="menu-item menu-item-go-&nbsp;&nbsp;&nbsp;编程"><a href="/Programming-Go/" rel="section"><i class="fa fa-hippo fa-fw"></i>Go &nbsp;&nbsp;&nbsp;编程</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#AGI%E7%B3%BB%E7%BB%9F%EF%BC%9A%E5%AE%9E%E7%8E%B0AGI%E6%9C%BA%E5%88%B6"><span class="nav-number">1.</span> <span class="nav-text">AGI系统：实现AGI机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AGI%E5%AF%B9%E9%BD%90%EF%BC%9A%E7%A1%AE%E4%BF%9DAGI%E6%BB%A1%E8%B6%B3%E5%90%84%E7%A7%8D%E9%9C%80%E6%B1%82"><span class="nav-number">2.</span> <span class="nav-text">AGI对齐：确保AGI满足各种需求</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="umbrella"
      src="/avatar.jpeg">
  <p class="site-author-name" itemprop="name">umbrella</p>
  <div class="site-description" itemprop="description">没事就多看看书</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">256</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fresh88888888" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fresh88888888" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fresh888888@foxmail.com" title="E-Mail → mailto:fresh888888@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.rust-lang.org/zh-CN/" title="https:&#x2F;&#x2F;www.rust-lang.org&#x2F;zh-CN&#x2F;" rel="noopener" target="_blank">Rust</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://go.dev/" title="https:&#x2F;&#x2F;go.dev&#x2F;" rel="noopener" target="_blank">Golang</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://isocpp.org/" title="https:&#x2F;&#x2F;isocpp.org&#x2F;" rel="noopener" target="_blank">C++</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.python.org/" title="https:&#x2F;&#x2F;www.python.org&#x2F;" rel="noopener" target="_blank">Python</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://doc.rust-lang.org/cargo/index.html" title="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;cargo&#x2F;index.html" rel="noopener" target="_blank">Cargo</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gist.github.com/rxaviers/7360908" title="https:&#x2F;&#x2F;gist.github.com&#x2F;rxaviers&#x2F;7360908" rel="noopener" target="_blank">Emoji</a>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.jpeg">
      <meta itemprop="name" content="umbrella">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="UMBRELLA">
      <meta itemprop="description" content="没事就多看看书">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习(ML)(二十七) — AGI探析 | UMBRELLA">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习(ML)(二十七) — AGI探析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-15 16:00:11" itemprop="dateCreated datePublished" datetime="2025-03-15T16:00:11+08:00">2025-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>50 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h5 id="AGI系统：实现AGI机制"><a href="#AGI系统：实现AGI机制" class="headerlink" title="AGI系统：实现AGI机制"></a>AGI系统：实现AGI机制</h5><p><strong>超越</strong><code>Transformers</code>：尽管<code>Transformers</code><strong>架构</strong>取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。<strong>混合专家</strong>(<code>MoEs</code>)使用多个“<strong>专家</strong>”子网络组成的条件模块替换了<code>Transformer</code><strong>模型</strong>中的稠密层。使用<strong>路由机制</strong>在<strong>词元级</strong>或<strong>任务级</strong>动态决定使用哪个<strong>专家</strong>。尽管拥有多个<strong>专家</strong>，稀疏的<code>MoEs</code>通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务中实现。然而，<code>MoEs</code>在推理过程中也带来了其他挑战，例如将所有专家加载到<code>VRAM</code>中的要求以及在多个节点上分发<strong>专家</strong>。</p>
<span id="more"></span>

<p><strong>状态空间模型</strong>(<code>SSMs</code>)最近被应用于建模<strong>序列到序列</strong>的转换，可以在各种模型架构拓扑中替代<strong>二次自注意力机制</strong>。（<strong>离散化</strong>）<code>SSM</code>通过一组可学习参数(<code>∆, A, ¯ B, C ¯</code>)在每个时间步（标记）上定义<strong>递归关系</strong>，大多数研究试图解决的主要挑战是如何以<strong>并行化</strong>的方式计算这种递归，以便有效利用<strong>现代硬件加速器</strong>（例如<code>FFTConv</code>）。在这一类别中，最简单的形式是<strong>线性注意力</strong>，它可以被视为一种退化的<code>SSM</code>。在其核心，<strong>线性注意力</strong>将<strong>自注意力表示</strong>为<strong>核特征映射</strong>的<strong>线性点积</strong>，并利用<strong>矩阵乘积</strong>的结合性质将复杂度降低到线性。<code>S4</code>通过<strong>低秩校正</strong>对<code>SSM</code>进行了表达性和高效的参数化，使其能够稳定<strong>对角化</strong>，并将<code>SSM</code>简化为<strong>柯西核</strong>的计算。在<code>S4</code>之后，还有许多后续研究尝试对<strong>转移矩阵</strong><code>A¯</code>（和其他矩阵）进行不同的参数化，以提高计算效率和建模能力。<code>H3</code>提出了一个由两个堆叠的独立<code>SSM</code>组成的<code>SSM</code>块，专门设计用于应对回忆早期标记的挑战，并支持跨序列的标记比较。<code>Hyena</code>通过用交错和隐式参数化的<strong>长卷积</strong>以及<strong>数据控制门</strong>替换<code>S4</code>层，推广了<code>H3</code>，从而将参数大小与滤波器大小解耦，增加了表达能力。后来提出了一种名为<code>Retentive Network</code>的基础架构，其中包含额外的<strong>门</strong>和<strong>多头注意力</strong>的变体，实现了<strong>恒定推理成本</strong>和<strong>线性长序列内存消耗</strong>。<code>RWKV</code>是一种新架构，结合了<code>Transformer</code>的高效并行训练和<code>RNN</code>的高效推理。本质上，主要的“<code>WKV</code>”操作涉及线性时不变(<code>LTI</code>)递归，可以解释为两个<code>SSM</code>的比率。为了克服以前<code>SSM</code>模型的主要弱点，即它们无法进行基于内容的推理，<code>Mamba</code>提出了<strong>选择性状态空间</strong>，可以使<code>SSM</code>参数成为输入的函数，从而将<code>SSM</code>从<code>LTI</code>转变为时变。尽管不再能够应用高效卷积，但他们设计了面向硬件的并行算法，用于递归计算，称为<strong>并行关联扫描</strong>，使其能够实现比<code>Transformer</code>高<code>5</code>倍的吞吐量，在多种模态上达到最先进的性能，并在实际数据上不断提高，直至百万长度的序列。重新审视<strong>循环神经网络</strong>（<code>RNN</code>）的兴趣火花也随着其在长上下文处理中的主要优势（即<strong>隐藏状态的线性时间</strong>和<strong>恒定内存</strong>）而出现。<code>RNN</code>的一个挑战是如何高效地扩展训练和推理。一种基于<code>RNN</code>的模型，具有<strong>门控线性递归</strong>，以及<code>Griffin</code>，它将<strong>门控线性递归</strong>与<strong>局部注意力</strong>相结合。他们展示了<code>Hawk</code>在下游任务上的优越性能，<code>Griffin</code>的性能与<code>Llama-2</code>相当，但训练标记数量减少了六倍。他们不仅展示了<strong>长上下文能力</strong>的潜力，还解释了如何在<strong>分布式训练</strong>和推理期间通过将 <code>Griffin</code>扩展到<code>14B</code>参数来有效利用硬件加速器。紧接着，一系列名为<code>RecurrentGemma</code>的模型以各种尺寸发布，包括预训练和指令调优版本。这些进展展示了在不依赖<code>Transformer</code>架构的情况下，训练一个数据高效、固定状态大小、长上下文和表达能力强的模型的可能性。最近的研究还探索了高层次架构混合策略，旨在结合不同变体的优势。并提出将<code>Transformer</code>与<code>Mamba</code>结合，通过<strong>交错层</strong>实现，在标准和长上下文任务上取得了令人印象深刻的结果，同时资源需求可控。除了手动设计，<code>MAD</code>将这一过程整合到一个端到端的流水线中，包含能够预测扩展规律的小规模能力单元测试。<code>MAD</code>成功找到了一种高效的架构，名为<code>Striped Hyena</code>，基于<strong>混合</strong>和<strong>稀疏</strong>，在计算最优预算和过度训练条件下，其扩展性能优于最先进的<code>Transformer</code>、<code>卷积</code>和<code>循环架构</code>（<code>Transformer++、Hyena、Mamba</code>）。这些研究很可能将继续激发对既高效又具扩展性的架构设计的进一步探索，打破当前的<code>Transformer</code>范式。</p>
<p><strong>大规模训练</strong>：扩展大模型的训练在现代硬件上面临许多挑战，例如由于内存需求增加，模型无法再容纳在单个<code>GPU</code>中，通过更多计算单元加速训练速度同时尽量减少开销（<strong>线性扩展</strong>），以及利用分散资源等。<strong>并行计算</strong>：大语言模型在具有多个计算单元的集群环境中的并行性通常可以分为四种主要模式，通常称为“<code>4D</code><strong>并行</strong>”。</p>
<ul>
<li><strong>分布式数据并行</strong>(<code>DDP</code>)：是最简单的设置，其中模型在各单元上复制，数据被切片并分别输入每个模型，通常在每次传递结束时进行同步步骤。<code>DDP</code>的更复杂版本，如<code>ZeRO</code>和<code>FSDP</code>，在现代大型训练框架（如 <code>DeepSpeed</code>、<code>FairScale</code>和<code>Megatron-LM</code>中被广泛使用。</li>
<li><strong>张量并行</strong>(<code>TP</code>)或<strong>模型并行</strong>：将模型权重分成多个块，分布在多个<code>GPU</code>上。这种水平分割允许数据在分片权重上并行处理，然后在每个步骤结束时聚合结果，通常涉及清晰融合以减少同步通信。</li>
<li><strong>流水线并行</strong>(<code>PP</code>)：将模型层垂直分配到不同的<code>GPU</code>上，数据将在不同单元的各个阶段之间移动。</li>
<li><strong>序列并行</strong>(<code>SP</code>)：主要针对长上下文任务，沿序列维度进行分割，以缓解计算和存储负载。</li>
</ul>
<p>结合不同的并行方式可能会带来高效的系统。然而，它们独特的权衡和集群配置，这并不容易实现。<code>Alpha、HexGen</code>和<code>FlexFlow</code>尝试自动化模型训练和推理的并行化过程，以最大化硬件利用率。<strong>集群配置</strong>（如内存、带宽和单个加速器的延迟、网络带宽等）通常是估计的，并使用<strong>动态规划</strong>和<strong>受限优化</strong>等搜索算法来找到最佳<strong>并行策略</strong>。通过自适应分配请求来支持<strong>非对称计算</strong>，以满足延迟要求。这些自动并行调度方法在许多涉及硬件和<strong>网络异构</strong>的情况下，已被证明可以与手动设计媲美甚至超越。</p>
<p><strong>内存管理</strong>：<strong>内存管理</strong>是训练和部署大模型的最关键方面之一，特别是在长上下文领域，<code>KV</code><strong>缓存</strong>的内存占用可能轻松超过模型权重和激活的总和。受传统操作系统设计启发，<code>vLLM</code>的<strong>分页注意力</strong>通过将<code>KV</code><strong>缓存</strong>分区为非连续的内存块来解决内存碎片问题，显著提高了内存利用率，从而提高了系统<strong>吞吐量</strong>和<strong>效率</strong>。<code>FastGen</code>引入了一种自适应的<code>KV</code>缓存压缩技术，通过结构分析动态驱逐非特殊标记，减少内存使用。<code>Scissorhands</code>和<code>H2O</code>也有类似的观察，即保留关键标记可以在最小的微调和内存节省下保留大部分性能。<code>Infinite-LLM</code>首先将注意力计算分割成可分配给不同单元的较小子程序。为了实现这些子程序的高效分配，开发了一种专用服务器，可以动态管理<code>KV</code><strong>缓存</strong>，并有效协调数据中心范围内的所有可用<code>GPU</code>和<code>CPU</code>内存。许多重要技术已被广泛采用于流行的<strong>深度学习框架</strong>中，以将更大的模型适配到固定内存的设备中。<code>CPU</code>卸载允许模型有选择地将<strong>权重</strong>（层）或<code>KV</code><strong>缓存</strong>转移到具有更多内存的<code>CPU</code>，并仅将必要的网络部分加载到<code>GPU</code>进行处理。在极端情况下，<code>FlexGen</code>可以在单个<code>16GB</code> <code>GPU</code>上实现<code>OPT-175B</code>的显著<strong>批处理吞吐量</strong>。<strong>梯度检查点</strong>通过在<strong>反向传播</strong>期间重新计算计算图的部分来减少<strong>峰值内存</strong>使用。毫无疑问，高效的内存管理将继续作为核心投资方向，使可扩展系统的部署和更大批次的并行处理成为可能。</p>
<p><strong>高效微调</strong>：预训练的大模型通常内化了大量知识，可以通过（<strong>指令</strong>）<strong>微调</strong>释放出来。然而，尽管通常只需少量示例即可成功<strong>微调</strong>，但其成本和时间仍然高昂且不经济。<strong>高效微调</strong>的主要目标是在成本（实现难度、数据需求、训练预算等）和<strong>持续预训练</strong>的性能差距之间找到平衡。为应对这一挑战，开发了一系列<strong>参数高效微调</strong>(<code>PEFT</code>)技术，这些技术只需训练少量新参数，通常比上下文学习表现更好。<code>LoRA</code>作为最受欢迎的<code>PEFT</code>方法之一，近来备受关注。<code>LoRA</code>及其许多变体（如<code>LoHA、AdaLoRA、Q-LoRA</code>和最近的<code>PiSSA</code>）通过<strong>插入低秩分解</strong>的可学习矩阵来代替<strong>增量权重矩阵</strong>。<code>LLaMA-Adapter</code>通过很少的计算预算高效地将<code>LLaMA</code>微调为指令遵循模型。一组可学习的适配提示首先被添加到上下文中，并通过零初始化的<strong>注意力机制</strong>和<strong>零门控机制</strong>，仅使用<code>52K</code>自我指导示例进行训练。来自适配器的额外<code>1.2M</code>参数可以产生高质量的输出，与完全微调的结果相当。与<code>LoRA</code>类似，<code>IA3</code>通过可学习向量而非矩阵来缩放模型激活。其他插入可学习组件的<code>PEFT</code>方法展示了强大的<strong>泛化能力</strong>，而基于提示的方法（如软提示）则在保持原始模型权重不变的情况下，向输入嵌入中添加额外的可学习参数。<strong>适配器</strong>在<strong>注意力块</strong>内添加可训练参数，而<strong>前缀调整</strong>则在<strong>注意力</strong>中的<code>KV</code>表示中附加<strong>可学习向量</strong>。与传统的<code>PEFT</code>技术不同，微调<code>Transformer</code>的<code>LayerNorm</code>层可以作为一个强大的基准，带来不错的性能。</p>
<p><strong>去中心化</strong>：许多研究致力于利用云端分散且硬件异构的计算设备进行模型训练和推理。位置上分离的集群面临的一个挑战是通信开销，这使得数据移动（如<strong>训练数据</strong>、<strong>梯度</strong>、<code>KV</code><strong>缓存</strong>等）成本高昂，从而抵消了<strong>去中心化</strong>的好处。<code>CacheGen</code>通过<strong>编码器</strong>将<code>KV</code><strong>缓存</strong>压缩成紧凑的<strong>比特流表示</strong>，减少了上下文获取和处理的延迟。<code>CocktailSGD</code>采用<strong>稀疏化</strong>和<strong>量化技术</strong>的组合，使得在慢速网络上微调高达<code>20B</code>的大语言模型成为可能，并且与数据中心的快速互连相比，仅有最小的减速。<code>DiLoCo</code>在连接不佳的设备岛上引入了一种新的<strong>联合平均算法</strong>，声称在<code>C4</code>数据集上的表现与完全同步优化相当，同时通信量减少了<code>500</code>倍。<strong>协作训练</strong>通过众包方式从个人用户那里获取普通<code>GPU</code>，其中最著名的例子是<code>Petals</code>，该系统能够使用多方的普通<code>GPU</code>以不错的性能（例如支持交互式会话）提供和<strong>微调</strong><code>BLOOM-176B</code>和<code>OPT-175B</code>。<strong>去中心化</strong>的<strong>人工智能系统</strong>开启了全球设备互联的可能性，确保了<strong>容错性</strong>和异构设备及网络的<strong>兼容性</strong>，同时优化了有限的<strong>网络带宽</strong>和<strong>数据隐私</strong>。</p>
<p><strong>训练动态与扩展</strong>：大语言模型的科学难以捉摸，理解它们可以极大地改进各种<strong>人工智能</strong>的开发。然而，大多数成功的<code>LLM</code>不仅在数据和模型权重方面不完全“<strong>开放</strong>”，还在中间检查点和工件日志等其他方面也不开放，而这些信息可以帮助<strong>推理训练动态</strong>，因为我们将模型扩展到不同的规模。<code>OPT</code>模型在各种下游任务上的中间检查点，试图强调<strong>困惑度</strong>作为模型性能的<strong>预测指标</strong>，而不是其大小，表明较大的模型出现<strong>幻觉</strong>的频率较低，并且模型在训练早期阶段表现出最小的回报。与此互补，研究了不同<strong>模型大小</strong>、<strong>数据集大小</strong>和学习率下的<strong>记忆能力</strong>，并提出了关于名词和数字作为记忆单个训练示例的<strong>唯一标识符</strong>的重要性的有趣假设。除了纯粹的分析，<code>Pythia</code>推出了一套由<code>16</code>个<code>LLM</code>组成的套件，这些模型在公共数据上训练，参数量从<code>70M</code>到<code>12B</code>不等。通过将这些中间检查点发布给更广泛的社区，研究人员可以更轻松、更高效地通过检查和基准测试单个保存的权重和损失来找到与<strong>训练动态</strong>相关的问题的答案。最后，<code>OLMo</code>发布了整个框架，包括训练数据和训练及评估代码，以便更容易研究<code>LLM</code>背后的科学。</p>
<p><strong>推理技术</strong>：<code>AGI</code><strong>推理系统</strong>需要确保用户响应性、可用性和效率，从而在<strong>训练阶段</strong>释放大模型的最大潜力，并改变用户与系统的交互方式。</p>
<p><strong>解码算法</strong>：在这里主要关注<strong>精确解码加速</strong>，目的是在不影响准确性的情况下最大化性能。对几种近似方法进行了全面的综述，包括<strong>采样策略</strong>、<strong>非自回归解码</strong>、<strong>半自回归解码</strong>、<strong>块并行解码</strong>等。大量研究探讨了推测解码的理念，通过并行计算来提高生成多个标记的机会。通常，<strong>推测解码</strong>过程从一个高效的<strong>草稿模型</strong>开始，该模型对多个步骤进行预测，生成的提案由我们想要采样的<strong>目标模型验证</strong>。然而，这其中存在许多挑战，包括：如何使草稿模型轻量化，同时仍能生成有用的猜测以实现高效进展；如何避免大量架构变化和微调以实现更快的适应；如何更有效地部署<strong>草稿模型</strong>。最简单但有效的变体称为<strong>提示查找解码</strong>，其中<strong>草稿模型</strong>被从现有数据库中进行简单前缀字符串匹配所取代，以生成<strong>候选标记</strong>。这种与模型无关的方法可以非常快速地解码，而无需任何微调或模型更改，但其性能严重依赖于字符串池的质量和多样性。为了加快对大量候选项的验证，<code>SpecInfer</code>将<strong>草稿模型</strong>的输出组织成一个标记树，每个节点都是一个<strong>候选标记</strong>，其正确性可以被基础模型并行高效地检查。类似地，<code>Medusa</code>引入了一种树<strong>注意力机制</strong>，同时检查来自<code>medusa</code>头的所有标记，这通过特殊的<strong>掩码模式</strong>实现了高效的并行计算。<strong>自推测解码</strong>提出完全放弃<strong>草稿模型</strong>的要求，通过选择性地跳过一部分中间层来生成候选序列。<strong>硬件感知算法</strong>在解码阶段特别有效和吸引人。遵循高效<strong>自注意力</strong>工作，<code>Flash-Decoding</code>沿<strong>序列维度</strong>进行分割，并与其<code>KV</code><strong>缓存</strong>和统计数据并行处理这些块，其结果将通过<strong>归约步骤聚合</strong>以获得精确输出。为了克服<code>Flash-Decoding</code>的局限性并应用更多系统级优化，<code>FlashDecoding++</code>引入了基于统一最大值的异步<code>softmax</code>（避免同步开销）、优化的平坦<code>GEMM</code>操作与<strong>双缓冲</strong>（<code>GEMM</code>的性能取决于矩阵形状）以及基于启发式的数据流与硬件资源适应，以加速解码过程，结果比<code>HuggingFace</code>快了<code>4</code>倍以上。</p>
<p><strong>请求调度</strong>：<strong>大语言模型</strong>(<code>LLM</code>)的请求调度与传统<strong>机器学习系统</strong>相比，具有几个独特的挑战。成熟的<strong>请求调度策略</strong>的重要特性包括：1、<strong>高效地预取上下文</strong>（用户信息、过去的<code>KV</code><strong>缓存</strong>和<strong>模型适配器</strong>等）以便为给定输入提供服务；2、处理具有<strong>可变序列长度</strong>的示例，以最大化<code>GPU</code>利用率；3、在时间到首个<strong>令牌</strong>(<code>TTFT</code>)、<strong>作业完成时间</strong>(<code>JCT</code>)、<strong>批量令牌吞吐量</strong>和<strong>推理延迟</strong>等请求级别指标之间进行权衡。<code>Orca</code>提出了一种迭代级别的<strong>调度机制</strong>，以满足<code>LLM</code>推理请求的<strong>自回归特性</strong>，当与<strong>选择性批处理</strong>的技术结合时，可以更好地利用硬件，从而在<strong>吞吐量</strong>和<strong>延迟</strong>方面超越之前的<strong>推理引擎</strong>，如<code>FastTransformer</code>(<code>NVIDIA, 2023a</code>)。其他动态批处理策略也被广泛探索，例如<code>vLLM</code>的<strong>连续批处理</strong>和<code>TensorRT-LLM</code>(<code>NVIDIA, 2023b</code>)的<strong>飞行批处理</strong>。与<strong>请求调度</strong>不同，<code>FastServe</code>利用<code>LLM</code>推理的<strong>自回归模式</strong>，使每个输出令牌的抢占成为可能，通过一种新的<strong>跳跃连接多级反馈队列调度器</strong>优化<code>JCT</code>，该调度器利用输入长度信息以提高效率。<strong>推理工作负载</strong>与示例的<strong>平均序列长度</strong>密切相关，因此我们希望最小化最长和最短句子之间的差距。<code>S³</code>预测批处理中每个示例的潜在响应长度，用于在相同的<strong>内存约束</strong>（例如<code>GPU</code>内存）下容纳更多示例。<code>DeepSpeed-FastGen</code>的动态<code>SplitFuse</code>利用<code>LLM</code>推理的思想（批量大小与令牌数量变化对模型性能的影响），提出了一种<strong>令牌组合策略</strong>。动态<code>SplitFuse</code>通过从提示中获取部分令牌并与生成进行组合，以一致的前向大小运行。例如，长提示被分割成几个前向迭代中的较小块，而短提示则被组合以与其他请求对齐。通过这种策略，系统不仅提供了更好的效率和响应能力，还减少了请求的差异。</p>
<p><strong>多模型服务</strong>：除了为同一模型提供多个副本外，能够高效部署大量任务专用模型成为许多应用场景（如<code>LLM</code><strong>智能体</strong>、<strong>个性化聊天机器人</strong>、<strong>隐私敏感助手</strong>等）的重要特性。然而，简单地扩展实例数量在计算上不仅成本高昂，而且浪费资源。随着<code>PEFT</code>技术的进步，使用<strong>多样化适配器</strong>为基础模型提供服务成为许多从业者青睐的范式，因为<code>PEFT</code>模型轻量且易于维护，同时具有灵活性和强大的功能。<code>多模型</code>(<code>PEFT</code>)<strong>服务</strong>的主要挑战是如何动态且高效地为每个示例加载“正确的”（以延迟或任务性能等衡量）<strong>适配器</strong>。<code>Punica</code>通过设计新的<code>CUDA</code><strong>内核</strong>，实现了在一个批次中对异构<code>LoRA</code>头的高效计算，该内核共享一个预训练模型，实现了高达<code>12</code>倍的吞吐量，同时仅增加了轻微的额外延迟。<code>S-LoRA</code>引入了<strong>统一分页</strong>，使用统一的<strong>内存池</strong>进行<strong>动态适配器管理</strong>，并使用高度优化的<code>CUDA</code><strong>内核</strong>并行化<code>LoRA</code>计算。<code>LoRAX</code>还提供<strong>适配器交换调度</strong>，在<code>GPU</code>和<code>CPU</code>内存之间<strong>异步预取</strong>和<strong>卸载适配器</strong>，并<strong>调度请求批处理</strong>以优化<strong>总吞吐量</strong>。通过这些系统，可以在单个<code>GPU</code>上为超过一千个不同的<code>LoRA头</code>提供服务，从而开启了<strong>模型协作</strong>、<strong>任务泛化</strong>和<strong>模型合并</strong>等更广泛的可能性。</p>
<p><strong>成本与效率</strong>：与模型训练和推理相关的成本往往容易被忽视，但在实际应用中，尤其是在工业环境中，这些因素常常影响许多决策，如<strong>模型架构设计</strong>、<strong>数据混合选择</strong>和<strong>服务定价</strong>。</p>
<p><strong>数据经济</strong>：数据在模型性能中起着至关重要的作用，数据价值的问题因此变得非常重要：1、我们应收集哪些数据以添加到现有数据混合中以提高性能；2、我们应如何支付给数据提供者；3、我们能否移除非必要的数据（异常值）以使我们的模型更健壮。为了回答这些问题，许多计算机科学和经济学（<strong>博弈论</strong>）的研究探讨了不同的形式来定义“<strong>数据价值</strong>”及其高效估计方法。来自<strong>经典博弈论</strong>的<strong>夏普利值</strong>(<code>Shapley value</code>)独特地满足了公平数据评估的几个自然属性。由于其丰富的理论成果，<strong>夏普利值</strong>在<strong>数据经济</strong>领域被广泛用作数据重要性的<strong>定量</strong>和<strong>替代性度量</strong>（例如，夏普利值估计可用于数据采样、清理、定价、异常检测等）。计算数据<strong>夏普利值</strong>需要指数时间，因此<strong>蒙特卡罗方法</strong>和<strong>基于梯度的方法</strong>被用来提高效率。<code>TracIn</code>采用了类似的思想，利用<strong>梯度信息</strong>追踪单个训练示例的影响。为了使这些算法实用且易于使用，<code>DataScope</code>被开发为一个端到端的系统，可以高效地计算整个管道中各种<strong>机器学习算法</strong>和<strong>数据转换的训练数据的夏普利值</strong>，使其成为数据调试的强大工具。随着数据评估的成熟，数据提供者更有动力做出贡献，从而促进了更健康和健壮的<strong>数据中心生态系统</strong>。</p>
<p><strong>模型组合</strong>(<code>Model Combination，MC</code>)旨在通过协调或合并一系列（专用）大模型来提高整个系统的性能。<strong>模型组合</strong>的主要优势在于无需<strong>显式训练</strong>，并且能带来更好的<strong>下游性能</strong>和<strong>任务泛化能力</strong>。例如，<code>FrugalGPT</code>以级联方式将请求路由到不同的<code>LLM</code>，并使用学习到的<strong>评分函数</strong>决定是否以灵活的方式返回中间结果，从而大幅降低成本并提高质量。合并多个<code>LLM</code>的权重已被广泛探索并证明其有效。流行的方法包括<strong>简单平均</strong>、<strong>任务算术</strong>、<strong>多模态（编码器）合并</strong>、<strong>基于学习路由函数的合并</strong>、<code>SLERP</code>以及<strong>加权</strong>（<strong>共轭梯度下降、随机和基于群体的优化算法</strong>）<strong>合并</strong>。<strong>模型组合</strong>对于<strong>联邦学习</strong>也非常有前景，因为只需要<strong>交换模型权重</strong>，从而更容易保证<strong>数据隐私</strong>。例如，<code>CoID Fusion</code>提出通过将基础模型的副本发送到工作者并在不传输数据的情况下合并学习到的权重来协作改进<strong>多任务学习</strong>。<strong>模型组合</strong>可以形成<strong>复合系统</strong>，这些系统由多个<code>LLM</code>通过<strong>合并</strong>、<strong>路由</strong>或<strong>知识共享</strong>方式协同工作。例如，<code>AIOS</code>设计了一种机制，将多个<code>LLM</code><strong>智能体</strong>集成到<strong>操作系统</strong>中，这些<strong>智能体</strong>的<strong>协同组合</strong>使得能够处理越来越复杂的<strong>多模态任务</strong>，这些任务需要推理、执行以及与物理世界的交互。<code>Tandom Transformer</code>通过让较小的模型关注较大模型的丰富表示（该表示可以同时处理多个令牌），形成了一个拼接的<strong>学生-教师系统</strong>，从而在下游任务中提高<strong>准确性</strong>和<strong>效率</strong>。然而，开发复杂的<strong>复合系统</strong>也面临着多个挑战：如何协同优化多个<code>LLM</code>；识别故障（不安全）组件比调试单体系统要困难得多；如何为大型系统的不同组件设计成熟的<strong>数据管道</strong>。</p>
<p><strong>自动化</strong>：随着大模型的复杂性不断增加，为了实现<strong>民主化</strong>和<strong>敏捷开发</strong>，需要一个更加成熟的<strong>自动化过程</strong>。<strong>自动机器学习</strong>(<code>AutoML</code>)在过去几年中已在许多机器学习任务中取得了显著的成功，这证明了它在大模型自动化中的前景。然而，将<code>AutoML</code>技术应用于<strong>大语言模型</strong>(<code>LLM</code>)面临诸多挑战，例如预训练成本、多个不同阶段以及性能指标的多样性，使得整体优化变得困难甚至不可行。例如，<code>PriorBand</code>尝试通过利用专家信念和廉价的代理任务来弥补<strong>传统机器学习</strong>和<strong>现代深度学习</strong>之间的超参数优化(<code>HPO</code>)成本差距。<code>AdaBERT</code>是一种基于<strong>可微分神经架构搜索</strong>(<code>NAS</code>)的<strong>自动化任务压缩算法</strong>，受任务导向的<code>知识蒸馏损失</code>和<strong>效率感知损失</strong>的指导。为了减轻提示工程的负担，自动提示工程师(<code>APE</code>)提出利用多个<code>LLM</code>之间的相互作用来实现自动提示生成和选择，其中一个<code>LLM</code>提出或修改提示，另一个<code>LLM</code>对其进行评分和选择。<code>EcoOptiGen</code>通过找到更好的超参数（如响应数量、温度和最大令牌数）来优化解码的效用和成本，展示了将<code>AutoML</code>应用于推理阶段的潜力。一种非常令人兴奋的方法是让多个<code>LLM</code>在分解的方式下合作解决大问题。一种实现方式是让<code>LLM</code>或<code>VLM</code>在管道中服务于不同的目的，这可能非常具有挑战性，需要<strong>调优、优化、模块化</strong>和<code>调试</code>。<code>DSPy</code>通过将系统的流程与每个步骤的参数（即模型提示和权重）分离，然后使用专用算法根据用户定义的指标进行调优来解决这一问题。</p>
<p><strong>计算平台</strong>：<strong>大语言模型</strong>的进步和实用性在很大程度上取决于不断演进的硬件加速器趋势。<code>GPU</code>是最常用的选择，它们通过<strong>优化并行计算</strong>和<strong>快速线程共享内存</strong>来提高性能。<code>GPU</code>非常适合现代深度学习，特别是在大量向量和矩阵乘法方面。<code>NVIDIA</code>的<code>Ampere</code>和<code>Hopper GPU</code>架构是许多最先进模型的基石，主要得益于其增强的<strong>内存容量、访问速度</strong>和<strong>计算性能</strong>（增加了张量核心）。这些<code>GPU</code>支持不同的算术精度（<code>32</code>位和<code>16</code>位浮点数）和格式（张量浮点数和脑浮点数），在数值精度和效率之间进行权衡。除了<code>NVIDIA</code>外，其他制造商也在投资专门用于深度学习应用的加速器，例如<code>TPU</code>、<code>FPGA</code>、<code>AWS Inferentia</code>和<code>Groq</code>的<code>LPU</code>，每种都有其优势。大模型需要巨大的内存容量来支持训练和推理（例如，未经额外优化的原生<code>Llama-70B</code>模型需要<code>8</code>块<code>A100 GPU</code>，总共<code>80GB</code>的<code>VRAM</code>）。然而，开发高效算法需要对底层硬件的规格有深入的理解（例如<strong>模型并行、内存层次、网络配置</strong>等）。随着我们将模型扩展到万亿甚至更大规模，需要更复杂的并行技术，这可能很难概念化、实现和维护。<code>NVIDIA DGX GH200</code>通过提供一个巨大的<strong>共享内存空间</strong>（最高<code>144TB</code>）来简化编程模型，该内存空间跨越了相互连接的<code>Grace Hopper</code> <code>Superchips</code>（一个<code>Grace CPU</code>与一个<code>Grace GPU</code>配对）。<code>Qualcomm Cloud AI 100 Ultra</code>可以在单个<code>150</code>瓦卡上支持<code>100</code>亿参数模型（与<code>LED</code>灯泡的功耗相同）。加速器的强大和效率伴随着灵活性，这得益于专门设计的编程语言，如<code>NVIDIA</code>的<code>CUDA</code>和<code>AMD</code>的<code>ROCm</code>，它们提供了对线程利用和计算逻辑的更细粒度的控制。许多工作，如<code>TVM</code>和<code>MLC-LLM</code>，试图通过<strong>编译器加速</strong>来普及<strong>机器学习</strong>和<strong>深度学习模型</strong>在各类设备上的部署，旨在最大化各种加速器的潜力。</p>
<img data-src="/2025/03/15/artificial-intelligence/ML27_theory_study/ml_1.png" class="" title="AGI系统的未来形式：集中式服务模型（左）、分布式云端模型（中）、边缘设备网络模型（右）">

<p>三种常见的<code>AGI系统范式</code>：</p>
<ul>
<li><strong>集中式服务模型</strong>（左）：这种模型通常将<code>AI</code>模型部署在中央服务器上，通过强连接的客户端提供快速稳定的服务。这种架构在当前的数据中心中非常常见，能够支持高吞吐量和复杂任务的处理。</li>
<li><strong>分布式云端模型</strong>（中）：这种模型将<code>AI</code>模型（完整副本或分片）分散到云端的异构设备上，这些设备通过不同的网络连接，请求可以在不经过单一节点的情况下处理。这种架构提高了系统的容错性和灵活性，能够更好地利用分散的计算资源。</li>
<li><strong>边缘设备网络模型</strong>（右）：这种模型不仅将高性能设备，还包括<strong>物联网</strong>(<code>IoT</code>)设备连接起来，只有必要的数据通过网络传输，以减少网络负担。这种架构优化了用户数据隐私、快速适应和响应式个人助手，非常适合需要保护用户数据隐私和提供快速响应的应用场景。</li>
</ul>
<p><strong>AGI系统的未来</strong>：<code>AGI</code><strong>系统</strong>作为支持各种应用的基础设施，其目标是不断改进<strong>稳定性、资源利用率、性能和安全性</strong>。受前期工作和最近硬件趋势启发的三种<code>AGI</code><strong>系统</strong>，设想了三种主要的<code>AGI</code><strong>系统</strong>，它们针对不同应用场景，具有各自的<strong>资源可用性、核心系统指标、安全性和性能要求</strong>。以下是这些系统的关键特征及其目标应用：</p>
<ul>
<li><strong>数据中心</strong><code>SoTA</code><strong>模型</strong>：这些模型正在随着新技术的发展而演进，以支持更高的吞吐量和解决复杂任务，如科学发现和世界模拟。它们与当前的最先进模型类似，通常部署在数据中心。我们可以预期网络、加速器和推理基础设施将继续演进，以支持超高吞吐量和解决更复杂的任务。</li>
<li><strong>去中心化社区驱动模型</strong>：这些模型实现了<strong>容错、透明</strong>和<strong>民主</strong>的计算资源利用。分散的计算资源如果能协同使用，将具有重要意义。这些模型将由多个服务器以<strong>去中心化</strong>方式维护，类似于<strong>分布式账本系统</strong>，任何单个参与者都难以破坏整个系统。通过精心设计的激励机制，<strong>去中心化</strong>的大模型具有<strong>容错性、透明性</strong>，并由整个社区驱动，用户可以同时贡献和受益，从而实现<strong>大模型民主</strong>。</li>
<li><strong>本地和专用模型</strong>：这些模型优化了用户数据隐私、快速适应和响应式个人助手。它们通常部署在较便宜、性能较低且异构的边缘设备上，可以在网络中<strong>异步交换</strong>必要信息。这些模型非常适合<strong>快速任务适应、保护用户数据隐私</strong>、<strong>提供较简单的个人助手</strong>，并确保闪电般的响应时间。</li>
</ul>
<p>这些系统的发展将在未来<code>AI</code>技术的进步中发挥重要作用，尤其是在<strong>数据中心</strong>的<strong>高性能计算</strong>、<strong>去中心化AI</strong>的<strong>透明性和安全性</strong>，以及<strong>本地模型的隐私保护方面</strong>。系统支持内部和外部<code>AGI</code>模块，系统研究和工程的进步如何促进内部和外部<code>AGI</code>模块的发展，可能性是无穷的。以下是一些例子，希望它们能够激发未来的潜力：</p>
<ul>
<li><strong>具有更长上下文长度和更强处理能力的系统</strong>：最常见的将多模态数据整合到一个共同空间（例如<code>LLM</code>中的令牌）的方式会导致数据长度爆炸，即使使用足够的压缩技术，我们仍然希望未来<code>AI</code>系统能够处理更多信息。同样的要求也出现在世界模型构建中，用户可能需要更频繁、大量地输入数据。其他需要长上下文理解的情况包括<strong>批量数据处理</strong>（用于金融和数据分析）、<strong>医疗史检查</strong>、<strong>人格聊天机器人</strong>等。这些应用要求模型能够处理更长的上下文输入，这需要专门设计的系统来应对高效扩展的挑战。</li>
<li><strong>与模型架构协同设计的系统以支持高效外部资源获取</strong>：能够使用多样化工具并获取外部知识是未来<code>AGI</code>系统的必备要求。我们可以设想持续投资于开发和协同设计模型友好的工具接口（例如与人类使用的<code>API</code>不同、适应模型输出模式的检索索引等），这可以大大提高模型获取外部知识的效率。<code>AI</code>系统的一个关键期望是<strong>终身学习</strong>，这需要复杂的<strong>记忆</strong>和<strong>能力存储</strong>，这是系统研究的一个有前途的方向。</li>
<li><strong>多个智能体的系统编排</strong>：协作<code>AI</code><strong>智能体</strong>之间的协同作用可以显著造福世界的各个方面。然而，达到这种<strong>多智能体系统</strong>的高效和有效并非易事，需要在支持<strong>智能体</strong>之间的<strong>通信、资源共享、调制和任务编排</strong>的基础设施上投入大量精力。此外，随着<strong>智能体</strong>数量和复杂性的增长，我们需要在系统技术上进行更多投资，例如日志记录和监控，这些技术使得调试和故障恢复变得更容易。<img data-src="/2025/03/15/artificial-intelligence/ML27_theory_study/ml_2.png" class="" title="AGI对齐：前向对齐、后向对齐、去中心化对齐"></li>
</ul>
<p><code>AGI</code>的期望包括其能力和伦理问题。<code>AGI</code>系统应该具备<strong>解决复杂问题的能力</strong>，同时也需要考虑其对社会的影响和潜在风险。<strong>伦理问题</strong>包括确保<code>AGI</code>不会对人类造成伤害，并且其行为符合人类的价值观。当前的对齐技术可以分为三个主要类别：</p>
<ul>
<li><strong>前向对齐</strong>(<code>Forward Alignment</code>)：通过训练<code>AI</code>系统从<strong>人类反馈中学习</strong>，并在数据分布变化时保持稳定。常用的方法包括<strong>强化学习</strong>和<strong>偏好建模</strong>。</li>
<li><strong>后向对齐</strong>(<code>Backward Alignment</code>)：通过评估和监管<code>AI</code>系统的行为，以确保其与人类价值观保持一致。这包括<strong>安全评估、可解释性</strong>和<strong>价值遵守</strong>等方面。</li>
<li><strong>去中心化对齐</strong>：强调<strong>开放性、包容性</strong>和<strong>与人类价值观的对齐</strong>，特别是在<strong>去中心化</strong>的<code>AGI</code>开发中。</li>
</ul>
<p><strong>基于接口的未来AGI对齐路线</strong>：未来<code>AGI</code>对齐可以通过设计和优化接口来实现。这种方法包括开发能够与人类有效交互的<code>AI</code>系统，并确保这些系统能够从<strong>人类反馈中学习</strong>。通过协同设计<code>AI</code>系统和其接口，可以提高对齐的效率和安全性。此外，<code>AGI</code><strong>对齐</strong>还需要解决诸如<strong>可靠性、可解释性、可控性</strong>和<strong>伦理性</strong>等关键原则。随着<code>AGI</code>的发展，确保其与人类价值观保持一致将变得越来越重要。</p>
<h5 id="AGI对齐：确保AGI满足各种需求"><a href="#AGI对齐：确保AGI满足各种需求" class="headerlink" title="AGI对齐：确保AGI满足各种需求"></a>AGI对齐：确保AGI满足各种需求</h5><p>这个标题强调了确保<strong>人工智能</strong>(<code>AGI</code>)<strong>系统</strong>与人类价值观和目标一致的重要性，以便它们能够满足不同的需求。<strong>对齐技术</strong>包括：<strong>在线人类监督、离线人类监督和交互式监督</strong>。为什么我们需要对齐？未来<code>AGI</code>系统的开发和部署面临复杂的挑战，其中一个核心期望是它们与<strong>人类价值观、目标和道德原则</strong>的一致。这种对齐要求<code>AGI</code>具备对社会规范和个人偏好的深刻理解，从而能够做出对所有人都有益且符合道德的决策和行动。确保这种对齐对于引导<code>AGI</code>系统朝着有益的结果发展并减少意外后果的风险至关重要。为了实现这一目标，研究人员提出了多种<code>AI</code>对齐的方法，例如<strong>价值学习、逆强化学习、合作逆强化学习</strong>以及最常见的与RLHF相关的策略。这些方法旨在推断并使AI系统与人类偏好和价值观一致。此外，开发包容广泛文化、哲学和伦理观点的道德框架和指南至关重要。此外，<code>AGI</code>的部署需要全面的测试和验证，以确保它在各种情境下符合人类价值观。这包括技术模拟和现实世界的受控实验，以评估<code>AGI</code>与人类及其环境的互动。还有必要对<code>AGI</code>系统施加限制，特别是在其与外部接口和环境互动方面。通过定义严格的操作限制、实施实时监督以及集成在检测到偏离安全行为时停止操作的保护机制，可以缓解与自主决策相关的风险以及潜在的系统漏洞被利用的风险。</p>
<ul>
<li><strong>公平性</strong>：<code>AI</code>系统可能会因训练数据中存在的不公正倾向而产生不公平和歧视性的结果。这些结果可能以多种方式导致伦理问题。</li>
<li><strong>可信性</strong>：<code>AI</code>系统也可能生成包含虚假或误导性声明的信息。最近的研究发现，<strong>大语言模型</strong>(<code>LLMs</code>)可能会产生<strong>幻觉信息</strong>，生成看似合理但实际错误的输出。</li>
<li><strong>透明性</strong>：透明性旨在使相关利益方能够对模型的机制、能力和局限性形成适当的理解。<strong>大语言模型</strong>(<code>LLMs</code>)的最新进展在透明性方面带来了巨大挑战，因为它们的模型能力复杂且不确定，架构也不透明。</li>
<li><strong>安全性</strong>：<code>AI</code>系统可以通过自动生成针对性的文本、图像或代码，增强个人故意造成伤害的能力。通过<code>AGI</code>系统，人们可以以更低的成本生成用于恶意目的的内容。攻击者可以利用<strong>大语言模型</strong>(<code>LLMs</code>)的最新进展，生成新的攻击方式，并提高现有攻击的速度和效力。</li>
<li><strong>隐私性</strong>：<code>AI</code>系统可能会在现实世界中导致各种数字隐私损害，这源于<code>AI</code>模仿人类或超人类水平表现的独特能力。根据之前的研究<code>AI</code>系统可能会产生新的隐私风险。</li>
</ul>
<p>当前的<strong>对齐技术</strong>可以根据预期的对齐目标进行分类。大多数当前的模型使用各种技术来实现这一任务，并采用人类监督。然而，为了预见比教师更强的模型（即<strong>对齐超级智能</strong>），需要一个可扩展的方法来实现这一过程，通常涉及人类监督和递归演化信号。与<strong>在线人类反馈对齐</strong>大多数当前经验证实的<strong>大语言模型</strong>(<code>LLMs</code>)对齐方法属于这一类。这些方法可以通过<strong>强化学习</strong>等技术，帮助<code>LLMs</code>与<strong>在线人类反馈对齐</strong>，或者仅在离线情况下询问人类监督。因此，我们进一步将这些技术分为<strong>在线人类监督</strong>和<strong>离线人类监督</strong>两组。值得注意的是，这两个子组中的方法都有潜力成为可扩展监督的组成部分。在线监督是在训练过程中从奖励模型获得的。<strong>人类反馈的强化学习</strong>(<code>RLHF</code>)是<strong>在线监督学习方法</strong>中最常见的方法之一。还提出了多种增强的<code>RLHF</code>变体。<code>RLHF</code>的改进方向主要集中在奖励建模、优化、数据和自我改进方面。</p>
<ul>
<li><strong>奖励建模</strong>：作为对齐过程中的主要监督手段，<strong>奖励建模</strong>是改进对齐技术的关键方式。<code>Sparrow</code>将对抗性探测和基于语言的规则纳入<code>RLHF</code><strong>奖励模型</strong>。研究使用纯<code>RL</code>为<code>LLMs</code>训练提供在线人类级别的监督，并详细探讨了输出有用性和无害性之间的权衡。其他统一奖励和策略模型的技术也已出现，这为对齐<code>AI</code>模型提供了更多方向。另一个方向则专注于通过更新评估协议、组合多个奖励模型以及用合成数据精炼奖励策略来缓解奖励黑客行为或过度优化问题。</li>
<li><strong>优化</strong>：如何将在线或离线监督纳入其中是一个值得探索的开放问题。例如，使用极小极大优化同时优化奖励模型和策略模型。最近的研究正在探索 <code>RLHF</code>方法的几种替代方案。<code>DPO</code>放弃了奖励模型，并使用数据中的标记偏好优化最终目标。<code>Muldrew et al.(2024)</code>提出了一种基于<code>DPO</code>的改进方法，采用主动学习策略。<code>NashLLM</code>利用成对的人类反馈，通过<strong>纳什学习训练策略模型</strong>。<code>ReMax</code>在传统强化学习算法中去除了<strong>价值模型</strong>，并引入了一种新的方差减少技术来稳定优化过程。</li>
<li><strong>数据</strong>：<code>Sensi</code>尝试通过语言模型在语言生成的每个步骤中嵌入人类价值判断，用于奖励分配（作为评论者）和生成控制（作为执行者）。<code>Baheti et al.(2023)</code>专注于通过赋予不同实例不同的权重来增强当前的训练数据，以最大化数据对语言模型的贡献。为了确保持续的高质量数据，<code>AI</code>生成的实例被用于适应<code>RLHF</code>，最近也用于<code>DPO</code>。</li>
<li><strong>自我改进</strong>：强大的<code>AI</code>模型应学会在有或无外部监督的情况下改进自身。最近的进展之一是从弱到强的<strong>泛化</strong>。为了改进当前的<code>RLHF</code>相关方法，<code>f-DPG</code>被构建为<code>RLHF</code>的泛化，使用任何<strong>f-发散度</strong>来近似任何可评估的目标分布，这使其与之前只能适应<strong>KL发散度</strong>的方法不同。<code>Zhu et al.(2023)</code>将<code>RLHF</code>与<strong>最大熵</strong><code>IRL</code>结合，并提出了一个具有样本复杂度界限的<strong>统一范式</strong>，适用于两种情况。</li>
</ul>
<p>除了<code>RLHF</code>，其他基于<strong>强化学习</strong>的方法也引起了研究人员的关注，以便进一步探索。<code>Second Thoughts</code>通过文本编辑过程增强训练数据，并进一步利用<code>RL</code>算法训练<code>LLM</code>。<code>RLAIF</code>开启了利用<code>AI</code>生成数据进行<strong>强化学习</strong>的新时代，使得从更具竞争力的生成模型中提取知识更加有效，同时保持了<code>RLHF</code>技术的优势。<code>Kim et al.(2023)</code>提出了<strong>基于合成反馈的强化学习</strong>(<code>RLSF</code>)，他们自动构建奖励模型的训练数据，而不是使用人工标注的偏好数据。为了有效地调整<strong>黑盒模型</strong>，各种方法引入了<code>RL</code>算法。<strong>方向性刺激提示</strong>(<code>DSP</code>)使用可训练的策略语言模型，通过<strong>监督微调</strong>(<code>SFT</code>)和<code>RL</code>调整的策略语言模型，引导黑盒冻结的<code>LLM</code>朝向期望的目标。与上述仅涉及单一模型的对齐方法不同，<code>RL4F</code>是一个<strong>多智能体协作框架</strong>，针对<code>LLM</code>进行微调，并使用一个小型评论模型，通过文本反馈对<code>LLM</code>的响应进行评论。与<code>DSP</code>直接修改初始提示不同，该框架通过逐步互动逐渐影响<code>LLM</code>的输出，使其适用于黑盒<code>LLM</code>的优化。</p>
<p><strong>与离线人类监督对齐</strong>：基于<strong>强化学习</strong>的方法提供了灵活的<strong>在线人类偏好监督</strong>，但代价是<strong>训练奖励模型</strong>可能会出现误对齐和系统性缺陷，以及<strong>强化学习训练</strong>的固有不稳定性。离线监督方法可以帮助缓解这些挑战，同时在大多数情景下仍能取得不错的性能。我们将<strong>离线监督调优方法</strong>分为基于<strong>文本</strong>和基于<strong>排名</strong>的反馈信号。</p>
<ul>
<li><strong>基于文本的反馈信号</strong>：基于文本的反馈信号涉及将人类意图和偏好转换为基于文本的反馈，以确保对齐，扩展了<code>SFT</code>过程。这些方法主要从改进训练数据入手。<code>CoH</code>受人类学习过程启发，专注于根据连续输出和之前推理步骤的总结反馈来调整模型，以微调预测偏好输出。<code>RAFT</code>使用奖励模型通过<code>SFT</code>将模型输出与人类偏好对齐，但以离线方式进行。<code>LIMA</code>旨在验证<code>LLMs</code>在预训练期间获取大部分知识的假设，仅需少量指令微调数据即可引导生成期望的输出。<code>ILF</code>引入了一个三阶段过程，基于语言反馈建模人类偏好，类似于<strong>贝叶斯推断</strong>。稳定对齐通过使用<code>Sandbox</code>模拟器，从多智能体社交互动中学习对齐，直接用偏好数据优化<code>LLMs</code>，避免奖励黑客行为。<code>SteerLM</code>赋予最终用户在推理过程中控制响应的能力，通过使响应符合明确定义的多维属性集来调节响应。<code>CLP</code>学习可操控的模型，能够在推理时有效权衡冲突目标，基于多任务训练和参数高效微调的技术。</li>
<li><strong>基于排名的反馈信号</strong>：<code>CRINGE</code>深入研究了<code>LLMs</code>应避免的负面示例，而<code>Xu et al.(2022)</code>通过训练另一个生成有害内容的模型来微调模型。然而，这种方法在资源消耗和潜在模型质量及多样性降低方面引发了担忧。<code>Schick et al.(2021)</code>提出了一种识别和生成与有害文本类型对应的文本的方法。<code>SLiC(Zhao et al., 2023a)</code>通过使用各种损失函数，将输出序列的概率与参考序列对齐。<code>RRHF(Yuan et al., 2023)</code>通过排名结果自动生成监督信号以实现对齐，而<code>DPO</code>则直接优化<code>LLMs</code>以与人类偏好对齐，类似于<code>RRHF</code>，但侧重于最大化奖励并集成<strong>KL散度</strong>正则化。<code>IPO</code>在<code>DPO</code>的基础上引入了一个<strong>正则化项</strong>，以稳定训练过程。偏好排名优化(<code>PRO</code>)与<code>IPO</code>和<code>DPO</code>类似，使用一个正样本和多个负样本优化<code>LLMs</code>，而不是成对比较。<code>Kahneman-Tversky</code>优化(<code>KTO</code>)仅基于标记为“好”或“坏”的单个示例定义<strong>损失函数</strong>，不需要成对偏好，使其训练数据更易获取。此外，<code>Best-of-N(BoN)</code>方法也是一种流行且有效的算法，用于在推理时将语言模型与人类偏好对齐。<code>BoNBoN</code>对齐微调<code>LLM</code>以模仿<code>Best-of-N</code>采样分布。<code>BOND</code>引入了一种新的<code>RLHF</code>算法，旨在模仿<code>Best-of-N</code>，但在推理时不会带来显著的计算开销。变分<code>BoN(vBoN)</code>通过最小化语言模型与<code>BoN</code>分布之间的<strong>逆KL散度</strong>，近似<code>BoN</code>算法诱导的<strong>概率分布</strong>。</li>
</ul>
<p><strong>可扩展监督</strong>：对齐模型的最终目标是规范超人类智能。可扩展对齐方法是一种有前景的手段，旨在解决监督复杂任务或超人类模型的挑战。通过使相对弱势的监督者（如人类）能够使用逐步演化的信号监督复杂任务或系统，可扩展对齐为超出人类能力的任务提供了解决方案。</p>
<ul>
<li><strong>通过任务分解</strong>：已提出各种范式和策略将复杂任务分解为更简单的子任务。<strong>因子认知</strong>涉及将复杂任务分解为同时处理的较小独立任务。<strong>过程监督</strong>将任务分解为顺序子任务，并为每个阶段提供监督信号。<strong>三明治方法</strong>将复杂任务委派给领域专家解决。<code>IDA</code>引入了一种迭代蒸馏和放大过程，通过任务分解提升模型能力。<code>RRM</code>用奖励建模替代<code>IDA</code>中的蒸馏模仿学习，使用人类对齐信号和强化学习优化模型。这些方法旨在通过迭代改进，增强人类与智能体之间的合作，以解决复杂任务。</li>
<li><strong>通过人类编写的原则</strong>：<strong>宪法</strong><code>AI</code>，也称为原则指导对齐，涉及人类为<code>AI</code>系统提供一般原则，使<code>AI</code>系统能够在此指导下生成训练实例。<code>Bai et al.(2022b)</code>提出了一种<strong>宪法</strong><code>AI</code>的两阶段训练方法，在<code>SL</code>阶段使用红队提示，在<code>RL</code>阶段训练偏好模型。类似地，<code>Sun et al.(2023c)</code>引入了<code>Dromedary</code>，一种基于人类编写原则的自我指导和自我对齐方法训练的模型，不使用<code>RL</code>。这些方法旨在扩展人类监督，以协助开发超人类<code>AI</code>系统。</li>
<li><strong>通过模型互动</strong>：其他可扩展监督的努力探索了模型之间互动优化的可能性。<strong>辩论范式</strong>使智能体能够提出问题的答案，并参与结构化辩论以证明和批评立场。类似地，<strong>市场制定</strong>部署市场和对手模型，通过生成论据影响市场对问题的回答。与此同时，对手目标通过论据改变市场的预测，从而形成动态决策流。</li>
</ul>
<p><strong>如何处理</strong><code>AGI</code><strong>对齐</strong>：基于接口类型的对齐，当<code>AGI</code>系统与各种接口（包括工具、<code>API</code>、其他<code>AI</code>智能体和人类）互动时，必须遵守不同的期望和约束，以确保符合伦理要求并产生有益的结果。</p>
<ul>
<li><strong>与工具和API的互动</strong>：在与工具和<code>API</code>互动时，我们主要关注<code>AGI</code>对齐中的效力、效率以及一些基本的限制规则：对齐的主要目标是赋予这些模型与工具和<code>API</code>高效互动的能力，并准确遵循指令。例如，在由<code>AGI</code>管理的自动化工厂中，<code>AGI</code>需要灵活地利用各种机械设备和制造工具来完成生产过程。在这种情景下，<code>AGI</code>需要通过对齐技术准确完成工厂工具的使用过程，并在规定时间内创造更高的利润。在与工具和<code>API</code>互动时，<code>AGI</code>系统应遵循基本协议，并尊重这些接口的预期用途。在数字世界中，这可能涉及正确使用搜索引擎、社交媒体平台或其他在线服务，而不参与恶意活动或传播虚假信息。<code>AGI</code>不能在互动过程中使用<code>API</code>或工具进行犯罪活动。在物理环境中，控制物理设备的<code>AGI</code>系统必须优先考虑安全，避免对环境造成伤害。例如，考虑一个在数字世界中从搜索引擎获取信息的<code>AGI</code>问答系统，它应遵循适当的搜索引擎优化(<code>SEO</code>)实践，避免操纵搜索结果，可能泄露提问者的隐私。同样，如果一个机器人工厂在物理世界中由 <code>AGI</code>指挥，除了确保工业生产过程的顺利进行外，还必须防止<code>AGI</code>执行潜在的破坏性活动。</li>
<li><strong>与其他智能体的互动</strong>：与之前的互动场景相比，在与其他智能体互动时，<code>AGI</code>对齐更注重相互合作，遵守开发者的规则以及保护智能体的隐私：在与其他<code>AI</code>智能体互动时，<code>AGI</code>系统应遵循合作、公平和互相尊重的原则。随着<code>AGI</code>的发展，各种领域可能会开发出具有专业知识、技能和目标的多样化<code>AGI</code>智能体。在这种多智能体环境中，<code>AGI</code>系统必须设计成能够与其他智能体有效合作，利用它们的互补能力来实现共同目标并解决复杂问题。同时，<code>AGI</code>系统不应试图对其他智能体进行敌对利用或操纵，以实现自身目标。它们应避免从事可能削弱其他智能体性能、完整性或决策能力的行为，认识到这些智能体具有自己的<strong>大脑、记忆、感知和推理能力</strong>。<code>AGI</code>系统必须抵制任何违背其预期用途或开发者设定约束的诱惑，因为这种行为可能导致意外后果，并对多智能体生态系统的稳定性和安全性构成重大风险。</li>
<li><strong>与人类的互动</strong>：与前两种互动场景相比，<code>AGI</code>在与人类互动时的对齐需要更多的约束，同时为人类带来便利和利益。这些约束主要是为了保护人类的<strong>隐私、伦理、安全和自主权</strong>，并与人类价值观保持一致：智能<code>AGI</code>必须设计成不仅遵循直接命令，还能稳健和安全地运行。在面对非典型或未预见情况时，这些模型应紧密符合积极的人类价值观和感知，以减轻潜在风险。因此，对齐过程不仅仅是服从指令，还包括伦理和安全考量的融合，确保<code>AGI</code>的行为在广泛的情景下始终有益且无害。<code>AGI</code>的自我发展需要对人类价值观的监督对齐。<code>AGI</code>的能力和知识库未来可能超越人类理解，使传统的监督方法效果不佳。因此，需要一套全面且精心设计的预防措施。这些措施应包括法规和伦理指南，以及先进的对齐策略，以预见并应对超人类智能的独特挑战。例如，北京人工智能研究院(<code>2023</code>)提出了一套<code>AI</code>发展的“红线”，以减轻先进<code>AI</code>系统的灾难性风险。由领先<code>AI</code>研究人员和利益相关者起草的共识声明强调，需要国际协调和治理，以确保<code>AI</code>的安全发展和部署。这种方法将有助于确保<code>AGI</code>系统在超越人类理解的智能水平上仍与人类价值观和社会福祉保持一致。<code>AGI</code>系统必须谨慎地感知和利用有关人类的信息，并遵守最高的伦理标准，如严格的安全和隐私要求。它们应主要依靠纯语言和视觉输出与人类交流，因为这些模态不太可能像物理行动那样造成意外伤害。它们还必须透明地表明自己是<strong>人工智能</strong>，避免欺骗人类或操纵他们的情感。</li>
</ul>
<p>上述三个<code>AGI</code>对齐方案针对不同的接口，其约束不断增加并变得更加严格。这是因为我们将<code>AGI</code>对齐的要求视为<code>AGI</code>在不同群体中的应用需求。在处理工具和<code>API</code>时，由于接口对象是客观存在的无生命实体，我们会更加关注它们在交互过程中带来的效益和价值，并制定一些轻微的规章以确保交互的正常秩序。对于<strong>智能体</strong>，既然不同的智能体可能代表不同开发者的利益，我们除了考虑自身的利益外，还需要尊重其他智能体的利益。最后，在与人类交互的过程中，基于以人为本的理念，我们将从多个方面考虑最严格的约束，以使<code>AGI</code>在人类使用中变得可靠和安全。</p>
<p>未来<code>AGI</code>模型在处理不同任务时变得更加强大，势必需要增加模型参数。为了确保它们的安全和有效部署，我们提议研究重点应放在开发<strong>可靠、有效</strong>和<strong>透明</strong>的对齐技术上：</p>
<ul>
<li><strong>一致的对齐确保可靠的部署</strong>：由于收集高质量监督数据的挑战，存在一些挑战，包括获取反馈的困难、人类标注者的数据污染、部分可观察性以及反馈数据中的偏见，这些都为当前的对齐方法构成了障碍。</li>
<li><strong>高效的对齐有助于AGI模型的发展</strong>：另一方面，这些方法严重依赖于任务可以并行化的假设。这种假设并不总是成立，因为一些任务，如排序算法，需要顺序处理步骤，这些步骤不能完全分解为并行部分，从而导致额外的处理时间。另一方面，这些对齐方法的训练阶段是不可避免的。随着参数规模的增加，这在实际应用中部署对齐算法时会带来问题。一些最近的工作已经开始寻找减少<code>AI</code>系统对齐整体训练成本的解决方案。</li>
<li><strong>透明的对齐保障下一代模型的安全</strong>：通常假设模型的意图对人类来说是透明的。然而，如果模型可以向人类监督者隐瞒其真实意图，实施可扩展的对齐方法将会面临挑战。</li>
<li><strong>统一的评估框架对于复杂任务是必要的</strong>：当前的对齐方法评估比生成更容易。虽然这在某些任务中可能是正确的，但对于具有复杂文本输出且语义标签较少的任务，这可能不成立。然而，从模型中评估综合性解释可能比创建它们更容易。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>umbrella
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/" title="机器学习(ML)(二十七) — AGI探析">https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/02/23/artificial-intelligence/ML26_theory_study/" rel="prev" title="机器学习(ML)(二十六) — 强化学习探析">
                  <i class="fa fa-chevron-left"></i> 机器学习(ML)(二十六) — 强化学习探析
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/23/artificial-intelligence/ML28_theory_study/" rel="next" title="机器学习(ML)(二十八) — AGI探析">
                  机器学习(ML)(二十八) — AGI探析 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备15012817号-2 </a>
  </div>
  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">umbrella</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">1.4m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">78:53</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/fresh88888888" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/pjax.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/tags/pdf.min.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/fancybox.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/pace.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":5000,"priority":true,"url":"https://fresh88888888.github.io/2025/03/15/artificial-intelligence/ML27_theory_study/"}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/quicklink.min.js"></script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"fresh88888888.github.io","issue_term":"title","theme":"github-light"}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.17.1/third-party/comments/utterances.min.js"></script>

</body>
</html>
