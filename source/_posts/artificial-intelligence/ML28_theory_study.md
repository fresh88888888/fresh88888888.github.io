---
title: 机器学习(ML)(二十八) — AGI探析
date: 2025-03-23 12:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

##### AGI路线图

我们定义了三个`AGI`级别及其主要特征。主要目标是定位当前的`AI`发展，量化现有的局限性，并激励以实现下一级别的能力、评估和对齐。如下图所示：
<!-- more -->
{% asset_img ml_1.png "雷达图展示：评估AGI的方法，涵盖四个核心领域：内部、接口、系统和对齐" %}

**内部领域**：评估基本**认知能力**，如**推理、记忆、感知**和**元认知**；**接口领域**：评估`AGI`的工具使用能力和连接智能的能力；**系统领域**：关注运营方面，包括**效率、规模**和**计算能力**；**对齐领域**：考察**伦理能力**和**安全性**。

- **级别一：萌芽期**`AGI`，这一级别的`AGI`通常在特定基准任务上表现优于或与人类相当。级别一的`AGI`代表了当前最先进的`AI`系统。例如，`GPT-4`和`DeepSeek-R1`在许多自然语言任务中表现出显著的能力，包括语言理解、生成连贯和上下文相关的响应，通常与人类相当或更胜一筹。这些系统在有足够大的人类数据集的情况下通常表现良好，并能在某些领域辅助人类。目前在许多领域处于这一级别的`AGI`。
- **级别二：超人类**`AGI`，从级别一到级别二的关键转折点是`AI`能够在现实世界的任务和应用中完全取代人类。它们在效率（例如，更高的准确性、更好的问题解决能力）、效率（例如，更快的处理速度、更高的吞吐量、处理大量数据的能力）和可靠性（例如，更高的成功率、抗疲劳能力、增强的安全性）方面表现出色。这些系统还可以从有限的数据中学习，跨领域推广知识，在较少的人类干预下适应新环境，并在其方法中表现出**创造力**和**创新**。它们还能进行复杂的决策过程，考虑多种因素，并根据预定义的目标优化结果。值得注意的是，级别二的`AGI`应准备好在现实世界中部署，并在没有任何人类干预的情况下解决当前由人类解决的复杂现实世界任务。在我们看来，除了在高度专业化的领域（例如，围棋游戏）之外，很少有`AI`系统达到了级别二。
- **级别三：终极**`AGI`，虽然级别二的`AGI`能够在解决许多任务时取代人类，但级别二的`AGI`仍然需要人类的努力。我们认为，级别三的`AGI`的本质是，给定某个可能模糊且高层次的目标，这种`AGI`系统能够在没有任何人类干预的情况下完全自我进化。这一级别标志着`AGI`发展的巅峰，代表了一个理想化且可能无法实现的`AI`系统。终极`AGI`将具备远超人类能力的学习、推理和决策能力，并解放人类在这种`AGI`系统发展过程中的参与。因此，在这个阶段，确保这种级别三的`AGI`与人类价值观和目标高度一致变得尤为重要。此外，级别三的`AGI`可能会展示出更深的人类情感，如同理心、社会意识，从而能够与人类和其他`AI`系统无缝协作，甚至具备自我意识的火花。然而，实现终极`AGI`仍然是一个理论概念，其可行性仍在不断研究和辩论中。

**自回归生成是通向AGI的途径吗**？下一个词预测是大模型成功的核心。这引发了一个问题：下一个词预测能否通向`AGI`？本质上，自回归生成利用大量自监督数据的方式代表了一种**大规模多任务学习**。通过预测语料库中给定文本的下一个词，它解决了从传统`NLP`任务（如语法、词汇语义和翻译）到常识推理和知识驱动推理的各种任务。学习**输入-输出关系**，或**上下文学习**，可以被视为下一个词预测。世界上的关系通常以词、视觉标记、时空片段或其他类型的标记编码，使它们可以通过下一个词预测来学习。关键问题仍然是：世界知识的范围，包括直觉、情感、文化和艺术表达等隐性知识，是否能够编码为简化的标记？**自回归方法**能否学习到世界知识中的所有**因果关系**，而不仅仅是**相关性**？此外，**扩散模型**的流行也对自回归生成的未来提出了挑战。这种方法在生成过程中不依赖于先前生成的数据点，而是依赖于逐渐减少噪声来恢复数据的过程。**扩散模型**在生成任务中的效果也导致了其在实际应用中的广泛使用。所有这些都使得**自回归生成**是否是通向`AGI`的途径成为一个持续的争论话题。

**扩展法则是否存在局限**？**扩展法则**表明，增加某些模型的规模和训练数据量可以带来各种任务在性能上的预测改进。这凸显了开发可扩展模型架构和获取更多高质量数据以供这些不断增长的系统使用的重要性。该前提假设表明，通过遵循这一轨迹，我们可以更接近创建具有`AGI`能力的模型。然而，**边际效益递减**现象表明，持续扩展需要指数级增加的资源，以换取微小的改进。此外，某些能力（如创造性思维、现实世界的直觉和伦理推理）可能无法通过单纯的扩展有效学习，因为它们需要更复杂的**推理**和**学习机制**。

**合成数据是未来还是风险**？`AGI`的成功依赖于大量、多样且高质量数据集的获取。尽管现有的高质量数据量将继续增长，但合成数据已成为一种可行且高效的解决方案，能够大规模生成复制现实世界模式的人工数据。然而，这一创新也带来了重大挑战。**合成数据**的滥用可能传播有偏见或误导性的信息，导致与人类期望的偏差。未来的努力应集中在提高合成数据的质量和多样性，并探索适用于其的**扩展法则**。此外，即使人们在模型训练中没有故意使用**合成数据**，`LLM`的广泛使用也可能导致互联网上充斥着**合成数据**。虽然自动区分**合成数据**和**真实数据**具有挑战性，但这可能会对训练数据集造成潜在的污染风险。

**计算优越性是否意味着智力优越性**？许多在游戏中表现出超人类的**智能系统**（如`AlphaGo、AlphaZero、AlphaStar、MuZero`等）不仅能够以大幅度战胜最佳人类世界冠军，还能帮助分析游戏并为高级玩法创建新策略。这些超级游戏`AI`的基础几乎都是一种**搜索计算**，它能够通过算法引导的剪枝巧妙地枚举大量可能性，从而超越人类的思维能力。然而，关于这种计算优越性是否构成真正智能，还是仅仅代表一个强大的程序，仍然存在争议。最近成功的`LLM`系统（例如，`ChatGPT`在国际象棋游戏中表现不佳）并不具备这种**搜索计算能力**，但仍被许多人认为同样的智能。在探索`AGI`的过程中，我们是否应期望未来的系统也具备这些特性，以及如何平衡这些特性与其他改变我们评估`AGI`系统方式的智能特质？

**如何走向完全自主**？当前的`AI`系统被设计用于特定任务和特定场景，展示出专业化的能力。随着我们向`AGI`迈进，期望转向`AI`在没有人类干预的情况下学习新技能和创新工具的自主性。这一进展要求复杂的**自我评估**和**自我改进机制**。此外，`AGI`的愿景包括**完全自主**，消除对人类持续监督的需求。这种自主性强调了先进的**自我调节**、**安全**和**风险预防措施**的重要性，确保未来的`AGI`系统做出的决策能够被信任。

**如何有效地将人类价值观融入**`AGI`？随着`AGI`的发展，将人类价值观和伦理融入这些系统变得至关重要。想象一个`AGI`与人类社会和谐共存的未来，这些系统必须被设计成能够执行任务，并理解和遵守伦理规范和价值观。我们目前依赖于法规和约束，但将“**人类价值观**”真正融入`AGI`将是一个重要的研究领域。`AGI`的发展将伦理原则编码到这种新形式的智能的核心提供了机会。人们期望伦理`AGI`系统能够在复杂的道德环境中指引，并做出反映全球文化多样价值观的决策。

**如何在推进过程中平衡风险和收益**？`AI`发展的初期阶段集中在增强特定能力和解决特定挑战，但`AI`技术的不断进步需要更加注重建立与安全和伦理相关的约束。呼吁停止所有可能导致失控`AI`的研究的呼声越来越高。然而，这一举措需要全球协调和监督，并可能扼杀`AI`的许多潜在好处。相反，主张继续推进`AI`，确保所有强大的`AI`系统都能负责任地构建和部署。这需要：加强对齐研究的关注和投资，创建`AI`必须遵守的普世价值观和目标集，并开发健壮的方法将 `AI`系统与这些原则对齐；确保任何有能力创建先进`AI`的团体都理解并使用这些技术；实施平衡`AI`发展满足最少干预需求与严格监督要求的法规。

`AGI`**的最大阻碍是什么**？
- **模仿类人推理和主动性**，开发具有类人推理和主动性的`AI`系统对于提升其能力至关重要。这些系统必须具备：**有意识的思考**、**推理**、**一致性**、**因果关系**和**注意力**，类似于人类的**有意识推理**。当前的`AI`训练范式主要依赖于大数据集，与人类的学习过程相比存在不足。这种不足部分是由于训练过程中缺乏深层次、统一的主动性，从而限制了`AI`系统在开放世界情境中的有效性。因此，研究人员应专注于创建能够复制这些高级推理技能的架构和技术，以克服现有的局限。
- **确保**`AI`**安全和与人类价值观的一致性**，`AI`安全被认为是实现`AGI`的最大障碍，没有充分的安全措施，`AGI`的发展被认为是不可行的。这涉及到在创建具有主动性的`AI`系统的同时，确保其目标与人类价值观一致的挑战。此外，在`AI`辅助决策中有效地表示不确定性对于确保与人类利益的一致性和实现理想结果很重要。
- **提高效率和可扩展性**，今天的人工智能模型常因其巨大的规模以及所需的大量能源和资源而受到批评，使得训练和部署变得昂贵。这种能源效率低下的根本原因被认为是数据移动，因此有必要减少数据移动以实现更高效的人工智能系统。此外，有一种观点认为，通过结合表现力和适当的归纳偏差，人工智能模型可以随着时间变得更加紧凑和有效，从而实现更小但更强大的模型。
- **理解和应对紧密相连的世界**，赋予人工智能系统理解由数据、人类和**智能体**组成的高度互联世界的能力，是实现**通用人工智能**(`AGI`)的重要挑战。人工智能需要理解这些实体之间复杂的依赖关系，以创建真正智能和适应性强的系统。此外，人工智能不仅应识别数据中的**模式**，还应提供深刻的理解。

**总结**，实现**通用人工智能**(`AGI`)需要解决几个关键的研究问题并克服障碍。这些挑战包括模仿类人的**推理**和**行动能力**，确保**人工智能的安全性和与人类价值观的一致性，提高效率和可扩展性，理解和应对紧密相连的世界**，以及**自动化的科学发现**和**泛化**。通过专注于这些关键领域并开发出创新的方法，研究人员可以在实现创造真正智能和适应性强的人工智能系统方面取得显著进展，从而造福整全人类。

**开源与**`AGI`**时代**？**开源人工智能系统**的最终目标是促进人工智能研究。然而，随着技术的不断发展，迫切需要在**开源人工智能系统**时平衡其利弊：
- 人工智能开源与武器管制的类比。可访问性带来了被滥用的风险，就像一些西方国家合法化武器一样。一旦人工智能系统的全部细节公开，就很容易削弱甚至移除其安全保障，使其容易被利用。例如，利用当前技术和大量互联网数据的精细调整，可以创建一个在社交媒体上具有高度说服力的**智能体**。因此，开源需要考虑不同精细调整和采用的巨大潜力。然而，值得一提的是，利用程度因许多因素而异，其中之一是用于此类精细调整的计算资源量。因此，评估开源特定软件的潜在后果是一个具有挑战性的问题。
- **模型自身的观点**，一项有趣的实验测试了**大语言模型**(`LLMs`)是否会投票支持或反对开源`LLMs`。也许令人惊讶的是，除了`Claude-3`之外，所有模型都投票支持开源，这引发了为什么只有`Claude-3`表现不同的问题。这一实验表明，一旦我们进入人工智能系统在决策中扮演更多角色的阶段，监管它们将变得越来越困难。这再次强调了谁应该和能够决定开源过程和标准，以最大程度地造福整个社会并促进科学研究。
- **开源的动机**，公开智力成果并分享更多细节需要动机。为了促进有益的开源，建立激励机制至关重要。对许多个人来说，动机更加明确：例如，年轻研究人员和学生将开源视为一种宣示所有权和宣传自己作品的方式。然而，许多其他实体（如科技公司）的动机则更加模糊，通常涉及技术壁垒和货币化。总体而言，激励人们为开源社区做出贡献是一项协作，是实现开放`AGI`的必要步骤。
- **开放模型与开放数据**，尽管已经有许多开源模型，但它们的数据配方很少透明。再加上在基准测试过程中数据泄露的潜在风险，越来越难以区分新模型的真正创新点与炒作点。尽管完全公开数据本身仍然具有挑战性，但建立一些保护数据隐私的机制（如本地检查）仍然可以为从业者和研究人员提供更多见解，以理解特定模型的各种行为和偏差，从而大大地帮助揭示这些模型的面具。

总体而言，研究人员支持并鼓励**开源人工智能系统**，但他们也意识到其好处，同时也关心如何使其安全和受控，特别是在我们越来越接近`AGI`的时候。社会各界的参与可能是必要的，以规范用户的行为，并建立激励贡献者分享其工作。未来`AGI`研究方法论。研究人员应考虑各种观点和方法，以促进人工智能系统的未来发展。研究人员不应仅专注于一个方向，而应探索多条建立`AGI`的道路，以避免“**局部最小值**”。在这个意义上，不同的研究人员可能有不同的观点。在安全方面，根据`Yoshua Bengio`的观点，了解不同方法的局限性至关重要。从`Yejin Choi`的角度来看，在通往`AGI`的道路上，精确理解语言模型及其基准测试非常重要。`Song Han`则强调高效硬件系统和网络架构的方向，以提升当前人工智能系统的训练和推理效率。通过采用多种研究方法，研究人员可以为人工智能的发展做出有意义的贡献。

**去中心化的人工智能**，硬件加速器的进步推动了规模达数十亿甚至数万亿的语言模型的成功。目前，大多数顶尖的**大语言模型**(`LLMs`)都是在配备以下设施的数据中心中训练和部署的：1、高端基础设施，如同质加速器；2、优化的网络拓扑，以实现超快的互连；3、稳定高效的电源供应；4、人类专家的精心维护。然而，从头开始训练一个像`GPT-3`这样的模型仍然超出了个人的承受能力：例如，完整地预训练一个`GPT-3`模型，即使它已不再是最强大的模型，估计仍需数月时间和上千个`V100 GPU`。在扩大批量规模的同时不影响响应延迟，模型的部署也面临许多挑战。迈向`AGI`时代，我们需要新的技术来克服当前主流模型训练和部署方式的局限性，其中一个显著的方向是**从数据中心转向去中心化的人工智能**。

去中心化和边缘`LLMs`的需求在扩展模型时，最突出的问题是所需的大量内存，这使得数据中心训练成为可能，因为它们拥有组织良好的`GPU`机架和高速互连。然而，有许多分布在地理上的闲置计算资源，如果以有意义的方式结合起来，可以作为一个高性能的超级服务器。此外，随着我们向`AGI`迈进，数据和用户隐私将越来越受到关注，拥有一个去中心化的人工智能系统，边缘设备只将必要的信息发送到云端，将确保不同层次的安全。对于许多应用系统（如具身智能体、自动驾驶汽车和健康监测器），极低的延迟和高可用性变得至关重要，这对于集中式服务器来说可能是一个具有挑战性的特性。随着`AGI`系统在日常生活中的应用越来越广泛，我们可以期待人工智能需要更多的透明度和个人的精细控制，而去中心化的`LLM`由于其去中心化的特性，成为一个有前景的候选者。缓解硬件限制，边缘服务器的一个期望特性是能够在普通加速器上运行`LLMs`。`FlexGen`首次展示了在单个`16GB GPU`上运行`OPT-175B`等大模型的文本生成是可能的。`FlexGen`通过线性规划高效搜索模式，并通过权重和缓存量化，从`GPU、CPU`和磁盘聚合内存以及计算，将`OPT-175B`以每秒`1`个令牌的速度解码，批量大小为`144`，且准确度损失可忽略不计。为了最大化不同硬件的潜力，`MLC-LLM`提供了一种通用解决方案，允许语言模型在各种硬件后端和本地应用程序上原生部署。例如，`MLCChat`是一款`iOS`应用程序，可以在最新的`iPhone`和`iPad`型号上运行；类似的`APK`也适用于`Android`设备（包括三星、红米和谷歌等制造商）。这种可能性还延伸到`Mac、Linux`和网络浏览器。最后，在硬件方面，越来越多强大且经济实惠的芯片被开发出来，以应对边缘`LLMs`的兴起，例如苹果的`M3`系列和高通的`Cloud AI 100 Ultra`（支持单张`150`瓦卡上的`1000`亿参数模型）。最后，**核电池**展示了其革新移动计算平台电源结构的潜力，其`50`年不充电的电池续航时间，可能使边缘设备更稳定，并适用于`LLMs`的多种应用。去中心化`LLM`的未来形态是不可否认，因为它能够满足用户对`AGI`系统的许多期望。随着新算法、系统和硬件的进步，将这些组件整合为一个连贯的整体只是时间问题。我们可以预见，不久将能够实现全球用户通过各自的设备和数据进行协作训练和推理，同时保持隐私、安全和透明控制，这才是真正民主和开放的人工智能形态。

**人类-人工智能协作**：是指人类与人工智能在不同环境中通过协作互动以实现特定目标的过程。随着我们向`AGI`迈进，人工智能将有更多机会与人类协作。以往关于**人类-人工智能协作**的研究涵盖了许多现实世界中的案例。其中一个代表性方向是**人类-人工智能协作内容创作**，例如撰写文章、绘画、编写代码或头脑风暴。例如，研究人类-人工智能协作写作的研究人员专注于研究作家如何与这些新型写作助手互动，以及它们如何影响人类写作。他们提出了一个设计空间，作为一种结构化的方式来探索和研究智能和互动写作助手的多维空间。另一个代表性方向是**人类-人工智能协作决策**，其中人工智能助手向人类提出建议，而人类负责做出最终决策。例如，人工智能系统可以预测可能的住院复发情况，以帮助医生做出相关的护理决策，或为公共服务的政策制定者提供资源分配决策。在这种情况下，研究人员认为，人类-人工智能团队中最准确的模型并不一定是最好的队友。相反，人工智能系统应以人为中心进行训练，直接优化团队的表现。人类-人工智能协作的方面 为了实现高效的协作，以往的研究集中在人类-人工智能协作的几个关键方面，包括**互动结果**和**互动过程**：
- **互动结果**：人类-人工智能协作的初衷之一是实现互补性能，利用人工智能和人类的优势，实现比单独任何一方都更好的互动结果。在大型语言模型的时代，这需要对语言模型能够执行的任务进行合理的描述和分配。设计有效的人类-人工智能协作通常从全面理解人类和人工智能在特定任务中的能力和局限性开始。在人类-人工智能协作写作的案例中，研究人员认为，人类擅长逻辑推理和长文档的一致性，而模型则擅长根据局部上下文快速生成多个版本的文本。因此，人类主导写作并编辑模型的建议，而模型则提出下一句建议并帮助快速写作。通过这种描述，为协作团队中的人类和人工智能分配合理的任务对于获得更好的结果至关重要。为解决这一问题，近期研究转向了**语言模型链技术**。链技术将任务分解为对语言模型的多次调用，每次调用语言模型只需完成几个基本操作。这些技术在**人类-语言模型协作**环境中被广泛采用，人类可以在语言模型可能无法充分处理的子任务中进行干预。
- **互动过程**：在人类-人工智能协作中，还有一些关键问题需要解决，以实现更好的互动过程。其中最重要的前提是确保人工智能的行为与人类在人类-人工智能团队中的期望一致。鉴于大语言模型的最新进展，提示已成为实现一致性的主要方法。**提示工程**也成为一个活跃的研究领域，专注于开发和优化提示，以便高效地将语言模型用于各种应用。然而，近期研究发现，非人工智能专家很难设计语言模型的提示。来自人类指导经验的期望和过度泛化的倾向是有效提示设计的障碍。此外，在人类-人工智能互动中建立适当的信任是另一个重要方面。为此，研究人员开发了多种技术，帮助负责任的人类了解何时信任人工智能的建议以及何时持怀疑态度，其中一种方法是通过可解释的人工智能。他们开发了许多以用户为中心的创新算法可视化、界面和工具包，支持不同人工智能素养水平的人在各种领域理解和信任人工智能。然而，许多因素可能会影响人类在现实世界中对人工智能队友的信任。例如，研究人员发现，提供决策建议和解释很少能帮助人们建立更多信任并做出更好的决策。

人类-人工智能协作的未来，随着人工智能接近人类水平的能力，人类-人工智能协作可能带来的利益和关切并存。未来的人工智能系统可以在人类-人工智能协作中扮演多种角色，提供解决复杂任务的机会，但也面临着协作环境中的不确定性和非确定性行为等挑战。
- **利益**：未来的`AGI`可以在人类-人工智能协作环境中承担更多不同的角色。随着`AGI`发展到具备人类水平的能力，人工智能将有许多机会与人类合作，解决超越内容创作或决策的复杂任务。`AGI`有可能同时承担类似真实人类的多种角色，例如集体教育儿童或照顾老年人。此外，大语言模型的最新进展展示了在人类-人工智能协作中赋予人类更多可能性。与传统模型不同，大语言模型可以为现实世界的各种任务提供支持。通过基于提示和示例的使用，人类可以在几乎没有人工智能知识的情况下创建特定用途的模型，从而降低非专家在人类-人工智能互动中创新的门槛。
- **关切**：将未来的`AGI`引入人类-人工智能团队带来了许多挑战。一方面，由于其非确定性特征、有限的推理能力和偶尔难以理解指令，最近的大语言模型在人类-人工智能互动过程中仍面临着固有的挑战。面对这些挑战，我们仍然远未掌握构建人类-人工智能协作系统的全面知识。另一方面，`AGI`的能力在人类-人工智能协作环境中高度依赖上下文和主观解释。因此，在积极影响的同时，如何以及何时建立人类-人工智能协作以最小化负面影响仍然是一个难题。

##### 总结

最终，我们的目标是引起关注并促使人们思考一个紧迫的研究问题：我们距离实现`AGI`还有多远，以及如何负责任地实现`AGI`。我们坚信，解决这些研究问题需要人工智能研究界及其他领域的共同努力。除了通过全面审视最新研究进展为人工智能研究人员建立共同基础外，我们还阐述了对`AGI`本质的愿景，并倡导负责任的发展方式。
- 我们提出了新的`AGI`定义、分层和特征。我们深入探讨了实现`AGI`所需的内部和外部（接口）能力的技术细节，以及系统努力使其实现成为可能。我们讨论了改进当前**评估范式**、高效部署越来越大的模型以及维护人工智能与人类共存生态系统的重要性。这些因素对于将研究理念转化为造福社会的实际产品至关重要。
- 我们还展示了一系列相关案例研究，说明人工智能系统在日常生活中的广泛应用，同时坦率承认其潜在局限性。
- 我们一直强调不断进步的**人工智能技术的伦理**、**社会**和**哲学影响**。通过纳入这些考虑，我们旨在指导工程师和研究人员构建以人类为中心的`AGI`系统，优先考虑人类的福祉和利益。

当我们站在这个转型时代的边缘时，以对社会潜在影响的敏锐意识来发展`AGI`至关重要。通过优先考虑**伦理问题**、**协作**和致力于人类的**改善**，我们可以共同努力，使`AGI`系统成为解决复杂问题、推动科学发现和提高全人类生活质量的强大工具。通往`AGI`的道路可能艰难，但只要有共同的愿景、坚定的决心和负责任的态度，我们就能释放其巨大潜力，为下一代创造更加光明的未来。

##### 论文解读——未来5年内AI将替代普通软件开发人员

这篇论文[`Measuring AI Ability to Complete Long Tasks`](https://arxiv.org/pdf/2503.14499)，主要提供了评估人工智能在处理需要较长时间任务能力的一种方法。尽管人工智能在基准测试上取得了快速进展，但基准测试的现实意义仍然不明确。为了从人类能力的角度量化人工智能系统的能力，作者提出了一种新的度量标准：`50%`**任务完成时间窗口**。这是指人类通常完成任务所需的时间，而人工智能模型能够以`50%`的成功率完成这些任务。首先测量了具有相关领域专业知识的人类在`RE-Bench、HCAST`以及`66`个新的较短任务组合上的完成时间。在这些任务上，当前人工智能模型如`Claude 3.7 Sonnet`的`50%`时间窗口大约为`50`分钟。此外，自`2019`年以来，前沿人工智能的时间窗口大约每七个月翻一番，尽管这一趋势可能在`2024`年加速。人工智能模型时间窗口的增加似乎主要是由于更高的可靠性和纠正错误的能力，结合更好的**逻辑推理**和**工具使用能力**。如果这些结果能够推广到现实世界的软件开发任务中，那么根据这一趋势的推测，**在五年内，人工智能系统将能够自动化许多目前需要人类一个月才能完成的软件开发任务**。
{% asset_img ml_2.png "智能体完成任务以50%的成功率，在过去六年中大约每七个月翻一番的速度，这一趋势也预示着在不到十年的时间里，我们将看到AI智能体能独立完成需要人类花费数天或数周才能完成的软件任务" %}

