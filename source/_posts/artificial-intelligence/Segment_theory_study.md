---
title: SAM模型—探析（PyTorch）
date: 2024-07-16 11:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

什么是图像分割？**图像分割**是将数字图像划分为多个区域（或段）的过程，使得属于同一区域（或段）的像素共享一些（语义）特征。应用领域：医学成像（定位肿瘤）；物体检测（行人检测、卫星图像中的物体检测）；基于内容的图像检索（查找所有包含猫/狗/披萨的图像）。面临的挑战：标记数据集困难且成本高昂（操作员需要创建像素完美的区域）；模型通常是特定于应用程序的（例如，仅针对特定类型的医疗应用进行训练，而不能应用于行人检测等其他领域）；以前的模型通常不可提示，也就是说，我们无法告诉模型只分割人、汽车或狗。
<!-- more -->

#### Vision Transformer(ViT) 

`Vision Transformer(ViT)`是一种将`Transformer`架构应用于计算机视觉任务的模型。我们将图像分割成固定大小的**块**，线性的嵌入每个块，添加位置嵌入，并将得到的向量序列发送到标准 `Transformer`编码器。为了执行分类，我们使用标准方法，即在序列中添加一个额外的可学习“分类标记”。
{% asset_img s_2.png %}

#### MAE模型

`Masked Autoencoder Vision Transformer(MAE)`是一种自监督学习方法,用于预训练`Vision Transformer`模型。`MAE`架构在预训练期间，会屏蔽掉一大批随机图像块子集（例如`75%`）。编码器应用于可见图像块的小子集。在编码器之后引入掩码`token`，全套编码图像块和掩码`token`由小型解码器处理，解码器会以像素为单位重建原始图像。预训练后，解码器被丢弃，编码器应用于未损坏的图像（全套图像块）以执行识别任务。特点：
- **非对称编码器-解码器架构**：编码器只处理未被遮蔽的图像`patches`,不包含`mask tokens`；解码器较轻量级,用于从潜在表示和`mask tokens`重建原始图像。
- **高比例遮蔽**：随机遮蔽输入图像的大部分区域(约`75%`的`patches`)；这创造了一个非平凡且有意义的自监督任务。
- **预训练目标**：模型需要重建被遮蔽的像素值；这迫使模型学习图像的语义和结构信息。
- **训练效率**：高比例遮蔽和非对称架构使大型模型的训练更加高效；加速训练(`3`倍或更多)并提高准确性。
- **迁移学习**：预训练后,丢弃解码器,仅保留编码器用于下游任务微调；在下游任务中表现优于有监督预训练,并展现出良好的可扩展性。

{% asset_img s_3.png %}

#### SAM模型

`Segment Anything Model(SAM)`是由`Meta AI`开发的一种先进的**图像分割模型**。 `SAM`是一个能够对任何图像中的任何对象进行分割的`AI`模型。它使用多种输入提示（如点击、框选或文本描述）来生成高质量的**对象掩码**。`SAM`由以下三个主要组件组成：
- **图像编码器**：用于提取图像特征。
- **提示编码器**：用于处理用户提供的提示信息。
- **掩码解码器**：用于生成最终的分割掩码。
{% asset_img s_1.png %}

**特点**：
- **通用性**：`SAM`能够分割几乎任何类型的对象，而不局限于预定义的类别；通过提示(`prompts`)机制，`SAM`可以根据用户提供的提示进行分割。
- **灵活性**：支持多种输入提示，如点、框和文本描述；适用于不同的图像分布和任务，具有零样本迁移能力(`zero-shot transfer`)。
- **高效性**：通过高效的模型设计和数据循环收集，`SAM`能够实时生成分割结果；使用了大型数据集进行训练，包括`11`亿个掩码和`1100`万张高分辨率图像。

**应用场景**：
- **图像编辑和处理**：如对象移除、背景替换等。
- **计算机视觉研究**：提供一个强大的基础模型，促进各种视觉任务的研究。
- **医学图像分析**：用于自动分割医学图像中的病变区域。
- **自动驾驶**：用于实时分割道路和障碍物。
- **增强现实**：实现实时对象分割和跟踪。

**优势**：
- **高精度**：在各种分割任务中表现优异，零样本性能甚至超过一些完全监督的模型。
- **快速**：能够实时生成分割结果，适用于实时应用。
- **易用性**：提供简单的`API`和开源代码，易于集成到各种应用中。

可提示分割任务，允许查找给定的掩码：点（例如用户通过鼠标单击选择的点）；框（用户定义的矩形）文本提示（例如“找到此图像中的所有狗”）上述内容的组合（例如框和背景点）。快速编码器-解码器模型（在网络浏览器中需要`50`毫秒才能根据提示生成掩码）；歧义感知：例如，给定一个点，它可能对应多个掩码，模型应该返回最有可能的三个（部分、子部分和整体）。`Segment Anything`引擎收集了`11`亿个分割掩码；所有掩码均已自动生成（无需人工监督）。总的来说，`Segment Anything Model(SAM)`是一个强大、灵活且高效的图像分割工具，能够适应广泛的应用场景，为计算机视觉领域带来了显著的进步。
```python
import torch
from segment_anything import SamPredictor, sam_model_registry

# 加载预训练模型
sam = sam_model_registry["vit_h"](checkpoint="${path}/to/checkpoint.pth")
predictor = SamPredictor(sam)

# 加载图像并进行分割
image = ...  # 读取图像
predictor.set_image(image)

# 使用提示进行分割
masks, _, _ = predictor.predict(point_coords=[[x, y]], point_labels=[1])
```
`SAM`：提示编码器。这里考虑两组提示：**稀疏提示**（点、框、文本）和**密集提示**（掩码）。我们用**位置编码**表示点和框，并与每个提示类型的学习嵌入和文本相加，使用`CLIP`的文本编码器。密集提示使用**卷积嵌入**，并与图像嵌入的每个元素相加。