---
title: 机器学习(ML)(十四) — 推荐系统探析
date: 2024-11-04 09:30:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 排序 - 多目标模型

我们先回顾一下**推荐系统**的链路，分为召回，粗排、精排、重排。有很多条召回通道，从几亿个物品选出几千个物品，做完召回之后，要从中选出用户最感兴趣的物品，这就要用到粗排和精排，粗排会给召回的物品逐一打分，保留分数最高的几百个物品，然后使用精排模型给粗排选中的几百个物品打分但不做截断，让几百个物品全都带着精排分数进入重排，最后一步是重排，做多样性抽样，并且把相似内容打散，最终由几十个物品被选中展示给用户。
<!-- more -->

**排序**的主要依据是用户对物品的兴趣，兴趣可以反映在用户与物品的交互上，对于每个物品，系统会记录下面几个统计量：**曝光次数**(`number of impressions`)，就是一个物品被展示给多少用户，展示之后，才会有点击等行为；**点击次数**(`number of clicks`)，就是一个物品被多少用户点开；**点赞次数**(`number of likes`)、**收藏次数**(`number of collects`)、**转发次数**(`number of shares`)。点击率 = 点击次数 / 曝光次数、点赞率 = 点赞次数 / 点击次数、收藏率 = 收藏次数 / 点击次数、转发率 = 转发次数 / 点击次数。排序的依据：排序模型通过机器学习预估点击率、点赞量、收藏率、转发率等，做完预估之后，要对这些预估分数做融合（比如加权和），最后按照融合的分数对物品做排序、截断，保留分数最高的物品。

**多目标模型**：排序模型的输入是各种各样的特征，**用户特征**主要是用户`ID`和用户画像；**物品特征**主要是物品`ID`、物品画像和作者信息；**统计特征**：包括**用户统计特征**和**物品统计特征**，比如用户在过去`30`天曝光了多少物品，点击了多少物品，点赞了多少物品，比如物品在过去获得了多少次曝光、点击、点赞等等；**场景特征**：是随着用户请求传过来的，不如按当前的时间和用户所在的地点。这些信息对推荐很有用，把这些特征做融合(`concatenation`)，输入神经网咯，神经网络可以是简单的全连接网络，也可以是更复杂的结构，神经网络会输出一个向量，这个向量在输入`4`个神经网络，每个神经网络有`2-3`个全连接层和输出层的**激活函数**是`Sigmoid`，`4`个神经网络分别输出点击率、点赞率、收藏率和转发率的预估值。`4`个预估值都是实数，介于`0~1`之间，**推荐系统**排序就主要靠这4个预估值，它们反映出用户对物品的兴趣。如下图所示：
{% asset_img ml_1.png "多目标模型" %}

把模型输出的点击率、点赞率、收藏率和转发率，分别记作{% mathjax %}p_1,p_2,p_3,p_4{% endmathjax %}。它们都是模型做出的预估，做训练的时候，要让这些预估值去拟合真实的目标。把真实的目标记作{% mathjax %}y_1,y_2,y_3,y_4{% endmathjax %}，分别对应：点击、点赞、收藏和转发的行为。{% mathjax %}y{% endmathjax %}要么是0、要么是1，举个例子，{% mathjax %}y_1 = 1,y_2 = 0,y_3 = 0,y_4 = 1{% endmathjax %}，表示用户对物品有点击、没点赞、没收藏和有转发。这些是用户的真实的行为，被记录下来。用这样的数据来训练模型，训练是要鼓励模型的预测接近目标，其实就是二元分类，就是判定用户是否会点击物品，有点击、点赞、收藏和转发4个任务，每个任务都是一个二元分类，可以使用**交叉熵损失函数**。对于点击这个任务使用{% mathjax %}y_1{% endmathjax %}和{% mathjax %}p_1{% endmathjax %}的**交叉熵作为损失函数**，记作{% mathjax %}\text{CrossEntropy}(y_1,p_1) = -(y_1\ln p_1 + (1 - y_1)\ln (1 - p_1)){% endmathjax %}，{% mathjax %}p_1{% endmathjax %}越接近{% mathjax %}y_1{% endmathjax %}，那么交叉熵损失就越小。我们把4个函数的加权和作为总的损失函数，记作{% mathjax %}\sum_{i=1}^4 \alpha_i \cdot \text{CrossEntropy}(y_i,p_i){% endmathjax %}，其中权重{% mathjax %}\alpha{% endmathjax %}是根据经验设置的。在收集的历史数据上训练神经网络的参数，最小化损失函数，损失函数越小，说明模型的预测越接近真实目标。做训练的时候，把**损失函数**关于**神经网络**的参数求**梯度**，做**梯度下降**更新**神经网络**的参数。

实际的训练中会有很多困难：做训练的时候存在类别不平衡的问题，**正样本少，负样本多**。比如说每`100`次曝光，约有`10`次点击，`90`次无点击。点击的是正样本，没有点击的负样本。要太多的负样本用途不大，白白浪费计算资源。解决方案就是负样本的降采样(`down-sampling`)，只保留其中一小部分负样本，让正负样本数量保持平衡，降低训练的计算代价。给定用户特征、物品特征，用神经网络预估出点击率、点赞率等之后，就要对这些预估分数做校准，做完校准之后才能把这些预估值做排序。为什么要做校准？首先设正样本、负样本的数量为{% mathjax %}n_{+}{% endmathjax %}和{% mathjax %}n_{-}{% endmathjax %}，以点击为例，曝光之后有点击就是正样本，否则就是负样本，负样本数量通常远大于正样本，在训练的时候要对负样本做降采样，抛弃一部分负样本，这样会使正、负样本的差距不太悬殊，把采样率记作{% mathjax %}\alpha{% endmathjax %}，它介于0~1之间，使用负样本的数量等于{% mathjax %}\alpha\cdot n_{-}{% endmathjax %}，由于减少了负样本的数量，模型预估点击率大于真实点击率，{% mathjax %}\alpha{% endmathjax %}越小，负样本越少，模型对点击率高估就会越严重，把真实的点击率记作{% mathjax %}p_{\text{true}} = \frac{n_{+}}{n_{+} + n_{-}}{% endmathjax %}，样本的总数等于{% mathjax %}n_{+} + n_{-}{% endmathjax %}，预估的点击率记作{% mathjax %}p_{\text{pred}} = \frac{n_{+}}{n_{+} + \alpha\cdot n_{-}}{% endmathjax %}，把上面两个公式合起来，记作{% mathjax %}p_{\text{true}} = \frac{\alpha\cdot p_{\text{pred}}}{(1 - p_{\text{pred}}) + \alpha\cdot p_{\text{pred}}}{% endmathjax %}，这个公式就是预估点击率的校准。等式左边的{% mathjax %}p_{\text{pred}}{% endmathjax %}是校准之后的点击率，等式右边是对预估点击率{% mathjax %}p_{\text{pred}}{% endmathjax %}做的变换，公式中用到了采样率{% mathjax %}alpha{% endmathjax %}，在线上排序的时候，首先让模型预估{% mathjax %}p_{\text{pred}}{% endmathjax %}，然后使用{% mathjax %}p_{\text{true}} = \frac{\alpha\cdot p_{\text{pred}}}{(1 - p_{\text{pred}}) + \alpha\cdot p_{\text{pred}}}{% endmathjax %}做校准，最后拿校准之后的点击率作为排序的依据。

#### 召回 - MMoE

`MMoE`(`Multi-gate Mixture-of-Exports`)：模型的输入是一个向量，包含**用户特征**、**物品特征**、**统计特征**、**场景特征**。把向量输入`3`个**神经网络**，每个神经网络结构都相同，都是由很多全连接层组成，但是这`3`个神经网络不共享参数，这`3`个神经网络各输出一个向量，这`3`个向量记作{% mathjax %}x_1,x_2,x_3{% endmathjax %}，这`3`个神经网络代表`3`个”专家“。把下面的特征向量输入到另一个神经网络，这个神经网络也有多个全连层，神经网络的最后加入一个`softmax`激活函数，输出一个`3`维的向量。由于是`softmax`输出，向量的`3`个元素都大于`0`，而且相加等于`1`，向量的`3`个元素记作{% mathjax %}p_1,p_2,p_3{% endmathjax %}，分别对应`3`个专家神经网络，之后用这`3`个元素做权重，对向量{% mathjax %}x_1,x_2,x_3{% endmathjax %}做**加权平均**；同样的方法，把下面的特征向量送入右边的神经网络，在神经网络的最后也是`softmax`激活函数，输出一个`3`个向量，分别记作{% mathjax %}q_1,q_2,q_3{% endmathjax %}，这三个元素也是之后做加权平均的权重。如下图所示：
{% asset_img ml_2.png "MMoE模型" %}

如下图所示，{% mathjax %}p_1,p_2,p_3{% endmathjax %}和{% mathjax %}q_1,q_2,q_3{% endmathjax %}都是权重，用于之后的加权平均，对向量{% mathjax %}x_1,x_2,x_3{% endmathjax %}做加权平均，权重是{% mathjax %}p_1,p_2,p_3{% endmathjax %}，得到左边的向量{% mathjax %}p_1x_1 +p_2x_2 + p_3x_3{% endmathjax %}；用右边的向量{% mathjax %}q_1,q_2,q_3{% endmathjax %}对向量{% mathjax %}x_1,x_2,x_3{% endmathjax %}做加权平均，得到右边的向量{% mathjax %}q_1x_1 +q_2x_2 + q_3x_3{% endmathjax %}。把左边的向量输入一个神经网络，神经网络可以有一个或多个全连接层，神经网络的输出取决于具体的任务，比如神经网络输出是对点击率的预估，是一个介于`0~1`之间的实数，把右边的一个向量输入另一个神经网络，这个神经网络会输出另一个指标的预估，比如对点赞率的预估，也是介于`0~1`之间的实数。这里假设**多目标模型**只有点击率、点赞率这两个目标，所以用了{% mathjax %}p,q{% endmathjax %}这两种权重，假如有`10`个目标，就要用`10`种权重。
{% asset_img ml_3.png "MMoE模型" %}

在实践中`MMoE`模型有一个问题：`softmax`会发生**极化现象**(`polarization`)。上个例子中有`2`个`softmax`函数，各自输出一个3维向量，每个向量都是概率分布，元素大于`0`，相加等于`1`。**极化现象**(`polarization`)是`softmax`输出值`1`个接近`1`，其余接近`0`。举例左边的`softmax`输出值为{% mathjax %}(0,0,1){% endmathjax %}，也就是说左边神经网络点击预估任务只使用了第`3`号专家网络，而没有使用其它的专家网络，这也就是没有使用`MMoE`，没有让`3`个专家网络输出融合，而是简单使用了一个专家；右边的`softmax`输出值为{% mathjax %}(0,1,0){% endmathjax %}，也就是说左边神经网络点击预估任务只使用了第`2`号专家网络，也没有对`3`个专家做融合，而第1号专家网络没有被使用。那么`MMoE`就是一个简单的模型，不会对专家做融合，则失去了`MMoE`的优势。解决**极化现象**的方案：如果有{% mathjax %}n{% endmathjax %}个专家神经网络，那么每个`softmax`的输出都是{% mathjax %}n{% endmathjax %}维向量。不希望输出值中其中一个接近`1`，其余的接近`0`，在训练的过程中，对`softmax`输出使用`dropout`。`softmax`输出的{% mathjax %}n{% endmathjax %}个数值被`mask`的概率都是`10%`，也就是说每个专家被丢弃的概率都是`10%`，这会强迫每个任务根据部分专家做预测，如果用`dropout`，则不太可能发生极化。`MMoE`模型请参考论文：[`Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts`](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)，极化现象的解决方案请参考：[`Recommending What Video to Watch Next: A Multitask Ranking System`](https://daiwk.github.io/assets/youtube-multitask.pdf)。

**融合预估分数**：最简单的分数融合方法就是**预估值的加权和**，记作{% mathjax %}p_{\text{click}} + w_1\cdot p_{\text{like}} + w_2\cdot p_{\text{collect}} + \ldots{% endmathjax %}，其中{% mathjax %}p_{\text{click}}{% endmathjax %}就是模型预估的点击率，对应的权重是`1`；{% mathjax %}p_{\text{like}}{% endmathjax %}是模型预估的点赞率，对应的权重的{% mathjax %}w_1{% endmathjax %}；{% mathjax %}p_{\text{collect}}{% endmathjax %}是模型预估的收藏率，对应的权重是{% mathjax %}w_2{% endmathjax %}。另一种融合方式：点击率乘以其它项的加权和，{% mathjax %}p_{\text{click}}(1 + w_1\cdot p_{\text{like}} + w_2\cdot p_{\text{collect}} + \ldots){% endmathjax %}。另一种融合方式：{% mathjax %}(1 + w_1\cdot p_{\text{time}})^{\alpha_1}\cdot (1 + w_2\cdot p_{\text{like}})^{\alpha_2}\;\ldots{% endmathjax %}，其中{% mathjax %}p_{\text{time}}{% endmathjax %}代表预估短视频的观看时长，比如预测用户会观看`10`秒，{% mathjax %}w_1, \alpha_1{% endmathjax %}是超参数，需要自己调整，{% mathjax %}p_{\text{like}}{% endmathjax %}代表预估用户的点赞率。

**视频播放建模**：视频排序的依据还有**播放时长**和**完播**。直接使用回归拟合播放时长效果不好，建议使用`YouTube`的时长建模（论文请参考：[`Deep Neural Networks for YouTube Recommendations`](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf)）。
- **对播放时长的预估**：这是排序模型使用的特征：**用户特征**、**物品特征**、**统计特征**、**场景特征**。把它输入多层神经网络，这个神经网络被所有任务共享，在这个神经网络之上有多个全连接层，一个全连接层对应`1`个目标，比如点击、点赞、收藏、播放时长等，这里只关注播放时长的预估，全连接层输出的实数，记作{% mathjax %}z{% endmathjax %}，对{% mathjax %}z{% endmathjax %}做`sigmoid`变换得到{% mathjax %}p = \text{sigmoid}(z){% endmathjax %}，记作{% mathjax %}p = \frac{\exp(z)}{1 + \exp(z)}{% endmathjax %}，然后让{% mathjax %}p{% endmathjax %}拟合{% mathjax %}y{% endmathjax %}（{% mathjax %}y{% endmathjax %}是自己定义的，记作{% mathjax %}y = \frac{t}{t + 1}{% endmathjax %}，其中{% mathjax %}t{% endmathjax %}是实际观看时长，{% mathjax %}t{% endmathjax %}越大，则{% mathjax %}y{% endmathjax %}也越大），为了让{% mathjax %}p{% endmathjax %}拟合{% mathjax %}y{% endmathjax %}，我们使用{% mathjax %}p{% endmathjax %}与{% mathjax %}y{% endmathjax %}的交叉熵作为损失函数，记作{% mathjax %}\text{CrossEnptoryLoss}(y,p) = y\cdot\log p + (1 - y)\cdot\log(1 - p){% endmathjax %}。最小化交叉熵损失，记作{% mathjax %}\frac{t}{1 + t}\cdot\log p + \frac{1}{1+t}\cdot\log(1 - p){% endmathjax %}，会让{% mathjax %}p{% endmathjax %}接近于{% mathjax %}y{% endmathjax %}，如果{% mathjax %}p = y{% endmathjax %}，那么{% mathjax %}\exp(z) = t{% endmathjax %}，也就是说，可以用{% mathjax %}\exp(z){% endmathjax %}作为播放时长的预估，对{% mathjax %}z{% endmathjax %}做指数变换，输出的{% mathjax %}\exp(z){% endmathjax %}就是对播放时长的预估。
- **对完播的预估**：一种方法是**回归**，例如视频长度为`10`分钟，实际播放是`4`分钟，则实际播放率是{% mathjax %}y = 0.4{% endmathjax %}。做训练的时候，让预估播放率{% mathjax %}p{% endmathjax %}去拟合{% mathjax %}y{% endmathjax %}，记作{% mathjax %}\text{loss} = y\cdot\log p + (1 - y)\cdot\log(1 - p){% endmathjax %}，其中{% mathjax %}y{% endmathjax %}的大小介于0~1之间，用{% mathjax %}p{% endmathjax %}和{% mathjax %}y{% endmathjax %}的交叉熵作为损失函数，在线上用{% mathjax %}p{% endmathjax %}作为**完播率**的预估，比如说模型输出{% mathjax %}p = 0.73{% endmathjax %}，意思是预计播放73%。视频的完播率会作为融分公式的一项，影响视频的排序。另一种视频完播的建模方法是**二元分类**，需要自己定义完播指标，比如完播`80%`，例如视频的长度是`10`分钟，那么播放超过8分钟就是正样本，否则作为负样本。训练要做**二元分类**，播放 `>=` `80%`属于正样本、播放 `<` `80%`属于负样本。做完训练之后，可以在线上预估完播率，例如模型输出{% mathjax %}p = 0.73{% endmathjax %}，意思是{% mathjax %}\mathsf{P}(\text{播放} > 80%) = 0.73{% endmathjax %}，预估的播放率会跟点击率一起作为排序的依据。
