---
title: 高质量人类数据—思考（深度学习）
date: 2024-06-21 18:50:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

高质量数据是深度学习模型训练的燃料。大多数特定任务的标记数据来自人工标注，例如分类任务或用于`LLM`对齐训练的`RLHF labeling`(基于人类反馈的强化学习标注)（可以构建为分类格式）。文章中的许多`ML`技术可以帮助提高数据质量，但从根本上讲，人工数据收集需要关注细节和谨慎行事。
<!-- more -->

{% asset_img qd_1.png "实现高质量数据的两个方向" %}

#### 人工评估员<->数据质量

收集人类数据涉及一系列操作步骤，每个步骤都会影响数据质量：
- 任务设计：设计任务工作流程以提高清晰度并降低复杂性。详细的指南很有帮助，但非常冗长和复杂的指南需要大量培训才能使用。
- 选择并培训一批评分员：选择具有匹配技能和一致性的注释者。培训课程是必要的。入职后，还需要定期反馈和校准。
- 收集和汇总数据。在这个阶段，可以应用更多 ML 技术来清理、过滤和智能聚合数据以识别真实标签。

{% asset_img qd_2.png "质量保证是指一系列行动，通过对质量模型中确定的质量属性采取行动，从而提高质量" %}

##### 群体的智慧

Vox populi（原意为`“Vox populi, vox Dei”`）是一个拉丁语短语，意为人民的声音。`1907`年，《自然》杂志发表了一篇同名短文。该文追踪了一年一度的展览会上的一项活动，即选出一头肥牛，人们猜测这头牛的重量，如果猜测接近真实数字，即可赢得奖品。最中间的估计值被视为`“vox populi”`，结果非常接近真实值。作者总结道：“我认为，这个结果比人们预期的更能证明民主判断的可信度。”这可能是最早提到众包（“群体的智慧”）如何发挥作用的文献。

近`100`年后，[`Callison-Burch(2009)`](https://aclanthology.org/D09-1030/)进行了一项早期研究，使用`Amazon Mechanical Turk(AMT)`对机器翻译(`MT`)任务进行非专家人工评估，甚至依靠非专家来创建新的黄金参考翻译。人工评估的设置很简单：向每个`turker`展示一个源句子、一个参考翻译和来自`5`个`MT`系统的`5`个翻译。他们被要求从最好到最差对`5`个翻译进行排名。每个任务由`5`个`turker`完成。毫无疑问，有些垃圾评论者会为了优化数量而制作低质量的标注。因此，在衡量专家和非专家之间的一致性时，需要应用不同的加权方案来降低垃圾评论者的贡献：(1)“专家加权”：使用与专家在`10`个示例的黄金数据集上的一致性率；(2)“非专家加权”：依靠与整个数据集上其余`turkers`的一致性率。在一项更艰巨的任务中，非专家级的人工标注者被要求创建新的黄金参考翻译。`Callison-Burch`将这项任务设计为两个阶段，第一阶段参考机器翻译输出创建新的翻译，第二阶段过滤看似由机器翻译系统生成的翻译。专家翻译和众包翻译之间的相关性高于专家翻译和机器翻译系统输出之间的相关性。
{% asset_img qd_3.png "（左）通过比较每对翻译句子（“A > B”、“A=B”、“A < B”）来衡量一致率，因此偶然一致性为1/3。上限由专家之间的一致率设定。（右）不同来源的翻译之间的BLEU分数比较。LCD（语言数据联盟）翻译人员提供专家翻译" %}

##### 评分一致性

我们通常认为标注针对的是单一的基本事实，并尝试根据具有一致标准的黄金答案来评估质量。寻找可靠的基本事实标签的常见做法是从多个评分者那里收集多个标签。假设每个评分者的质量水平不同，我们可以使用标注的加权平均值，但要用熟练度分数加权。这个分数通常由一个评分者同意其他评分者的频率来近似。
- 多数投票：多数投票是最简单的聚合方式。
- 原始一致性：原始一致性计算其他人同意的百分比。这与多数投票间接相关，因为所有成员都有望获得更高的标注者间一致性率。
- Cohen的Kappa值([Landis & Koch，1977](https://www.jstor.org/stable/2529310))：`Cohen`的`Kappa`值以以下形式衡量评分者之间的一致性{% mathjax %}\kappa = (p_o - p_e) / (1 - p_c){% endmathjax %}，在这里{% mathjax %}p_o{% endmathjax %}是原始一致率，{% mathjax %}p_e{% endmathjax %}是偶然的一致性。`Cohen`的`kappa`有一个用于偶然一致的校正项，但如果一个标签更普遍，则此校正可能会被高估。
- 概率图建模：有一系列工作依赖于概率图建模来对标注决策中的不同因素进行建模，例如任务难度、任务潜在主题、评估者偏见、评估者信心，然后据此预测真实标签。比较了众包中`17`种真值推断算法，其中大多数是概率图模型。
##### 评分者分歧和两种范式

上述聚合过程依赖于一个假设，即存在一个潜在的标准答案，因此我们可以据此评估注释者的表现。然而，在许多主题中，特别是在安全、社会或文化领域，人们可能会意见不一，而且这种分歧往往是合理的，然后就归结为我们在多大程度上想要应用严格的规则而不是拥抱多样性。[Aroyo & Welty,2015](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564)讨论了人工注释收集实践中的一系列“误区”，发现它们都有些不准确，主要发现包括：
- 一些样本通常有不止一种正确的解释。我们需要多种视角，例如让多个人来审查注释质量。
- 分歧并不总是坏事。我们应该减少由错误或设计不良的流程引起的分歧，但其他分歧也可以为我们提供丰富的信息。如果是因为任务定义不明确，就应该加强指导，但更详细的指导并不能解决固有的意见分歧。
- 专家不一定总是比外行人优秀，但是在考虑什么是重要的方面，他们会有很大的差距。
- 基本事实标注会随着时间而改变，尤其是与及时事件或新闻相关的标注。

后来，[`Rottger`等人,`2021`](https://arxiv.org/abs/2112.07475)将这种差异概括为`NLP`任务数据标注的两个对比范式。
**描述性**：
- 定义：鼓励标注者的主观性，尝试模拟多种信念。
- 优点：有助于识别哪些条目更加主观；拥抱多样性。
- 缺点：评估者分歧等指标不能用于衡量数据质量或注释者表现；不能用于针对输出一种预设行为进行优化的训练模型。

**规定性**：
- 定义：不鼓励标注者的主观性，尝试始终如一地坚持一种信念。
- 优点：更加符合标准`NLP`设置。通过测量分歧或进行标签聚合更容易进行`QC`。
- 缺点：在实践中，创建高质量的标注指南成本高昂且具有挑战性，而且永远不可能完美；培训标注者熟悉指南以便正确应用它也具有挑战性；无法捕捉可解释的多样性或始终如一地编码一种特定的概念。

**描述范式**使我们能够理解许多重要的影响，并解释不同的观点。例如，标注者身份（例如非裔美国人、`LGBTQ`）被发现是他们将与身份相关的内容标记为有毒的统计学上显着的因素([`Goyal`等人，`2022`](https://arxiv.org/abs/2205.00501))。**主题**可能是导致意见分歧的另一个主要驱动因素。([Wang,2023](https://research.google/pubs/all-that-agrees-is-not-gold-evaluating-ground-truth-labels-and-dialogue-content-for-safety/))研究了人工智能对话系统安全性的人工评估过程，并比较了信任与安全(`T&S`) 专业人员和众包标注者的标签结果。他们有意收集与人群标注者相关的丰富元数据，例如人口统计或行为信息。通过比较`T&S`专家标签和人群标注，他们发现一致率因语义主题和严重程度而异：
- 不同主题的同意率差别很大；从暴力/血腥主题的`0.96`到个人主题的`0.25`不等。
- 鉴于标记“良性”，“有争议”，“中等”到“极端”的四个标签选项，“极端”和“良性”对话的同意率更高。

{% asset_img qd_4.png "非专家和专家标注之间的相关性在不同主题之间差异很大" %}

[`Zhang`等人,`2023`](https://arxiv.org/abs/2311.04345)提出了一种评估者分歧分类法来分析其根本原因。在列出的原因中，应避免因随机误差或个人层面的不一致而导致的分歧。当评估者在多次被问到同一个任务时给出不同的标签时，其中一些很可能是由人为错误造成的。基于这种直觉，分歧反卷积方法([`Gordon`等人,`2021`](https://dl.acm.org/doi/abs/10.1145/3411764.3445423))通过将每个人的意见锚定到他们自己的主要标签上，将稳定的意见与错误区分开来，从而鼓励评估者内部的一致性。
{% asset_img qd_5.png "评估者意见分歧原因分类" %}

分歧反卷积依赖于概率图建模：
- 估计注释者返回非主要标签的频率，{% mathjax %}p_{\text{flip}}{% endmathjax %}。
- 每个样本获取调整后的标签分布{% mathjax %}p^*{% endmathjax %}主要标签基于{% mathjax %}p_{\text{flip}}{% endmathjax %}。
- 样品来自{% mathjax %}p^*{% endmathjax %}作为新的测试集。
- 根据新的测试集测量性能指标。

鉴于{% mathjax %}C{% endmathjax %}-类别分类，生成模型的采样过程表述如下：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{aligned}
y^*\mid x &\sim \text{Categorial}([C], p^*(y\mid x)) \\
y_\text{other}\mid y^* &\sim \text{Categorial}([C]\setminus\{y^*\}, \frac{1}{C-1}) \\
z_\text{flip} \mid x &\sim \text{Bernoulli}(p_\text{flip}(x)) \\
y\mid y^*, y_\text{other}, z_\text{flip} &= y^* (1 - z_\text{flip}) + y_\text{other} z_\text{flip}
\end{aligned}
{% endmathjax %}
鉴于真实情况{% mathjax %}p(y|x){% endmathjax %}和{% mathjax %}p_{\text{flip}}{% endmathjax %}根据数据估计，我们将更新主要标签的标签分布：
{% mathjax '{"conversion":{"em":14}}' %}
p^*(y\mid x) = \frac{p(y\mid x) - \frac{p_\text{flip}(x)}{C-1}}{1 - \frac{C \cdot p_\text{flip}(x)}{C - 1}}
{% endmathjax %}
新的测试集取自{% mathjax %}p^* (y|x){% endmathjax %}表示已消除个体不一致噪声的主标签。它可用作无噪声测试集进行评估。为了捕捉注释者在学习预测标签时的系统性分歧，[`Davani`等人,`2021`](https://arxiv.org/abs/2110.05719)尝试了一个多注释者模型，其中预测每个注释者的标签被视为一个子任务。假设分类任务是在带标注的数据集上定义的{% mathjax %}D=(X,A,Y){% endmathjax %}，在这里{% mathjax %}X{% endmathjax %}是文本实例，{% mathjax %}A{% endmathjax %}是标注者的集合，{% mathjax %}Y{% endmathjax %}是标注矩阵，{% mathjax %}y_{ij}\in Y{% endmathjax %}表示分配的二进制标签{% mathjax %}a_j\in A{% endmathjax %}样品{% mathjax %}x_i\in X{% endmathjax %}。多数人投票赞成{% mathjax %}x_i{% endmathjax %}表示为{% mathjax %}\bar{y}_i{% endmathjax %}。实验是在预先训练的`BERT`模型上训练分类头，并比较`4`种设置：
- 基线：直接预测多数票{% mathjax %}\bar{y}_i{% endmathjax %}，不使用完整标注矩阵{% mathjax %}Y{% endmathjax %}。
- 集成：每个标注器分别训练一个模型来预测{% mathjax %}y_{ij}{% endmathjax %}然后根据多数票汇总结果。
- 多标签：学习预测{% mathjax %}|A|{% endmathjax %}标签表示每个样本所有标注者的标签{% mathjax %}\langle y_{i1}, \dots, y_{i\vert A \vert} \rangle{% endmathjax %}，具有共享的`MLP`层，然后聚合输出。
- 多任务：类似于多标签，但每个标注器的预测头都是从分离的`MLP`层学习的，这样我们分配额外的计算来学习标注器之间的差异。

在`GHC`([Gab Hate Corpus](https://osf.io/edua3/))数据集上的实验结果表明，该多任务模型取得了最佳`F1`分数，并且能够自然地提供与标注分歧相关的预测不确定性估计。
{% asset_img qd_6.png "用于对多个标注者标签进行建模的不同架构的说明" %}

**陪审团学习**（[Gordon等人，2022年](https://arxiv.org/abs/2202.02950)）通过根据不同标注者的特征对其标记行为进行建模来模拟陪审团过程。从包含每个标注者的标签和人口统计特征的数据集开始，我们训练一个模型来学习预测每个标注者（每个标注者都是潜在的陪审员）所标记的标签。在决策时，从业者可以指定一组陪审员的组成，以确定抽样策略。最终决定是通过汇总来自多个审判的陪审员的标签来做出的。
{% asset_img qd_7.png "陪审团学习的工作原理说明" %}

陪审团学习模型是一个`DCN`([深度和交叉网络](https://arxiv.org/abs/2008.13535))，通常用于推荐用例，它经过联合训练以学习评论嵌入、标注者嵌入和组（标注者的特征）嵌入。文本内容由预先训练的`BERT`处理，它也经过联合微调，但时间较短，以避免过度拟合。
{% asset_img qd_8.png "用于陪审团学习的DCN模型架构" %}

他们的实验在毒性多样性数据集上运行，并将陪审团学习与基线模型进行比较，基线模型是经过微调的`BERT`，用于在不使用元数据的情况下预测单个注释者的标签。性能以`MAE`（平均绝对误差）来衡量。陪审团学习在整个测试集以及每个组片段上的表现始终优于标注者者的基线。
{% asset_img qd_9.png "将标注者不可知基线与陪审团学习进行比较的实验结果" %}

#### 数据质量<->模型训练

一旦构建了数据集，许多方法都可以根据训练动态帮助识别错误标签。请注意，我们只关注查找和排除可能带有错误标签的数据集的方法，而不是如何使用嘈杂数据训练模型。
##### Influence Functions (影响函数)

影响函数是稳健统计([`Hampel，1974`](https://www.jstor.org/stable/2285666))中的一种经典技术，它通过描述当我们将训练点的权重增加无穷小量时模型参数如何变化来衡量训练数据点的效果。[`Koh & Liang,2017`](https://arxiv.org/abs/1703.04730)引入了该概念并将其应用于深度神经网络。
