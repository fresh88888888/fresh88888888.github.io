---
title: 检索增强生成（RAG）：嵌入向量 & Sentence BERT & HNSW
date: 2024-07-08 17:30:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

语言模型是一种概率模型，它为单词序列分配概率。实际上，语言模型允许我们计算以下内容：我们通常训练一个神经网络来预测这些概率。在大量文本上训练的神经网络被称为大型语言模型(`LLM`)。
{% asset_img r_1.png %}
<!-- more -->

我们如何训练和推理语言模型？
- **训练**：语言模型是在文本语料库（即大量文档）上进行训练的。通常，语言模型是在整个维基百科和数百万个网页上进行训练的。这使语言模型能够获取尽可能多的知识。我们通常训练基于`Transformer`的神经网络作为语言模型。
- **推理**：为了语言模型实现推理，我们构建一个**提示词**，并让语言模型通过迭代添加`token`来生成剩余的部分。

语言模型只能输出经过训练的文本和信息。这意味着，如果我们只用英语内容训练语言模型，它可能无法输出中文。为了教授新的概念，我们需要对模型进行微调。它可能代价高昂。模型的参数数量可能不足以捕获我们想要教给它的所有知识。这就是为什么引入了`7B、13B`和`70B`个参数的`LLaMA`。微调不是附加的。它可能用新知识取代模型的现有知识。例如，一个用英语训练的语言模型，如果用中文进行（大量）微调，可能会“忘记”英语。

#### 检索增强生成（RAG）

`RAG`(`Retrieval-Augmented Generation`)是一种结合了信息检索系统和生成式大语言模型(`LLM`)优势的`AI`框架。`RAG`通过在生成过程中引入外部知识源来增强`LLM`的输出。它首先检索与用户查询相关的信息,然后将这些信息与原始查询一起输入`LLM`,从而生成更准确、更相关的回答。主要步骤:
- **创建外部数据**:将各种来源的数据转换为向量表示,存储在向量数据库中。
- **检索相关信息**:将用户查询转换为向量,在向量数据库中搜索相关信息。
- **增强LLM提示**:将检索到的相关信息与用户输入结合,形成增强的提示。
- **生成回答**:`LLM`基于增强的提示生成最终回答。

`RAG`的优势：
- **提高准确性**:通过引入外部最新信息,减少过时或不准确的回答。
- **增强上下文理解**:提供相关背景信息,使回答更加连贯和针对性强。
- **提高可信度**:可以提供信息来源,增加用户信任。
- **灵活性**:可以轻松更新外部知识库,无需重新训练整个模型。

`RAG`通过结合外部知识和`LLM`的生成能力,提供了一种更加灵活、准确和可信的`AI`解决方案,适用于各种需要最新、专业信息的应用场景。
{% asset_img r_2.png %}

#### 嵌入向量

为什么要用向量来表示单词？给定单词`“cherry”, “digital”`和`“information”`，如果我们仅使用`2`个维度（`X`，`Y`）表示**嵌入向量**并绘制它们，我们希望看到类似这样的结果：具有相似含义的单词之间的角度很小，而具有不同含义的单词之间的角度很大。因此，**嵌入**将单词投影到高维空间来“捕获”它们所表示的单词的含义。
{% asset_img r_3.png %}

我们通常使用**余弦相似度**，它基于两个向量之间的点积。同义词往往出现在相同的上下文中（被相同的单词包围）。例如，`“teacher”`和`“professor”`通常出现在`“school”、“university”、“exam”、“lecture”、“course”`等单词的包围中。反之亦然：出现在相同上下文中的单词往往具有相似的含义。这被称为**分布假设**。这意味着要捕捉单词的含义，我们还需要访问其上下文（围绕它的单词）。这就是我们在`Transformer`模型中采用自注意力机制来捕获每个`token`的上下文信息的原因。自注意力机制将每个`token`与句子中的所有其他`token`相关联。