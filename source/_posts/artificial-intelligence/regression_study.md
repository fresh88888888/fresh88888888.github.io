---
title: 线性回归（线性神经网络）(PyTorch)
date: 2024-04-25 16:29:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 线性回归

**回归**(`regression`)是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。在机器学习领域中的大多数任务通常都与预测(`prediction`)有关。当我们向预测一个数值时，就会涉及到回归问题。常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、预测需求（零售销量等）。但不是所有的预测都是回归问题。分类问题的目标是预测数据属于一组类别中的哪一个。
<!-- more -->
##### 线性回归的基本元素

线性回归(`linear regression`)可以追溯到19世纪初，它在回归的各种工具中最简单且最流行。线性回归基于几个简单的假设：首先，自变量{% mathjax %}x{% endmathjax %}与因变量{% mathjax %}y{% endmathjax %}之间的关系是线性的，即{% mathjax %}y{% endmathjax %}可以表示为{% mathjax %}x{% endmathjax %}中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

为了解释线性回归，我们举一个实际的例子：我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋的价格（美元）。为了开发一个能够预测房价的模型，我们需要收集一些真实的数据集。这个数据集包括了房屋的销售价格、面积和房龄。在机器学习术语中，该数据集称为训练数据集和训练集。每行数据（比如一次房屋交易相对应的数据）称为养样本，也可以称为数据点或数据样本，我们把试图预测的目标（比如预测房屋价格）称为标签(`label`)或目标(`target`)。预测所依据的自变量（面积和房龄）称为特征(`feature`)或协变量(`covariate`)。通常，我们用{% mathjax %}n{% endmathjax %}来表示数据集中的样本数。对索引为{% mathjax %}i{% endmathjax %}的样本其输入表示为{% mathjax %}x^{(i)}=[x_1^{(i)},x_2^{(i)}]^{\mathsf{T}}{% endmathjax %}。
##### 线性模型

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：
{% mathjax '{"conversion":{"em":14}}' %}
price=w_{area}\cdot area + w_{age}\cdot age + b
{% endmathjax %}

{% mathjax %}w_{area}{% endmathjax %}和{% mathjax %}w_{age}{% endmathjax %}称为权重`(weight)`，权重决定了每个特征对我们预测值的影响，{% mathjax %}b{% endmathjax %}称为偏置`(bias)`、偏移量`(offset)`或截距`(intercept)`。偏置是指当所有特征取值为`0`时，预测值应该为多少。既使现实生活中不会有房屋的面积为`0`或房龄正好为`0`年，我们仍然需要偏置项。如果没有偏置项，我们模型的表达能力将会受到限制。严格来说，它是输入特征的一个**仿射变换**`(affine transformation)`。仿射变换的特点是通过加权和对特征进行线性变换`(linear transformation)`，并通过偏置项来进行平移`(translation)`。