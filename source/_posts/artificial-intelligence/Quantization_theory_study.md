---
title: 量化(Quantization)（深度学习）
date: 2024-07-03 18:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

**量化**(`Quantization`)是一种用于减少深度学习模型计算和存储成本的技术，**量化**是将高精度数据(通常是`32`位浮点数)转换为低精度数据类型(如`8`位整数)的过程。目标是减小模型大小、降低内存带宽需求、加快推理速度、减少能耗。量化方案：**对称量化**(`Symmetric Quantization`)、**非对称量化**(`Asymmetric Quantization`)。**量化**是一种强大的模型优化技术,能够在保持模型性能的同时显著减少资源需求,使得复杂的深度学习模型能够在资源受限的环境中高效运行。
<!-- more -->

大多数现代深度神经网络由数`10`亿个参数组成。例如，最小的`LLaMA 2`有`70`亿个参数。如果每个参数都是`32`位，那么我们需要{% mathjax %}\frac{7\times 10^9\times 32}{8\times 10^9} = 28 GB{% endmathjax %}才能将这些参数存储在磁盘上。当我们推理一个模型时，我们需要将其所有参数加载到内存中，这意味着大型模型无法轻松加载到标准`PC`或智能手机上。就像人类一样，与整数运算相比，计算机在计算浮点运算时速度较慢。尝试执行{% mathjax %}3\times 6{% endmathjax %}并将其与{% mathjax %}1.21\times 2.897{% endmathjax %}进行比较，哪一个可以计算得更快？

量化的目的是减少表示每个参数所需的总位数，通常是通过将浮点数转换为整数来实现的。这样，通常占用`10 GB`的模型就可以“压缩”到`1GB`以下（取决于所使用的量化类型）。加载模型时内存消耗更少（对于智能手机等设备很重要），由于数据类型更简单，推理时间更短，能耗更少，因此推理总体上需要的计算更少。
{% note warning %}
**请注意**：量化并不意味着截断/舍入。我们不会将所有浮点数向上或向下舍入！我们稍后会看到它是如何工作的，量化还可以加快计算速度，因为处理较小的数据类型速度更快（例如，将两个整数相乘比将两个浮点数相乘更快）。
{% endnote %}
