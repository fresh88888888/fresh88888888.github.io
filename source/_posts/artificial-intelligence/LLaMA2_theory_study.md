---
title: LLaMA 2 模型—探析（PyTorch）
date: 2024-07-12 15:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

`LLaMA 2`是`Meta AI`(原`Facebook AI`)在`2023`年`7`月发布的大型语言模型系列,是`LLaMA`模型的第二代版本。**模型规模**：包含`70`亿、`130`亿和`700`亿参数三种规模的模型。比`LLaMA 1`增加了一个`700`亿参数的大型模型。**训练数据**：使用`2`万亿个`tokens`进行预训练,比`LLaMA 1`增加了`40%`；完全使用公开可用的数据集,不依赖专有数据。**性能改进**：在多数基准测试中,性能超过了同等规模的开源模型；`130`亿参数版本在某些任务上甚至超过了`GPT-3`(`1750`亿参数)。**对话优化**：提供了针对对话场景优化的`LLaMA 2-Chat`版本；使用了超过`100`万人工标注进行微调。**安全性**：在模型训练中加入了安全性改进措施；使用**人类反馈强化学习**(`RLHF`)来确保安全性和有用性。**技术创新**：使用分组查询注意力(`GQA`)机制提高效率；上下文长度增加到`4096 tokens`,是`LLaMA 1`的两倍。
<!-- more -->

`LLaMA 2`采用了经典的`Transformer`架构，但在多个方面进行了优化，以提高模型的性能和效率：
- `Transformer`架构：`LLaMA 2`基于经典的`Transformer`架构，利用注意力机制来理解文本的上下文关系。
- 解码器结构：`LLaMA 2`采用了仅解码器的`Transformer`架构，这种架构在生成任务中表现出色。
- `RMSNorm(Root Mean Square Layer Normalization)`：取代了传统的`Layer Normalization`，`RMSNorm`有助于提高训练的稳定性和效率。
- `SwiGLU`激活函数：采用了`SwiGLU`激活函数，而不是标准的`ReLU`激活函数，这种选择有助于提升模型的表现。
- `RoPE(Rotary Positional Embedding)`位置编码：使用旋转位置编码来处理位置信息，这种方法在处理长序列时表现更好。
- `Grouped Query Attention(GQA)`：引入了**分组查询注意力机制**，以加速推理过程。

{% asset_img ll_1.png %}

{% asset_img ll_2.png %}

#### RMSNorm

`Llama 2`系列模型。标记计数仅指预训练数据。所有模型均使用`4M`标记的全局批处理大小进行训练。更大的模型（`34B`和`70B`）使用**分组查询注意力**(`GQA`)来提高推理可扩展性。

什么是**归一化**？**归一化**(`Normalization`)是一种数据处理技术，主要用于调整不同尺度的数据到一个共同的尺度。**定义**：将数据按照一定的规则转换到特定的范围内，通常是`[0,1]`或`[-1,1]`。**主要目的**：使不同量纲的数据可比较；消除数据的单位影响；改善数据的稳定性和可解释性。**常见的归一化方法**：最小-最大归一化(`Min-Max Normalization`)、`Z-score`归一化、小数定标归一化。

`Root Mean Square Normalization(RMSNorm)`是一种数据归一化技术，主要用于信号处理、统计学和机器学习等领域。**定义**：`RMS Normalization`将数据缩放，使得数据的均方根值等于`1`。它是通过将每个值除以所有值平方的平均值的平方根来计算的。
{% asset_img ll_3.png %}

就像层规范化一样，我们也有一个可学习的参数{% mathjax %}\gamma{% endmathjax %}（左边公式中的`g`），它乘以归一化的值。RMSNorm的好处：与层归一化相比，需要的计算量更少；在实践中效果更好。

#### 旋转位置编码 (RoPE)

**旋转位置编码**(`Rotary Positional Encoding, RoPE`) 是一种用于`Transformer`模型的位置编码技术。原理：通过将绝对位置信息编码到**查询**(`query`)和**键**(`key`)向量中,实现相对位置编码的效果；使用复数旋转的方式来编码位置信息。

什么是**旋转矩阵**？**旋转矩阵**可以定义为对向量进行操作并产生旋转向量的变换矩阵，使得坐标轴始终保持固定。这些矩阵将向量沿逆时针方向旋转角度{% mathjax %}\theta{% endmathjax %}。旋转矩阵始终是具有实数的方阵。这意味着它始终具有相同数量的行和列。
{% asset_img ll_4.png %}

我们看到**旋转矩阵**保留了原始向量的大小（或长度），如上图中`“r”`所示，唯一改变的是与`x`轴的角度。这里旋转位置编码中使用的旋转矩阵是二维旋转矩阵的多个块。如下图所示：
{% asset_img ll_5.png %}

**绝对位置编码**是固定向量，它们被添加到`token`的嵌入中以表示其在句子中的绝对位置。因此，它一次处理一个`token`。您可以将其视为地图上的一对（纬度，经度）：地球上的每个点都有一对唯一的`token`。绝对位置编码的缺点：它没有考虑句子中的相对位置信息。

**相对位置编码**一次处理两个`token`，并且在我们计算注意力时会涉及它：由于注意力机制捕获了两个单词相互关联的“强度”，相对位置编码会告诉注意力机制其中涉及的两个单词之间的距离。因此，给定两个`token`，我们创建一个表示它们距离的向量。相对位置编码的缺点：是计算效率低下，导致成本高，不适合推理（因为每个`token`的嵌入会随着每个新的时间步长而改变）。
