---
title: 利用推测采样加速大型语言模型解码（LLM）
date: 2024-06-05 17:40:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

论文中提出了**推测性采样算法**，这是一种通过从每个`Transformer`调用生成多个`token`来加速`Transformer`解码的算法。**推测采样算法**依赖于以下观察：由更快但能力较弱的`draft`模型生成的短连续并行评分的延迟与从较大的目标模型中采样单个`token`的延迟相当。这与一种新颖的改进拒绝采样方案相结合，该方案在硬件数值内保留了目标模型的分布。使用`Chinchilla`（一个`700`亿参数语言模型）对推测性采样进行基准测试，在分布式设置中实现了`2-2.5`倍解码速度的提高，而且不会影响样本质量或对模型本身的变更。
<!-- more -->
#### 介绍

将`Transformer`模型扩展到`500B+`参数已导致许多自然语言、计算机视觉和强化学习任务的性能得到了大幅提升。当然，在这种情况下，`Transformer`解码仍然是一个成本高昂且效率低下的过程。`Transformer`采样通常受内存带宽限制，`Transformer`采样通常受内存带宽的限制，因此假设给定的一组硬件，在`Transformer`模型中生成单个`token`的时间、参数大小跟`Transformer`内存大小的一阶近似成正比。语言模型的大小还需要使用模型并行性——增加通信开销和资源消耗。由于每个新的`token`都依赖于过去的，因此需要许多`Transformer`的调用来对新序列进行采样。论文中提出了一种算法来加速`Transformer`采样，我们称之为**推测采样**(`SpS`)。通过以下方式来实现：
- 生成长度为`𝐾`的短`draft`。这可以通过并行模型或通过调用更快的自回归模型`𝐾`次来实现。我们将此模型称为`draft`模型，并且它是自回归的。
- 使用我们希望从中采样的更大、更强的模型对草稿进行评分。我们将此模型称为目标模型。
- 使用修改后的拒绝采样方案，从左到右接受`𝐾 draft tokens`的子集，在此过程中恢复目标模型的分布。

直观来说，存在下一个`token`是比较明显的序列。因此，如果`draft`模型和目标模型在给定`token`或`token`子序列的分布之间存在很强的一致性，则此设置允许每次调用目标模型时生成多个`token`。表明`draft token`的预期接受率足以抵消大语言模型的`draft`过程的开销，从而产生一种有效且实用的方法来降低采样延迟。且无需修改目标模型或偏差样本分布。

#### 自回归采样

虽然可以在`TPU`和`GPU`上高效且并行地训练`Transformer`，但样本通常还是以自回归方式绘制（参见算法1）。对于大多数应用，**自回归采样**(`ArS`)受到内存带宽的高度限制，因此无法有效利用现代加速器。内存绑定模型调用仅为批次中的每个序列生成一个`token`，因此生成多个`token`会在使用它的系统中引入大量延迟。随着模型中参数数量的增加，显得尤为严重。由于所有模型参数都需要通过至少一个加速器芯片，因此模型大小除以所有芯片的总内存带宽为最大自回归采样速度的上限。更大的模型还需要在多个加速器上运行，由于设备间通信开销而引入了另一个延迟的源头。

#### 引用

[利用推测采样加速大型语言模型解码](https://arxiv.org/abs/2302.01318)