---
title: 利用推测采样加速大型语言模型解码（LLM）
date: 2024-06-05 17:40:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

论文中提出了**推测性采样算法**，这是一种通过从每个`Transformer`调用生成多个`token`来加速`Transformer`解码的算法。**推测采样算法**依赖于以下观察：由更快但能力较弱的`draft`模型生成的短连续并行评分的延迟与从较大的目标模型中采样单个`token`的延迟相当。这与一种新颖的改进拒绝采样方案相结合，该方案在硬件数值内保留了目标模型的分布。使用`Chinchilla`（一个`700`亿参数语言模型）对推测性采样进行基准测试，在分布式设置中实现了`2-2.5`倍解码速度的提高，而且不会影响样本质量或对模型本身的变更。
<!-- more -->
#### 介绍

将`Transformer`模型扩展到`500B+`参数已导致许多自然语言、计算机视觉和强化学习任务的性能得到了大幅提升。当然，在这种情况下，`Transformer`解码仍然是一个成本高昂且效率低下的过程。`Transformer`采样通常受内存带宽限制，`Transformer`采样通常受内存带宽的限制，因此假设给定的一组硬件，在`Transformer`模型中生成单个`token`的时间、参数大小跟`Transformer`内存大小的一阶近似成正比。语言模型的大小还需要使用模型并行性——增加通信开销和资源消耗。由于每个新的`token`都依赖于过去的，因此需要许多`Transformer`的调用来对新序列进行采样。论文中提出了一种算法来加速`Transformer`采样，我们称之为**推测采样**(`SpS`)。通过以下方式来实现：
- 生成长度为`𝐾`的短`draft`。这可以通过并行模型或通过调用更快的自回归模型`𝐾`次来实现。我们将此模型称为`draft`模型，并且它是自回归的。
- 使用我们希望从中采样的更大、更强的模型对草稿进行评分。我们将此模型称为目标模型。
- 使用修改后的拒绝采样方案，从左到右接受`𝐾 draft tokens`的子集，在此过程中恢复目标模型的分布。

直观来说，存在下一个`token`是比较明显的序列。因此，如果`draft`模型和目标模型在给定`token`或`token`子序列的分布之间存在很强的一致性，则此设置允许每次调用目标模型时生成多个`token`。表明`draft token`的预期接受率足以抵消大语言模型的`draft`过程的开销，从而产生一种有效且实用的方法来降低采样延迟。且无需修改目标模型或偏差样本分布。

#### 自回归采样

虽然可以在`TPU`和`GPU`上高效且并行地训练`Transformer`，但样本通常还是以自回归方式绘制（参见算法1）。对于大多数应用，**自回归采样**(`ArS`)受到内存带宽的高度限制，因此无法有效利用现代加速器。内存绑定模型调用仅为批次中的每个序列生成一个`token`，因此生成多个`token`会在使用它的系统中引入大量延迟。随着模型中参数数量的增加，显得尤为严重。由于所有模型参数都需要通过至少一个加速器芯片，因此模型大小除以所有芯片的总内存带宽为最大自回归采样速度的上限。更大的模型还需要在多个加速器上运行，由于设备间通信开销而引入了另一个延迟的源头。

以下是自回归采样算法：
{% asset_img ss_1.png %}

以下是推测性采样算法：
{% asset_img ss_2.png %}

#### 推测采样

对于推测性采样（参见算法`2`），我们首先观察到，并行`𝐾 token`的对数计算的延迟与单个`token`采样的延迟非常相似。我们将注意力主要集中在以`Megatron`风格分割的`Transformer`上。对于这些模型，大部分采样花费的时间包含三个部分：
- **线性层**：对于小批量，每个线性层仅处理少量的嵌入。这会导致前馈层、查询、键、值计算和最终注意力投影中的密集矩阵乘法受到内存限制。对于较小的`𝐾`值，这将继续受到内存的限制，因此同样需要花费大量的时间。
- **注意力机制**：注意力机制也会受到内存的限制。在采样期间，我们需要维护序列中先前标记的所有键和值的缓存，以避免重新计算。这些`KV`缓存很大，占注意力机制内存带宽的大部分。但是，由于`KV`缓存大小不会随着我们增加`𝐾`而变化，因此该组件几乎没有增量。
- **全归约**：随着模型规模的扩大，其参数需要分布在多个加速器上，从而导致通信开销。对于`Megatron`，这表现为每个前馈和注意层之后的全归约。由于只传输少量`token`的激活，因此采样和评分（对于较小的`𝐾`）通常受延迟的限制，而不是吞吐量的限制。同时，这将会导致两种情况下花费的时间相似。

可能存在其他方面开销，具体取决于`Tranformer`的实现方式。因此，编码、解码方法的选择（例如，可能需要对核采样进行排序）、硬件限制等有可能在评分和采样之间存在一些差异。但是，如果满足上述条件，则对于较小的`𝐾`，评分数值不应该变慢。
##### 改进的拒绝采样

我们需要一种方法来从`draft`模型的样本中恢复目标模型的分布，以及来自两个模型的`tokens`的对数。为了实现这一点，我们引入了以下对草稿令牌的拒绝采样方案。给定由{% mathjax %}p(.|.){% endmathjax %}生成的`token`序列{% mathjax %}x_1,\ldots,x_n{% endmathjax %}和`𝐾 draft tokens`{% mathjax %}\tilde{x}_{n+1},\ldots,\tilde{x}_{n + K}{% endmathjax %}，{% mathjax %}\tilde{x}_{n+1}{% endmathjax %}的概率为：
{% mathjax '{"conversion":{"em":14}}' %}
\min(1,\frac{q(\tilde{x}_{n+1}|x_1,\ldots,x_n)}{p(\tilde{x}_{n+1}|x,\ldots,x_n)})
{% endmathjax %}
其中{% mathjax %}q(\tilde{x}_{n+1}|x_1,\ldots,x_n){% endmathjax %}和{% mathjax %}p(\tilde{x}_{n+1}|x,\ldots,x_n){% endmathjax %}分别是根据目标模型和`draft`模型得出{% mathjax %}\tilde{x}_{n+1}{% endmathjax %}的概率。如果`token`被接受，我们设置{% mathjax %}x_{n+1}\leftarrow \tilde{x}_{n+1}{% endmathjax %}，并对{% mathjax %}\tilde{x}_{n+2}{% endmathjax %}重复此过程，直到`token`被拒绝或所有`token`都被接受。如果{% mathjax %}\tilde{x}_{n+1}{% endmathjax %}被拒绝，我们将根据以下分布重新采样{% mathjax %}x_{n+1}{% endmathjax %}：
{% mathjax '{"conversion":{"em":14}}' %}
x_{n+1} \sim (q(x|x_1,\ldots,x_n) - p(x|x_1,\ldots,x_n))_+
{% endmathjax %}
其中{% mathjax %}(.)_+{% endmathjax %}表示为：
{% mathjax '{"conversion":{"em":14}}' %}
(f(x))_+ = \frac{\max(0, f(x))}{\sum_x \max(0,f(x))}
{% endmathjax %}

#### 引用

[利用推测采样加速大型语言模型解码](https://arxiv.org/abs/2302.01318)