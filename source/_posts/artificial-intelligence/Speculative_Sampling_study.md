---
title: 利用推测采样加速大型语言模型解码（LLM）
date: 2024-06-05 17:40:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

论文中提出了**推测性采样算法**，这是一种通过从每个`Transformer`调用生成多个`token`来加速`Transformer`解码的算法。**推测采样算法**依赖于以下观察：由更快但能力较弱的`draft`模型生成的短连续并行评分的延迟与从较大的目标模型中采样单个`token`的延迟相当。这与一种新颖的改进拒绝采样方案相结合，该方案在硬件数值内保留了目标模型的分布。使用`Chinchilla`（一个`700`亿参数语言模型）对推测性采样进行基准测试，在分布式设置中实现了`2-2.5`倍解码速度的提高，而且不会影响样本质量或对模型本身的变更。
<!-- more -->
#### 介绍

将`Transformer`模型扩展到`500B+`参数已导致许多自然语言、计算机视觉和强化学习任务的性能得到了大幅提升。当然，在这种情况下，`Transformer`解码仍然是一个成本高昂且效率低下的过程。`Transformer`采样通常受内存带宽限制，`Transformer`采样通常受内存带宽的限制，因此假设给定的一组硬件，在`Transformer`模型中生成单个`token`的时间、参数大小跟`Transformer`内存大小的一阶近似成正比。语言模型的大小还需要使用模型并行性——增加通信开销和资源消耗。由于每个新的`token`都依赖于过去的，因此需要许多`Transformer`的调用来对新序列进行采样。论文中提出了一种算法来加速`Transformer`采样，我们称之为**推测采样**(`SpS`)。通过以下方式来实现：
- 生成长度为`𝐾`的短`draft`。这可以通过并行模型或通过调用更快的自回归模型`𝐾`次来实现。我们将此模型称为`draft`模型，并且它是自回归的。
- 使用我们希望从中采样的更大、更强的模型对草稿进行评分。我们将此模型称为目标模型。
- 使用修改后的拒绝采样方案，从左到右接受`𝐾 draft tokens`的子集，在此过程中恢复目标模型的分布。

直观来说，存在下一个`token`是比较明显的序列。因此，如果`draft`模型和目标模型在给定`token`或`token`子序列的分布之间存在很强的一致性，则此设置允许每次调用目标模型时生成多个`token`。表明draft token的预期接受率足以抵消大语言模型的draft过程的开销，从而产生一种有效且实用的方法来降低采样延迟。且无需修改目标模型或偏差样本分布。

#### 引用

[利用推测采样加速大型语言模型解码](https://arxiv.org/abs/2302.01318)