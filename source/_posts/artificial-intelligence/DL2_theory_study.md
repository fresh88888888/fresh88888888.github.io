---
title: 深度学习(DL)(二) — 探析
date: 2024-09-05 12:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

我们一直在使用词汇表来表示单词，词汇表可能有`10,000`个单词。我们一直在使用`1-hot`向量（`1-Hot`编码是一种用于表示分类数据的技术，广泛应用于机器学习和深度学习中。它将每个类别转换为一个二进制向量，向量的长度等于类别的总数。每个向量中只有一个元素为`1`，其余元素均为`0`。）来表示单词。例如，如果`man`是本词典中的第`5391`个单词，那么你可以用一个在位置`5391`处为`1`的向量来表示。我还将使用{% mathjax %}0_5931{% endmathjax %}来表示这个因子，其中`O`代表`1-hot`。如果`woman`是第`9853`个单词，那么你可以用{% mathjax %}0_9853{% endmathjax %}来表示，它在位置`9853`处只有一个`1`，其他地方都是`0`。然后其他单词`king、queen、apple、orange`将同样用`1-hot`向量表示。这种表示的缺点之一是它将每个单词视为一个独立的事物，并且它不允许算法概括交叉单词。
<!-- more -->

例如，假设你有一个**语言模型**，它已经学会了当你看到我想要一杯橙子__时。那么，你认为下一个词会是什么？很有可能是果汁。但是，即使学习算法已经知道我想要一杯橙汁，如果它看到我想要一杯苹果，没有任何关系。苹果和橙子之间的关系并不比其他任何单词男人、女人、国王、王后和橙子之间的关系更紧密。因此，学习算法很难知道橙汁是一种流行的东西，推广到苹果汁也可能是一种流行的东西或一个流行的短语。这是因为任何两个不同的`1-hot`向量之间的任何乘积都是`0`。如果你取任何两个向量，比如说，女王和国王，它们的乘积是`0`。如果你取苹果和橙子，它们的乘积是`0`。无法让这些向量中的任何一对之间的距离也相同。所以它只是不知道苹果和橙子比国王和橙子或王后和橙子更相似。那么，如果我们不使用`1-hot`向量表示，而是学习每个单词的**特征表示**，男人、女人、国王、王后、苹果、橙子，字典中的每个单词，我们可以学习每个单词的一组特征和值，那不是很好吗？例如，如果性别从男性的`-1`变为女性的`+1`，那么与男人相关的性别可能是`-1`，与女人相关的性别可能是`+1`。最终，学习这些东西，对于国王，你可能会得到`-0.95`，对于王后，你会得到`+0.97`，而对于苹果和橙子，你可能会得到无性别。另一个特征是这些东西有多高贵。男人和女人并不是真正的高贵，所以他们的特征值可能接近于`0`。而国王和王后则非常高贵。而苹果和橙子并不是真正的高贵。年龄呢？男人和女人与年龄没有太大关系。也许男人和女人暗示是成年人，但可能既不一定年轻也不一定年老。所以可能值接近于`0`。而国王和王后几乎总是成年人。苹果和橙子在年龄方面可能更中性。这里的另一个特征是，这是食物吗？男人不是食物，女人不是食物，国王和王后也不是，但苹果和橙子是食物。它们还可以是许多其他特征，大小是多少？价格是多少？这是生命吗？这是一个动词还是名词等等。所以你可以想象出很多特征。为了便于说明，我们假设有`300`个不同的特征，它的作用是获取这个数字列表，这里只写了四个，但这可能是包含`300`个数字的列表，然后变成一个`300`维向量来表示单词`man`。使用符号 {% mathjax %}e_5391{% endmathjax %}来表示。这个`300`维向量，表示为{% mathjax %}e_9853{% endmathjax %}，可以用来表示单词`woman`的`300`维向量。对于这里的其他示例，也是如此。如果用此来表示单词`orange`和`apple`，那么请注意，`orange`和`apple`的表示非常相似。某些特征会因橙子的颜色、苹果的颜色、味道而有所不同，或者某些特征会有所不同。但总的来说，`apple`和`orange`的很多特征实际上是相同的，或者具有非常相似的值。这使其能够更好地表示不同的单词。
{% asset_img dl_1.png %}

这些表示将在`300`维空间中使用这些**特征化**的表示，这些被称为**嵌入**。它们不能在二维空间中绘制，因为它是`3D`空间。你要做的是，把每个单词（如 `orange`）都取出来，并得到一个三维特征向量，这样单词`orange`就嵌入到这个`300`维空间中的一个点。而单词`apple`则嵌入到`300`维空间的另一个点。当然，为了将其可视化，像`t-SNE`这样的算法会将其映射到一个低维空间，你可以绘制二维数据并查看它。但这就是嵌入的由来。**词嵌入**一直是`NLP`中最重要的思想之一。

#### 词嵌入

让我们从一个例子开始。继续使用命名实体识别示例，如果您尝试检测人的名字。给出一个句子，如`Sally Johnson is an orange farmer`，希望能弄清楚`Sally Johnson`是一个人的名字，确保`Sally Johnson`必须是一个人，而不是说公司的名称，您知道`orange farmer`是一个人。之前讨论了`1-hot`来表示这些单词，{% mathjax %}x^{<1>},x^{<2>}{% endmathjax %}等等。
{% asset_img dl_2.png %}

但是如果使用**特征化表示**，在训练了一个使用词嵌入作为输入的模型之后，如果看到一个新的输入，`Robert Lin is an apple farmer`。橙子和苹果非常相似，将使学习算法更容易表示，从而找出`Robert Lin`也是一个人的名字。如果在测试集中没有看到`Robert Lin is an apple farmer`，但看到了不常见的单词，那会怎么样？如果你看到`Robert Lin is a dusian cultiztor`，那会怎么样？榴莲是一种稀有的水果。但是如果你有一个用于命名实体识别任务的小标签训练集，没有在训练集中看到榴莲这个词，也没有看到种植者这个词。如果已经学习了**词嵌入**，告诉你榴莲是一种水果，所以它就像一个橙子，而种植者，种植的人就像一个农民，那么你可能仍然会在你的训练集中看到橙子农民的表示知道榴莲种植者也可能是一个人。词向量能够做到这一点的原因是，学习词向量的算法可以检查非常大的文本语料库，这些文本语料库可能是从互联网上找到的。因此，可以检查非常大的数据集，也许有十亿个单词，甚至多达`1000`亿个单词也是相当合理的。只有未标记的文本的训练集非常大。通过检查大量未标记的文本），你会发现橙子和榴莲很相似。农民和种植者也很相似，因此，学习**嵌入**，将它们归为一类。现在，通过阅读大量互联网文本，你发现橙子和榴莲都是水果，你可以将这个词嵌入应用到命名实体识别任务中，而命名实体识别任务的训练集可能要小得多，训练集中可能只有`10`万个单词，甚至更小。这允许进行**迁移学习**，从大量未标记的文本中获取信息，这些信息基本上是免费的，可以从互联网上获取，从而找出橙子、苹果和榴莲是水果。然后将这些知识迁移到命名实体识别等任务中，而命名实体识别任务的标记训练集可能相对较小。当然，为了简单起见，将其绘制为单向`RNN`。如果你真的想执行命名实体识别任务，当然应该使用双向`RNN`，而不是简单的`RNN`。

总而言之，这就是使用词向量进行**迁移学习**的方法。第一步是从大型文本语料库（非常大的文本语料库）中学习**词向量**，或者也可以在线下载预先训练好的词向量。然后，您可以利用这些词向量，将**嵌入迁移**到新任务中，在那里，您有一个小得多的标记训练集。使用这个`300`维嵌入来表示您的单词。这样做还有一个好处，就是可以使用相对低维的**特征向量**。因此，可以使用`300`维的密集向量，而不是使用`10,000`维的`1-hot`向量。尽管`1-hot`向量很快，但嵌入学习的`300`维向量是一个密集向量。当你在新的任务上训练模型时，在具有较小标签数据集的命名实体识别任务上，你可以继续微调，继续使用新数据调整**词嵌入**。只有当这个任务具有相当大的数据集时，你才会这样做。如果标签数据集非常小，通常不会费心继续**微调词嵌入**。对许多`NLP`任务都很有用。它对命名实体识别、文本摘要、解析都很有用。这些可能是非常标准的`NLP`任务。它对于语言建模、机器翻译的用处不大，特别是语言建模或机器翻译任务，而你有大量专门用于该任务的数据。正如在其他迁移学习设置中所看到的，如果从某个任务`A`迁移到某个任务`B`，迁移学习过程只有在恰好拥有大量`A`数据和相对较小的`B`数据集时才最有用。这对于`NLP`任务来说都是如此，但对于某些语言建模和机器翻译来说则不那么如此。

最后，编码和嵌入这两个词的意思相当相似。在人脸识别文献中，人们也使用编码来指代这些向量{% mathjax %}f(x^{(i)}){% endmathjax %}和{% mathjax %}f(x^{(j)}){% endmathjax %}。人脸识别文献与我们在词嵌入中所做的工作之间的一个区别是，对于人脸识别，您希望训练一个神经网络，该神经网络可以将任何脸部图片作为输入，即使是您从未见过的图片，让神经网络为该新图片计算编码。学习**词嵌入**，将拥有一个固定的词汇表，例如`10,000`个单词。我们将学习向量{% mathjax %}\{\vec{e}_1,\ldots,\vec{e}_{10000}\}{% endmathjax %}，它只学习固定的编码或词汇表中每个单词的**固定嵌入**。**编码**和**嵌入**在某种程度上是可以互换使用的。
{% asset_img dl_3.png %}

<span style="color:#295F98;font-weight:900;">词嵌入</span>最迷人的特性之一是可以进行**类比推理**。这里词**嵌入**可以捕获的一组单词的特征表示。假设提出一个问题，男人之于女人就像国王之于什么？许多人会说，男人之于女人就像国王之于王后。但是，是否能让算法自动解决这个问题？可以这样做，假设使用四维向量来表示男人。这将是{% mathjax %}e_{\text{man}}{% endmathjax %}。这是女人的嵌入向量，{% mathjax %}e_{\text{woman}}{% endmathjax %}，国王和王后的嵌入向量也类似。在这个例子中，使用的是四维嵌入，而不是`50~1,000`维。这些向量的一个有趣特性是，如果{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx \begin{bmatrix}-2 \\0 \\0 \\0 \end{bmatrix} {% endmathjax %}。同样，如果您取{% mathjax %}e_{\text{king}} - e_{\text{queen}} \approx \begin{bmatrix}-2 \\0 \\0 \\0 \end{bmatrix} {% endmathjax %}，结果大致相同。这大约是`1-1`，因为国王和王后的皇室地位大致相同。所以这是`0`，然后是年龄差异，食物差异，`0`。所以这捕捉到的是男人和女人之间的主要差异是性别。而国王和王后之间的主要差异，正如这些向量所表示的，也是性别。这就是为什么差异{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx e_{\text{king}} - e_{\text{queen}}{% endmathjax %}，大致相同。进行**类比推理**的方法是，男人和女人的关系相当于国王和什么的关系？它可以做的是计算{% mathjax %}e_{\text{man}} - e_{\text{woman}}{% endmathjax %}，并找到一个向量，尝试找到一个词，使{% mathjax %}e_{\text{man}} - e_{\text{woman}}{% endmathjax %}接近于{% mathjax %}e_{\text{king}} - e_{\text{?}}{% endmathjax %}这个新词。事实证明，当这里插入的单词是女王时，左侧接近右侧。所以这些想法是由`Tomas Mikolov、Wentau Yih`和`Geoffrey Zweig`首次提出的。这是关于`词嵌入`最具影响力的结果之一。这有助于整个社区更好地了解**词嵌入**的作用。

**词嵌入**可能存在于`300`维空间中。单词`man`表示为空间中的一个点，单词`woman`表示为空间中的一个点。单词`king`表示为另一个点，单词`queen`表示为另一个点。男人和女人之间的向量差异与国王和王后之间的向量差异非常相似。箭头实际上是表示性别差异的向量。为了**类比推理**来弄清楚，男人和女人的关系是国王和什么的关系，你可以尝试找到单词`w`，使这个等式成立：{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx e_{\text{king}} - e_{\text{?}} {% endmathjax %}，所以你想要找到一个单词，使{% mathjax %}w:\;\arg \underset{w}{\max} \text{sim}(e_w, e_{\text{king}} - e_{\text{man}} + e_{\text{woman}}){% endmathjax %}的相似度最大化。有一些合适的相似度函数来衡量某个单词`w`的嵌入与右边的相似度。然后找到使相似度最大化的单词。值得注意的是，这确实有效。如果学习一组**单词嵌入**并找到一个使这种相似度最大化的单词`w`，可以得到完全正确的答案。只有当它正确猜出单词时，类比才算正确。只有在这种情况下，选出了单词`“queen”`。`t-SAE`所做的是获取`300`维数据，并以非线性的方式将其映射到`2D`空间。`t-SAE`学习的映射是一种非常复杂且非线性的映射。实际上，在这个`300`维空间中，在通过`t-SAE`映射后，平行四边形关系可能成立，但在大多数情况下，由于`t-SAE`的非线性映射，不应指望这一点。 `t-SAE`会破坏许多平行四边形类比关系。最常用的**相似性函数**称为**余弦相似性**。在余弦相似性中，你将两个向量{% mathjax %}u{% endmathjax %}和 {% mathjax %}v{% endmathjax %}之间的相似性定义为{% mathjax %}u{% endmathjax %}转置{% mathjax %}v{% endmathjax %}除以长度再除以欧几里得长度({% mathjax %}\text{sim}(e_w, e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})\;,\; \text{sim}(u,v) = \frac{u^{\mathsf{T}}v}{\|u\|_2\|v\|_2}{% endmathjax %})。所以现在忽略分母，这基本上是{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}之间的内积。所以如果{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}非常相似，它们的内积就会很大。这被称为**余弦相似性**，因为这实际上是两个向量{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}之间角度的**余弦**。这就是角度{% mathjax %}\phi{% endmathjax %}，所以这个公式是它们之间角度的**余弦**({% mathjax %}\cos\phi{% endmathjax %})。如果它们之间的角度为`0`，则余弦相似度等于`1`。如果它们的角度是`90`度，余弦相似度就是`0`。如果它们的角度是`180`度，**余弦相似度**最终就是`-1`。它对这些**类比推理任务**非常有效。你也可以使用平方距离或欧几里得距离，{% mathjax %}\|u-v\|^2{% endmathjax %}。从技术上讲，这将是对差异的度量，而不是对**相似度**的度量。所以需要取它的负数。它们之间的主要区别在于它如何规范向量{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}的长度。**词嵌入**的一个显着结果是它们可以学习**类比关系**的普遍性。例如，它可以学习男人和女人的关系就像男孩和女孩的关系，因为男人和女人之间的向量差异，类似于国王和王后以及男孩和女孩，只是性别。它可以学习到渥太华是加拿大的首都，渥太华之于加拿大相当于内罗毕之于肯尼亚。这就是城市首都之于国家名称。它可以学习到大之于更大，高之于更高，它可以学习诸如此类的东西。日元之于日本，因为日元是日本的货币，卢布之于俄罗斯。所有这些事情都可以通过在大型文本语料库上运行**词嵌入**学习算法来学习。

当你实现一个算法来学习**词嵌入**时，最终会学习一个**嵌入矩阵**。假设使用`10,000`个单词的**词汇表**。词汇表中有`A、Aaron、Orange、Zulu`，也许还有未知单词作为标记。学习嵌入矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，它是一个{% mathjax %}300\times 10000{% endmathjax %}的矩阵，如果你有`10,000`个单词的词汇表，`10,001`是未知单词标记，那么就会有一个额外的标记。`Orange`是`10,000`个单词的词汇表中的第`6257`个单词。符号{% mathjax %}o_{6257}{% endmathjax %}是一个`1-hot`向量，只有位置`6257`处是`1`，其它都是`0`。它应该和左边的嵌入矩阵一样高。如果**嵌入矩阵**{% mathjax %}\mathbf{E}\cdot \mathbf{o}_{6257}{% endmathjax %}，如果将{% mathjax %}\mathbf{E}{% endmathjax %}乘以`1-hot`向量，再乘以`6257`中的`0`，那么这是一个`300`维的向量。{% mathjax %}\mathbf{E}{% endmathjax %}是{% mathjax %}300\times 10000{% endmathjax %}，{% mathjax %}10000\times 1{% endmathjax %}。乘积是{% mathjax %}300\times 1{% endmathjax %}，要计算这个`300`维向量的第一个元素，您要做的就是将矩阵{% mathjax %}\mathbf{E}{% endmathjax %}的第一行与其相乘。但是除了元素`6257`之外，其它元素都是`0`，最终你会得到第一个元素，即上面的`Orange`列下的元素。然后，计算`300`维向量的第二个元素，你需要取向量{% mathjax %}o_{6257}{% endmathjax %}与矩阵{% mathjax %}\mathbf{E}{% endmathjax %}的第二行相乘。同样，依此类推。这就是为什么**嵌入矩阵**{% mathjax %}\mathbf{E}{% endmathjax %}乘以这个`1-hot`向量选出这个对应于单词`Orange`的`300`维列。这等于{% mathjax %}\mathbf{E}\cdot o_{6257} = e_{6257}{% endmathjax %}，这是用来表示单词`Orange`的`300`个一维向量的**嵌入向量**的符号。

#### 单词嵌入: Word2vec & GloVe

