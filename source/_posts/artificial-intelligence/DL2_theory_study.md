---
title: 深度学习(DL)(二) — 探析
date: 2024-09-05 12:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

我们一直在使用词汇表来表示单词，词汇表可能有`10,000`个单词。我们一直在使用`1-hot`向量（`1-Hot`编码是一种用于表示分类数据的技术，广泛应用于机器学习和深度学习中。它将每个类别转换为一个二进制向量，向量的长度等于类别的总数。每个向量中只有一个元素为`1`，其余元素均为`0`。）来表示单词。例如，如果`man`是本词典中的第`5391`个单词，那么你可以用一个在位置`5391`处为`1`的向量来表示。我还将使用{% mathjax %}o_{5931}{% endmathjax %}来表示这个因子，其中`O`代表`1-hot`。如果`woman`是第`9853`个单词，那么你可以用{% mathjax %}o_{9853}{% endmathjax %}来表示，它在位置`9853`处只有一个`1`，其他地方都是`0`。然后其他单词`king、queen、apple、orange`将同样用`1-hot`向量表示。这种表示的缺点之一是它将每个单词视为一个独立的事物，并且它不允许算法概括交叉单词。
<!-- more -->

例如，假设你有一个**语言模型**，它已经学会了当你看到我想要一杯橙子__时。那么，你认为下一个词会是什么？很有可能是果汁。但是，即使学习算法已经知道我想要一杯橙汁，如果它看到我想要一杯苹果，没有任何关系。苹果和橙子之间的关系并不比其他任何单词男人、女人、国王、王后和橙子之间的关系更紧密。因此，学习算法很难知道橙汁是一种流行的东西，推广到苹果汁也可能是一种流行的东西或一个流行的短语。这是因为任何两个不同的`1-hot`向量之间的任何乘积都是`0`。如果你取任何两个向量，比如说，女王和国王，它们的乘积是`0`。如果你取苹果和橙子，它们的乘积是`0`。无法让这些向量中的任何一对之间的距离也相同。所以它只是不知道苹果和橙子比国王和橙子或王后和橙子更相似。那么，如果我们不使用`1-hot`向量表示，而是学习每个单词的**特征表示**，男人、女人、国王、王后、苹果、橙子，字典中的每个单词，我们可以学习每个单词的一组特征和值，那不是很好吗？例如，如果性别从男性的`-1`变为女性的`+1`，那么与男人相关的性别可能是`-1`，与女人相关的性别可能是`+1`。最终，学习这些东西，对于国王，你可能会得到`-0.95`，对于王后，你会得到`+0.97`，而对于苹果和橙子，你可能会得到无性别。另一个特征是这些东西有多高贵。男人和女人并不是真正的高贵，所以他们的特征值可能接近于`0`。而国王和王后则非常高贵。而苹果和橙子并不是真正的高贵。年龄呢？男人和女人与年龄没有太大关系。也许男人和女人暗示是成年人，但可能既不一定年轻也不一定年老。所以可能值接近于`0`。而国王和王后几乎总是成年人。苹果和橙子在年龄方面可能更中性。这里的另一个特征是，这是食物吗？男人不是食物，女人不是食物，国王和王后也不是，但苹果和橙子是食物。它们还可以是许多其他特征，大小是多少？价格是多少？这是生命吗？这是一个动词还是名词等等。所以你可以想象出很多特征。为了便于说明，我们假设有`300`个不同的特征，它的作用是获取这个数字列表，这里只写了四个，但这可能是包含`300`个数字的列表，然后变成一个`300`维向量来表示单词`man`。使用符号 {% mathjax %}e_{5391}{% endmathjax %}来表示。这个`300`维向量，表示为{% mathjax %}e_{9853}{% endmathjax %}，可以用来表示单词`woman`的`300`维向量。对于这里的其他示例，也是如此。如果用此来表示单词`orange`和`apple`，那么请注意，`orange`和`apple`的表示非常相似。某些特征会因橙子的颜色、苹果的颜色、味道而有所不同，或者某些特征会有所不同。但总的来说，`apple`和`orange`的很多特征实际上是相同的，或者具有非常相似的值。这使其能够更好地表示不同的单词。
{% asset_img dl_1.png %}

这些表示将在`300`维空间中使用这些**特征化**的表示，这些被称为**嵌入**。它们不能在二维空间中绘制，因为它是`3D`空间。你要做的是，把每个单词（如 `orange`）都取出来，并得到一个三维特征向量，这样单词`orange`就嵌入到这个`300`维空间中的一个点。而单词`apple`则嵌入到`300`维空间的另一个点。当然，为了将其可视化，像`t-SNE`这样的算法会将其映射到一个低维空间，你可以绘制二维数据并查看它。但这就是嵌入的由来。**词嵌入**一直是`NLP`中最重要的思想之一。

#### 词嵌入

让我们从一个例子开始。继续使用命名实体识别示例，如果您尝试检测人的名字。给出一个句子，如`Sally Johnson is an orange farmer`，希望能弄清楚`Sally Johnson`是一个人的名字，确保`Sally Johnson`必须是一个人，而不是说公司的名称，您知道`orange farmer`是一个人。之前讨论了`1-hot`来表示这些单词，{% mathjax %}x^{<1>},x^{<2>}{% endmathjax %}等等。
{% asset_img dl_2.png %}

但是如果使用**特征化表示**，在训练了一个使用词嵌入作为输入的模型之后，如果看到一个新的输入，`Robert Lin is an apple farmer`。橙子和苹果非常相似，将使学习算法更容易表示，从而找出`Robert Lin`也是一个人的名字。如果在测试集中没有看到`Robert Lin is an apple farmer`，但看到了不常见的单词，那会怎么样？如果你看到`Robert Lin is a dusian cultiztor`，那会怎么样？榴莲是一种稀有的水果。但是如果你有一个用于命名实体识别任务的小标签训练集，没有在训练集中看到榴莲这个词，也没有看到种植者这个词。如果已经学习了**词嵌入**，告诉你榴莲是一种水果，所以它就像一个橙子，而种植者，种植的人就像一个农民，那么你可能仍然会在你的训练集中看到橙子农民的表示知道榴莲种植者也可能是一个人。词向量能够做到这一点的原因是，学习词向量的算法可以检查非常大的文本语料库，这些文本语料库可能是从互联网上找到的。因此，可以检查非常大的数据集，也许有十亿个单词，甚至多达`1000`亿个单词也是相当合理的。只有未标记的文本的训练集非常大。通过检查大量未标记的文本），你会发现橙子和榴莲很相似。农民和种植者也很相似，因此，学习**嵌入**，将它们归为一类。现在，通过阅读大量互联网文本，你发现橙子和榴莲都是水果，你可以将这个词嵌入应用到命名实体识别任务中，而命名实体识别任务的训练集可能要小得多，训练集中可能只有`10`万个单词，甚至更小。这允许进行**迁移学习**，从大量未标记的文本中获取信息，这些信息基本上是免费的，可以从互联网上获取，从而找出橙子、苹果和榴莲是水果。然后将这些知识迁移到命名实体识别等任务中，而命名实体识别任务的标记训练集可能相对较小。当然，为了简单起见，将其绘制为单向`RNN`。如果你真的想执行命名实体识别任务，当然应该使用双向`RNN`，而不是简单的`RNN`。

总而言之，这就是使用词向量进行**迁移学习**的方法。第一步是从大型文本语料库（非常大的文本语料库）中学习**词向量**，或者也可以在线下载预先训练好的词向量。然后，您可以利用这些词向量，将**嵌入迁移**到新任务中，在那里，您有一个小得多的标记训练集。使用这个`300`维嵌入来表示您的单词。这样做还有一个好处，就是可以使用相对低维的**特征向量**。因此，可以使用`300`维的密集向量，而不是使用`10,000`维的`1-hot`向量。尽管`1-hot`向量很快，但嵌入学习的`300`维向量是一个密集向量。当你在新的任务上训练模型时，在具有较小标签数据集的命名实体识别任务上，你可以继续微调，继续使用新数据调整**词嵌入**。只有当这个任务具有相当大的数据集时，你才会这样做。如果标签数据集非常小，通常不会费心继续**微调词嵌入**。对许多`NLP`任务都很有用。它对命名实体识别、文本摘要、解析都很有用。这些可能是非常标准的`NLP`任务。它对于语言建模、机器翻译的用处不大，特别是语言建模或机器翻译任务，而你有大量专门用于该任务的数据。正如在其他迁移学习设置中所看到的，如果从某个任务`A`迁移到某个任务`B`，迁移学习过程只有在恰好拥有大量`A`数据和相对较小的`B`数据集时才最有用。这对于`NLP`任务来说都是如此，但对于某些语言建模和机器翻译来说则不那么如此。

最后，编码和嵌入这两个词的意思相当相似。在人脸识别文献中，人们也使用编码来指代这些向量{% mathjax %}f(x^{(i)}){% endmathjax %}和{% mathjax %}f(x^{(j)}){% endmathjax %}。人脸识别文献与我们在词嵌入中所做的工作之间的一个区别是，对于人脸识别，您希望训练一个神经网络，该神经网络可以将任何脸部图片作为输入，即使是您从未见过的图片，让神经网络为该新图片计算编码。学习**词嵌入**，将拥有一个固定的词汇表，例如`10,000`个单词。我们将学习向量{% mathjax %}\{\vec{e}_1,\ldots,\vec{e}_{10000}\}{% endmathjax %}，它只学习固定的编码或词汇表中每个单词的**固定嵌入**。**编码**和**嵌入**在某种程度上是可以互换使用的。
{% asset_img dl_3.png %}

<span style="color:#295F98;font-weight:900;">词嵌入</span>最迷人的特性之一是可以进行**类比推理**。这里词**嵌入**可以捕获的一组单词的特征表示。假设提出一个问题，男人之于女人就像国王之于什么？许多人会说，男人之于女人就像国王之于王后。但是，是否能让算法自动解决这个问题？可以这样做，假设使用四维向量来表示男人。这将是{% mathjax %}e_{\text{man}}{% endmathjax %}。这是女人的嵌入向量，{% mathjax %}e_{\text{woman}}{% endmathjax %}，国王和王后的嵌入向量也类似。在这个例子中，使用的是四维嵌入，而不是`50~1,000`维。这些向量的一个有趣特性是，如果{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx \begin{bmatrix}-2 \\0 \\0 \\0 \end{bmatrix} {% endmathjax %}。同样，如果您取{% mathjax %}e_{\text{king}} - e_{\text{queen}} \approx \begin{bmatrix}-2 \\0 \\0 \\0 \end{bmatrix} {% endmathjax %}，结果大致相同。这大约是`1-1`，因为国王和王后的皇室地位大致相同。所以这是`0`，然后是年龄差异，食物差异，`0`。所以这捕捉到的是男人和女人之间的主要差异是性别。而国王和王后之间的主要差异，正如这些向量所表示的，也是性别。{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx e_{\text{king}} - e_{\text{queen}}{% endmathjax %}，差异大致相同。进行**类比推理**的方法是，男人和女人的关系相当于国王和什么的关系？它可以做的是计算{% mathjax %}e_{\text{man}} - e_{\text{woman}}{% endmathjax %}，并找到一个向量，尝试找到一个词，使{% mathjax %}e_{\text{man}} - e_{\text{woman}}{% endmathjax %}接近于{% mathjax %}e_{\text{king}} - e_{\text{?}}{% endmathjax %}这个新词。事实证明，当这里插入的单词是女王时，左侧接近右侧。所以这些想法是由`Tomas Mikolov、Wentau Yih`和`Geoffrey Zweig`首次提出的。这是关于`词嵌入`最具影响力的结果之一。这有助于整个社区更好地了解**词嵌入**的作用。

**词嵌入**可能存在于`300`维空间中。单词`man`表示为空间中的一个点，单词`woman`表示为空间中的一个点。单词`king`表示为另一个点，单词`queen`表示为另一个点。男人和女人之间的向量差异与国王和王后之间的向量差异非常相似。箭头实际上是表示性别差异的向量。为了**类比推理**来弄清楚，男人和女人的关系是国王和什么的关系，你可以尝试找到单词`w`，使这个等式成立：{% mathjax %}e_{\text{man}} - e_{\text{woman}} \approx e_{\text{king}} - e_{\text{?}} {% endmathjax %}，所以你想要找到一个单词，使{% mathjax %}w:\;\arg \underset{w}{\max} \text{sim}(e_w, e_{\text{king}} - e_{\text{man}} + e_{\text{woman}}){% endmathjax %}的相似度最大化。有一些合适的相似度函数来衡量某个单词`w`的嵌入与右边的相似度。然后找到使相似度最大化的单词。值得注意的是，这确实有效。如果学习一组**单词嵌入**并找到一个使这种相似度最大化的单词`w`，可以得到完全正确的答案。只有当它正确猜出单词时，类比才算正确。只有在这种情况下，选出了单词`“queen”`。`t-SAE`所做的是获取`300`维数据，并以非线性的方式将其映射到`2D`空间。`t-SAE`学习的映射是一种非常复杂且非线性的映射。实际上，在这个`300`维空间中，在通过`t-SAE`映射后，平行四边形关系可能成立，但在大多数情况下，由于`t-SAE`的非线性映射，不应指望这一点。 `t-SAE`会破坏许多平行四边形类比关系。最常用的**相似性函数**称为**余弦相似性**。在余弦相似性中，你将两个向量{% mathjax %}u{% endmathjax %}和 {% mathjax %}v{% endmathjax %}之间的相似性定义为{% mathjax %}u{% endmathjax %}转置{% mathjax %}v{% endmathjax %}除以长度再除以欧几里得长度({% mathjax %}\text{sim}(e_w, e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})\;,\; \text{sim}(u,v) = \frac{u^{\mathsf{T}}v}{\|u\|_2\|v\|_2}{% endmathjax %})。所以现在忽略分母，这基本上是{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}之间的内积。所以如果{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}非常相似，它们的内积就会很大。这被称为**余弦相似性**，因为这实际上是两个向量{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}之间角度的**余弦**。这就是角度{% mathjax %}\phi{% endmathjax %}，所以这个公式是它们之间角度的**余弦**({% mathjax %}\cos\phi{% endmathjax %})。如果它们之间的角度为`0`，则余弦相似度等于`1`。如果它们的角度是`90`度，余弦相似度就是`0`。如果它们的角度是`180`度，**余弦相似度**最终就是`-1`。它对这些**类比推理任务**非常有效。你也可以使用平方距离或欧几里得距离，{% mathjax %}\|u-v\|^2{% endmathjax %}。从技术上讲，这将是对差异的度量，而不是对**相似度**的度量。所以需要取它的负数。它们之间的主要区别在于它如何规范向量{% mathjax %}u{% endmathjax %}和{% mathjax %}v{% endmathjax %}的长度。**词嵌入**的一个显着结果是它们可以学习**类比关系**的普遍性。例如，它可以学习男人和女人的关系就像男孩和女孩的关系，因为男人和女人之间的向量差异，类似于国王和王后以及男孩和女孩，只是性别。它可以学习到渥太华是加拿大的首都，渥太华之于加拿大相当于内罗毕之于肯尼亚。这就是城市首都之于国家名称。它可以学习到大之于更大，高之于更高，它可以学习诸如此类的东西。日元之于日本，因为日元是日本的货币，卢布之于俄罗斯。所有这些事情都可以通过在大型文本语料库上运行**词嵌入**学习算法来学习。

当你实现一个算法来学习**词嵌入**时，最终会学习一个**嵌入矩阵**。假设使用`10,000`个单词的**词汇表**。词汇表中有`A、Aaron、Orange、Zulu`，也许还有未知单词作为标记。学习嵌入矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，它是一个{% mathjax %}300\times 10000{% endmathjax %}的矩阵，如果你有`10,000`个单词的词汇表，`10,001`是未知单词标记，那么就会有一个额外的标记。`Orange`是`10,000`个单词的词汇表中的第`6257`个单词。符号{% mathjax %}o_{6257}{% endmathjax %}是一个`1-hot`向量，只有位置`6257`处是`1`，其它都是`0`。它应该和左边的嵌入矩阵一样高。如果**嵌入矩阵**{% mathjax %}\mathbf{E}\cdot \mathbf{o}_{6257}{% endmathjax %}，如果将{% mathjax %}\mathbf{E}{% endmathjax %}乘以`1-hot`向量，再乘以`6257`中的`0`，那么这是一个`300`维的向量。{% mathjax %}\mathbf{E}{% endmathjax %}是{% mathjax %}300\times 10000{% endmathjax %}，{% mathjax %}10000\times 1{% endmathjax %}。乘积是{% mathjax %}300\times 1{% endmathjax %}，要计算这个`300`维向量的第一个元素，您要做的就是将矩阵{% mathjax %}\mathbf{E}{% endmathjax %}的第一行与其相乘。但是除了元素`6257`之外，其它元素都是`0`，最终你会得到第一个元素，即上面的`Orange`列下的元素。然后，计算`300`维向量的第二个元素，你需要取向量{% mathjax %}o_{6257}{% endmathjax %}与矩阵{% mathjax %}\mathbf{E}{% endmathjax %}的第二行相乘。同样，依此类推。这就是为什么**嵌入矩阵**{% mathjax %}\mathbf{E}{% endmathjax %}乘以这个`1-hot`向量选出这个对应于单词`Orange`的`300`维列。这等于{% mathjax %}\mathbf{E}\cdot o_{6257} = e_{6257}{% endmathjax %}，这是用来表示单词`Orange`的`300`个一维向量的**嵌入向量**的符号。

#### 单词嵌入: Word2vec & GloVe

假设正在构建一个语言模型，并且使用**神经网络**来实现。在训练期间，您可能希望神经网络执行一些操作，例如输入：`I want a glass of orange`，然后预测序列中的下一个单词。构建神经语言模型是学习一组嵌入的方法。可以这样构建神经网络来预测序列中的下一个单词。拿一个单词列表，`I want a glass of orange`，让我们从第一个单词`I`开始。构建一个与单词`I`相对应的加法向量。在位置`4343`处有一个值为`1`的加法向量。这将是`10,000`维的向量。得到一个参数矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，然后用{% mathjax %}\mathbf{E}\cdot O = e_{4343}{% endmathjax %}。然后对所有其他单词执行相同的操作。单词`want`是位置为`9665`的一个加向量，乘以{% mathjax %}\mathbf{E}{% endmathjax %}得到**嵌入向量**。对于其他单词也是如此。`A`是字典中的第一个单词。对于这个短语中的其他单词也是如此。现在你有一堆三维嵌入，每个都是`300`维的嵌入向量。将它们全部填充到神经网络中。然后神经网络会将数据馈送到`softmax`，它也有自己的参数。`softmax`会在词汇表`10,000`个可能输出中对预测的单词进行分类。如果在训练中看到`juice`这个词，那么训练中`softmax`的目标就是预测后面的另一个单词`juice`。这个隐藏的名称会有自己的参数。将其称为{% mathjax %}w^{<1>}{% endmathjax %}，还有{% mathjax %}b^{<1>}{% endmathjax %}。`softmax`有自己的参数{% mathjax %}w^{<2>},b^{<2>}{% endmathjax %}，它们使用`300`维词嵌入，这里有六个单词。所以，这是{% mathjax %}6\times 300{% endmathjax %}。这一层将是一个`1,800`维向量，它通过将六个嵌入向量堆叠在一起而获得。实际上，更常见的做法是使用固定的历史窗口。例如，您可能根据前四个单词来预测下一个单词，其中`4`是算法的超参数。如果您始终使用四个单词的历史记录，意味着神经网络将输入一个`1,200`维**特征向量**，进入这一层，然后使用`softmax`预测输出。使用固定的历史记录意味着可以处理任意长的句子，因为输入大小始终是固定的。因此，这个模型的参数将是这个矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，并且对所有单词使用相同的矩阵{% mathjax %}\mathbf{E}{% endmathjax %}。对于程序中的四个单词，不需要为不同的位置使用不同的矩阵，而是使用相同的矩阵{% mathjax %}\mathbf{E}{% endmathjax %}。这些权重也是算法的参数，可以使用裁剪来执行**梯度下降**，以最大化训练集重复预测给定序列中的四个单词的可能性，即文本语料库中的下一个单词是什么？该算法将学习到不错的**词向量**。如果还记得我们的橙汁、苹果汁示例，那么算法会激励学习橙汁和苹果的相似的**词向量**，因为可以使其更好地适应训练集，因为它有时会看到橙汁，有时会看到苹果汁，如果只有一个`300`维特征向量来表示所有这些单词，算法会发现它最适合训练集。如果是苹果、橙子、葡萄、梨等等，也许还有榴莲，这是一种非常稀有的水果，具有相似的特征向量。所以，这是学习**词向量**的早期且非常成功的算法之一，用于学习这个矩阵{% mathjax %}\mathbf{E}{% endmathjax %}。但现在让我们概括一下这个算法，看看如何推导出更简单的算法。我想用一个更复杂的句子作为例子来说明其他算法。假设在你的训练集中，你有这个较长的句子，我想要一杯橙汁来搭配我的麦片。算法的工作是预测一些词，我们将其称为**目标词**，并为其提供一些**上下文**，即最后四个词。所以，如果你的目标是**嵌入**。如果它要建立一个语言模型，那么上下文自然是**目标词**之前的几个词。但是如果你的目标不是学习语言模型本身，那么你可以选择其他上下文。例如，你可以提出一个学习问题，其中上下文是左右四个单词。因此，你可以将左右四个单词作为上下文，意味着我们提出一个学习问题，其中算法在左侧有四个单词。`a glass of orange`，右边有四个单词，你将左侧四个单词和右侧四个单词的嵌入输入到神经网络中，预测中间的单词，将其放在中间的**目标单词**，这也可以用于学习**单词嵌入**。或者如果你想使用更简单的上下文，你只使用最后一个单词。那么只给出单词`orange`，那么`orange`后面是什么？

##### Word2vec

上下文不是总是最后四个单词或紧接在目标单词之前的最后一个单词，而是随机选择一个单词作为上下文单词。假设我们选择了单词`orange`。随机选择某个窗口中的另一个单词。比如说，加上上下文单词的五个单词或加上上下文单词的十个单词，选择它作为**目标单词**。或者可能在之前选择两个词。目标可能是`glass`，或者偶然选择了`my`这个单词作为目标单词。给定**上下文词**，预测一个随机选择的单词是什么，比如说，在输入上下文词的正负十个单词窗口内，或者正负五或十个单词窗口内。显然，这不是一个非常简单的学习问题，因为在`orange`这个单词的正负十个单词内，可能有很多不同的单词。但是监督学习问题的目标不是在监督学习问题本身上做得很好，而是想用这个学习问题来学习好的**词嵌入**。。假设使用`10,000`个单词的**词汇表**。但是要解决的监督学习问题是学习从某个上下文{% mathjax %}c\rightarrow t{% endmathjax %}（例如单词`orange`）到某个目标（称为{% mathjax %}t{% endmathjax %}）的映射，该目标可能是单词`juice`、`glass` 或`my`。在词汇表中，`orange`单词是`6257`，而单词`juice`是`10,000`个单词中的单词`4834`。要表示诸如单词`orange`之类的输入，可以从某个`1-hot`向量开始，该向量将写为{% mathjax %}o_c\rightarrow E \rightarrow e_c \rightarrow o\rightarrow \hat{y}{% endmathjax %}，上下文单词有一个`1-hot`向量。可以取嵌入矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，{% mathjax %}\mathbf{E}\times o_c{% endmathjax %}提供了输入上下文单词的**嵌入向量**，因此{% mathjax %}e_c= E_{o_c}{% endmathjax %}。然后在这个新网络中，我们将取这个向量{% mathjax %}e_c{% endmathjax %}并将其馈送到`softmax`单元。将`softmax`单元绘制为神经网络中的节点。这是`softmax`单元{% mathjax %}\text{Softmax}: \mathbf{P}(t|c) = \frac{e^{o^T_t e_c}}{\sum_{j=1}^{10000}e^{o^T_j e_c}}{% endmathjax %}。最后，`softmax`的损失函数将是常用的。使用{% mathjax %}y{% endmathjax %}来表示目标单词。对{% mathjax %}\hat{y}{% endmathjax %}和{% mathjax %}y{% endmathjax %}使用`1-hot`表示。那么损失将是**负对数似然**，{% mathjax %}\mathcal{L}(\hat{y},y) = -\sum_{i=1}^{10000}y_i\log\hat{y}_i{% endmathjax %}。这是`softmax`的损失，将目标{% mathjax %}y{% endmathjax %}表示为`1-hot`向量。这将是一个{% mathjax %}\begin{bmatrix}1 \\ 1 \\ \vdots\end{bmatrix}{% endmathjax %}`1-hot`向量。如果目标单词是`juice`，那么它将是上面的元素`4834`。它等于`1`，其余的将等于`0`。{% mathjax %}y{% endmathjax %}将是`softmax`单元输出的`10,000`维向量，其中包含所有`10,000`个可能目标词的概率：{% mathjax %}\mathbf{P}(t|c) = \frac{e^{\theta^T_t e_c}}{\sum_{j=1}^{10000}e^{\theta^T_j e_c}}{% endmathjax %}。总结一下，这是一个小型神经网络。矩阵{% mathjax %}\mathbf{E}{% endmathjax %}将有很多参数，矩阵{% mathjax %}\mathbf{E}{% endmathjax %}具有与这些**嵌入向量**相对应的参数，{% mathjax %}e_c{% endmathjax %}。`softmax`单元还有{% mathjax %}\theta^{\mathsf{T}},b{% endmathjax %}参数，但通过所有这些参数优化这个损失函数，会得到一组不错的嵌入向量。所以这被称为`skip-gram`模型，它将一个单词（如`orange`）作为输入，然后尝试预测一些跳过左侧或右侧几个单词的单词。预测上下文单词之前和之后的单词。事实证明使用这种算法存在一些问题。主要问题是**计算速度**。特别是对于`softmax`模型，每次想要评估这个概率时，需要对词汇表中`10,000`个单词进行求和。也许`10,000`个还不错，但如果使用的词汇量为`100,000`或`1,000,000`，那么每次对这个分母求和就会变得非常慢。事实上`10,000`已经很慢了，但这会使扩展到更大的词汇量变得更加困难。对此有几种解决方案，文献中提到的一种是使用分层`softmax`分类器。不必一次性将某个词归类到`10,000`个词中。想象一下，如果您有一个**分类器**，它会告诉目标词是否在词汇表的前`5,000`个词中？还是在词汇表的后`5,000`个词中？假设这个分类器告诉您这是在词汇表的前`5,000`个词中，那么第二个分类器会告诉您这是在词汇表的前`2,500`个词中，还是在词汇表的后`2,500`个词中，依此类推。直到最终您开始对它进行分类，这样这棵树的叶子，以及像这样的**分类器树**，树的每个检索器节点都只是一个**绑定分类器**。您无需对全部`10,000`个单词求和，否则它将无法进行单一分类。事实上，像这样的计算分类树的扩展方式与词汇量成**对数关系**，而不是与词汇量成**线性关系**。这被称为分层`softmax`分类器。在实践中，分层`softmax`分类器不使用**完全平衡的树**或**完全对称的树**，每个分支的左侧和右侧的单词数量相等。在实践中，可以开发分层软件分类器，以便将常用词放在顶部，而将不太常见的词埋在树中更深的地方。可能只需要几次遍历就可以找到像`the`和`of`这样的常用词。很少看到像`durian`这样的不太常见的词，可以将它们埋在树的深处。有各种启发式方法可用于构建树，就像构建分层软件尖塔一样。加速`softmax`分类。对于加速`softmax`分类器和需要对分母中的整个上限大小求和的问题也非常有效。

##### GloVe

`GloVe`算法是由`Jeffrey Pennington、Richard Socher`和`Chris Manning`创建的。`GloVe`用于词表示的全局向量。通过选择文本语料库中彼此接近的两个单词来采样**单词对**、**上下文单词**和**目标单词**。首先假设 {% mathjax %}x_{ij}{% endmathjax %}是单词{% mathjax %}i{% endmathjax %}在{% mathjax %}j{% endmathjax %}的上下文中出现的次数。因此，这里{% mathjax %}i{% endmathjax %}和{% mathjax %}j{% endmathjax %}扮演{% mathjax %}t{% endmathjax %}和{% mathjax %}c{% endmathjax %}的角色，可以将{% mathjax %}x_{ij}{% endmathjax %}视为{% mathjax %}x_{tc}{% endmathjax %}。计算一下单词{% mathjax %}i{% endmathjax %}在不同单词{% mathjax %}j{% endmathjax %}的上下文中出现了多少个单词。单词{% mathjax %}t{% endmathjax %}在不同单词{% mathjax %}c{% endmathjax %}的上下文中出现了多少次。根据上下文和目标词的定义，会让{% mathjax %}x_{ij} = x_{ji}{% endmathjax %}。根据上下文和目标词是否出现在彼此的`10`个单词范围内来定义对称关系。虽然，如果你选择的上下文是目标词之前的单词，那么{% mathjax %}x_{ij}{% endmathjax %}和{% mathjax %}x_{ij}{% endmathjax %}可能不对称。但对于`GloVe`算法来说，可以将上下文和目标词定义为这两个单词是否出现在彼此的近距离内，比如说彼此的`10`个单词范围内。{% mathjax %}x_{ij}{% endmathjax %}是一个计数，它捕获了单词{% mathjax %}i{% endmathjax %}和{% mathjax %}j{% endmathjax %}出现在一起或彼此接近的**频率**。`GloVe`模型优化了以下内容。最小化{% mathjax %}\text{minimize}\;\sum_{i=1}^{10000}\sum_{j=1}^{10000}f(x_{ij})(\theta^T_i e_j + b_i + b'_j- \log x_{ij})^2{% endmathjax %}。单词{% mathjax %}t{% endmathjax %}和{% mathjax %}c{% endmathjax %}有多相关？单词{% mathjax %}i{% endmathjax %}和{% mathjax %}j{% endmathjax %}有多相关，以它们彼此出现的频率来衡量？受这个{% mathjax %}x_{ij}{% endmathjax %}的影响。我们要做的就是使用**梯度下降法**求解参数{% mathjax %}\theta{% endmathjax %}和{% mathjax %}e{% endmathjax %}，以最小化{% mathjax %}i{% endmathjax %}之和等于{% mathjax %}1,\ldots,10000{% endmathjax %}和{% mathjax %}j{% endmathjax %}之和等于{% mathjax %}1,\ldots,10000{% endmathjax %}之间的差值。所以你只需要学习向量，这样它们的最终乘积就可以很好地预测这两个词一起出现的频率。如果{% mathjax %}x_{ij} = 0{% endmathjax %}，那么`0`的对数是未定义的，是负无穷大。因此，我们要做的就是对{% mathjax %}x_{ij} = 0{% endmathjax %}的项求和。我们要做的就是添加一个额外的加权项。如果{% mathjax %}x_{ij} = 0{% endmathjax %}。我们将使用{% mathjax %}0\log 0 = 0{% endmathjax %}的约定。意味着，如果{% mathjax %}x_{ij} = 0{% endmathjax %}，就不必对{% mathjax %}x_{ij}{% endmathjax %}对求和。因此，这个对数是不相关的。意味着求和仅对在该上下文-目标关系中至少同时出现过一次的单词对求和。{% mathjax %}f(x_{ij}){% endmathjax %}所做的另一件事是，有些词在英语中出现的频率很高，例如`this、is、of、a`等等。但实际上，在常用词和不常用词之间存在一个连续体。还有一些不常用词，如`durian`，实际上仍然要考虑这些词，但它们的出现频率不如常用词高。因此，加权因子可以是一个函数，它为不常用词（如`durian`）提供有意义的计算量，并为诸如`this、is、of、a`之类的词提供更多权重，但不会过大，因为这些词在语言中似乎已经消失了。因此，有各种启发式方法来选择这个加权函数{% mathjax %}f{% endmathjax %}，要么给这些词过多的权重，要么给不常用词过少的权重。如果你想了解如何选择{% mathjax %}f{% endmathjax %}作为启发式方法来实现这一点，你可以看看`GloVe`论文。最后，这个算法的一个有趣之处在于，{% mathjax %}\theta{% endmathjax %}和{% mathjax %}e{% endmathjax %}的角色现在完全对称了。因此，{% mathjax %}\theta_i{% endmathjax %}和{% mathjax %}e_j{% endmathjax %}是对称的，如果你从数学角度看，它们扮演的角色几乎相同，你可以将它们反转或排序，它们最终会得到相同的优化目标。训练算法的一种方法是围绕**梯度下降**均匀地初始化{% mathjax %}\theta{% endmathjax %}和{% mathjax %}e{% endmathjax %}，以最小化其目标，然后在每个单词完成后取平均值。对于给定的单词{% mathjax %}w{% endmathjax %}，最终的{% mathjax %}e{% endmathjax %}等于通过**梯度下降法**训练的**词向量**加上通过**梯度下降法**训练的{% mathjax %}e_w^{\text{final}} = \frac{e_w + \theta_w}{2}{% endmathjax %}，因为在这个特定的公式中，{% mathjax %}\theta{% endmathjax %}和{% mathjax %}e{% endmathjax %}起着对称的作用，不像早期模型，{% mathjax %}\theta{% endmathjax %}和{% mathjax %}e{% endmathjax %}实际上起着不同的作用，不能像这样取平均值。这就是`GloVe`算法。我认为这个算法的一个令人困惑的部分是，如果你看一下这个等式，它似乎太简单了。怎么可能仅仅通过最小化平方**成本函数**就可以学习有意义的**词向量**呢？但事实证明这是可行的。发明者最终得到这个算法的方式是，他们建立在更复杂的算法的历史之上，比如较新的语言模型，后来出现了`Word2Vec skip-gram`模型，然后是这个。希望简化所有早期的算法。在结束对**词向量算法**的讨论之前，我们应该简要讨论一下它们的另一个属性。也许**嵌入向量**的第一个组件表示性别，第二个组件表示皇室的等级，然后是年龄，然后是它是否是食物，等等。”但是，当您使用我们所见过的算法之一（例如`GloVe`算法）**学习词嵌入**时，会发生什么，您无法保证嵌入的各个组件是可解释的。为什么会这样？假设在某个空间中，第一个轴是性别，第二个轴是皇室。您可以做的是保证**嵌入向量**的第一个轴与这个意义轴（性别、皇室、年龄和食物）对齐。特别是，学习算法可能会选择这个作为第一维的轴。因此，给定一个单词上下文，那么第一个维度可能是这个轴，第二个维度可能是这个。甚至可能不正交，也许它会是第二个非正交轴，可能是实际学习的**词向量**的第二个组成部分。当我们看到这一点时，如果你对线性代数有理解，如果存在某个可逆矩阵{% mathjax %}A{% endmathjax %}，那么它可以很容易地替换为{% mathjax %}(A \theta_i)^{\mathsf{T}}(A^{-\mathsf{T}}e_j) = \theta_i^{\mathsf{T}}A^{\mathsf{T}}A^{-\mathsf{T}}e_j{% endmathjax %}。中间项抵消了，我们剩下{% mathjax %}\theta_i^{\mathsf{T}}e_j{% endmathjax %}，与之前相同。具体来说，第一个特征可能是性别、皇室、年龄、食物、成本、大小、名词还是动词以及所有其他特征的组合。查看嵌入矩阵的各个组件、各个行并为其分配人类解释非常困难。尽管存在这种线性变换，在描述类比时计算出的平行四边形图仍然有效。

#### 词嵌入应用

##### 情感分类

**情感分类**是一项任务，即查看一段文本并判断某人是喜欢还是不喜欢他们正在谈论的内容。它是`NLP`中最重要的构建块之一，并用于许多应用程序。**情感分类**的挑战之一是没有大量的标签训练集。但是使用**词嵌入**，即使只有中等大小的标签训练集，您也可以构建良好的**情感分类器**。这是一个**情感分类问题**的示例。输入{% mathjax %}x{% endmathjax %}是一段文本，您想要预测输出{% mathjax %}y{% endmathjax %}的情感，例如餐厅评论的星级。如果有人说“甜点很棒”，他们会给出四星评价，“服务很慢”给出两星评价，“适合快餐但没什么特别的”给出三星评价。这是一个相当苛刻的评论，“完全缺乏良好的品味，良好的服务和良好的氛围。”这是一个一星评论。因此，如果您可以训练一个系统，根据这样的标签数据集从{% mathjax %}x{% endmathjax %}或{% mathjax %}y{% endmathjax %}进行映射，那么您就可以使用它来监控人们对经营的餐厅的评论。人们可能还会在社交媒体、`X、Facebook、Instagram` 或其他形式的社交媒体上发布有关餐厅的消息。如果有一个**情感分类器**，它们可以只看一段文本，就能判断出发帖人对餐厅的情绪是积极还是消极。还可以跟踪是否存在任何问题，或者餐厅随着时间的推移是变好还是变坏。**情感分类**的挑战之一是没有庞大的**标签数据集**。对于**情感分类任务**，包含`10,000`到`100,000`个单词的训练集并不罕见。有时，甚至小于`10,000`个单词，可以使用的**词嵌入**帮助您更好地理解，尤其是当您有一个小型训练集时。这就是可以做的事情。

您可以取一个句子，例如`“dessert is Excellent”`，然后在字典中查找这些单词。我们像往常一样使用`10,000`词的字典。让我们构建一个分类器，将其映射到输出{% mathjax %}y{% endmathjax %}，即这是四颗星。给定这四个词，像往常一样，可以取这四个词并查找`1-hot`向量{% mathjax %}o_{8928}\rightarrow e_{8928}\rightarrow \text{Avg}\rightarrow \underset{\text{softmax}}{O}\rightarrow \hat{y}{% endmathjax %}，这是一个`1-hot`向量乘以嵌入矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，它可以从更大的文本语料库中学习。可以从十亿个单词或一千亿个单词中学习嵌入，并使用它提取单词`“the”`的**嵌入向量**，然后对`“dessert”`执行相同的操作，对`“is”`执行相同的操作，对`“excellent”`执行相同的操作。如果这是在一个非常大的数据集上训练的，比如一千亿个单词，那么你可以从不常用的单词中获取大量知识，并将它们应用到你的问题中，即使这些单词不在你的标记训练集中。现在这里有一种构建分类器的方法，你可以取这些向量，假设它们是`300`维向量，然后你可以对它们求和或求平均值。我在这里放一个更大的平均值运算符，你可以使用求和或平均值。这会给你一个`300`维的**特征向量**，然后你将其传递给`soft-max`分类器，然后输出{% mathjax %}\hat{y}{% endmathjax %}。`softmax`可以输出从一星到五星的五种可能结果的**概率**。这将是预测{% mathjax %}y{% endmathjax %}的五种结果的组合。通过使用此处的平均运算，此算法适用于短评论或长评论，因为即使评论只有`100`个字，您也可以对所有`100`个字的所有**特征向量**求和或平均，这样您就可以得到一个表示，一个`300`维的**特征表示**，然后您可以将其传递到**情感分类器**中。因此，这个平均值会相当好用。它所做的是真正平均所有单词的含义或对示例中所有单词的含义求和。此算法的问题之一是它忽略了**词序**。特别是，这是一条非常负面的评论，“完全缺乏良好的品味、良好的服务和良好的氛围”。但“好”这个词出现了很多次。如果你使用这样的算法，忽略单词顺序，只对不同单词的所有嵌入进行求和或平均，那么最终的**特征向量**中就会有很多“好”的表示，你的分类器可能会认为这是一个好的评价，即使这实际上非常苛刻。这是一个一星评价。所以这里有一个更复杂的模型，它不是简单地对所有单词嵌入进行求和，您可以改用`RNN`进行**情感分类**。您可以查看该评论“完全缺乏良好的品味、良好的服务和良好的氛围”，然后为每个评论找到一个`1-hot`向量。所以我将跳过`1-hot`向量表示，而是取`1-hot`向量，像往常一样将其乘以嵌入矩阵{% mathjax %}\mathbf{E}{% endmathjax %}，然后这会为您提供**嵌入向量**，然可以将它们输入`RNN`。`RNN`的工作是计算最后一个时间步的表示，以便预测{% mathjax %}\hat{y}{% endmathjax %}。有了这样的算法，它会更好地考虑单词序列，并意识到“事物缺乏良好的品味”是一个负面评论，“不好”也是一个负面评论，而不像以前的算法那样，只是将所有内容加在一起形成一个大词向量，并没有意识到“不好”与“好”或“缺乏良好的品味”等词的含义的不同。如果你训练这个算法，最终会得到一个不错的**情感分类算法**，而且因为你的词向量可以从更大的数据集中训练，所以它可以更好地推广到你现在训练集中的新词，例如，如果有人说“完全没有好品味、没有好服务和好氛围”之类的话，那么即使“没有”这个词不在你的标签训练集中，如果它在用于训练词向量的`10`亿或`1000`亿个语料库中，它仍然能正确做到这一点，甚至可以更好地推广到用于**训练词向量**的训练集中的单词，但不一定在标签训练集中。

##### 词嵌入去重

人们越来越信任机器学习和人工智能算法，它们可以帮助做出极其重要的决定。我们希望尽可能确保它们没有不良形式的偏见，例如性别偏见、种族偏见等。偏见指性别、种族、性取向偏见。**词向量**可以反映用于训练模型的文本的性别、种族、年龄、性取向和其他偏见。我特别感兴趣的是与社会经济地位有关的偏见。我认为每个人，无论你来自富裕家庭、低收入家庭还是介于两者之间的任何家庭，我都认为每个人都应该拥有巨大的机会。因为机器学习算法被用于做出非常重要的决策。它们影响着一切，从大学录取，到人们找工作的方式，到贷款申请是否被批准，到刑事司法系统量刑指南。学习算法正在做出非常重要的决定，所以我认为我们要尝试改变学习算法，尽可能减少或者消除这些不良偏见。现在就**词嵌入**而言，它们可以拾取用于训练模型的文本偏见。我认为也许对人工智能来说幸运的是有更好的想法来快速减少人工智能中的偏见，而不是快速减少人类中的偏见。虽然我认为我们在人工智能方面的研究还远远没有结束，我们还需要进行大量的研究和艰苦的工作来减少学习算法中的偏见。假设我们已经学习了一个**词嵌入**，所以单词`babysitter`，`doctor`在这里。我们有`grandparents`和`grandgrafts`。单词`girl`，`boy`，`she`，`he`嵌入在那里。所以我们要做的第一件事就是确定我们想要减少或消除的特定偏见所对应的方向。这里将重点介绍性别偏见，在这个例子中，你如何确定偏见相对应的方向？对于性别的情况，我们可以做的是取“`he`”的**嵌入向量**，减去“`she`”的**嵌入向量**，因为这因性别而异。取{% mathjax %}e_{\text{man}} - e_{\text{woman}}{% endmathjax %}，取其中几个差异并取平均值。而这个方向与我们试图解决的特定偏见无关。所以这是无偏见方向。在这种情况下，偏见方向，可以将其视为`1`维子空间，而非偏见方向，将是`299`维子空间。偏差方向可以高于`1`维，而且它不是取平均值，而是使用一种更复杂的算法找到的，称为奇异值分解(`SVU`)。如果你熟悉`pr`，它与**主成分分析算法**类似的思想。下一步是中和步骤。因此，对于每个非定义性的单词，将其投影以消除偏见。有些单词本质上捕捉了性别。像祖母、祖父、女孩、男孩、她、他这样的词，性别在定义中是固有的。而像医生和保姆这样的词，它们是性别中立的。一般的情况下，希望医生或保姆这样的词是种族中立或性取向中立的，但对于每个非定义性的单词，这基本上意味着不像祖母和祖父这样的词，它们确实具有性别成分，根据定义，祖母是女性，而祖父是男性。对于像医生和保姆这样的词，只需将它们投影到这个轴上，在偏见方向上减少它们的成分，或消除它们的成分。在水平方向上减少它们的分量。最后一步称为**均衡**，例如祖母和祖父，或女孩和男孩，它们嵌入中的唯一差异是性别。在这个例子中，保姆和祖母之间的距离或相似度实际上小于保姆和祖父之间的距离。因此，这可能强化了一种不健康的或可能不受欢迎的偏见，即祖母最终比祖父更愿意照顾孩子。因此，在最后的均衡步骤中，我们希望确保像祖母和祖父这样的词与性别中立的词（例如保姆或医生）具有完全相同的相似度或完全相同的距离。因此，有几个线性代数步骤可以实现这一点。但它基本上会将祖母和祖父移动到与中间轴等距的一对点上。这样做的效果是，`babysitter`与这两个词之间的距离将完全相同。所以，比如祖母-祖父、男孩-女孩、姐妹会-兄弟会、女孩-男孩、姐妹-兄弟、侄女-侄子、女儿-儿子，希望通过这个**均衡步骤**来实现。你如何决定要中和哪个词？例如，单词`doctor`似乎是一个应该中和的词，以使其不具有性别或种族特征。而单词`grandsitter`和`grandparents`不应该具有性别特征。还有像`beard`这样的词，这只是一个统计，男性比女性更有可能有胡须，所以也许`beards`应该更接近`male`而不是`female`。因此，找出哪些词是定义性的，哪些词应该是性别特定的，哪些词不应该是。英语中的大多数词都不是定义性的，意味着性别不是定义的一部分。而且，像这样的词子集相对较小，例如祖母-祖父、女孩-男孩、姐妹会-兄弟会等，不应该被中和。**线性分类器**可以告诉您哪些词需要通过**中和步骤**来投射出这种偏差方向，将其投射到`299`维的子空间上。