---
title: 深度卷积神经网络(AlexNet) (TensorFlow)
date: 2024-05-15 17:24:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

在`LeNet`提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。这是因为虽然`LeNet`在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。事实上，在上世纪`90`年代初到`2012`年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机(`support vector machines`)。在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。
<!-- more -->
虽然上世纪`90`年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。因此，与训练端到端（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：
- 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就`100万`像素）。
- 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。
- 通过标准的特征提取算法，如`SIFT`（尺度不变特征变换）(`Lowe, 2004`)和`SURF`（加速鲁棒特征）或其他手动调整的流水线来输入数据。
- 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。

当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实—推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。
#### 学习表征

另一种预测这个领域发展的方法—观察图像特征的提取方法。在`2012`年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。`SIFT`、`SURF`、`HOG`（定向梯度直方图）、`bags of visual words`和类似的特征提取方法占据了主导地位。有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。下图是从`AlexNet论文 (Krizhevsky et al., 2012)`复制的，描述了底层图像特征。
{% asset_img alex_1.png "AlexNet第一层学习到的特征抽取器" %}

`AlexNet`的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。**深度卷积神经网络**的突破出现在`2012`年。突破可归因于两个关键因素。