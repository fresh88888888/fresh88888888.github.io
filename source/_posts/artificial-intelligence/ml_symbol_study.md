---
title: 数学符号&名词解释（机器学习）
date: 2024-04-19 17:32:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 引言

为了解决各种各样的机器学习问题，深度学习提供了强大的工具。虽然许多深度学习方法都是最近才有重大突破，但使用数据和神经网络编程的核心思想已经研究了几个世纪。事实上，人类长期以来就有分析数据和预测未来结果的愿望，而自然科学大部分都植根于此。例如，伯努利分布是以雅各布•伯努利（`1654-1705`）命名的。而高斯分布是由卡尔•弗里德里希•高斯（`1777-1855`）发现的，他发明了最小均方算法，至今仍用于解决从保险计算到医疗诊断的许多问题。这些工具算法催生了自然科学中的一种实验方法—例如，电阻中电流和电压的欧姆定律可以用线性模型完美地描述。
<!-- more -->
机器学习的关键组件：
- 可以用来学习的数据（`data`）。
- 如何转换数据的模型（`model`），我觉得也可以称为结构体。
- 一个目标函数（`objective function`）或者叫评估指标，用来量化模型的有效性。
- 调整模型参数以优化目标函数的算法（`algorithm`）。

#### 数学符号

##### 数字

|符号|描述|
|:--|:--|
|{% mathjax %} x{% endmathjax %}|标量|
|{% mathjax %} \mathrm {x}{% endmathjax %}|向量|
|{% mathjax %} \mathbf {X}{% endmathjax %}|矩阵|
|{% mathjax %} \mathsf {X}{% endmathjax %}|张量|
|{% mathjax %} \mathbf {I}{% endmathjax %}|单位矩阵|
|{% mathjax %} x_i, \left [ \mathrm {x}_i \right ]{% endmathjax %}|向量{% mathjax %} x{% endmathjax %}第{% mathjax %} i{% endmathjax %}个元素|
|{% mathjax %} x_{ij}, \left [ \mathbf {X}_{ij} \right ]{% endmathjax %}|矩阵{% mathjax %} \mathbf {X}{% endmathjax %}第{% mathjax %} i{% endmathjax %}行第{% mathjax %} j{% endmathjax %}列的元素|

##### 集合论

|符号|描述|
|:--|:--|
|{% mathjax %} \chi{% endmathjax %}|集合|
|{% mathjax %} \mathbb{Z}{% endmathjax %}|整数集合|
|{% mathjax %} \mathbb{R}{% endmathjax %}|实数集合|
|{% mathjax %} \mathbb{R}^n{% endmathjax %}|{% mathjax %} n{% endmathjax %}维实数向量集合|
|{% mathjax %} \mathbb{R}^{a\times b}{% endmathjax %}|包含{% mathjax %} a{% endmathjax %}行和{% mathjax %} b{% endmathjax %}列的实数矩阵集合|
|{% mathjax %} A\cup B{% endmathjax %}|集合{% mathjax %} A{% endmathjax %}和{% mathjax %} B{% endmathjax %}的并集|
|{% mathjax %} A\cap B{% endmathjax %}|集合{% mathjax %} A{% endmathjax %}和{% mathjax %} B{% endmathjax %}的交集|
|{% mathjax %} A\\ B{% endmathjax %}|集合{% mathjax %} A{% endmathjax %}与集合{% mathjax %} B{% endmathjax %}相减，{% mathjax %} B{% endmathjax %}关于{% mathjax %} A{% endmathjax %}的相对补集|
##### 函数与运算符

|符号|描述|
|:--|:--|
|{% mathjax %} f(\cdot){% endmathjax %}|函数|
|{% mathjax %} log(\cdot){% endmathjax %}|自然对数|
|{% mathjax %} exp(\cdot){% endmathjax %}|指数函数|
|{% mathjax %} 1_x{% endmathjax %}|指示函数|
|{% mathjax %} (\cdot)^T{% endmathjax %}|向量或矩阵的转置|
|{% mathjax %} \mathbf {X^{-1}}{% endmathjax %}|矩阵的逆|
|{% mathjax %} \odot{% endmathjax %}|按元素相乘|
|{% mathjax %} [\cdot ,\cdot]{% endmathjax %}|连结|
|{% mathjax %} \mid \chi\mid{% endmathjax %}|集合的基数|
|{% mathjax %} \parallel \cdot\parallel_p{% endmathjax %}|{% mathjax %} L_p{% endmathjax %}正则|
|{% mathjax %} \parallel \cdot\parallel{% endmathjax %}|{% mathjax %} L_2{% endmathjax %}正则|
|{% mathjax %} \langle x, y\rangle{% endmathjax %}|向量{% mathjax %} x(\cdot){% endmathjax %}和{% mathjax %} y{% endmathjax %}的点积|
|{% mathjax %} \sum{% endmathjax %}|连加|
|{% mathjax %} \prod{% endmathjax %}|连乘|
|{% mathjax %} \stackrel{def}{=}{% endmathjax %}|定义|
##### 微积分

|符号|描述|
|:--|:--|
|{% mathjax %} \frac {dy}{dx}{% endmathjax %}|{% mathjax %} y{% endmathjax %}关于{% mathjax %} x{% endmathjax %}的导数|
|{% mathjax %} \frac {\partial y}{\partial x}{% endmathjax %}|{% mathjax %} y{% endmathjax %}关于{% mathjax %} x{% endmathjax %}的偏导数|
|{% mathjax %} \nabla_xy{% endmathjax %}|{% mathjax %} y{% endmathjax %}关于{% mathjax %} x{% endmathjax %}的梯度|
|{% mathjax %} \int\nolimits_{a}^{b}f(x)dx{% endmathjax %}|{% mathjax %} f{% endmathjax %}在{% mathjax %} a{% endmathjax %}到{% mathjax %} b{% endmathjax %}区间上关于{% mathjax %} x{% endmathjax %}的定积分|
|{% mathjax %} \int f(x)dx{% endmathjax %}|{% mathjax %} f{% endmathjax %}关于{% mathjax %} x{% endmathjax %}的不定积分|

##### 概率与信息论

|符号|描述|
|:--|:--|
|{% mathjax %} P(\cdot){% endmathjax %}|概率分布|
|{% mathjax %} z\sim P{% endmathjax %}|随机变量{% mathjax %} z{% endmathjax %}具有概率分布{% mathjax %} P{% endmathjax %}|
|{% mathjax %} P(X\mid Y){% endmathjax %}|{% mathjax %} X\mid Y{% endmathjax %}的条件概率|
|{% mathjax %} p(x){% endmathjax %}|概率的密度函数|
|{% mathjax %} E_x[f(x)]{% endmathjax %}|函数{% mathjax %} f{% endmathjax %}对{% mathjax %} x{% endmathjax %}的数学期望|
|{% mathjax %} X\bot Y{% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}和{% mathjax %} Y{% endmathjax %}是独立的|
|{% mathjax %} X\bot Y\mid Z{% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}和{% mathjax %} Y{% endmathjax %}在给定随机变量{% mathjax %} Z{% endmathjax %}的条件下是独立的|
|{% mathjax %} Var(X){% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}的方差|
|{% mathjax %} \sigma x{% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}的标准差|
|{% mathjax %} Co\mathrm{v} (X,Y){% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}和{% mathjax %} Y{% endmathjax %}的协方差|
|{% mathjax %} \rho (X,Y){% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}和{% mathjax %} Y{% endmathjax %}的相关性|
|{% mathjax %} H(X){% endmathjax %}|随机变量{% mathjax %} X{% endmathjax %}的熵|
|{% mathjax %} D_{KL}(P\parallel Q){% endmathjax %}|{% mathjax %} P{% endmathjax %}和{% mathjax %} Q{% endmathjax %}的`KL`散度|

##### 复杂度

|符号|描述|
|:--|:--|
|{% mathjax %} O{% endmathjax %}|复杂度标记|

#### 名词解释

##### 数据

毋庸置疑，如果没有数据，那么数据科学好无用武之地。每个数据集由一个个样本组成，大多时候，它们遵循独立和相同分布。样本有时也叫数据点(`data point`)或者叫做数据实例(`data instance`)，通常每个样本由一组称为特征(`feature`，或协变量(`covariates`))的属性组成。机器学习模型会根据这些属性进行预测。要预测的是一个特殊的属性，它被称为标签(`label`，或目标(`target`))。当处理图像数据时，每一张单独的照片即为一个样本，它的特征由每个像素数值的有序列表表示。 比如，{% mathjax %}200\times 200{% endmathjax %}彩色照片由{% mathjax %}200\times 200\times 3{% endmathjax %}个数值组成，其中的“`3`”对应于每个空间位置的红、绿、蓝通道的强度。 再比如，对于一组医疗数据，给定一组标准的特征（如年龄、生命体征和诊断），此数据可以用来尝试预测患者是否会存活。当每个样本的特征类别数量都相同时，其特征向量的长度是固定的，这个长度被称为数据的维数(`dimensionality`)。

然而并不是所有的数据都可以用“固定长度”的向量来表示，以图像数据为例，如果他们全部都来自标准显微镜设备，那么固定长度是可取的，但如果图像数据来自互联网，他们很难具有相同的分辨率和形状。这时将图像裁剪成标准尺寸是一种方法，但这种方法很局限，有丢失信息的风险，此外文本数据跟不符合固定长度的要求。比如，对于亚马逊等电子商务网站上的客户评论，有些文本数据很简短（比如“好极了”），有些则长篇大论。 与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。一般来说，拥有越多数据的时候，工作就越容易。更多的数据可以被用来训练出更强大的模型，从而减少对预先设想假设的依赖。数据集的由小变大为现代深度学习的成功奠定基础。在没有大数据集的情况下许多令人兴奋的深度学习模型黯然失色。就算一些深度学习模型在小数据集上能够工作，但其效能并不比传统方法高。

请注意，仅仅拥有海量数据是不够的，我们还需要正确的数据。如果数据中充满了错误，或者如果数据的特征不能预测任务目标，那么模型很可能无效。有一句古话说得好：“输入的是垃圾，输出的也是垃圾。”，此外，糟糕的预测性能甚至会加倍放大事态的严重性。在一些敏感应用中，如预测性监管、简历筛选和用于信贷的分险模型，我们必须特别警惕垃圾数据带来的后果。一种常见的问题来自不均衡的数据集，比如在一个有关医疗的的数据集中，某些人群没有样本表示。想象一下，我们要训练一个皮肤癌识别模型，但它（在训练数据集中）从未见过黑色皮肤的人群，这个模型就会顿时束手无策。再比如，如果“用过去的招聘决策数据”来训练一个筛选简历的模型，那么机器学习模型就会捕捉到历史残留的不公正，并将其自动化。然而，这一切都可能在不知情的情况下发生。因此，当数据不具有充分代表性，甚至包含了一些社会偏见性时，模型就很有可能有偏见。

##### 模型

大多数机器学习会涉及到数据的转换。比如一个“摄取照片并预测笑脸”的系统，再比如通过摄取到的一组传感器读数预测读数的正常与异常程度。虽然简单的模型能够解决如上简单的问题，但本文中关注到的问题超出了经典方法的极限。深度学习与经典方法的区别主要在于：前者关注的是功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习（`deep learning`）。

##### 目标函数

前面的内容将机器学习介绍为“从经验中学习”。这里所说的“学习”，是指**自主提高模型完成某些任务的效能**。但是，什么才算真正的提高呢？在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，这被称之为**目标函数**（`objective function`）。我们通常定义一个目标函数，并希望优化它到最低点。因为越低越好，所以这些函数有时被称为**损失函数**（`loss function`，或`cost function`）。但这只是一个惯例，我们也可以取一个新的函数，优化到它的最高点。这两个函数本质上是相同的，只是翻转一下符号。当任务在视图预测数值时，最常见的损失函数是**平方误差**(`squared error`)，即预测值与实际值之差的平方。当试图解决分类问题时最常见的损失函数是“**最小化错误率**”，即预测与实际情况不符的样本比例。有些损失函数（平方误差）很容易被优化；有些损失函数（如错误率）由于不可微性或其他复杂性难以直接优化。在这些情况下，通常会优化替代目标。通常损失函数是根据模型参数定义的，并取决于数据集。在一个数据集上，我们可以通过最小化总损失来学习模型参数的最佳值，该数据集由一些为训练而收集的样本组成，称为训练数据集(`training dataset`)。然而，在训练数据上表现良好的模型，并不一定在新数据集上有同样的性能，这里的“新数据集”通常称为**测试数据集**(`test dataset`)。

综上所述，可用数据集通常分成两部分：**训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型**。然后我们观察模型在这两部分数据集上的性能。“一个模型在训练数据集上的性能”可以被想象成“一个学生在模拟考试中的分数”。这个分数用来为一些真正的期末考试做参考，即使成绩令人鼓舞，也不能保证期末考试成功。换言之，测试性能可能会偏离训练性能。当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为过拟合(`overfitting`)的。就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。

##### 优化算法

当我们获得了一些数据及其表示、一个模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。深度学习中，大多数流行的优化算法通常基于一个基本方法—**梯度下降**(`gradient descent`)。简而言之，在每个步骤中，梯度下降法都会检测每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝着哪个方向移动。然后，它在可以减少损失的方向上优化参数。

##### 监督学习

监督学习(`supervised learning`)擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都称为一个样本(example)。有时，即使标签是未知的，样本也可以指代输入特征。我们的目标是生成一个模型能够将任何输入特征映射到标签（即预测）。举一个具体的例子：假设我们需要预测患者的心脏病是否会发作，那么观察结果“心脏病发作”或“心脏病没有发作”将是样本的标签。输入特征可能是生命体征，如心率、舒张压和收缩压等。监督学习之所以能发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实的标签。用概率论术语来说，我们希望预测“估计给定输入特征的标签”的条件概率。虽然监督学习只是几类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都使用了监督学习。这是因为在一定程度上，许多重要任务可以清晰的描述为，在给定一组特定的可用数据的情况下估计未知事物的概率。比如：
- 根据计算机断层扫描(`Computed Tomography，CT`)肿瘤图像，预测是否为癌症。
- 给出一个英语句子，预测正确的法语翻译。
- 根据本月的财务报告数据，预测下个月股票的价格。

监督学习的学习过程一般可以分为三大步骤：
- 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签（例如，患者是否在一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集。
- 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”。
- 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。

综上所述，即使使用简单的描述给定输入特征的预测标签，监督学习也可以采取多种形式的模型，并且需要不同的建模决策，这取决于输入输出的类型、大小和数量。例如，我们使用不同的模型来处理“任意长度的序列”或“固定长度的序列”
###### 回归

**回归**（`regression`）是最简单的监督学习任务之一。假设有一组房屋销售数据表格，其中每行对应一个房子，每列对应一个相关的属性，例如房屋的面积、卧室的数量、浴室的数量以及到镇中心的步行距离，等等。每一行的属性构成了一个房子样本的特征向量。如果一个人住在纽约或旧金山，而且他不是亚马逊、谷歌、微软或Facebook的首席执行官，那么他家的特征向量（房屋面积，卧室数量，浴室数量，步行距离）可能类似于：`[600,1,1,60]`。如果一个人住在匹兹堡，这个特征向量可能更接近`[3000,4,3,10]`…… 当人们在市场上寻找新房子时，可能需要估计一栋房子的公平市场价值。 为什么这个任务可以归类为回归问题呢？本质上是输出决定的。销售价格（即标签）是一个数值。当标签取任意数值时，我们称之为回归问题，此时的目标是生成一个模型，使它的预测非常接近实际标签值。再比如，预测病人在医院的住院时间也是一个回归问题。 总而言之，判断回归问题的一个很好的经验法则是，**任何有关“有多少”的问题很可能就是回归问题**。比如：这个手术需要多少小时？在未来`6`小时，这个镇会有多少降雨量等。然而，以上假设有时并不可取。例如，一些差异是由于两个特征之外的几个因素造成的。 在这些情况下，我们将尝试学习最小化“预测值和实际标签值的差异”的模型。本书大部分章节将关注平方误差损失函数的最小化。
###### 分类

这种“哪一个”的问题叫做**分类**（`classification`）问题。分类问题希望模型能够预测样本属于哪个类别（`category`，正式称为类（`class`））。例如，手写数字可能有10类，标签被设置为数字`0～9`。最简单的分类问题是只有两类，这被称之为**二项分类**（`binomial classification`）。例如，数据集可能由动物图像组成，标签可能是[猫,狗]两类。回归是训练一个**回归函数**来输出一个数值；分类是训练一个**分类器**来输出预测的类别。然而模型怎么判断得出这种“是”或“不是”的硬分类预测呢？我们可以试着用概率语言来理解模型。给定一个样本特征，模型为每个可能的类分配一个概率。比如，之前的猫狗分类例子中，分类器可能会输出图像是猫的概率为`0.9`。`0.9`这个数字表达什么意思呢？可以这样理解：分类器`90%`确定图像描绘的是一只猫。预测类别的概率的大小传达了一种模型的不确定性。当有两个以上的类别时，我们把这个问题称为**多项分类**（`multiclass classification`）问题。与解决回归问题不同，分类问题的常见损失函数被称为**交叉熵**（`cross-entropy`）。

分类可能变得比二项分类、多项分类复杂得多。例如，有一些分类任务的变体可以用于寻找**层次结构**，层次结构假定在许多类之间存在某种关系。因此，并不是所有的错误都是均等的。人们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为**层次分类**(`hierarchical classification`)。早期的一个例子是卡尔·林奈，他对动物进行了层次分类。在动物分类的应用中，把一只狮子狗误认为雪纳瑞可能不会太糟糕。但如果模型将狮子狗与恐龙混淆，就滑稽至极了。层次结构相关性可能取决于模型的使用者计划如何使用模型。例如，响尾蛇和乌梢蛇血缘上可能很接近，但如果把响尾蛇误认为是乌梢蛇可能会是致命的。因为响尾蛇是有毒的，而乌梢蛇是无毒的。
###### 标记问题

有些分类很适合二项分类或多项分类。例如，我们可以训练一个普通的二项分类器来区分猫和狗。运用计算机视觉算法，这个模型可以很轻松的被训练。尽管如此，无论模型有多精确。当分类器遇到新的动物时可能会束手无策。学习预测不相互排斥的类别的问题称为**多标签分类**。举个例子，人们在技术博客上贴的标签，比如“机器学习”“技术”“小工具”“编程语言”“`Linux`”“云计算”“`AWS`”。一篇典型的文章可能会用`5～10`个标签，因为这些概念是相互关联的。关于“云计算”的帖子可能会提到“`AWS`”，而关于“机器学习”的帖子也可能涉及“编程语言”。
###### 搜索

有时，我们不仅仅希望输出一个类别或一个实值。在信息检索领域，我们希望对一组项目进行排序。以网络搜索为例，目标不是简单的查询(`query`)-网页(`page`)分类，而是在海量搜索结果中找到用户最需要的那部分。搜索结果的排序也十分重要，学习算法需要输出有序的元素子集。换句话说，如果要求我们输出字母表中的5个字母，返回“`A、B、C、D、E`”和“`C、A、B、E、D`”是不同的，即使结果是相同的，结果集内的顺序有时却很重要。该问题的一种可能的解决方案：首先为集合中的每个元素分配相应相关性分数，然后检索评级最高的元素。`PageRank`，谷歌搜索引擎背后最初的秘密武器就是这种评分系统的早期例子，但它的奇特之处在于它不依赖于实际的查询。 在这里，他们依靠一个简单的相关性过滤来识别一组相关条目，然后根据`PageRank`对包含查询条件的结果进行排序。如今，搜索引擎使用机器学习和用户行为模型来获取网页相关性得分，很多学术会议也致力于这一主题。
###### 推荐系统

另一类与搜索排名相关的问题是推荐系统（`recommender system`），它的目标是向用户进行“个性化”推荐。例如，对于电影推荐，科幻迷和喜剧爱好者的推荐结果页面可能会有很大不同。类似的应用也会出现在零售产品、音乐和新闻推荐等等。在某些应用中，客户会提供明确反馈，表达他们对特定产品的喜爱程度。例如，亚马逊上的产品评级和评论。在其他一些情况下，客户会提供隐性反馈。例如，某用户跳过播放列表中的某些歌曲，这可能说明这些歌曲对此用户不大合适。总的来说，推荐系统会为“给定用户和物品”的匹配性打分，这个“分数”可能是估计的评级或购买的概率。由此，对于任何给定的用户，推荐系统都可以检索得分最高的对象集，然后将其推荐给用户。以上只是简单的算法，而工业生产的推荐系统要先进得多，它会将详细的用户活动和项目特征考虑在内。

尽管推荐系统具有巨大的应用价值，但单纯用它作为预测模型仍存在一些缺陷。首先，我们的数据只包含“审查后的反馈”：用户更倾向于给他们感觉强烈的事物打分。此外，推荐系统有可能形成反馈循环：推荐系统首先会优先推送一个购买量较大（可能被认为更好）的商品，然而目前用户的购买习惯往往是遵循推荐算法，但学习算法并不总是考虑到这一细节，进而更频繁地被推荐。综上所述，关于如何处理审查、激励和反馈循环的许多问题，都是重要的开放性研究问题。
###### 序列学习

以上大多数问题都具有固定大小的输入和产生固定大小的输出。例如，在预测房价的问题中，我们考虑从一组固定的特征：房屋面积、卧室数量、浴室数量、步行到市中心的时间；图像分类问题中，输入为固定尺寸的图像，输出则为固定数量（有关每一个类别）的预测概率；在这些情况下，模型只会将输入作为生成输出的“原料”，而不会“记住”输入的具体内容。如果输入的样本之间没有任何关系，以上模型可能完美无缺。但是如果输入是连续的，模型可能就需要拥有“记忆”功能。比如，我们该如何处理视频片段呢？在这种情况下，每个视频片段可能由不同数量的帧组成。通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。语言也是如此，机器翻译的输入和输出都为文字序列。再比如，在医学上序列输入和输出就更为重要。设想一下，假设一个模型被用来监控重症监护病人，如果他们在未来24小时内死亡的风险超过某个阈值，这个模型就会发出警报。我们绝不希望抛弃过去每小时有关病人病史的所有信息，而仅根据最近的测量结果做出预测。

这些问题是**序列学习**的实例，是机器学习最令人兴奋的应用之一。序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。具体来说，输入和输出都是可变长度的序列，例如机器翻译和从语音中转录文本。虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提。

**标记和解析**。这涉及到用属性注释文本序列。换句话说，输入和输出的数量基本上是相同的。例如，我们可能想知道动词和主语在哪里，或者可能想知道哪些单词是命名实体。通常，目标是基于结构和语法假设对文本进行分解和注释，以获得一些注释。这听起来比实际情况要复杂得多。下面是一个非常简单的示例，它使用“标记”来注释一个句子，该标记指示哪些单词引用命名实体。标记为“`Ent`”，是实体（`entity`）的简写。
```bash
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```
**自动语音识别**。在语音识别中输入序列是说话人的录音，输出序列是说话人所说内容的文本记录。它的挑战在于，与文本相比，音频帧更多（声音通常是在`8khz`或`16khz`采样）。也就是说，音频很文本之间没有`1:1`的对应关系，因为数千个样本可能对应于一个单独的单词，这也是序列到序列的学习问题，其中输出比输入短得多。

**文本到语音**。这与自动语音识别相反。换句话说，输入是文本，输出是音频文件。在这种情况下，输出比输入长得多。虽然人类很容易判断发音别扭的音频文件，但这对计算机来说并不是那么简单。

**机器翻译**。在语音识别中，输入和输出的出现顺序基本相同。而在机器翻译中，颠倒输入和输出的顺序非常重要。换句话说，虽然那我们仍将一个序列转化为另一个序列，但是输入和输出的数量以及相应的序列顺序大都不会相同。其他学习任务也有序列学习的应用。例如，确定“用户阅读网页的顺序”是二维布局分析问题。再比如，对话问题对序列的学习更为复杂：确定下一轮对话，需要考虑对话历史状态以及现实世界的知识……如上这些都是热门的序列学习研究领域。
##### 无监督学习

##### 与环境互动

##### 强化学习

##### 深度学习