---
title: 深度学习(DL)(四) — 探析
date: 2024-09-09 18:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 自注意力

要将**自注意力**与`CNN`一起使用，需要计算**自注意力**，即为输入句子中的每个单词创建基于**注意力**的表示。示例`Jane, visite, l'Afrique, en, septembre`，我们的目标是为每个单词计算一个基于注意力的表示。最终会得到五个，因为句子有五个单词。即{% mathjax %}A^{<1>},\ldots,A^{<5>}{% endmathjax %}。然后对句子中的各个单词进行计算。表示`l'Afrique`的一种方法是查找`l'Afrique`的词嵌入。根据对`l'Afrique`的理解，可以选择不同的方式来表示它({% mathjax %}A^{<3>}{% endmathjax %})。它将查看周围的单词，试图弄清楚在这个句子中的含义，并找到最合适的表示。就实际计算而言，它与之前在`RNN`上下文中看到的**注意力机制**没有太大区别，只是并行计算句子中所有单词的表示。
<!-- more -->

在`RNN`之上构建注意力时，使用了以下方程{% mathjax %}\alpha^{<t,t'>} = \frac{\text{exp}(e^{<t,t'>})}{\sum_{t'=1}^{T_x}\text{exp}(e^{<t,t'>})}{% endmathjax %}。使用`Transformer`的**自注意力机制**，方程将如下所示{% mathjax %}\mathbf{A}(q,K,V) = \sum_i \frac{\text{exp}(q\cdot k^{<i>})}{\sum_j \text{exp}(q\cdot k^{<j>})}v^{<i>}{% endmathjax %}。可以看到这些方程有一些相似之处。这里的内部项也涉及`softmax`。但主要的区别在于，比如`l'Afrique`，有三个值，称为`Query、Key`和`Value`。这些向量是计算每个单词的**注意力值**的关键输入。让我们逐步完成从单词l`'Afrique`到自注意力表示{% mathjax %}A^{<3>}{% endmathjax %}所需的计算。首先，将每个单词与三个值关联起来，如果{% mathjax %}X^{<3>}{% endmathjax %}是`l'Afrique`的**单词嵌入**，则计算{% mathjax %}Q^{<3>} = WQ \times X^{<3>}{% endmathjax %}是作为一个学习矩阵，键和值也是如此，{% mathjax %}K^{<3>} = WK \times X^{<3>}{% endmathjax %}，{% mathjax %}V^{<3>} = WV \times X^{<3>}{% endmathjax %}。这些矩阵{% mathjax %}WQ,WK{% endmathjax %}和{% mathjax %}WV{% endmathjax %}是此学习算法的参数。那么这些查询，键和值向量应该做什么呢？{% mathjax %}Q^{<3>}{% endmathjax %}是要问的关于非洲的问题。{% mathjax %}Q^{<3>}{% endmathjax %}可能代表这样的问题，例如，那里发生了什么？您可能想知道在计算{% mathjax %}A^{<3>}{% endmathjax %}时，发生了什么。计算{% mathjax %}q^{<3>}\cdot k^{<1>}{% endmathjax %}之间的内积，答案为`1`时问题的答案有多好。{% mathjax %}q^{<3>}\cdot k^{<2>}{% endmathjax %}，答案为`1`时问题的答案有多好，此操作的目的是提取信息，并计算出此处最有用的表示{% mathjax %}A^{<3>}{% endmathjax %}。如果{% mathjax %}k^{<1>}{% endmathjax %}表示这个词是一个人，因为`Jane`是一个人，而{% mathjax %}k^{<2>}{% endmathjax %}表示第二个词`visite`是一个 `action`，那么会发现{% mathjax %}q^{<3>}\cdot k^{<2>}{% endmathjax %}的内积具有最大值，取这一行中的这五个值，并对它们计算`softmax`。{% mathjax %}q^{<3>}\cdot k^{<2>}{% endmathjax %}对应于单词`visite`具有最大值。将这些`softmax`值与{% mathjax %}v^{<1>},v^{<2>}{% endmathjax %}相乘。最后，它们全部加起来。因此将所有这些值相加会得到{% mathjax %}A^{<3>}{% endmathjax %}。这种表示的主要优势在于`l'Afrique`这个词不是某种固定的**词嵌入**。相反，它让**自注意力机制**意识到`l'Afrique`是访问的目的地，从而为这个词计算出更丰富、更有用的表示。如果将这五个计算放在一起，文献中使用的表示法如下所示，其中{% mathjax %}Q,K,V{% endmathjax %}是包含所有这些值的矩阵，这只是此处方程的压缩或矢量化表示{% mathjax %}\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}(\frac{\mathbf{QK}^{\mathsf{T}}}{\sqrt{d_k}}) \mathbf{V}{% endmathjax %}。分母中的项只缩放点积，因此它不会爆炸。但这种注意力的另一个名称是**缩放点积注意力**。总结一下，与这五个单词中的每一个相关联，您最终会得到一个查询、一个键和一个值。查询可以让您提出有关该单词的问题，例如非洲发生了什么。关键是查看所有其他单词，并通过与查询的相似性，帮助您找出哪些单词给出了与该问题最相关的答案。
{% asset_img dl_1.png %}

#### 多头注意力

每次计算序列的自注意力时，称为**头部**。通过将每个输入项乘以这几个矩阵`WQ，WK`和`WV`，获得了向量`QK`和`V`。使用**多头注意力**，将同一组查询键和值向量作为输入。{% mathjax %}q,k,v{% endmathjax %}值并计算多个自注意力{% mathjax %}\text{head}_i = \text{Attention}(W^Q_iQ,W^K_iK,W^V_iV){% endmathjax %}。其中第一个，将{% mathjax %}q,k,v{% endmathjax %}矩阵与权重矩阵相乘，{% mathjax %}W^Q_1Q^{<1>},W^K_1K^{<1>}{% endmathjax %}和{% mathjax %}W^V_1V^{<1>}{% endmathjax %}。这三个值提供了一组新的查询键和值向量，用于第一个单词。对其他单词也做同样的操作。完成后，单词`visite`给出了最佳答案，表示`l'Afrique`的键与`visite`的查询之间的内积具有最高值。这就是获得`l'Afrique`的表示，对`Jane、visite`和其他单词`en septembre`做同样的操作。最终得到五个向量来表示序列中的五个单词。这是对多头注意力机制中的第一个头进行的计算。对`l'Afrique`和其他单词进行的完全相同的计算，并最终得到注意力值。现在执行不止一次，而是多次。到目前为止，用第一个头计算了注意力的数量。让我们用第二个头进行计算。有一组新的矩阵。{% mathjax %}W^Q_2Q^{<1>},W^K_2K^{<1>}{% endmathjax %}和{% mathjax %}W^V_2V^{<1>}{% endmathjax %}，重复与第一个完全相同的计算，但使用这组新矩阵。在这种情况下，您最终得到的可能是`september`键和`l'Afrique`查询之间的内积最高。也许我们现在要问的第三个问题，由{% mathjax %}W^Q_3Q^{<1>},W^K_3K^{<1>}{% endmathjax %}和{% mathjax %}W^V_3V^{<1>}{% endmathjax %}表示，当第三次进行计算时，也许`Jane`的键向量和`l'Afrique`查询向量之间的内积将最高。`Jane`的值将在此表示中具有最大的权重，将其堆叠在后面。在文献中，头部的数量通常用{% mathjax %}h{% endmathjax %}表示。你可以将每个头部视为不同的特征。当你将这些特征传递给新网络时，你可以计算出句子的表示。计算三个头部或八个头部，这三个{% mathjax %}A{% endmathjax %}值的串联用于计算多头注意力的输出。因此最终值是{% mathjax %}h{% endmathjax %}个头部的串联{% mathjax %}\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{concat}(\text{head}_1,\text{head}_2,\ldots,\text{head}_h)\mathbf{W}_o{% endmathjax %}。最后乘以矩阵{% mathjax %}\mathbf{W}{% endmathjax %}。实际上可以并行计算所有头，而不是按顺序计算。
{% asset_img dl_2.png %}

#### Transformer

再次从句子“`Jane visite L'Afrique en septembre`”及其对应的嵌入开始。如何将句子从法语翻译成英语。这里添加了句子开头和句子结尾的标记。为了简单起见，只讨论了句子中单词的嵌入，但在许多序列到序列的翻译任务中，添加句子开头(`SOS`)和句子结尾(`EOS`)标记会很有用，`Transformer`的第一步是，这些嵌入被输入到具有**多头注意力层**的**编码器块**中。根据**嵌入**和**权重矩阵**{% mathjax %}\mathbf{W}{% endmathjax %}计算出{% mathjax %}\mathbf{Q},\mathbf{K},\mathbf{V}{% endmathjax %}的值。这一层生成一个矩阵，该矩阵可以传递到**前馈神经网络**中，从而帮助确定句子中有哪些有趣的特征。在`Transformer`论文中，这个**编码块**重复了`n`次，`n`的默认值是`6`。大约经过六次这个块之后，将**编码器**的输出输入到**解码器块**中。接下来开始构建**解码器块**。**解码器块**的工作是输出英文翻译。第一个输出将是句子开头的标记。在每一步，**解码器块**都会输入前几个单词，无论生成了什么翻译。刚开始时，唯一知道以句子开头的标记开始。句子开头标记被输入到这个**多头注意力块**中，仅使用这个标记(`SOS`)来计算**多头注意力块**的{% mathjax %}\mathbf{Q},\mathbf{K},\mathbf{V}{% endmathjax %}。第一个块的输出用于生成下一个多头注意力块的{% mathjax %}\mathbf{Q}{% endmathjax %}矩阵，**编码器**的输出用于生成{% mathjax %}\mathbf{K},\mathbf{V}{% endmathjax %}。第二个**多头注意力块**，其输入与之前一样{% mathjax %}\mathbf{Q},\mathbf{K},\mathbf{V}{% endmathjax %}。然后从{% mathjax %}\mathbf{Q},\mathbf{K}{% endmathjax %}中提取上下文，然后决定要生成的序列中的下一个单词是什么。**多头注意力块**输出输入到**前馈神经网络**的值。这个**解码器块**也重复{% mathjax %}n{% endmathjax %}次，可能是`6`次，将输出反馈给输入，并让它重复。神经网络的工作是预测句子中单词。希望确定英语翻译中的第一个单词是`Jane`。然后还要将`Jane`提供给输入。下一个查询来自`SOS`和`Jane`，给定`Jane`，最合适的下一个单词是什么？找到正确的键和值，然后生成最合适的下一个单词，希望会生成`visite`。然后再次运行这个神经网络生成`Africa`。然后我们将`Africa`反馈给输入。希望它生成`in`然后是`September`，有了这个输入，希望它能生成句子结尾的标记，然后就完成了。输入的**位置编码**。没有任何东西可以指示单词的位置。这个词是句子中的第一个词，还是句子中的中间词，还是句子中的最后一个词？句子中的位置对于翻译来说非常重要。对输入中元素位置进行编码的方式是使用**正弦**和**余弦**方程的组合。例如，假设您的**词嵌入**是一个具有四个值的向量。在这种情况下，**词嵌入**的维度{% mathjax %}D = 4{% endmathjax %}，在这个例子中，创建一个相同维度的**位置嵌入向量**，也是四维的。把这个**位置嵌入**称为{% mathjax %}p^{<1>}{% endmathjax %}，假设是第一个单词 `Jane`的位置嵌入。在下面的等式中，位置(`pos`)表示单词的数字位置。对于单词`Jane`，{% mathjax %}pos = 1{% endmathjax %}，这里的{% mathjax %}i{% endmathjax %}是编码的不同维度。第一个元素对应于{% mathjax %}i=0{% endmathjax %}。{% mathjax %}PE_{(\text{pos},2i)} = \sin(\frac{\text{pos}}{10000\frac{2i}{d}}),\;PE_{(\text{pos},2i+1)} = \cos(\frac{\text{pos}}{10000\frac{2i}{d}}){% endmathjax %}。其中`pos`是单词的位置，{% mathjax %}i = \{0,1\}{% endmathjax %}，{% mathjax %}d=4{% endmathjax %}，是这个向量的维度。**位置编码**对正弦和余弦所做的就是创建一个唯一的**位置编码向量**。每个单词都是唯一的，即对第三个单词`l'Afrique`的位置进行编码的向量{% mathjax %}p^{<3>}{% endmathjax %}，这些值将不同于第一个单词`Jane`的代码位置中使用的四个值。{% mathjax %}i =\{0,0,1,1\}{% endmathjax %}。{% mathjax %}i=0{% endmathjax %}将具有这样的**正弦曲线**。{% mathjax %}i=1{% endmathjax %}将以较低频率的正弦曲线，而{% mathjax %}i=1{% endmathjax %}提供了**余弦曲线**。**编码块**的输出包含**上下文语义嵌入**和**位置编码**信息。嵌入层的输出是{% mathjax %}\mathbf{D}{% endmathjax %}，除了将这些**位置编码**添加到嵌入之外，还会残差连接的网络传递。在这种情况下，它们的目的是将位置信息传递到整个架构中。除了**位置编码**之外，`Transformer`网络还使用与批量规范非常相似的层。它们的目的是将**位置信息**传递到**位置编码**中。`Transformer`还使用了一个批量规范层。最后，对于**解码器块**的输出，实际上还有一个**线性层**和一个`softmax`层，用于一次一个单词地预测下一个单词。您可能还会听到一种称为**掩码多头注意力**。**掩码多头注意力**仅在训练过程中很重要，在该过程中，您将使用正确的法语到英语翻译的数据集来训练`Transformer`。
{% asset_img dl_3.png %}
