---
title: NLPï¼ˆGloVe & BERT & TF-IDF & LSTMï¼‰
date: 2024-04-03 14:09:11
tags:
  - AI
categories:
  - äººå·¥æ™ºèƒ½
---

**è‡ªç„¶è¯­è¨€å¤„ç†**ï¼ˆ`NLP`ï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè´Ÿè´£è¿æ¥æœºå™¨ä»¥è‡ªç„¶è¯­è¨€ç†è§£äººç±»ã€‚è‡ªç„¶è¯­è¨€å¯ä»¥æ˜¯æ–‡æœ¬æˆ–å£°éŸ³çš„å½¢å¼ã€‚`NLP`å¯ä»¥ç”¨äººç±»çš„æ–¹å¼ä¸æœºå™¨è¿›è¡Œäº¤æµã€‚**æ–‡æœ¬åˆ†ç±»**æ˜¯æƒ…æ„Ÿåˆ†æä¸­æ¶‰åŠçš„å†…å®¹ã€‚å®ƒæ˜¯å°†äººç±»çš„æ„è§æˆ–è¡¨è¾¾åˆ†ç±»ä¸ºä¸åŒçš„æƒ…ç»ªã€‚**æƒ…ç»ª**åŒ…æ‹¬æ­£é¢ã€ä¸­ç«‹å’Œè´Ÿé¢ã€è¯„è®ºè¯„çº§ä»¥åŠå¿«ä¹ã€æ‚²ä¼¤ã€‚ æƒ…ç»ªåˆ†æå¯ä»¥é’ˆå¯¹ä¸åŒçš„ä»¥æ¶ˆè´¹è€…ä¸ºä¸­å¿ƒçš„è¡Œä¸šè¿›è¡Œï¼Œåˆ†æäººä»¬å¯¹ç‰¹å®šäº§å“æˆ–ä¸»é¢˜çš„çœ‹æ³•ã€‚è‡ªç„¶è¯­è¨€å¤„ç†èµ·æºäº`20`ä¸–çºª`50`å¹´ä»£ã€‚æ—©åœ¨`1950`å¹´ï¼Œè‰¾ä¼¦Â·å›¾çµå°±å‘è¡¨äº†ä¸€ç¯‡é¢˜ä¸ºã€Šè®¡ç®—æœºå™¨ä¸æ™ºèƒ½ã€‹çš„æ–‡ç« ï¼Œæå‡ºäº†å›¾çµæµ‹è¯•ä½œä¸ºæ™ºèƒ½çš„æ ‡å‡†ï¼Œè¿™é¡¹ä»»åŠ¡æ¶‰åŠè‡ªç„¶è¯­è¨€çš„è‡ªåŠ¨è§£é‡Šå’Œç”Ÿæˆï¼Œä½†å½“æ—¶å°šæœªæ˜ç¡®é˜è¿°ã€‚åœ¨æ­¤å†…æ ¸ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æéƒ¨åˆ†ã€‚
<!-- more -->

#### åŠ è½½æ•°æ®

åªéœ€åŠ è½½æ•°æ®é›†å’Œé¢œè‰²ç­‰å…¨å±€å˜é‡å³å¯ã€‚
```python
import re
import string
import numpy as np 
import random
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from plotly import graph_objs as go
import plotly.express as px
import plotly.figure_factory as ff
from collections import Counter

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tqdm import tqdm
import os
import nltk
import spacy
import random
from spacy.util import compounding
from spacy.util import minibatch
from collections import defaultdict
from collections import Counter
import keras
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers import (LSTM, 
                          Embedding, 
                          BatchNormalization,
                          Dense, 
                          TimeDistributed, 
                          Dropout, 
                          Bidirectional,
                          Flatten, 
                          GlobalMaxPool1D)
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.optimizers import Adam

from sklearn.metrics import (
    precision_score, 
    recall_score, 
    f1_score, 
    classification_report,
    accuracy_score
)

# Defining all our palette colours.
primary_blue = "#496595"
primary_blue2 = "#85a1c1"
primary_blue3 = "#3f4d63"
primary_grey = "#c6ccd8"
primary_black = "#202022"
primary_bgcolor = "#f4f0ea"
primary_green = px.colors.qualitative.Plotly[2]

# åŠ è½½æ•°æ®
df = pd.read_csv("/kaggle/input/sms-spam-collection-dataset/spam.csv", encoding="latin-1")
df = df.dropna(how="any", axis=1)
df.columns = ['target', 'message']
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
	target	message
0	ham	Go until jurong point, crazy.. Available only ...
1	ham	Ok lar... Joking wif u oni...
2	spam	Free entry in 2 a wkly comp to win FA Cup fina...
3	ham	U dun say so early hor... U c already then say...
4	ham	Nah I don't think he goes to usf, he lives aro...
```
```python
df['message_len'] = df['message'].apply(lambda x: len(x.split(' ')))
df.head()
max(df['message_len'])
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
	target	message	message_len
0	ham	Go until jurong point, crazy.. Available only ...	20
1	ham	Ok lar... Joking wif u oni...	6
2	spam	Free entry in 2 a wkly comp to win FA Cup fina...	28
3	ham	U dun say so early hor... U c already then say...	11
4	ham	Nah I don't think he goes to usf, he lives aro...	13

171
```
#### EDA 

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹ç›®æ ‡åˆ†å¸ƒå’Œæ¶ˆæ¯é•¿åº¦ã€‚**å¹³è¡¡æ•°æ®é›†**ï¼š- è®©æˆ‘ä»¬ä¸¾ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œå¦‚æœåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æˆ‘ä»¬æœ‰æ­£å€¼ä¸è´Ÿå€¼å¤§è‡´ç›¸åŒã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬çš„æ•°æ®é›†å¤„äº**å¹³è¡¡çŠ¶æ€**ã€‚å°†æ©™è‰²è§†ä¸ºæ­£å€¼ï¼Œå°†è“è‰²è§†ä¸ºè´Ÿå€¼ã€‚å¯ä»¥è¯´æ­£å€¼å’Œè´Ÿå€¼çš„æ•°é‡å¤§è‡´ç›¸åŒã€‚**ä¸å¹³è¡¡æ•°æ®é›†**ï¼šâ€” å¦‚æœæ­£å€¼å’Œè´Ÿå€¼ä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ä¸å¹³è¡¡æ•°æ®é›†ã€‚
```python
balance_counts = df.groupby('target')['target'].agg('count').values
balance_counts

# array([4825,  747])
```
```python
fig = go.Figure()
fig.add_trace(go.Bar(
    x=['ham'],
    y=[balance_counts[0]],
    name='ham',
    text=[balance_counts[0]],
    textposition='auto',
    marker_color=primary_blue
))
fig.add_trace(go.Bar(
    x=['spam'],
    y=[balance_counts[1]],
    name='spam',
    text=[balance_counts[1]],
    textposition='auto',
    marker_color=primary_grey
))
fig.update_layout(title='<span style="font-size:32px; font-family:Times New Roman">Dataset distribution by target</span>')
fig.show()
```
{% asset_img na_1.png %}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œç±»åˆ«æ˜¯ä¸å¹³è¡¡çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è€ƒè™‘ä½¿ç”¨æŸç§é‡é‡‡æ ·ã€‚
```python
ham_df = df[df['target'] == 'ham']['message_len'].value_counts().sort_index()
spam_df = df[df['target'] == 'spam']['message_len'].value_counts().sort_index()

fig = go.Figure()
fig.add_trace(go.Scatter(
    x=ham_df.index,
    y=ham_df.values,
    name='ham',
    fill='tozeroy',
    marker_color=primary_blue,
))
fig.add_trace(go.Scatter(
    x=spam_df.index,
    y=spam_df.values,
    name='spam',
    fill='tozeroy',
    marker_color=primary_grey,
))
fig.update_layout(
    title='<span style="font-size:32px; font-family:Times New Roman">Data Roles in Different Fields</span>'
)
fig.update_xaxes(range=[0, 70])
fig.show()
```
{% asset_img na_2.png %}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ­£å¸¸é‚®ä»¶çš„é•¿åº¦å¾€å¾€ä½äºåƒåœ¾é‚®ä»¶çš„é•¿åº¦ã€‚

#### æ•°æ®é¢„å¤„ç†

ç°åœ¨æˆ‘ä»¬å°†å¯¹æ•°æ®è¿›è¡Œå·¥ç¨‹è®¾è®¡ï¼Œä»¥ä½¿æ¨¡å‹æ›´å®¹æ˜“åˆ†ç±»ã€‚è¿™ä¸€éƒ¨åˆ†å¯¹äºç¼©å°é—®é¢˜çš„ç»´åº¦éå¸¸é‡è¦ã€‚

##### æ¸…ç†è¯­æ–™åº“

```python
def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

df['message_clean'] = df['message'].apply(clean_text)
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
  target                                            message  message_len                                      message_clean
0    ham  Go until jurong point, crazy.. Available only ...           20  go until jurong point crazy available only in ...
1    ham                      Ok lar... Joking wif u oni...            6                            ok lar joking wif u oni
2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28  free entry in  a wkly comp to win fa cup final...
3    ham  U dun say so early hor... U c already then say...           11        u dun say so early hor u c already then say
4    ham  Nah I don't think he goes to usf, he lives aro...           13  nah i dont think he goes to usf he lives aroun...
```
##### Stopwords

`Stopwords`æ˜¯è‹±è¯­ä¸­å¸¸ç”¨çš„å•è¯ï¼Œåœ¨å¥å­ä¸­æ²¡æœ‰ä¸Šä¸‹æ–‡å«ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨åˆ†ç±»ä¹‹å‰å°†å®ƒä»¬åˆ é™¤ã€‚åˆ é™¤`Stopwords`çš„ä¸€äº›ç¤ºä¾‹æ˜¯ï¼š
{% asset_img na_3.png %}

```python
stop_words = stopwords.words('english')
more_stopwords = ['u', 'im', 'c']
stop_words = stop_words + more_stopwords

def remove_stopwords(text):
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    return text
    
df['message_clean'] = df['message_clean'].apply(remove_stopwords)
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
  target                                            message  message_len                                      message_clean
0    ham  Go until jurong point, crazy.. Available only ...           20  go jurong point crazy available bugis n great ...
1    ham                      Ok lar... Joking wif u oni...            6                              ok lar joking wif oni
2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28  free entry  wkly comp win fa cup final tkts  m...
3    ham  U dun say so early hor... U c already then say...           11                      dun say early hor already say
4    ham  Nah I don't think he goes to usf, he lives aro...           13        nah dont think goes usf lives around though
```
##### è¯å¹²æå–

###### è¯å¹²æå–/ç‰¹å¾åŒ–

å‡ºäºè¯­æ³•åŸå› ï¼Œæ–‡æ¡£å°†ä½¿ç”¨å•è¯çš„ä¸åŒå½¢å¼ï¼Œä¾‹å¦‚`writeã€writing`å’Œ`writes`ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›å…·æœ‰ç›¸ä¼¼å«ä¹‰çš„æ´¾ç”Ÿç›¸å…³è¯æ—ã€‚**è¯å¹²æå–**å’Œ**è¯å½¢è¿˜åŸ**çš„ç›®æ ‡éƒ½æ˜¯å°†å•è¯çš„å±ˆæŠ˜å½¢å¼å’Œæœ‰æ—¶æ´¾ç”Ÿç›¸å…³çš„å½¢å¼å‡å°‘ä¸ºå…±åŒçš„åŸºæœ¬å½¢å¼ã€‚
- **è¯å¹²æå–**é€šå¸¸æ˜¯æŒ‡ä¸ºäº†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ­£ç¡®å®ç°ç›®æ ‡è€Œç æ‰å•è¯æœ«å°¾çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”é€šå¸¸åŒ…æ‹¬åˆ é™¤æ´¾ç”Ÿè¯ç¼€ã€‚
- **è¯å½¢è¿˜åŸ**é€šå¸¸æ˜¯æŒ‡ä½¿ç”¨è¯æ±‡å’Œå•è¯çš„å½¢æ€åˆ†ææ¥æ­£ç¡®åœ°è¿›è¡Œæ“ä½œï¼Œé€šå¸¸æ—¨åœ¨ä»…åˆ é™¤å±ˆæŠ˜è¯å°¾å¹¶è¿”å›å•è¯çš„åŸºæœ¬å½¢å¼å’Œå­—å…¸å½¢å¼ã€‚

{% asset_img na_4.png %}

###### è¯å¹²æå–ç®—æ³•

`NLTK Python`åº“ä¸­å®ç°äº†å¤šç§è¯å¹²æå–ç®—æ³•ï¼š
- `PorterStemmer`ä½¿ç”¨åç¼€å‰¥ç¦»æ¥ç”Ÿæˆè¯å¹²ã€‚`PorterStemmer`ä»¥å…¶ç®€å•å’Œé€Ÿåº¦è€Œé—»åã€‚è¯·æ³¨æ„`PorterStemmer`å¦‚ä½•é€šè¿‡ç®€å•åœ°åˆ é™¤`cat`åé¢çš„'`s`'æ¥ç»™å‡ºå•è¯â€œ`cats`â€çš„è¯æ ¹ï¼ˆè¯å¹²ï¼‰ã€‚è¿™æ˜¯æ·»åŠ åˆ°`cat`ä¸Šçš„åç¼€ï¼Œä½¿å…¶æˆä¸ºå¤æ•°ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ çœ‹ä¸€ä¸‹â€œ`trouble`â€ã€â€œ`trouble`â€å’Œâ€œ`troubled`â€ï¼Œå®ƒä»¬å°±ä¼šè¢«å½’ä¸ºâ€œ`trouble`â€ï¼Œå› ä¸º`PorterStemmer`ç®—æ³•ä¸éµå¾ªè¯­è¨€å­¦ï¼Œè€Œæ˜¯éµå¾ªä¸€ç»„é€‚ç”¨äºä¸åŒæƒ…å†µçš„`05`æ¡è§„åˆ™ï¼Œè¿™äº›è§„åˆ™åˆ†é˜¶æ®µï¼ˆé€æ­¥ï¼‰åº”ç”¨äºç”Ÿæˆè¯å¹²ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ`PorterStemmer`ä¸ç»å¸¸ç”Ÿæˆå®é™…è‹±è¯­å•è¯çš„è¯å¹²çš„åŸå› ã€‚å®ƒä¸ä¿ç•™å•è¯å®é™…è¯å¹²çš„æŸ¥æ‰¾è¡¨ï¼Œè€Œæ˜¯åº”ç”¨ç®—æ³•è§„åˆ™æ¥ç”Ÿæˆè¯å¹²ã€‚å®ƒä½¿ç”¨è§„åˆ™æ¥å†³å®šåˆ é™¤åç¼€æ˜¯å¦æ˜æ™ºã€‚
- äººä»¬å¯ä»¥ä¸ºä»»ä½•è¯­è¨€ç”Ÿæˆè‡ªå·±çš„ä¸€ç»„è§„åˆ™ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ`Python nltk`å¼•å…¥äº†`SnowballStemmers`ï¼Œç”¨äºåˆ›å»ºéè‹±è¯­`Stemmers`ï¼
- `LancasterStemmer`ï¼ˆ`Paice-Husk`è¯å¹²åˆ†æå™¨ï¼‰æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œè§„åˆ™ä¿å­˜åœ¨å¤–éƒ¨ã€‚ä¸€ä¸ªè¡¨åŒ…å«çº¦`120`æ¡è§„åˆ™ï¼ŒæŒ‰åç¼€çš„æœ€åä¸€ä¸ªå­—æ¯è¿›è¡Œç´¢å¼•ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒéƒ½ä¼šå°è¯•é€šè¿‡å•è¯çš„æœ€åä¸€ä¸ªå­—ç¬¦æ‰¾åˆ°é€‚ç”¨çš„è§„åˆ™ã€‚æ¯æ¡è§„åˆ™æŒ‡å®šåˆ é™¤æˆ–æ›¿æ¢ç»“å°¾éƒ¨åˆ†ã€‚å¦‚æœæ²¡æœ‰è¿™æ ·çš„è§„åˆ™ï¼Œåˆ™ç»ˆæ­¢ã€‚å¦‚æœä¸€ä¸ªå•è¯ä»¥å…ƒéŸ³å¼€å¤´å¹¶ä¸”åªå‰©ä¸‹ä¸¤ä¸ªå•è¯ï¼Œæˆ–è€…å¦‚æœä¸€ä¸ªå•è¯ä»¥è¾…éŸ³å¼€å¤´å¹¶ä¸”åªå‰©ä¸‹ä¸‰ä¸ªå­—ç¬¦ï¼Œå®ƒä¹Ÿä¼šç»ˆæ­¢ã€‚å¦åˆ™ï¼Œåº”ç”¨è¯¥è§„åˆ™å¹¶é‡å¤è¯¥è¿‡ç¨‹ã€‚
```python
stemmer = nltk.SnowballStemmer("english")

def stemm_text(text):
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    return text

df['message_clean'] = df['message_clean'].apply(stemm_text)
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
  target                                            message  message_len                                      message_clean
0    ham  Go until jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...
1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni
2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...
3    ham  U dun say so early hor... U c already then say...           11                      dun say earli hor alreadi say
4    ham  Nah I don't think he goes to usf, he lives aro...           13          nah dont think goe usf live around though
```
##### å…¨éƒ¨

```python
def preprocess_data(text):
    # Clean puntuation, urls, and so on
    text = clean_text(text)
    # Remove stopwords
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    # Stemm all the words in the sentence
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    
    return text

df['message_clean'] = df['message_clean'].apply(preprocess_data)
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
  target                                            message  message_len                                      message_clean
0    ham  Go until jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...
1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni
2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...
3    ham  U dun say so early hor... U c already then say...           11                        dun say ear hor alreadi say
4    ham  Nah I don't think he goes to usf, he lives aro...           13          nah dont think goe usf live around though
```

##### ç›®æ ‡ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(df['target'])

df['target_encoded'] = le.transform(df['target'])
df.head()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
  target                                            message  message_len                                      message_clean  target_encoded
0    ham  Go until jurong point, crazy.. Available only ...           20  go jurong point crazi avail bugi n great world...               0
1    ham                      Ok lar... Joking wif u oni...            6                                ok lar joke wif oni               0
2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28  free entri  wkli comp win fa cup final tkts  m...               1
3    ham  U dun say so early hor... U c already then say...           11                        dun say ear hor alreadi say               0
4    ham  Nah I don't think he goes to usf, he lives aro...           13          nah dont think goe usf live around though               0
```
#### Tokens å¯è§†åŒ–

```python
twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 'ham', 'message_clean']))
plt.figure(figsize=(18,10))
plt.title('Top words for HAM messages', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()
```
{% asset_img na_5.png %}

```python
twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 'spam', 'message_clean']))
plt.figure(figsize=(18,10))
plt.title('Top words for HAM messages', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()
```
{% asset_img na_6.png %}

#### çŸ¢é‡åŒ–

ç›®å‰ï¼Œæˆ‘ä»¬å°†æ¶ˆæ¯ä½œä¸ºæ ‡è®°åˆ—è¡¨ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦å°†æ¯æ¡æ¶ˆæ¯è½¬æ¢ä¸º`SciKit Learn`ç®—æ³•æ¨¡å‹å¯ä»¥ä½¿ç”¨çš„**å‘é‡**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`bag-of-words`æ¨¡å‹åˆ†ä¸‰ä¸ªæ­¥éª¤æ¥å®Œæˆæ­¤æ“ä½œï¼š
- è®¡ç®—æŸä¸ªå•è¯åœ¨æ¯æ¡æ¶ˆæ¯ä¸­å‡ºç°çš„æ¬¡æ•°ï¼ˆç§°ä¸ºé¢‘ç‡ï¼‰ã€‚
- æƒè¡¡è®¡æ•°ï¼Œä½¿é¢‘ç¹çš„æ ‡è®°è·å¾—è¾ƒä½çš„æƒé‡ï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰ã€‚
- å°†å‘é‡æ ‡å‡†åŒ–ä¸ºå•ä½é•¿åº¦ï¼Œä»åŸå§‹æ–‡æœ¬é•¿åº¦ä¸­æŠ½è±¡å‡ºæ¥ï¼ˆ`L2`èŒƒæ•°ï¼‰ã€‚

è®©æˆ‘ä»¬å¼€å§‹ç¬¬ä¸€æ­¥ï¼š
æ¯ä¸ªå‘é‡çš„ç»´åº¦ä¸`SMS`è¯­æ–™åº“ä¸­å”¯ä¸€å•è¯çš„ç»´åº¦ä¸€æ ·å¤šã€‚æˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨`SciKit Learn`çš„`CountVectorizer`ã€‚è¯¥æ¨¡å‹ä¼šå°†æ–‡æœ¬æ–‡æ¡£é›†åˆè½¬æ¢ä¸ºæ ‡è®°è®¡æ•°çŸ©é˜µã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶æƒ³è±¡ä¸ºä¸€ä¸ªäºŒç»´çŸ©é˜µã€‚å…¶ä¸­ä¸€ç»´æ˜¯æ•´ä¸ªè¯æ±‡è¡¨ï¼ˆæ¯ä¸ªå•è¯`1`è¡Œï¼‰ï¼Œå¦ä¸€ä¸ªç»´åº¦æ˜¯å®é™…æ–‡æ¡£ï¼Œåœ¨æœ¬ä¾‹ä¸­æ¯æ¡æ–‡æœ¬æ¶ˆæ¯ä¸€åˆ—ã€‚
{% asset_img na_7.png %}

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
# how to define X and y (from the SMS data) for use with COUNTVECTORIZER
x = df['message_clean']
y = df['target_encoded']

print(len(x), len(y))

# Split into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)
print(len(x_train), len(y_train))
print(len(x_test), len(y_test))

# instantiate the vectorizer
vect = CountVectorizer()
vect.fit(x_train)

# Use the trained to create a document-term matrix from train and test sets
x_train_dtm = vect.transform(x_train)
x_test_dtm = vect.transform(x_test)
```
##### è°ƒæ•´CountVectorizer

`CountVectorizer`æœ‰ä¸€äº›åº”è¯¥çŸ¥é“çš„å‚æ•°ã€‚
- `stop_words`ï¼šç”±äº`CountVectorizer`åªæ˜¯è®¡ç®—è¯æ±‡è¡¨ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ï¼Œå› æ­¤åƒâ€œ`the`â€ã€â€œ`and`â€ç­‰æå…¶å¸¸è§çš„å•è¯å°†æˆä¸ºéå¸¸é‡è¦çš„ç‰¹å¾ï¼Œä½†å®ƒä»¬å¯¹æ–‡æœ¬çš„æ„ä¹‰ä¸å¤§ã€‚å¦‚æœæ‚¨ä¸è€ƒè™‘è¿™äº›å› ç´ ï¼Œæ‚¨çš„æ¨¡å‹é€šå¸¸å¯ä»¥æ”¹è¿›ã€‚åœç”¨è¯åªæ˜¯æ‚¨ä¸æƒ³ç”¨ä½œç‰¹å¾çš„å•è¯åˆ—è¡¨ã€‚æ‚¨å¯ä»¥è®¾ç½®å‚æ•°`stop_words='english'`ä»¥ä½¿ç”¨å†…ç½®åˆ—è¡¨ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥å°†`stop_words`è®¾ç½®ä¸ºç­‰äºæŸä¸ªè‡ªå®šä¹‰åˆ—è¡¨ã€‚è¯¥å‚æ•°é»˜è®¤ä¸ºæ— ã€‚
- `ngram_range`ï¼š`n-gram`å°±æ˜¯ä¸€ä¸²è¿ç»­çš„`n`ä¸ªå•è¯ã€‚ä¾‹å¦‚ã€‚å¥å­â€œ`I am Groot`â€åŒ…å«`2-grams`â€œ`I amâ€å’Œâ€œam Groot`â€ã€‚è¯¥å¥å­æœ¬èº«å°±æ˜¯ä¸€ä¸ª`3`å…ƒè¯­æ³•ã€‚è®¾ç½®å‚æ•°`ngram_range=(a,b)`ï¼Œå…¶ä¸­`a`æ˜¯è¦åŒ…å«åœ¨ç‰¹å¾ä¸­çš„`ngram`çš„æœ€å°å€¼ï¼Œ`b`æ˜¯æœ€å¤§å€¼ã€‚é»˜è®¤`ngram_rangeä¸º(1,1)`ã€‚åœ¨æœ€è¿‘çš„ä¸€ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å¯¹åœ¨çº¿æ‹›è˜ä¿¡æ¯è¿›è¡Œäº†å»ºæ¨¡ï¼Œæˆ‘å‘ç°å°†`2-gram`åŒ…å«åœ¨å†…å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™å¾ˆç›´è§‚ã€‚è®¸å¤šèŒä½åç§°ï¼Œä¾‹å¦‚â€œæ•°æ®ç§‘å­¦å®¶â€ã€â€œæ•°æ®å·¥ç¨‹å¸ˆâ€å’Œâ€œæ•°æ®åˆ†æå¸ˆâ€éƒ½æ˜¯ä¸¤ä¸ªè¯é•¿ã€‚
- `min_df`ã€`max_df`ï¼šè¿™äº›æ˜¯å•è¯`/n-gram`å¿…é¡»ç”¨ä½œç‰¹å¾çš„æœ€å°å’Œæœ€å¤§æ–‡æ¡£é¢‘ç‡ã€‚å¦‚æœè¿™äº›å‚æ•°ä¸­çš„ä»»ä½•ä¸€ä¸ªè®¾ç½®ä¸ºæ•´æ•°ï¼Œå®ƒä»¬å°†ç”¨ä½œæ¯ä¸ªç‰¹å¾å¿…é¡»ä½äºçš„æ–‡æ¡£æ•°é‡çš„ç•Œé™æ‰èƒ½è¢«è§†ä¸ºç‰¹å¾ã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªè®¾ç½®ä¸ºæµ®ç‚¹æ•°ï¼Œåˆ™è¯¥æ•°å­—å°†è¢«è§£é‡Šä¸ºé¢‘ç‡è€Œä¸æ˜¯æ•°å€¼é™åˆ¶ã€‚`min_df`é»˜è®¤ä¸º`1(int)`ï¼Œ`max_df`é»˜è®¤ä¸º`1.0(float)`ã€‚
- `max_features`ï¼šè¿™ä¸ªå‚æ•°ä¸è¨€è‡ªæ˜ã€‚`CountVectorizer`å°†é€‰æ‹©æœ€å¸¸å‡ºç°åœ¨å…¶è¯æ±‡è¡¨ä¸­çš„å•è¯/ç‰¹å¾ï¼Œå¹¶ä¸¢å¼ƒå…¶ä»–æ‰€æœ‰å†…å®¹ã€‚

æ‚¨å¯ä»¥åœ¨åˆå§‹åŒ–`CountVectorizer`å¯¹è±¡æ—¶è®¾ç½®è¿™äº›å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
```python
vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)
```
##### TF-IDF

åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼Œ`tfâ€“idf`ã€`TF-IDF`æˆ–`TFIDF`æ˜¯**æœ¯è¯­é¢‘ç‡â€“é€†æ–‡æ¡£é¢‘ç‡**çš„ç¼©å†™ï¼Œæ˜¯ä¸€ç§æ•°å€¼ç»Ÿè®¡é‡ï¼Œæ—¨åœ¨åæ˜ ä¸€ä¸ªå•è¯å¯¹äºé›†åˆæˆ–è¯­æ–™åº“ä¸­çš„æ–‡æ¡£çš„é‡è¦æ€§ï¼Œç»å¸¸ä½¿ä½œä¸ºä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬æŒ–æ˜å’Œç”¨æˆ·å»ºæ¨¡æœç´¢ä¸­çš„æƒé‡å› å­ã€‚`tf-idf`å€¼ä¸å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ¯”ä¾‹å¢åŠ ï¼Œå¹¶ç”±è¯­æ–™åº“ä¸­åŒ…å«è¯¥å•è¯çš„æ–‡æ¡£æ•°é‡æŠµæ¶ˆï¼Œè¿™æœ‰åŠ©äºè°ƒæ•´æŸäº›å•è¯é€šå¸¸å‡ºç°é¢‘ç‡æ›´é«˜çš„äº‹å®ã€‚`tfâ€“idf`æ˜¯å½“ä»Šæœ€æµè¡Œçš„æœ¯è¯­åŠ æƒæ–¹æ¡ˆä¹‹ä¸€ã€‚`2015`å¹´è¿›è¡Œçš„ä¸€é¡¹è°ƒæŸ¥æ˜¾ç¤ºï¼Œæ•°å­—å›¾ä¹¦é¦†ä¸­`83%`çš„åŸºäºæ–‡æœ¬çš„æ¨èç³»ç»Ÿä½¿ç”¨`tf-idf`ã€‚
```python
from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
tfidf_transformer.fit(x_train_dtm)
x_train_tfidf = tfidf_transformer.transform(x_train_dtm)

x_train_tfidf

# <4179x5684 sparse matrix of type '<class 'numpy.float64'>' with 32201 stored elements in Compressed Sparse Row format>
```
##### è¯åµŒå…¥ï¼šGloVe

```python
texts = df['message_clean']
target = df['target_encoded']

# Calculate the length of our vocabulary
word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(texts)

vocab_length = len(word_tokenizer.word_index) + 1
vocab_length

def embed(corpus): 
    return word_tokenizer.texts_to_sequences(corpus)

longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
    embed(texts), 
    length_long_sentence, 
    padding='post'
)

train_padded_sentences

# array([[   2, 3179,  274, ...,    0,    0,    0],
#        [   8,  236,  527, ...,    0,    0,    0],
#        [   9,  356,  588, ...,    0,    0,    0],
#        ...,
#        [6724, 1002, 6725, ...,    0,    0,    0],
#        [ 138, 1251, 1603, ...,    0,    0,    0],
#        [1986,  378,  170, ...,    0,    0,    0]], dtype=int32)
```
`GloVe`æ–¹æ³•å»ºç«‹åœ¨ä¸€ä¸ªé‡è¦çš„æƒ³æ³•ä¹‹ä¸Šï¼Œæ‚¨å¯ä»¥ä»`co-occurrence`çŸ©é˜µå¯¼å‡ºå•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚ä¸ºäº†è·å¾—å•è¯çš„å‘é‡è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç§åä¸º`GloVe`ï¼ˆå•è¯è¡¨ç¤ºçš„å…¨å±€å‘é‡ï¼‰çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸“æ³¨äºæ•´ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„`co-occurrence`ã€‚å®ƒçš„åµŒå…¥ä¸ä¸¤ä¸ªå•è¯ä¸€èµ·å‡ºç°çš„æ¦‚ç‡æœ‰å…³ã€‚**è¯åµŒå…¥**æ˜¯ä¸€ç§è¯è¡¨ç¤ºå½¢å¼ï¼Œå®ƒå°†äººç±»å¯¹è¯­è¨€çš„ç†è§£ä¸æœºå™¨å¯¹è¯­è¨€ç†è§£è”ç³»èµ·æ¥ã€‚ä»–ä»¬å·²ç»å­¦ä¹ äº†`n`ç»´ç©ºé—´ä¸­æ–‡æœ¬çš„è¡¨ç¤ºï¼Œå…¶ä¸­å…·æœ‰ç›¸åŒå«ä¹‰çš„å•è¯å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºã€‚è¿™æ„å‘³ç€ä¸¤ä¸ªç›¸ä¼¼çš„å•è¯ç”±å‘é‡ç©ºé—´ä¸­éå¸¸æ¥è¿‘çš„å‘é‡è¡¨ç¤ºã€‚å› æ­¤ï¼Œå½“ä½¿ç”¨è¯åµŒå…¥æ—¶ï¼Œæ‰€æœ‰å•ä¸ªè¯éƒ½è¢«è¡¨ç¤ºä¸ºé¢„å®šä¹‰å‘é‡ç©ºé—´ä¸­çš„**å®å€¼å‘é‡**ã€‚æ¯ä¸ªå•è¯éƒ½æ˜ å°„åˆ°ä¸€ä¸ªå‘é‡ï¼Œå¹¶ä¸”ç±»ä¼¼äºç¥ç»ç½‘ç»œçš„æ–¹å¼å­¦ä¹ å‘é‡å€¼ã€‚
```python
embeddings_dictionary = dict()
embedding_dim = 100

# Load GloVe 100D embeddings
with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:
    for line in fp.readlines():
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary [word] = vector_dimensions

# embeddings_dictionary
# Now we will load embedding vectors of those words that appear in the
# Glove dictionary. Others will be initialized to 0.

embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
        
embedding_matrix

# array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,
#          0.        ,  0.        ],
#        [-0.57832998, -0.0036551 ,  0.34658   , ...,  0.070204  ,
#          0.44509   ,  0.24147999],
#        [-0.078894  ,  0.46160001,  0.57779002, ...,  0.26352   ,
#          0.59397   ,  0.26741001],
#        ...,
#        [ 0.63009   , -0.036992  ,  0.24052   , ...,  0.10029   ,
#          0.056822  ,  0.25018999],
#        [-0.12002   , -1.23870003, -0.23303001, ...,  0.13658001,
#         -0.61848003,  0.049843  ],
#        [ 0.        ,  0.        ,  0.        , ...,  0.        ,
#          0.        ,  0.        ]])
```
#### å»ºæ¨¡

åˆ›å»ºå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ¨¡å‹ï¼š
```python
import plotly.figure_factory as ff

x_axes = ['Ham', 'Spam']
y_axes =  ['Spam', 'Ham']

def conf_matrix(z, x=x_axes, y=y_axes):
    
    z = np.flip(z, 0)

    # change each element of z to type string for annotations
    z_text = [[str(y) for y in x] for x in z]

    # set up figure 
    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')

    # add title
    fig.update_layout(title_text='<b>Confusion matrix</b>',
                      xaxis = dict(title='Predicted value'),
                      yaxis = dict(title='Real value')
                     )

    # add colorbar
    fig['data'][0]['showscale'] = True
    
    return fig

# Create a Multinomial Naive Bayes model
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

# Train the model
nb.fit(x_train_dtm, y_train)
```
##### æœ´ç´ è´å¶æ–¯ DTM

åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œ**æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨**æ˜¯ç®€å•çš„â€œ**æ¦‚ç‡åˆ†ç±»å™¨**â€ï¼ŒåŸºäºåº”ç”¨è´å¶æ–¯å®šç†ä»¥åŠç‰¹å¾ä¹‹é—´çš„å¼ºï¼ˆæœ´ç´ ï¼‰ç‹¬ç«‹æ€§å‡è®¾ã€‚å®ƒä»¬æ˜¯æœ€ç®€å•çš„è´å¶æ–¯ç½‘ç»œæ¨¡å‹ä¹‹ä¸€ï¼Œä½†ä¸æ ¸å¯†åº¦ä¼°è®¡ç›¸ç»“åˆï¼Œå®ƒä»¬å¯ä»¥è¾¾åˆ°æ›´é«˜çš„ç²¾åº¦æ°´å¹³ã€‚**æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨**å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œéœ€è¦å­¦ä¹ é—®é¢˜ä¸­çš„è®¸å¤šå‚æ•°ä¸å˜é‡ï¼ˆç‰¹å¾/é¢„æµ‹å˜é‡ï¼‰çš„æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚`Maximum-likelihood`è®­ç»ƒå¯ä»¥é€šè¿‡è¯„ä¼°å°é—­è¡¨è¾¾å¼æ¥å®Œæˆï¼Œè¿™éœ€è¦çº¿æ€§æ—¶é—´ï¼Œè€Œä¸æ˜¯åƒè®¸å¤šå…¶ä»–ç±»å‹çš„åˆ†ç±»å™¨é‚£æ ·é€šè¿‡æ˜‚è´µçš„è¿­ä»£è¿‘ä¼¼æ¥å®Œæˆã€‚
{% asset_img na_8.png %}

```python
# Make class anf probability predictions
y_pred_class = nb.predict(x_test_dtm)
y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]

# calculate accuracy of class predictions
from sklearn import metrics
print(metrics.accuracy_score(y_test, y_pred_class))
# Calculate AUC
metrics.roc_auc_score(y_test, y_pred_prob)

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))

# 0.9784637473079684
# 0.974296765425861
```
{% asset_img na_9.png %}

##### æœ´ç´ è´å¶æ–¯

```python
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), ('tfid', TfidfTransformer()), ('model', MultinomialNB())])

# Fit the pipeline with the data
pipe.fit(x_train, y_train)
y_pred_class = pipe.predict(x_test)
print(metrics.accuracy_score(y_test, y_pred_class))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))

# 0.9597989949748744
```
{% asset_img na_10.png %}

##### XGBoost

```python
import xgboost as xgb

pipe = Pipeline([
    ('bow', CountVectorizer()), 
    ('tfid', TfidfTransformer()),  
    ('model', xgb.XGBClassifier(
        learning_rate=0.1,
        max_depth=7,
        n_estimators=80,
        use_label_encoder=False,
        eval_metric='auc',
        # colsample_bytree=0.8,
        # subsample=0.7,
        # min_child_weight=5,
    ))
])

# Fit the pipeline with the data
pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)
y_pred_train = pipe.predict(x_train)

print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))
print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))

# Train: 0.9834888729361091
# Test: 0.9662598707824839
```
{% asset_img na_11.png %}

#### é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)

```python
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    train_padded_sentences, 
    target, 
    test_size=0.25
)

def glove_lstm():
    model = Sequential()
    
    model.add(Embedding(
        input_dim=embedding_matrix.shape[0], 
        output_dim=embedding_matrix.shape[1], 
        weights = [embedding_matrix], 
        input_length=length_long_sentence
    ))
    
    model.add(Bidirectional(LSTM(
        length_long_sentence, 
        return_sequences = True, 
        recurrent_dropout=0.2
    )))
    
    model.add(GlobalMaxPool1D())
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence, activation = "relu"))
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence, activation = "relu"))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
    
    return model

model = glove_lstm()
model.summary()
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 80, 100)           672600    
_________________________________________________________________
bidirectional (Bidirectional (None, 80, 160)           115840    
_________________________________________________________________
global_max_pooling1d (Global (None, 160)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 160)               640       
_________________________________________________________________
dropout (Dropout)            (None, 160)               0         
_________________________________________________________________
dense (Dense)                (None, 80)                12880     
_________________________________________________________________
dropout_1 (Dropout)          (None, 80)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 80)                6480      
_________________________________________________________________
dropout_2 (Dropout)          (None, 80)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 81        
=================================================================
Total params: 808,521
Trainable params: 808,201
Non-trainable params: 320
_________________________________________________________________
```
```python
# Load the model and train!!
model = glove_lstm()

checkpoint = ModelCheckpoint(
    'model.h5', 
    monitor = 'val_loss', 
    verbose = 1, 
    save_best_only = True
)
reduce_lr = ReduceLROnPlateau(
    monitor = 'val_loss', 
    factor = 0.2, 
    verbose = 1, 
    patience = 5,                        
    min_lr = 0.001
)
history = model.fit(
    X_train, 
    y_train, 
    epochs = 7,
    batch_size = 32,
    validation_data = (X_test, y_test),
    verbose = 1,
    callbacks = [reduce_lr, checkpoint]
)
```
è®©æˆ‘ä»¬çœ‹çœ‹ç»“æœï¼š
```python
def plot_learning_curves(history, arr):
    fig, ax = plt.subplots(1, 2, figsize=(20, 5))
    for idx in range(2):
        ax[idx].plot(history.history[arr[idx][0]])
        ax[idx].plot(history.history[arr[idx][1]])
        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)
        ax[idx].set_xlabel('A ',fontsize=16)
        ax[idx].set_ylabel('B',fontsize=16)
        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)

plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])
```
{% asset_img na_12.png %}

```python
y_preds = (model.predict(X_test) > 0.5).astype("int32")
conf_matrix(metrics.confusion_matrix(y_test, y_preds))
```
{% asset_img na_13.png %}

#### BERT

`BERT`ï¼ˆæ¥è‡ª`Transformer`çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼‰æ˜¯`Google AI Language`çš„ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ã€‚å®ƒåœ¨å„ç§`NLP`ä»»åŠ¡ä¸­å±•ç¤ºäº†ä¼˜ç§€çš„ç»“æœï¼ŒåŒ…æ‹¬é—®ç­”(`SQuAD v1.1`)ã€è‡ªç„¶è¯­è¨€æ¨ç† (`MNLI`) ç­‰ï¼Œåœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºå¼•èµ·äº†è½°åŠ¨ã€‚`BERT`çš„å…³é”®æŠ€æœ¯åˆ›æ–°æ˜¯å°†æµè¡Œçš„æ³¨æ„åŠ›æ¨¡å‹`Transformer`çš„åŒå‘è®­ç»ƒåº”ç”¨äºè¯­è¨€å»ºæ¨¡ã€‚è¿™ä¸ä¹‹å‰çš„ç ”ç©¶å½¢æˆå¯¹æ¯”ï¼Œä¹‹å‰çš„ç ”ç©¶ä»å·¦åˆ°å³æˆ–ä»å³åˆ°å·¦ç»„åˆè®­ç»ƒæ–‡æœ¬åºåˆ—ã€‚è®ºæ–‡ç»“æœæ˜¾ç¤ºï¼ŒåŒå‘è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¯”å•å‘è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´æ·±å…¥åœ°äº†è§£è¯­è¨€ä¸Šä¸‹æ–‡å’Œæµç¨‹ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œç ”ç©¶äººå‘˜è¯¦ç»†ä»‹ç»äº†ä¸€ç§åä¸º`Masked LM (MLM)`çš„æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å…è®¸åœ¨æ¨¡å‹ä¸­è¿›è¡ŒåŒå‘è®­ç»ƒï¼Œè€Œè¿™åœ¨ä»¥å‰æ˜¯æ— æ³•æƒ³è±¡çš„ã€‚
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import transformers
from tqdm.notebook import tqdm
from tokenizers import BertWordPieceTokenizer
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from transformers import BertTokenizer
from transformers import TFBertModel

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
    
except:
    strategy = tf.distribute.get_strategy()
    
print('Number of replicas in sync: ', strategy.num_replicas_in_sync)

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')

def bert_encode(data, maximum_length) :
    input_ids = []
    attention_masks = []

    for text in data:
        encoded = tokenizer.encode_plus(
            text, 
            add_special_tokens=True,
            max_length=maximum_length,
            pad_to_max_length=True,

            return_attention_mask=True,
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
        
    return np.array(input_ids),np.array(attention_masks)

texts = df['message_clean']
target = df['target_encoded']

train_input_ids, train_attention_masks = bert_encode(texts,60)

def create_model(bert_model):
    
    input_ids = tf.keras.Input(shape=(60,),dtype='int32')
    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')

    output = bert_model([input_ids,attention_masks])
    output = output[1]
    output = tf.keras.layers.Dense(32,activation='relu')(output)
    output = tf.keras.layers.Dropout(0.2)(output)
    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)
    
    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])
    return model

bert_model = TFBertModel.from_pretrained('bert-base-uncased')
model = create_model(bert_model)
model.summary()
```
ç»“æœè¾“å‡ºä¸ºï¼š
```bash
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 60)]         0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 60)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 32)           24608       tf_bert_model[0][1]              
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 32)           0           dense_6[0][0]                    
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            33          dropout_43[0][0]                 
==================================================================================================
Total params: 109,506,881
Trainable params: 109,506,881
Non-trainable params: 0
__________________________________________________________________________________________________
```
```python

# è®­ç»ƒæ¨¡å‹
history = model.fit([train_input_ids, train_attention_masks],target,validation_split=0.2, epochs=3,batch_size=10)
```
```bash
Epoch 1/3
446/446 [==============================] - 1803s 4s/step - loss: 0.2607 - accuracy: 0.9095 - val_loss: 0.0709 - val_accuracy: 0.9767
Epoch 2/3
446/446 [==============================] - 1769s 4s/step - loss: 0.0636 - accuracy: 0.9838 - val_loss: 0.0774 - val_accuracy: 0.9767
Epoch 3/3
446/446 [==============================] - 1781s 4s/step - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.0452 - val_accuracy: 0.9857
```
```python
plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])
```
{% asset_img na_14.png %}

#### NLP: Disaster Tweets

```python
df = pd.read_csv("/kaggle/input/nlp-getting-started/train.csv", encoding="latin-1")
test_df = pd.read_csv("/kaggle/input/nlp-getting-started/test.csv", encoding="latin-1")

df = df.dropna(how="any", axis=1)
df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))

df.head()
```
ç»“æœè¾“å‡ºä¸ºï¼š
```bash
	id	text	target	text_len
0	1	Our Deeds are the Reason of this #earthquake M...	1	13
1	4	Forest fire near La Ronge Sask. Canada	1	7
2	5	All residents asked to 'shelter in place' are ...	1	22
3	6	13,000 people receive #wildfires evacuation or...	1	9
4	7	Just got sent this photo from Ruby #Alaska as ...	1	17
```
```python
balance_counts = df.groupby('target')['target'].agg('count').values
balance_counts

fig = go.Figure()
fig.add_trace(go.Bar(
    x=['Fake'],
    y=[balance_counts[0]],
    name='Fake',
    text=[balance_counts[0]],
    textposition='auto',
    marker_color=primary_blue
))
fig.add_trace(go.Bar(
    x=['Real disaster'],
    y=[balance_counts[1]],
    name='Real disaster',
    text=[balance_counts[1]],
    textposition='auto',
    marker_color=primary_grey
))
fig.update_layout(
    title='<span style="font-size:32px; font-family:Times New Roman">Dataset distribution by target</span>'
)
fig.show()
```
{% asset_img na_15.png %}

```python
disaster_df = df[df['target'] == 1]['text_len'].value_counts().sort_index()
fake_df = df[df['target'] == 0]['text_len'].value_counts().sort_index()

fig = go.Figure()
fig.add_trace(go.Scatter(
    x=disaster_df.index,
    y=disaster_df.values,
    name='Real disaster',
    fill='tozeroy',
    marker_color=primary_blue,
))
fig.add_trace(go.Scatter(
    x=fake_df.index,
    y=fake_df.values,
    name='Fake',
    fill='tozeroy',
    marker_color=primary_grey,
))
fig.update_layout(
    title='<span style="font-size:32px; font-family:Times New Roman">Data Roles in Different Fields</span>'
)
fig.show()
```
{% asset_img na_16.png %}

##### æ•°æ®é¢„å¤„ç†

```python
def remove_url(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', text)


def remove_emoji(text):
    emoji_pattern = re.compile(
        '['
        u'\U0001F600-\U0001F64F'  # emoticons
        u'\U0001F300-\U0001F5FF'  # symbols & pictographs
        u'\U0001F680-\U0001F6FF'  # transport & map symbols
        u'\U0001F1E0-\U0001F1FF'  # flags (iOS)
        u'\U00002702-\U000027B0'
        u'\U000024C2-\U0001F251'
        ']+',
        flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)


def remove_html(text):
    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
    return re.sub(html, '', text)

# Special thanks to https://www.kaggle.com/tanulsingh077 for this function
def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub(
        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 
        '', 
        text
    )
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    
    text = remove_url(text)
    text = remove_emoji(text)
    text = remove_html(text)
    
    return text

# Test emoji removal
remove_emoji("Omg another Earthquake ğŸ˜”ğŸ˜”")

stop_words = stopwords.words('english')
more_stopwords = ['u', 'im', 'c']
stop_words = stop_words + more_stopwords

stemmer = nltk.SnowballStemmer("english")

def preprocess_data(text):
    # Clean puntuation, urls, and so on
    text = clean_text(text)
    # Remove stopwords and Stemm all the words in the sentence
    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stop_words)

    return text

test_df['text_clean'] = test_df['text'].apply(preprocess_data)

df['text_clean'] = df['text'].apply(preprocess_data)
df.head()
```
ç»“æœè¾“å‡ºä¸ºï¼š
```bash
   id                                               text  target  text_len                                         text_clean
0   1  Our Deeds are the Reason of this #earthquake M...       1        13          deed reason earthquak may allah forgiv us
1   4             Forest fire near La Ronge Sask. Canada       1         7               forest fire near la rong sask canada
2   5  All residents asked to 'shelter in place' are ...       1        22  resid ask shelter place notifi offic evacu she...
3   6  13,000 people receive #wildfires evacuation or...       1         9       peopl receiv wildfir evacu order california 
4   7  Just got sent this photo from Ruby #Alaska as ...       1        17  got sent photo rubi alaska smoke wildfir pour ...
```

##### è¯äº‘

```python
def create_corpus_df(tweet, target):
    corpus=[]
    
    for x in tweet[tweet['target']==target]['text_clean'].str.split():
        for i in x:
            corpus.append(i)
    return corpus

corpus_disaster_tweets = create_corpus_df(df, 1)

dic=defaultdict(int)
for word in corpus_disaster_tweets:
    dic[word]+=1
        
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]
print(top)

# [('fire', 266),('bomb', 179),('kill', 158),('news', 132),('via', 121),('flood', 120),('disast', 116),('california', 115),
#  ('crash', 110),('suicid', 110)]
```
```python
twitter_mask = np.array(Image.open('twitter_mask3.jpg'))

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 1, 'text_clean']))
plt.figure(figsize=(12,6))
plt.title('Top words for Real Disaster tweets', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()
```
{% asset_img na_17.png %}

```python
corpus_disaster_tweets = create_corpus_df(df, 0)

dic=defaultdict(int)
for word in corpus_disaster_tweets:
    dic[word]+=1
        
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]
top

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 0, 'text_clean']))
plt.figure(figsize=(18,10))
plt.title('Top words for Fake messages', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()

[('like', 306),('get', 222),('amp', 192),('new', 168),('go', 142),('dont', 139),('one', 134),('bodi', 116),
 ('love', 115),('bag', 108)]
```
{% asset_img na_18.png %}

##### å»ºæ¨¡

```python
# how to define X and y (from the SMS data) for use with COUNTVECTORIZER
x = df['text_clean']
y = df['target']

# Split into train and test sets
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)
print(len(x_train), len(y_train))
print(len(x_test), len(y_test))

pipe = Pipeline([
    ('bow', CountVectorizer()), 
    ('tfid', TfidfTransformer()),  
    ('model', xgb.XGBClassifier(
        use_label_encoder=False,
        eval_metric='auc',
    ))
])
from sklearn import metrics

# Fit the pipeline with the data
pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)
y_pred_train = pipe.predict(x_train)

print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))
print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))

# 5709 5709
# 1904 1904

# Train: 0.8567174636538798
# Test: 0.7725840336134454
```
{% asset_img na_19.png %}

##### GloVe - LSTM

æˆ‘ä»¬å°†ä½¿ç”¨`LSTM`ï¼ˆé•¿çŸ­æœŸè®°å¿†ï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦æ‰§è¡Œæ ‡è®°åŒ–â€”â€”å°†æ–‡æœ¬åˆ†å‰²æˆå•è¯å¥å­çš„å¤„ç†ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ‰”æ‰äº†æ ‡ç‚¹ç¬¦å·å’Œé¢å¤–çš„ç¬¦å·ã€‚æ ‡è®°åŒ–çš„å¥½å¤„åœ¨äºï¼Œå®ƒæ›´å®¹æ˜“è½¬æ¢ä¸ºåŸå§‹æ•°å­—çš„æ ¼å¼ï¼Œè¿™å®é™…ä¸Šå¯ä»¥ç”¨äºå¤„ç†ï¼š
```python
train_tweets = df['text_clean'].values
test_tweets = test_df['text_clean'].values
train_target = df['target'].values

# Calculate the length of our vocabulary
word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(train_tweets)

vocab_length = len(word_tokenizer.word_index) + 1
vocab_length

def show_metrics(pred_tag, y_test):
    print("F1-score: ", f1_score(pred_tag, y_test))
    print("Precision: ", precision_score(pred_tag, y_test))
    print("Recall: ", recall_score(pred_tag, y_test))
    print("Acuracy: ", accuracy_score(pred_tag, y_test))
    print("-"*50)
    print(classification_report(pred_tag, y_test))
    
def embed(corpus): 
    return word_tokenizer.texts_to_sequences(corpus)

longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
    embed(train_tweets), 
    length_long_sentence, 
    padding='post'
)
test_padded_sentences = pad_sequences(
    embed(test_tweets), 
    length_long_sentence,
    padding='post'
)

train_padded_sentences

# 13704

# array([[3635,  467,  201, ...,    0,    0,    0],
#        [ 136,    2,  106, ...,    0,    0,    0],
#        [1338,  502, 1807, ...,    0,    0,    0],
#        ...,
#        [ 448, 1328,    0, ...,    0,    0,    0],
#        [  28,  162, 2637, ...,    0,    0,    0],
#        [ 171,   31,  413, ...,    0,    0,    0]], dtype=int32)
```
ä¸ºäº†è·å¾—å•è¯çš„å‘é‡è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç§åä¸º`GloVe`ï¼ˆå•è¯è¡¨ç¤ºçš„å…¨å±€å‘é‡ï¼‰çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸“æ³¨äºæ•´ä¸ªè¯­æ–™åº“ä¸­å•è¯çš„å…±ç°ã€‚å®ƒçš„åµŒå…¥ä¸ä¸¤ä¸ªå•è¯ä¸€èµ·å‡ºç°çš„æ¦‚ç‡æœ‰å…³ã€‚
```python
# Load GloVe 100D embeddings
# We are not going to do it here as they were loaded earlier.

# Now we will load embedding vectors of those words that appear in the
# Glove dictionary. Others will be initialized to 0.
embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
        
embedding_matrix

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    train_padded_sentences, 
    train_target, 
    test_size=0.25
)


# Load the model and train!!
model = glove_lstm()

checkpoint = ModelCheckpoint(
    'model.h5', 
    monitor = 'val_loss', 
    verbose = 1, 
    save_best_only = True
)
# æ¨¡å‹ LSTM
reduce_lr = ReduceLROnPlateau(
    monitor = 'val_loss', 
    factor = 0.2, 
    verbose = 1, 
    patience = 5,                        
    min_lr = 0.001
)
history = model.fit(
    X_train, 
    y_train, 
    epochs = 7,
    batch_size = 32,
    validation_data = (X_test, y_test),
    verbose = 1,
    callbacks = [reduce_lr, checkpoint]
)

plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])
```
{% asset_img na_20.png %}

```python
preds = model.predict_classes(X_test)
show_metrics(preds, y_test)
```
ç»“æœè¾“å‡ºä¸ºï¼š
```bash
F1-score:  0.7552910052910053
Precision:  0.6709753231492362
Recall:  0.8638426626323752
Acuracy:  0.805672268907563
--------------------------------------------------
              precision    recall  f1-score   support

           0       0.91      0.77      0.84      1243
           1       0.67      0.86      0.76       661

    accuracy                           0.81      1904
   macro avg       0.79      0.82      0.80      1904
weighted avg       0.83      0.81      0.81      1904
```
