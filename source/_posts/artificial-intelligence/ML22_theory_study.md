---
title: 机器学习(ML)(二十二) — 强化学习探析
date: 2024-12-26 10:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### RLHF

**人类反馈的强化学习**(`RLHF`)是一种结合了**人类反馈**与**强化学习**技术的**机器学习**方法，旨在提高人工智能模型的表现，尤其是在生成式人工智能（如`LLM`）中的应用。**人类反馈的强化学习**(`RLHF`)的核心思想是利用人类提供的反馈来优化**机器学习**模型，使其能够更好地满足用户需求和期望。传统的**强化学习**依赖于预定义的**奖励函数**来指导学习，而`RLHF`则将人类的主观反馈纳入其中，以便更灵活地捕捉复杂任务中的细微差别和主观性。
<!-- more -->
`RLHF`通常包括以下几个步骤：
- **预训练语言模型**：首先，使用大量标注数据对语言模型进行预训练。这一步骤通常通过**监督学习**完成，以确保模型能够生成合理的初步输出。
- **训练奖励模型**：在此阶段，生成多个可能的问答，并由人类评估这些问答的质量。人类反馈被用于训练一个**奖励模型**，该模型能够评估生成内容的好坏。
- **强化学习微调**：最后，使用训练好的**奖励模型**对**语言模型**进行**微调**，通过**强化学习算法**（如**近端策略优化**：`PPO`）进一步优化其表现，以便更好地符合人类反馈和偏好。

**人类反馈的强化学习**(`RLHF`)在多个领域展现了其重要性，尤其是在**自然语言处理**(`NLP`)和**生成式**`AI`中。通过引入**人类反馈**，`RLHF`能够：**提高生成内容的人性化程度**，使得`AI`生成的文本更符合人类的沟通习惯和情感表达；**增强适应性**，`AI`系统能够根据实时反馈调整其行为，**解决复杂任务**，在一些难以明确量化成功标准的任务中，`RLHF`提供了一种有效的方法来利用人类直观判断作为反馈。适应不断变化的用户需求和偏好。**人类反馈的强化学习**(`RLHF`)是一种前沿技术，通过将**人类直观反馈**与**强化学习**结合起来，为生成式`AI`的发展提供了新的方向。它不仅提高了`AI`系统与用户之间的互动质量，也为复杂任务提供了新的解决方案。

##### 预训练语言模型

首先，使用经典的预训练目标训练一个语言模型，对这一步模型，`OpenAI`在其第一个`RLHF`模型的`InstructGPT`中使用了较小版本的`GPT-3`；`Anthropic`使用了`100`0万 `～` `520`亿参数的`Transformer`模型进行训练；`DeepMind`使用了自家的`2800`亿参数模型`Gopher`。这里可以用额外的文本或者条件对这个`LM`进行微调，例如`OpenAI`采用 “**更可取**”(`preferable`)的人工生成文本进行了微调；而`Anthropic`采用了“**有用、诚实和无害**” 的标准在上下文线索上蒸馏了原始的`LM`。这里或许使用了昂贵的增强数据，但并不是`RLHF`必要的一步。由于`RLHF`还是一个尚待探索的领域，对于” 哪种模型” 适合作为`RLHF`的起点并没有明确的答案。
{% asset_img ml_1.png %}

##### 训练奖励模型

**奖励模型** (`RM`，也叫**偏好模型**)的训练是`RLHF`区别于旧范式的开端。这一模型接收一系列文本并返回一个标量奖励，数值上对应人的偏好。我们可以用端到端的方式用`LM`建模，或者用模块化的系统建模 (比如对输出进行排名，再将排名转换为奖励) 。这一奖励数值将对后续无缝接入现有的`RL`算法至关重要。关于模型选择方面，**奖励模型**可以是另一个经过微调的`LM`，也可以根据偏好数据从头开始训练的`LM`。例如`Anthropic`提出了一种特殊的预训练方式，即用**偏好模型预训练**(`Preference Model Pretraining，PMP`)来替换一般预训练后的微调过程。因为前者被认为对样本数据的利用率更高。但对于哪种**奖励模型**更好尚无定论。

关于训练文本方面，**奖励模型** 的提示-生成对文本是从预定义数据集中采样生成的，并用初始的`LM`给这些提示生成文本。`Anthropic`的数据主要是通过`Amazon Mechanical Turk`上的聊天工具生成的，并在`Hub`上可用，而 `OpenAI`使用了用户提交给`GPT API`的`prompt`。

关于训练奖励数值方面，这里需要人工对`LM`生成的回答进行排名，起初可能会认为应该直接对文本标注分数来训练**奖励模型**，但是由于标注者的价值观不同导致这些分数未经过校准并且充满噪声。通过排名可以比较多个模型的输出并构建更好的规范数据集。对具体的排名方式，是对不同的`LM`在相同提示下的输出进行比较，然后使用`Elo`(评分系统，是一种用于计算棋手和其他竞技游戏玩家相对技能水平的方法)系统建立一个完整的排名。这些不同的排名结果将被归一化为用于训练的**标量奖励值**。
{% asset_img ml_2.png %}

##### 强化学习微调

长期以来出于工程和算法原因，人们认为用**强化学习**训练LM是不可能的。而目前多个组织找到的可行方案是使用**策略梯度强化学习**(`Policy Gradient RL`)**算法**、**近端策略优化**(`Proximal Policy Optimization，PPO`)微调初始`LM`的部分或全部参数。因为微调整个`10B～100B+`参数的成本过高。首先将微调任务表述为**强化学习**问题。该策略是一个接受提示并返回一系列文本(或文本的概率分布)的`LM`。这个策略的**动作空间**(`action space`)是`LM`的词表对应的所有**词元** (一般在`50k`数量级)，**观察空间**(`observation space`)是输入**词元序列**，也比较大(词汇量 `x` 输入标记的数量)。**奖励函数**是奖**励模型**和**策略转变约束**(`Policy shift constraint`)的结合。PPO算法的奖励函数计算如下：将提示(`prompt`){% mathjax %}x{% endmathjax %}输入初始LM和当前微调的LM，分别得到了输出文本{% mathjax %}y_1,y_2{% endmathjax %}将来自当前策略的文本传递给**奖励模型**得到一个标量的奖励{% mathjax %}r_{\theta}{% endmathjax %}。将两个模型的生成文本进行比较计算差异的**惩罚项**，在来自`OpenAI、Anthropic`和`DeepMind`的多篇论文中设计为输出词分布序列之间的`Kullback–Leibler (KL) divergence`散度的缩放，即{% mathjax %}r = r_{\theta} - \lambda r_{KL}{% endmathjax %}。这一项被用于惩罚**强化学习**策略在每个训练批次中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。此外，`OpenAI`在`InstructGPT`上实验了在`PPO`添加新的**预训练梯度**，可以预见到**奖励函数**的公式会随着`RLHF`研究的进展而继续进化。最后根据`PPO`算法，按当前批次数据的**奖励指标**进行优化(来自`PPO`算法`on-policy`的特性)。`PPO`算法是一种**信赖域优化**(`Trust Region Optimization，TRO`)算法，它使用**梯度约束**确保更新步骤不会破坏学习过程的稳定性。`DeepMind`对`Gopher`使用了类似的奖励设置，但是使用`A2C`(`synchronous advantage actor-critic`)算法来优化**梯度**。
{% asset_img ml_3.png %}

作为一个可选项，`RLHF`可以通过迭代**奖励模型**和策略共同优化。随着策略模型更新，用户可以继续将输出和早期的输出进行合并排名。`Anthropic`在他们的论文中讨论了迭代在线`RLHF`，其中策略的迭代包含在跨模型的`Elo`**排名系统**中。这样引入策略和**奖励模型**演变的复杂动态，代表了一个复杂和开放的研究问题。收集人类偏好数据的质量和数量决定了`RLHF`系统性能的上限。`RLHF`系统需要两种人类偏好数据：**人工生成的文本**和**对模型输出的偏好标签**。除开数据方面的限制，一些有待开发的设计选项可以让`RLHF`取得长足进步。例如对`RL`**优化器**的改进方面，`PPO`是一种较旧的算法，但目前没有什么结构性原因让其他算法可以在现`有RLHF`工作中更具有优势。另外，微调`LM`策略的成本是策略生成的文本都需要在`RM`上进行评估，通过离线`RL`优化策略可以节约这些大模型`RM`的预测成本。最近，出现了新的`RL`算法如**隐式语言**`Q-Learning`(`Implicit Language Q-Learning，ILQL`) 也适用于当前`RL`的优化。在`RL`训练过程的其他核心权衡，例如探索和开发(`exploration-exploitation`) 的平衡也有待尝试和记录。