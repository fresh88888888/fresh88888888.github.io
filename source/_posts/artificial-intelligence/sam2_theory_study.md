---
title: SAM2模型-探析(深度学习)
date: 2024-08-02 15:44:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

[`SAM2(Segment Anything Model 2)`](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)是`Meta AI`最新发布的图像和视频分割模型,是`Segment Anything Model(SAM)`的下一代模型。`SAM2`是一个统一的模型,可以同时处理**图像**和**视频**的**分割任务**。这种统一的架构简化了部署,并在不同媒体类型中实现了一致的性能。`SAM2`采用了**提示式视觉分割**(`Promptable Visual Segmentation, PVS`)的方法。用户可以通过**点击**、**边界框**或**掩码**等方式在视频的任何帧上提供提示,模型会立即生成相应的分割掩码,并将其传播到整个视频中。
<!-- more -->

`SAM2`还可以分割任何视频或图像中的任何对象（通常称为**零样本泛化**），这意味着它可以应用于以前从未见过的视觉内容，而无需进行自定义调整。
{% asset_img s_1.png  "SAM2模型在SA-V的数据集上进行训练，主要解决基于提示的视觉分割任务" %}

**图像分割**：`Segment Anything`（`Kirillov`等人，`2023`年）引入了一种可以提示的图像分割任务，其目标是在给定输入提示（例如边界框或指向感兴趣对象的点）的情况下输出有效的分割掩码。在`SA-1B`数据集上训练的`SAM`允许使用灵活提示进行零样本分割，从而使其能够应用于广泛的下游应用。最近的工作通过提高其质量扩展了`SAM`。例如，`HQ-SAM`（`Ke`等人，`2024`年）通过引入高质量输出`token`并在细粒度掩码上训练模型来增强`SAM`。另一项工作重点是提高`SAM`的效率，使其在现实世界和移动应用中得到更广泛的应用，例如`EfficientSAM`（`Xiong`等人，`2023`年）、`MobileSAM`（`Zhang`等人，`2023`年）和`FastSAM`（`Zhao`等人，`2023`年）。

**交互式视频对象分割**(`iVOS`)：交互式视频对象分割已成为一项关键任务，可在用户指导下有效获得视频中的对象分割(`masklets`)，通常以涂鸦、点击或边界框的形式出现。一些早期方法 (`Wang`等人，`2005`年；`Bai & Sapiro`，`2007`年；`Fan`等人，`2015`年) 部署基于图的优化来指导分割注释过程。较新的方法 (`Heo`等人，`2020`年；`Cheng`等人，`2021`年；`Delatolas`等人，`2024`年) 通常采用模块化设计，将用户输入转换为单个帧上的掩码表示，然后将其传播到其他帧。在视频中分割对象并实现良好的交互体验，并且我们建立了一个强大的模型和一个庞大而多样化的数据集来实现这一目标。具体来说，`DAVIS`交互式基准 (`Caelles`等人，`2018`年) 允许通过多帧上的涂鸦输入以交互方式分割对象。然而，这些方法有局限性：跟踪器可能无法适用于所有对象，`SAM`可能无法很好地处理视频中的图像帧，并且除了使用`SAM`从头开始​​对错误帧进行重新注释并从那里重新启动跟踪之外，没有其他机制可以交互地改进模型的错误。

**半监督视频对象分割**(`VOS`)。半监督`VOS`通常以第一帧中的对象掩码作为输入开始，必须在整个视频中准确跟踪该掩码(`Pont-Tuset`等，`2017`)。之所以被称为“半监督”，是因为**输入掩码**看作是仅适用于第一帧的对象轮廓的监督信号。这项任务因其与各种应用的相关性而引起了广泛关注，包括视频编辑、机器人技术和自动背景去除。半监督`VOS`可以看作是**可提示视觉分割**(`PVS`)任务的一个特例，因为它相当于仅在第一个视频帧中提供掩码提示。然而，在第一帧中注释高质量的对象掩码实际上具有挑战性且耗时。

**视频分割数据集**：已经提出了许多数据集来支持`VOS`任务。然而当前的视频分割数据集缺乏足够的覆盖范围来实现“分割视频中的所有内容”的能力。它们的注释通常覆盖整个对象（而不是部分），数据集通常以特定对象类别为中心，例如人、车辆和动物。与这些数据集相比，当前发布的`SA-V`数据集不仅关注整个对象，还广泛覆盖对象的子部分，并包含超过一个数量级的掩码。
{% asset_img s_2.png %}

使用`SAM2`进行交互式分割：
- 步骤1（选择）：我们在第`1`帧中提示`SAM2`获取目标对象（舌头）的片段。绿点/红点分别表示正/负提示。`SAM2`自动将片段传播到后续帧（蓝色箭头）以形成`masklet`。如果`SAM2`丢失了对象（第2帧之后），我们可以通过在新帧（红色箭头）中提供额外提示来更正`masklet`。
- 步骤2（细化）：在第`3`帧中单击一次就足以恢复对象并传播它以获得正确的`masklet`。分离的`SAM`+ 视频跟踪器方法需要在第`3`帧（如第`1`帧）中单击几次才能在从头开始重新开始正确地分割并重新注释对象。借助`SAM2`的内存，单击一次即可恢复舌头。

#### 提示视觉分割(PVS)任务

`PVS`任务允许在视频的任何帧上向模型提供提示。提示可以是正/负点击、边界框或蒙版，用于定义要分割的对象或细化模型预测的对象。为了提供交互式体验，在收到特定帧上的提示后，模型应立即响应该帧上对象的有效分割蒙版。在收到初始（一个或多个）提示（在同一帧或不同帧上）后，模型应传播这些提示以获取整个视频中对象的蒙版，其中包含每个视频帧上目标对象的分割蒙版。可以在任何帧上向模型提供其他提示，以细化整个视频中的片段。

#### 模型

`SAM2`支持在单个帧上提示(点、框和掩码)，以定义视频中要分割的对象的空间范围。对于图像输入，该模型的行为类似于`SAM`。提示的掩码解码器接受当前帧上的帧嵌入和提示（如果有），并输出该帧的分割掩码。可以在帧上迭代添加提示以细化掩码。与`SAM`不同，`SAM2`解码器使用的帧嵌入，以过去预测和提示帧的记忆为条件。提示帧也可能来自相对于当前帧的“未来”。帧的记忆由**记忆编码器**根据当前的预测创建，并放置在记忆库中以供后续帧使用。**记忆注意力**操作从图像编码器获取每帧嵌入，并在记忆库上对其进行调节以生成嵌入，然后将其传递给**掩码解码器**。
{% asset_img s_3.png %}

SAM2的架构。对于给定帧，分割预测取决于当前提示`and / or`先前观察到的记忆。视频以流式方式处理，图像编码器一次消耗一个帧，并与先前帧中的目标对象的记忆交叉关注。掩码解码器（也可以选择接受输入提示）预测该帧的分割掩码。最后，记忆编码器变换预测和图像编码器嵌入以用于未来的帧。
- **图像编码器**：对于任意长视频的实时处理，我们采用流式传输方式，在视频帧可用时使用。图像编码器在整个交互过程中仅运行一次，其作用是提供代表每个帧的无约束`tokens`（特征嵌入）。我们使用`MAE`（`He`等人，`2022`年）预训练的`Hiera`（`Ryali`等人，`2023`年；`Bolya`等人，`2023`年）图像编码器，它是分层的，允许我们在解码过程中使用多尺度特征。
- **记忆注意力**(`Memory Attention`)。记忆注意力的作用是根据过去帧的特征和预测以及新的提示来调节当前帧的特征。这里堆叠`L`个转换器块，第一个将当前帧的图像编码作为输入。每个块执行自注意力操作，然后对存储在内存中的（提示/未提示）帧和对象指针的记忆进行交叉注意，然后是`MLP`。我们使用原始的注意力操作进行自我注意力和交叉注意力，这样能够从高效注意力内核的最新发展中受益（`Dao，2023`年）。
- 