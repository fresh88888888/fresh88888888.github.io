---
title: BERT模型—探析（Transformer）
date: 2024-07-08 18:30:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

语言模型是一种概率模型，它为单词序列分配概率。实际上，语言模型允许我们计算以下内容：我们通常训练一个神经网络来预测这些概率。在大量文本上训练的神经网络被称为大型语言模型(`LLM`)。
<!-- more -->
{% asset_img b_1.png %}

怎样训练&推理一个语言模型？假设我们想要训练一个中文诗歌语言模型，例如下面这个：
{% asset_img b_2.png %}

假设你是一个（懒惰的）学生，必须记住李白的诗，但只记得前两个字。你如何背诵出全诗？
{% asset_img b_3.png %}

{% asset_img b_4.png %}

#### Transformer Encoder架构

让我们将**输入**转换为**输入嵌入**。定义{% mathjax %}d_{\text{model}} = 512{% endmathjax %}，表示每个单词的嵌入向量的大小。
{% asset_img b_5.png %}

为什么我们要用向量来表示单词？给定单词`“cherry”`、`“digital”和“information”`，如果我们仅使用`2`个维度(`X，Y`)表示嵌入向量并绘制它们，我们希望看到类似这样的结果：具有相似含义的单词之间的角度很小，而具有不同含义的单词之间的角度很大。因此，通过嵌入将单词投影到大小为{% mathjax %}d_{\text{model}}{% endmathjax %}的高维空间中来“捕获”它们所表示的单词的含义。
{% asset_img b_6.png %}

我们通常使用**余弦相似度**，它是基于两个向量之间的**点积**，接下来让我们添加位置编码。每个`token`都转换为词汇表中的位置(`input_id`)，然后我们将每个`input_id`转换成大小为`512`的**嵌入向量**。接下来，我们给每个`token`添加一个大小为`512`的向量，以指示其在句子中的位置（**位置编码**）。位置编码在训练和推理期间仅计算一次并对每个句子重复使用。
{% asset_img b_7.png %}

如何计算位置编码？注意，我们只需要计算一次位置编码，然后在每个句子中重复使用它们，无论是训练还是推理。
{% asset_img b_8.png %}

**自注意力机制**：输入。每个`token`都被转换成它在词汇表中的位置(`input_id`)，然后我们将每个`input_id`转换成大小为`512`的嵌入向量并添加其位置向量（**位置编码**）。
{% asset_img b_9.png %}

形状为(`10, 512`)的矩阵，其中每一行代表输入序列中的一个`token`。
{% asset_img b_10.png %}

**自注意力机制**：`Q、K`和`V`。在大型语言模型(`LLM`)中，我们采用自注意力机制，这意味着查询(`Q`)、键(`K`)和值(`V`)是同一个矩阵。
{% asset_img b_11.png %}

自注意力机制让模型能够将单词相互关联。在我们的例子中，{% mathjax %}d_k = d_{\text{model}} = 512{% endmathjax %}。
{% mathjax '{"conversion":{"em":14}}' %}
\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
{% endmathjax %}
{% asset_img b_12.png %}

**自注意力机制**：因果掩码，语言模型是一种概率模型，它为单词序列分配概率。实际上，语言模型允许我们计算以下内容：
{% asset_img b_13.png %}

为了对上述概率分布进行建模，每个单词应该只依赖于它之前的单词（左上下文）。稍后会看到，在`BERT`中我们同时使用了左上下文和右上下文。
{% asset_img b_14.png %}

“**注意力输出**”矩阵的每一行代表输出序列的嵌入：它不仅捕获每个标记的含义、位置，还捕获每个标记与所有其他标记的交互，但只捕获`softmax`分数不为零的交互。每个向量的`512`个维度仅取决于非零的注意力分数。
{% asset_img b_15.png %}

#### BERT模型

`BERT`的架构由`Transformer`模型的编码器层组成：
- `BERT BASE`：`12`个编码器层、前馈层隐藏层的大小为`3072`、`12`个注意力头。
- `BERT LARGE`：`24`个编码器层、前馈层隐藏层的大小为`4096`、`16`个注意力头。

与`vanilla Transformer`的区别：
- 两个模型的嵌入向量分别为`768`和`1024`。
- 位置嵌入是绝对的，在训练过程中学习，并且限制为`512`个位置。
- 线性层头根据应用程序而变化。

使用`WordPiece`标记器，它还允许使用子词`token`。词汇表大小约为`30,000`个`token`。
{% asset_img b_16.png %}

`BERT`代表`Transformer`的双向编码器模型。
- 与常见语言模型不同，`BERT`不会处理带有提示的“`special tasks`”，而是可以通过微调专门处理特定任务。
- 与常见语言模型不同，`BERT`已使用左上下文和右上下文进行训练。
- 与常见语言模型不同，`BERT`并非专门为文本生成而构建。
- 与常见语言模型不同，`BERT`不在下一个`Token`预测任务上进行训练，而是在`Masked Language Model`和下一个句子预测任务上进行训练。

##### 掩蔽语言模型 (MLM)

也称为**完形填空任务**。这意味着句子中随机选择的单词被屏蔽，并且模型必须根据左右上下文预测正确的单词，`BERT`中的左/右上下文。
{% asset_img b_17.png %}

掩蔽语言模型(`MLM`)：训练。
{% asset_img b_18.png %}

下一句预测（`NSP`）。许多下游应用（例如在`4`个选项中选择正确答案）需要学习句子之间的关系而不是单个`token`，这就是为什么`BERT`已在下一句预测任务上进行了预训练。
