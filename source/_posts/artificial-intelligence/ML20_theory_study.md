---
title: 机器学习(ML)(二十) — 强化学习探析
date: 2024-12-11 12:30:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### ML-Agents

**强化学习**(`RL`)的挑战之一是**创建环境**。幸运的是，我们可以使用**游戏引擎**来实现它。这些引擎（例如`Unity`、`Godot`或`Unreal Engine`）是为创建视频游戏而开发的工具包。它们非常适合创建环境：它们提供物理系统、`2D/3D`渲染等。`Unity ML-Agents Toolkit`是一个`Unity`**游戏引擎**的插件，可以使用`Unity`**游戏引擎**作为**环境构建器**来训练**智能体**(`Agent`)。`Unity ML-Agents Toolkit`提供了许多出色的预制环境。
<!-- more -->
**深度强化学习**中的好奇心是什么？要理解好奇心是什么，我们首先需要了解**强化学习**(`RL`)的两个主要问题：
- **稀疏奖励问题**：即大多数奖励不包含信息，因此被设置为`0`。**强化学习**(`RL`)基于**奖励假设**，即每个目标都可以描述为奖励的最大化。因此，奖励充当**强化学习**(`RL`)**智能体**的反馈；如果没有收到任何反馈，就无法判定执行的动作合不合适。
- **奖励函数**是人为创建的；在每个环境中，都必须创建**奖励函数**。

创建一个**智能体**(`Agent`)固有的**奖励函数**，即由**智能体**(`Agent`)自身生成的**奖励函数**。**智能体**(`Agent`)将充当自学者，因为它将是学生和自己的反馈大师。这种内在奖励机制被称为好奇心，因为这种奖励会促使**智能体**(`Agent`)探索新奇/不熟悉的状态。为了实现这一点，**智能体**(`Agent`)在探索新轨迹时将获得高额奖励。这种奖励的灵感来源于人类的行为方式。人类天生就有探索环境、发现新事物的内在欲望。计算这种内在奖励的方法有很多种。经典方法是将**好奇心**计算为**智能体**(`Agent`)在给定当前状态和所采取的动作的情况下预测下一个状态的误差。

#### Actor-Critic

在基于策略的方法中，我们的目标是直接优化策略，而不使用价值函数。更准确地说，`Reinforce`是基于策略的方法的一个子类，称为**策略梯度方法**。这个子类通过使用**梯度上升**估计最优策略的权重来直接优化策略。虽然`Reinforce`效果很好。但是，由于使用了**蒙特卡洛抽样**来估计回报（使用整个回合来计算回报），因此在**策略梯度预测**中存在显著差异。**策略梯度预测**是收益增长最快的方向。换句话说，如何更新**策略权重**，以便有更高的概率采取良好收益的动作。`Actor-Critic`方法，这是一种结合价值和策略的方法的**混合架构**，它通过减少**方差**来帮助稳定训练：控制智能体(`Agent`)动作的`Actor`（基于策略的方法）；衡量所采取动作好坏的`Critic`（基于价值的方法）。

在`Reinforce`中，根据回报率的高低按比例增加轨迹中动作的概率。记作：{% mathjax %}\nabla_{\theta}J(\theta) = \sum\limits_{t=0} \nabla_{\theta}\log_{\pi_{\theta}}(a_t|s_t)R(\tau){% endmathjax %}。
- 如果回报很高，就将提高（状态，动作）组合的概率。
- 如果回报很低，就将降低（状态，动作）组合的概率。

此回报{% mathjax %}R(\tau){% endmathjax %}是使用**蒙特卡洛抽样**计算的。通过收集一条轨迹并计算折扣回报，并使用此分数来增加或减少该轨迹中采取的每个动作的概率。如果回报良好，所有动作都会通过增加其被采取的可能性而得到“**强化**”。{% mathjax %}R(\tau) = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \ldots{% endmathjax %}，这种方法的优点是**无偏**。由于不预测回报，因此只使用获得的真实回报。鉴于环境的随机性（一个回合期间的随机事件）和策略的随机性，轨迹可能产生不同的回报，从而导致较高的**方差**。因此，相同的起始状态可能产生截然不同的回报。正因为如此，从相同状态开始的回报在各个回合之间可能会有很大差异。解决办法是通过使用大量轨迹来减少**方差**，希望任何一条轨迹引入的方差总体上都会减少，并提供对回报的“真实”预测。

利用`Actor-Critic`方法减少方差：减少强化算法的方差并更快更好地训练**智能体**(`Agent`)的办法是利用基于策略和价值的方法的组合，即`Actor-Critic`方法。要理解`Actor-Critic`，想象一下你正在玩电子游戏。你可以和一个朋友一起玩，他会给你一些反馈。你是演员，你的朋友是评论家。一开始你不知道怎么玩，所以你随机尝试一些动作。评论家观察你的动作并提供反馈。通过这些反馈，您可以更新您的策略并更好地玩该游戏。另一方面，你的朋友（评论家）也会更新来提供反馈，以便下次反馈得更好。这就是`Actor-Critic`背后的思想。学习两个函数近似：控制智能体如何采取动作的策略：{% mathjax %}\pi_{\theta}(s){% endmathjax %}；通过衡量所采取的动作有多好来协助策略更新**价值函数**：{% mathjax %}\hat{q}_w(s,a){% endmathjax %}。

`Actor-Critic`过程：正如所见，使用`Actor-Critic`方法，有两个函数近似（两个神经网络）。在每个时间步{% mathjax %}t{% endmathjax %}，从环境中获取当前状态{% mathjax %}S_t{% endmathjax %}​并将其作为输入传递给`Actor`和`Critic`。我们的策略输入状态并输出动作。
{% asset_img ml_1.png %}

评论家(`Critic`)将该动作作为输入，并使用{% mathjax %}S_t{% endmathjax %}​和{% mathjax %}A_t{% endmathjax %}​计算在该状态下采取该动作的价值：`Q`值。
{% asset_img ml_2.png %}

在环境中执行的动作{% mathjax %}A_t{% endmathjax %}​输出新的状态{% mathjax %}S_{t+1}{% endmathjax %}和奖励{% mathjax %}R_{t+1}{% endmathjax %}​。
{% asset_img ml_3.png %}

演员(`Actor`)使用`Q`值更新其策略参数。记作：{% mathjax %}\Delta\theta = \alpha\nabla_{\theta}(\log_{\pi_{\theta}}(s,a))\hat{q_w}(s,a){% endmathjax %}。由于其更新了参数，演员(`Actor`)在给定新状态{% mathjax %}S_{t+1}{% endmathjax %}​的情况下生成在{% mathjax %}A_{t+1}{% endmathjax %}时要采取的下一个动作。然后，评论家(`Critic`)会更新其价值参数。
{% asset_img ml_4.png %}

我们可以使用`Advantage`函数作为评论家(`Critic`)来代替**动作值值函数**来进一步提高学习的稳定性。其思想是`Advantage`函数计算某个动作相对于某个状态下其他动作的相对优势：与该状态下的平均值相比，在某个状态下采取该动作更好，它从**状态-动作对**中减去状态的平均值，记作{% mathjax %}A(s,a) = Q(s,a) - V(s){% endmathjax %}。通过这个函数计算，如果在该状态下采取这个动作，得到的额外奖励与在该状态下得到的平均奖励的差值。额外的奖励是超出该状态预期值的奖励。
- 如果{% mathjax %}A(s,a) > 0{% endmathjax %}：**梯度**就会被推向那个方向。
- 如果{% mathjax %}A(s,a) < 0{% endmathjax %}，**梯度**就会被推向相反的方向。

实现`Advantage`函数需要两个值函数{% mathjax %}-Q(s,a){% endmathjax %} 和{% mathjax %}V(s){% endmathjax %}。可以使用`TD`误差作为`Advantage`函数的良好预测值。记作{% mathjax %}Q(s,a) = r + \gamma V(s')\;,\;A(s,a) = r + \gamma V(s') - V(s){% endmathjax %}。