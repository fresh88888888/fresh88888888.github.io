---
title: 联邦学习(Federated Learning)-探析(分布式机器学习)
date: 2024-07-31 11:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---


#### 介绍

**联邦学习**(`Federated Learning，FL`)是一种**分布式机器学习**技术，旨在保护**数据隐私**的同时，利用分散在多个边缘设备或服务器上的本地数据进行模型训练。该方法由谷歌在`2016`年首次提出，主要用于解决**数据孤岛**和**隐私保护**问题。它本质上是一种保护隐私的多方协作机器学习框架，它允许参与方建立一个联合训练模型，但参与方均在本地维护其底层数据而不将原始数据进行共享。**联邦学习**的核心思想是将模型训练过程分布在多个本地设备上，而不是将所有数据集中到一个中央服务器。每个设备在本地使用其数据进行模型训练，然后将模型参数（而非原始数据）发送到中央服务器进行聚合。通过这种方式，联邦学习能够有效保护**数据隐私**，减少数据传输的风险和成本。
<!-- more -->

**联邦学习**的典型工作流程如下：
- **初始化模型**：中央服务器初始化一个全局模型，并将其发送到各个客户端设备。
- **本地训练**：每个客户端设备在本地数据上训练模型，并更新模型参数。
- **参数上传**：各个客户端将更新后的模型参数发送回中央服务器。
- **参数聚合**：中央服务器对接收到的模型参数进行聚合，更新全局模型。
- **重复迭代**：重复上述步骤，直到模型收敛或达到预期的性能指标。

根据数据**样本空间和特征空间**的分布模式不同，**联邦学习**可以分为以下三类：
- **水平联邦学习**(`Horizontal Federated Learning`)：适用于数据特征重叠较多，但用户重叠较少的情况。数据集按用户维度水平分割，各个参与者的数据特征是对齐的。
- **垂直联邦学习**(`Vertical Federated Learning`)：适用于用户重叠较多，但数据特征重叠较少的情况。数据集按特征维度垂直分割，各个参与者的数据样本是对齐的。
- **联邦迁移学习**(`Federated Transfer Learning`)：适用于数据样本和数据特征重叠都很少的情况，通过迁移学习的方法进行模型训练。

**联邦学习**的优势：
- **数据隐私保护**：数据不离开本地设备，仅传输模型参数，减少了数据泄露的风险。
- **降低数据传输成本**：避免了将大量数据上传到中央服务器的需求，降低了带宽和存储成本。
- **适应性强**：能够处理异构数据，适用于各种分布式数据环境。

**联邦学习**通过在分布式环境中进行模型训练，解决了传统集中式机器学习在**数据隐私**和**数据孤岛**方面的挑战。随着技术的不断发展，**联邦学习**在各种应用场景中的潜力将不断被挖掘和实现。

多个数据拥有方{% mathjax %}\mathbf{F}_i(i = 1,2,\ldots,N){% endmathjax %}的目的是将各自的数据{% mathjax %}\mathbf{D}_i{% endmathjax %}联合，共同训练机器学习模型。传统做法是把数据整合到一起，形成全局数据集{% mathjax %}\mathbf{D} = \{\mathbf{D}_i, i=1,2,\ldots,N\}{% endmathjax %}，并利用{% mathjax %}\mathbf{D}{% endmathjax %}训练生成模型{% mathjax %}\mathbf{M}_{\text{sum}}{% endmathjax %}。然而，该方案因违背数据隐私保护而难以实施。为了解决这一问题**联邦学习**定义如下：联邦学习是指使得这些数据拥有方{% mathjax %}\mathbf{F}_i{% endmathjax %}在不用给出己方数据{% mathjax %}\mathbf{D}_i{% endmathjax %}的情况下可以进行模型训练并得到全局模型{% mathjax %}\mathbf{M}_{\text{fed}}{% endmathjax %}的计算过程，并能够保证模型{% mathjax %}\mathbf{M}_{\text{fed}}{% endmathjax %}的效果{% mathjax %}\mathbf{V}_{\text{fed}}{% endmathjax %}与传统模型{% mathjax %}\mathbf{M}_{\text{sum}}{% endmathjax %}的效果{% mathjax %}\mathbf{V}_{\text{sum}}{% endmathjax %}间的差距足够小，即：
{% mathjax '{"conversion":{"em":14}}' %}
|\mathbf{V}_{\text{fed}} - \mathbf{V}_{\text{sum}}| < \delta
{% endmathjax %}
其中，{% mathjax %}\delta{% endmathjax %}为设定的非负实数。

**水平联邦学习**(`HFL`)适用于联邦学习的参与方的数据有重叠的数据特征，即数据特征在参与方之间是对齐的，但是参与方拥有的数据样本是不同的。
{% asset_img fl_2.png  %}

**垂直联邦学习**(`VFL`)适用于联邦学习参与方的训练数据有重叠的数据样本，即参与方之间的数据样本是对齐的，但是在数据特征上有所不同。
{% asset_img fl_1.png  %}

**联邦迁移学习**(`FTL`)适用于当两个数据集不仅在样本大小上不同，而且在特征空间上也不同时。将中国的一家银行和美国的一家电子商务公司视为两个独立的实体。由于地理限制，两家机构的用户群体重叠很小。然而，由于企业不同，两家公司的特征空间只有一小部分重叠。具体而言，通过应用受限的一般样本集来学习两个特征空间的典型描述，然后将其用于为仅具有单侧特征的样本生成预测结果。`FTL`解决了当前联合学习方法无法解决的困难，这就是为什么它是该领域的一个重要补充。
{% asset_img fl_3.png  %}

