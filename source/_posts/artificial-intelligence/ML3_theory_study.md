---
title: 机器学习(ML)(三) — 探析
date: 2024-08-28 12:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 逻辑回归

**线性回归**不是解决分类问题的算法。另一种称为**逻辑回归**的算法。它是当今最流行和使用最广泛的学习算法之一。确定电子邮件是否为垃圾邮件的示例。您要输出的答案要么是“否”，要么是“是”。能否判断这笔交易是否是欺诈性的、试图将肿瘤分类为恶性还是非恶性。在每个问题中，你想要预测的变量只能是两个可能值中的一个。否或是。这种只有两个可能输出的分类问题称为**二元分类**。**二元**这个词指的是只有两个可能的类或两个可能的类别。它们基本上是同一个意思。
<!-- more -->

按照惯例，我们可以用几种常见的方式来指代这两个类或类别。我们经常将其指定为否或是，或者有时等同于假或真，或者非常普遍地使用`0`或`1`。按照计算机科学中的常见惯例，`0`表示假，`1`表示真。常用的方式是将假或`0`类称为**负示例**，将真或`1`类称为**正示例**。例如，对于垃圾邮件分类，非垃圾邮件的电子邮件可能被称为负类。因为问题的输出是垃圾邮件。输出为否或`0`。相反，包含垃圾邮件的电子邮件可能被称为正类。为了明确起见，负和正不一定意味着坏与好或邪恶与好。只是使用负示例和正示例来传达或零或假与存在或真或`1`可能正在寻找的某物的概念。在非垃圾邮件和垃圾邮件之间。您将哪个称为假或`0`，将哪个称为真或`1`有点随意。通常两种选择都可以。那么你如何构建分类算法呢？

这是一个用于对肿瘤是否为恶性进行分类的训练集示例。`1`类，阳性类别，`0`类，阴性类别。在横轴上绘制了肿瘤大小，在纵轴上绘制了标签`y`。**线性回归**并尝试将直线拟合到数据中。线性回归不仅预测`0`和`1`的值。而是`0`和`1`之间的所有数字，甚至小于`0`或大于`1`。但在这里我们想要预测类别。你可以尝试选择一个阈值，比如`0.5`。如果模型输出的值低于`0.5`，那么你就可以预测{% mathjax %}y=0{% endmathjax %}。如果模型输出{% mathjax %}y\geq 0.6 {% endmathjax %}，那么预测{% mathjax %}y=1{% endmathjax %}。请注意，这个阈值{% mathjax %}0.5{% endmathjax %}与最佳拟合直线相交。所以如果你在这里画一条垂直线，左边的所有内容最终都会预测{% mathjax %}y=0{% endmathjax %}。而右边的所有内容最终都会预测{% mathjax %}y=1{% endmathjax %}。对于这个特定的数据集，**线性回归**可以做一些合理的工作。但现在让我们看看如果你的数据集有一个更多的训练示例会发生什么。让我们也延伸一下横轴。请注意，这个训练示例不应该真正改变你对数据点分类的方式。我们刚才画的这条垂直分界线仍然有意义，因为它是小于这个的肿瘤应该被归类为`0`的临界值。大于这个的肿瘤应该被归类为`1`。但是一旦你在右边添加了这个额外的训练示例。**线性回归**的最佳拟合线将像这样移动。如果你继续使用`0.5`的阈值，你现在会注意到这个点左边的所有内容都被预测为`0`（非恶性）。而这个点右边的所有内容都被预测为`1`。这不是我们想要的，因为向右添加这个示例不会改变我们关于如何对恶性肿瘤和良性肿瘤进行分类的任何结论。但是如果你尝试使用**线性回归**来做到这一点，添加不应该改变任何东西的示例。最终，我们学习到的这个分类问题函数。显然，当肿瘤很大时，我们希望算法将其归类为恶性。所以我们刚才看到的是**线性回归**导致最佳拟合线。当我们在右侧添加一个示例时，它会移动。分界线（也称为**决策边界**）会向右移动。

##### 介绍

**逻辑回归**，它可能是世界上使用最广泛的分类算法。让我们继续以分类肿瘤是/否为恶性为例。而之前我们将使用标签`1`或“是”表示正类，以表示恶性肿瘤，使用`0`或“否”和负类表示良性肿瘤。下面数据集的图表，其中横轴是肿瘤大小，纵轴仅取`0`和`1`的值，因为这是一个分类问题。**逻辑回归**是将一条曲线拟合成这样的曲线，即此数据集的`S`形曲线。对于此示例，如果患者患有此大小的肿瘤，则算法将输出`0.7`，表明更接近是恶性和良性的。输出标签{% mathjax %}y{% endmathjax %}绝不会是`0.7`，而只能是`0`或`1`。为了构建**逻辑回归算法**，可以描述这样一个数学函数，称为`S`型函数，有时也称为**逻辑函数**。`S`型函数如下所示。左右两边图表的{% mathjax %}x{% endmathjax %}轴不同。左侧图表的{% mathjax %}x{% endmathjax %}轴表示肿瘤大小。而在右侧图表中，这里有`0`，横轴同时取负值和正值，并将横轴标记为`Z`。在这里显示的只是从`[-3,+3]`的范围。因此，`S`型函数的输出值介于`0`和`1`之间。如果我用{% mathjax %}g(z){% endmathjax %}来表示这个函数，那么{% mathjax %}g(z) = \frac{1}{1 + e^{-z}}{% endmathjax %}。这里的`e`是一个数学常数，其值约为`2.7`。注意，如果{% mathjax %}z=100{% endmathjax %}，那么{% mathjax %}e^{-z} = e^{-100}{% endmathjax %}，这是一个很小的数。因此最终结果是`1`加上一个很小的数，分母基本上非常接近`1`。这就是为什么当{% mathjax %}z{% endmathjax %}很大时，{% mathjax %}g(z) \approx 1{% endmathjax %}。这就是为什么`S`型函数具有这种形状，它从非常接近`0`开始，然后缓慢增加或增长到`1`的值。另外，在`Sigmoid`函数中，当{% mathjax %}z=0{% endmathjax %}时，因此 {% mathjax %}\frac{g}{z} = \frac{1}{1+1} = 0.5{% endmathjax %}，这就是它在`0.5`处通过纵轴的原因。
{% asset_img ml_1.png %}

现在，让我们用它来构建**逻辑回归算法**。我们将分两步进行。在第一步中，直线函数（如线性回归函数）可以定义为{% mathjax %}f_{\vec{w},b}(\vec{x}) = \vec{w}\cdot \vec{x} +b{% endmathjax %}。我们将此值存储在一个变量中，称之为{% mathjax %}z{% endmathjax %}。下一步是获取此{% mathjax %}z{% endmathjax %}值并将其传递给`Sigmoid`函数（也称为**逻辑函数**）{% mathjax %}g{% endmathjax %}。现在，{% mathjax %}g(z) = \frac{1}{1 + e^{-z}}{% endmathjax %}。介于`0`和`1`之间。将这两个方程放在一起，它们会给出`x`的逻辑回归模型`f`，{% mathjax %}z = \vec{w}\cdot\vec{x} + b,\;\; f_{\vec{w},b}(\vec{x}) = g(\vec{w}\cdot\vec{x} + b) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}。这是**逻辑回归模型**，它的作用是输入特征或特征集{% mathjax %}x{% endmathjax %}，并输出一个介于`0`和`1`之间的数字。
{% asset_img ml_2.png %}

接下来，让我们看看如何解释**逻辑回归**的输出。我们回到肿瘤分类的例子。给定某个输入{% mathjax %}x{% endmathjax %}，输出类别或标签{% mathjax %}y=1{% endmathjax %}的概率。例如，在这个应用中，{% mathjax %}x{% endmathjax %}是肿瘤大小，{% mathjax %}y{% endmathjax %}是`0`或`1`，如果一个病人来就诊，她的肿瘤大小为{% mathjax %}x{% endmathjax %}，如果基于这个输入{% mathjax %}x{% endmathjax %}，模型会加上`0.7`，那么这意味着模型预测认为这个病人的真实标签{% mathjax %}y=1{% endmathjax %}的可能性是`70%`。换句话说，模型告诉我们，它认为病人的肿瘤有`70%`的可能性是恶性的。我们知道 {% mathjax %}y{% endmathjax %}必须是`0`或`1`，所以如果{% mathjax %}y{% endmathjax %}有`70%`的机会是`1`，那么它为`0`的可能性是多少？因此这两个数字相加的概率必须等于`1`或`100%`。这就是为什么如果{% mathjax %}y=1{% endmathjax %}的概率为`0.7`或`70%`，那么它为`0`的概率就必须是`0.3`或`30%`，则{% mathjax %}\mathbf{P}(y=1) + \mathbf{P}(y=0) = 1{% endmathjax %}。有时你会看到这种符号，即给定输入特征{% mathjax %}x{% endmathjax %}和参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}，{% mathjax %}f_{\vec{w},b}(\vec{x}) = \mathbf{P}(y = 1|\vec{x};\vec{w},b){% endmathjax %}。这里的分号只是用来表示{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}是影响计算的参数，即给定输入特征{% mathjax %}x{% endmathjax %}，{% mathjax %}y=1{% endmathjax %}的概率是多少？

##### 决策边界

回顾一下，逻辑回归模型输出的计算分为两个步骤。在第一步中，您将{% mathjax %}z{% endmathjax %}计算为{% mathjax %}z = \vec{w}\cdot\vec{x} + b{% endmathjax %}。然后将`Sigmoid`函数{% mathjax %}g{% endmathjax %}应用于{% mathjax %}z{% endmathjax %}。`Sigmoid`函数的公式：{% mathjax %}g(z) = \frac{1}{1+e^{-z}}{% endmathjax %}。另一种写法是，{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(\vec{w}\cdot\vec{x}+ b) = \frac{1}{1 + e^{-(\vec{w}\cdot\vec{x}+b)}}{% endmathjax %}。将其解释为给定{% mathjax %}x{% endmathjax %}并具有参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}时{% mathjax %}\hat{y}=1{% endmathjax %}的概率：{% mathjax %}f_{\vec{w},b}(\vec{x}) = \mathbf{P}(y=1|\vec{x};\vec{w},b){% endmathjax %}。这可能是一个概率值，比如`0.7`或`0.3`。现在，如果用学习算法来预测。可以设置一个阈值，高于这个阈值你预测{% mathjax %}\hat{y}{% endmathjax %}为`1`，低于这个阈值预测{% mathjax %}\hat{y}{% endmathjax %}为`0`。选择一个阈值为`0.5`，如果{% mathjax %}f_{\vec{w},b}(\vec{x}) \geq 0.5{% endmathjax %}，则预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。如果{% mathjax %}f_{\vec{w},b}(\vec{x}) \leq 0.5{% endmathjax %}，则预测{% mathjax %}\hat{y}{% endmathjax %}为`0`，{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z){% endmathjax %}。只要{% mathjax %}g(z) \geq 0.5{% endmathjax %}。但是，什么时候{% mathjax %}f_{\vec{w},b}(\vec{x}) \geq 0.5{% endmathjax %}？只要{% mathjax %}z geq 0{% endmathjax %}，{% mathjax %}g(z) \geq 0.5{% endmathjax %}。也就是说，只要{% mathjax %}z{% endmathjax %}位于该轴的右半部分。最后，什么时候{% mathjax %}z \geq 0{% endmathjax %}？{% mathjax %}z = \vec{w}\cdot\vec{x} + b{% endmathjax %}，只要{% mathjax %}\vec{w}\cdot\vec{x} + b \geq 0{% endmathjax %}，{% mathjax %}z \geq 0{% endmathjax %}。
{% asset_img ml_3.png %}

总结一下，只要{% mathjax %}\vec{w}\cdot\vec{x} + b \geq 0{% endmathjax %}，模型就会预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。当{% mathjax %}\vec{w}\cdot\vec{x} + b < 0{% endmathjax %}时，算法会预测 {% mathjax %}\hat{y}{% endmathjax %}为`0`。鉴于此，这里举一个分类问题的例子，其中有两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}，这不仅仅是一个特征。这是训练集，其中小红叉表示正例，小蓝圈表示负例。红叉对应{% mathjax %}\hat{y}= 1{% endmathjax %}，蓝圈对应{% mathjax %}y=0{% endmathjax %}。**逻辑回归模型**将使用{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1 + w_2x_2 + b){% endmathjax %}函数进行预测，其中{% mathjax %}z = w_1x_1 + w_2x_2 +b{% endmathjax %}是这里的表达式，因为有两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}。我们假设这个例子中的参数值为{% mathjax %}w_1 = 1,\;w_2 = 1,/;b=-3{% endmathjax %}。现在让我们看看逻辑回归如何进行预测。具体来说，弄清楚{% mathjax %}\vec{w}\cdot\vec{x} + b{% endmathjax %}何时大于等于 `0`，何时小于`0`。要弄清楚这一点，有一条非常有趣的线需要查看，即当{% mathjax %}\vec{w}\cdot\vec{x} + b = 0{% endmathjax %}时。这条线也称为**决策边界**，因为在这条线上，你几乎可以中立地判断{% mathjax %}\hat{y}=0{% endmathjax %}还是{% mathjax %}\hat{y}=1{% endmathjax %}。现在，对于上面的参数{% mathjax %}w_1,w_2,b{% endmathjax %}的值，这个**决策边界**就是{% mathjax %}x_1 + x_2 -3{% endmathjax %}。什么时候{% mathjax %}x_1 + x_2 -3 = 0{% endmathjax %}？这将对应于{% mathjax %}x_1 + x_2= 3{% endmathjax %}这条线。这条线就是**决策边界**，如果特征{% mathjax %}x{% endmathjax %}位于这条线的右侧，逻辑回归将预测`1`，而位于这条线的左侧，逻辑回归将预测`0`。换句话说，当参数{% mathjax %}w_1 =1,\;w_2 = 1,\;b=-3{% endmathjax %}时**逻辑回归**的**决策边界**。当然，如果您选择了不同的参数，**决策边界**将是一条不同的线。
{% asset_img ml_4.png %}

现在让我们看一个更复杂的例子，其中**决策边界**不再是直线。和以前一样，`十`字表示类{% mathjax %}\hat{y}=1{% endmathjax %}，小圆圈表示类{% mathjax %}\hat{y}=0{% endmathjax %}。您了解了如何在线性回归中使用**多项式**，您可以在逻辑回归中做同样的事情。将{% mathjax %}z{% endmathjax %}设置为{% mathjax %}z = w_1x_1^2 + w_2x_2^2 + b{% endmathjax %}。通过这种特征选择，多项式特征进入**逻辑回归**。{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + b){% endmathjax %}。假设我们最终选择{% mathjax %}w_1=1,\;w_2=1,\;b=-1{% endmathjax %}。{% mathjax %}z = x_1^2 + x_2^2 -1{% endmathjax %}。**决策边界**与前面一样，对应于{% mathjax %}z = 0{% endmathjax %}的情况。当{% mathjax %}x_1^2 + x_2^2 = 1{% endmathjax %}时，该表达式等于`0`。如果你在左侧的图表上绘制{% mathjax %}x_1^2 + x_2^2 = 1{% endmathjax %}对应的曲线，它就是一个圆圈。当{% mathjax %}x_1^2 + x_2^2 \geq 1{% endmathjax %}时，这就是圆圈外的区域，这时预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。相反，当{% mathjax %}x_1^2 + x_2^2 < 1{% endmathjax %}时，这就是圆圈内的区域，这时你预测{% mathjax %}\hat{y}{% endmathjax %}为`0`。
{% asset_img ml_5.png %}

能想出比这更复杂的决策边界吗？是的。你可以通过使用更高阶的多项式项来实现这一点{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 + w_6x_1^3 + \ldots + b){% endmathjax %}。那么你就有可能得到更复杂的**决策边界**。模型可以定义**决策边界**，比如这个例子，像这样的椭圆，或者选择不同的参数。可以得到更复杂的**决策边界**。这种逻辑回归的实现将预测在这个形状内{% mathjax %}\hat{y}=1{% endmathjax %}，而在形状外将预测{% mathjax %}\hat{y}=0{% endmathjax %}。有了这些多项式特征，你可以得到非常复杂的**决策边界**。换句话说，**逻辑回归**可以学习拟合相当复杂的数据。但是，如果不包含这些高阶多项式，那么使用的特征只有{% mathjax %}x_1,x_2,x_3{% endmathjax %}等，**逻辑回归**的**决策边界**将始终是线性的（一条直线）。
{% asset_img ml_6.png %}

#### 逻辑回归的成本函数

**逻辑损失函数**可以帮助我们为**逻辑回归**选择更好的参数。**逻辑回归模型**的训练集可能如下图所示。这里的每一行可能对应于正在看医生的患者，其中一位患者的一些诊断。我们将使用{% mathjax %}m{% endmathjax %}表示训练示例的数量。每个训练示例都有一个或多个特征，例如肿瘤大小、患者年龄等，总共有{% mathjax %}n{% endmathjax %}个特征。我们将这些特征称为{% mathjax %}x_1,x_2,\ldots,x_n{% endmathjax %}。由于这是一个**二元分类任务**，目标标签{% mathjax %}y{% endmathjax %}只采用两个值，即`0`或`1`。**逻辑回归模型**由该方程定义。你要回答的问题是，给定这个训练集，你如何选择参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}？回想一下**线性回归**，是**平方误差成本函数**：{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{m}\sum_{i=1}^m \frec{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2{% endmathjax %}。我唯一改变的是，我将一半放在求和内而不是求和外。你可能还记得，在**线性回归**的情况下，{% mathjax %}f_{\vec{x},b}(\vec{x}) = \vec{w}\cdot\vec{x} + b{% endmathjax %}。**成本函数**看起来是一个**凸函数**或碗形或锤形。**梯度下降**像收敛到**全局最小值**。现在你可以尝试对**逻辑回归**使用相同的**成本函数**。但事实证明，如果我将{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}绘制**成本函数**，那么成本函数将看起来像**非凸成本函数**，它不是凸的。如果你尝试使用**梯度下降**。有很多**局部最小值**会让你感到厌烦。对于**逻辑回归**，这个**平方误差成本函数**并不是一个好的选择。相反，会有一个不同的成本函数，可以使成本函数再次凸出。**梯度下降**可以保证收敛到**全局最小值**。唯一改变的是将一半放在总和内而不是总和外。
{% asset_img ml_7.png %}

为了构建一个新的成本函数。我将稍微改变{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的成本函数{% mathjax %}\mathbf{J}{% endmathjax %}的定义。具体来说，如果你看看这个总和，这个术语称为单个训练示例的损失。通过这个大写的{% mathjax %}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}){% endmathjax %}表示损失，并将其作为学习算法的**预测函数**。在这种情况下，给定{% mathjax %}x{% endmathjax %}的预测变量{% mathjax %}f{% endmathjax %}和真实标签{% mathjax %}y{% endmathjax %}的损失等于平方差的`1.5`。我们很快就会看到，为该损失函数选择不同的形式，将能够保持**总成本函数**为凸函数。如果标签{% mathjax %}y^{(i)} = 1{% endmathjax %}，则损失为{% mathjax %}-\log f(x){% endmathjax %}；如果标签{% mathjax %}y^{(i)} = 0{% endmathjax %}，则损失为{% mathjax %}-\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。为什么这个损失函数有意义。首先考虑{% mathjax %}y^{(i)} = 1{% endmathjax %}的情况，并绘制此函数的样子，以获得有关此损失函数正在做什么。请记住，**损失函数**衡量的是你在某一个训练样本上的表现，它是通过对所有训练样本的损失求和得到的，**成本函数**衡量的是你在整套训练样本上的表现。如果你绘制{% mathjax %}-\log (f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}，它看起来像这条曲线，其中{% mathjax %}f{% endmathjax %}位于横轴上。{% mathjax %}f{% endmathjax %}的对数的负数图看起来像这样，我们只需沿横轴翻转曲线即可。
{% asset_img ml_8.png %}

请注意，它在{% mathjax %}f(x) = 1{% endmathjax %}处与横轴相交，并从那里继续向下。现在，{% mathjax %}f(x) = 1{% endmathjax %}是**逻辑回归**的输出。因此，{% mathjax %}f{% endmathjax %}始终介于`0` 和`1`之间，因为**逻辑回归**的输出始终介于`0`和`1`之间。如果算法预测的概率接近`1`，而真实标签为`1`，则损失非常小。它几乎为`0`，因为你非常接近正确答案。现在继续使用真实标签{% mathjax %}y^{(i)} = 1{% endmathjax %}的例子，假设都是恶性肿瘤。如果算法预测为`0.5`，那么损失就是这里的这个点，这个点稍高一点，但不是那么高。相反，如果算法认为肿瘤恶性的可能性只有`10%`，但{% mathjax %}y^{(i)} = 1{% endmathjax %}，那么输出为`0.1`。如果真的是恶性的，那么损失就是就是很高。当{% mathjax %}y^{(i)} = 1{% endmathjax %}时，**损失函数**会激励，推动算法做出更准确的预测，因为当它预测的值接近`1`时，损失最低。当{% mathjax %}y^{(i)} = 1{% endmathjax %}时损失是多少。对应于{% mathjax %}y^{(i)} = 0{% endmathjax %}时。损失是{% mathjax %}-\log 1 - f(x){% endmathjax %}。当绘制此函数时。 {% mathjax %}f \in [0,1]{% endmathjax %}，因为逻辑回归只输出`0~1`之间的值。如果我们放大，它看起来就是这样。在此图中，对应于{% mathjax %}y^{(i)} = 0{% endmathjax %}，纵轴显示不同{% mathjax %}f(x){% endmathjax %}值时的损失值。当{% mathjax %}f(x) = 0{% endmathjax %}或非常接近`0`时，损失也会非常小，如果真实标签{% mathjax %}y^{(i)} = 0{% endmathjax %}且模型的预测非常接近`0`，那么，是正确的，因此损失非常接近`0`。{% mathjax %}f(x){% endmathjax %} 值越大，损失越大，因为预测离真实标签{% mathjax %}y^{(i)} = 0{% endmathjax %}越远。事实上，当预测接近`1`时，损失实际上接近无穷大。

回到肿瘤预测的例子，如果模型预测患者的肿瘤几乎是恶性的，比如说，恶性的概率为`99.9%`，但结果不是恶性的，所以{% mathjax %}y^{(i)} = 0{% endmathjax %}，那么我们就会用非常高的**损失惩罚模型**。{% mathjax %}x{% endmathjax %}的预测{% mathjax %}f{% endmathjax %}距离{% mathjax %}y^{(i)}{% endmathjax %}的真实值越远，损失就越高。事实上，如果{% mathjax %}f(x){% endmathjax %}接近`0`，这里的损失实际上会变得非常大，实际上接近无穷大。当真实标签为`1`时，算法会受到强烈的激励，不要预测太接近`0`的值。
{% asset_img ml_9.png %}

事实证明，选择这种**损失函数**后，总体成本函数将呈现凸形，因此您可以使用梯度下降法迭代到**全局最小值**。成本函数是整个训练集的函数，因此是单个训练示例的损失函数总和的平均值或{% mathjax %}\frac{1}{m}{% endmathjax %}倍。特定参数集{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的成本等于训练示例损失的所有训练示例总和的{% mathjax %}\frac{1}{m}{% endmathjax %}倍。
{% asset_img ml_10.png %}