---
title: 机器学习(ML)(三) — 探析
date: 2024-08-28 12:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 逻辑回归

**线性回归**不是解决分类问题的算法。另一种称为**逻辑回归**的算法。它是当今最流行和使用最广泛的学习算法之一。确定电子邮件是否为垃圾邮件的示例。您要输出的答案要么是“否”，要么是“是”。能否判断这笔交易是否是欺诈性的、试图将肿瘤分类为恶性还是非恶性。在每个问题中，你想要预测的变量只能是两个可能值中的一个。否或是。这种只有两个可能输出的分类问题称为**二元分类**。**二元**这个词指的是只有两个可能的类或两个可能的类别。它们基本上是同一个意思。
<!-- more -->

按照惯例，我们可以用几种常见的方式来指代这两个类或类别。我们经常将其指定为否或是，或者有时等同于假或真，或者非常普遍地使用`0`或`1`。按照计算机科学中的常见惯例，`0`表示假，`1`表示真。常用的方式是将假或`0`类称为**负示例**，将真或`1`类称为**正示例**。例如，对于垃圾邮件分类，非垃圾邮件的电子邮件可能被称为负类。因为问题的输出是垃圾邮件。输出为否或`0`。相反，包含垃圾邮件的电子邮件可能被称为正类。为了明确起见，负和正不一定意味着坏与好或邪恶与好。只是使用负示例和正示例来传达或零或假与存在或真或`1`可能正在寻找的某物的概念。在非垃圾邮件和垃圾邮件之间。您将哪个称为假或`0`，将哪个称为真或`1`有点随意。通常两种选择都可以。那么你如何构建分类算法呢？

这是一个用于对肿瘤是否为恶性进行分类的训练集示例。`1`类，阳性类别，`0`类，阴性类别。在横轴上绘制了肿瘤大小，在纵轴上绘制了标签`y`。**线性回归**并尝试将直线拟合到数据中。线性回归不仅预测`0`和`1`的值。而是`0`和`1`之间的所有数字，甚至小于`0`或大于`1`。但在这里我们想要预测类别。你可以尝试选择一个阈值，比如`0.5`。如果模型输出的值低于`0.5`，那么你就可以预测{% mathjax %}y=0{% endmathjax %}。如果模型输出{% mathjax %}y\geq 0.6 {% endmathjax %}，那么预测{% mathjax %}y=1{% endmathjax %}。请注意，这个阈值{% mathjax %}0.5{% endmathjax %}与最佳拟合直线相交。所以如果你在这里画一条垂直线，左边的所有内容最终都会预测{% mathjax %}y=0{% endmathjax %}。而右边的所有内容最终都会预测{% mathjax %}y=1{% endmathjax %}。对于这个特定的数据集，**线性回归**可以做一些合理的工作。但现在让我们看看如果你的数据集有一个更多的训练示例会发生什么。让我们也延伸一下横轴。请注意，这个训练示例不应该真正改变你对数据点分类的方式。我们刚才画的这条垂直分界线仍然有意义，因为它是小于这个的肿瘤应该被归类为`0`的临界值。大于这个的肿瘤应该被归类为`1`。但是一旦你在右边添加了这个额外的训练示例。**线性回归**的最佳拟合线将像这样移动。如果你继续使用`0.5`的阈值，你现在会注意到这个点左边的所有内容都被预测为`0`（非恶性）。而这个点右边的所有内容都被预测为`1`。这不是我们想要的，因为向右添加这个示例不会改变我们关于如何对恶性肿瘤和良性肿瘤进行分类的任何结论。但是如果你尝试使用**线性回归**来做到这一点，添加不应该改变任何东西的示例。最终，我们学习到的这个分类问题函数。显然，当肿瘤很大时，我们希望算法将其归类为恶性。所以我们刚才看到的是**线性回归**导致最佳拟合线。当我们在右侧添加一个示例时，它会移动。分界线（也称为**决策边界**）会向右移动。

##### 介绍

**逻辑回归**，它可能是世界上使用最广泛的分类算法。让我们继续以分类肿瘤是/否为恶性为例。而之前我们将使用标签`1`或“是”表示正类，以表示恶性肿瘤，使用`0`或“否”和负类表示良性肿瘤。下面数据集的图表，其中横轴是肿瘤大小，纵轴仅取`0`和`1`的值，因为这是一个分类问题。**逻辑回归**是将一条曲线拟合成这样的曲线，即此数据集的`S`形曲线。对于此示例，如果患者患有此大小的肿瘤，则算法将输出`0.7`，表明更接近是恶性和良性的。输出标签{% mathjax %}y{% endmathjax %}绝不会是`0.7`，而只能是`0`或`1`。为了构建**逻辑回归算法**，可以描述这样一个数学函数，称为`S`型函数，有时也称为**逻辑函数**。`S`型函数如下所示。左右两边图表的{% mathjax %}x{% endmathjax %}轴不同。左侧图表的{% mathjax %}x{% endmathjax %}轴表示肿瘤大小。而在右侧图表中，这里有`0`，横轴同时取负值和正值，并将横轴标记为`Z`。在这里显示的只是从`[-3,+3]`的范围。因此，`S`型函数的输出值介于`0`和`1`之间。如果我用{% mathjax %}g(z){% endmathjax %}来表示这个函数，那么{% mathjax %}g(z) = \frac{1}{1 + e^{-z}}{% endmathjax %}。这里的`e`是一个数学常数，其值约为`2.7`。注意，如果{% mathjax %}z=100{% endmathjax %}，那么{% mathjax %}e^{-z} = e^{-100}{% endmathjax %}，这是一个很小的数。因此最终结果是`1`加上一个很小的数，分母基本上非常接近`1`。这就是为什么当{% mathjax %}z{% endmathjax %}很大时，{% mathjax %}g(z) \approx 1{% endmathjax %}。这就是为什么`S`型函数具有这种形状，它从非常接近`0`开始，然后缓慢增加或增长到`1`的值。另外，在`Sigmoid`函数中，当{% mathjax %}z=0{% endmathjax %}时，因此 {% mathjax %}\frac{g}{z} = \frac{1}{1+1} = 0.5{% endmathjax %}，这就是它在`0.5`处通过纵轴的原因。
{% asset_img ml_1.png %}

现在，让我们用它来构建**逻辑回归算法**。我们将分两步进行。在第一步中，直线函数（如线性回归函数）可以定义为{% mathjax %}f_{\vec{w},b}(\vec{x}) = \vec{w}\cdot \vec{x} +b{% endmathjax %}。我们将此值存储在一个变量中，称之为{% mathjax %}z{% endmathjax %}。下一步是获取此{% mathjax %}z{% endmathjax %}值并将其传递给`Sigmoid`函数（也称为**逻辑函数**）{% mathjax %}g{% endmathjax %}。现在，{% mathjax %}g(z) = \frac{1}{1 + e^{-z}}{% endmathjax %}。介于`0`和`1`之间。将这两个方程放在一起，它们会给出`x`的逻辑回归模型`f`，{% mathjax %}z = \vec{w}\cdot\vec{x} + b,\;\; f_{\vec{w},b}(\vec{x}) = g(\vec{w}\cdot\vec{x} + b) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}。这是**逻辑回归模型**，它的作用是输入特征或特征集{% mathjax %}x{% endmathjax %}，并输出一个介于`0`和`1`之间的数字。
{% asset_img ml_2.png %}

接下来，让我们看看如何解释**逻辑回归**的输出。我们回到肿瘤分类的例子。给定某个输入{% mathjax %}x{% endmathjax %}，输出类别或标签{% mathjax %}y=1{% endmathjax %}的概率。例如，在这个应用中，{% mathjax %}x{% endmathjax %}是肿瘤大小，{% mathjax %}y{% endmathjax %}是`0`或`1`，如果一个病人来就诊，她的肿瘤大小为{% mathjax %}x{% endmathjax %}，如果基于这个输入{% mathjax %}x{% endmathjax %}，模型会加上`0.7`，那么这意味着模型预测认为这个病人的真实标签{% mathjax %}y=1{% endmathjax %}的可能性是`70%`。换句话说，模型告诉我们，它认为病人的肿瘤有`70%`的可能性是恶性的。我们知道 {% mathjax %}y{% endmathjax %}必须是`0`或`1`，所以如果{% mathjax %}y{% endmathjax %}有`70%`的机会是`1`，那么它为`0`的可能性是多少？因此这两个数字相加的概率必须等于`1`或`100%`。这就是为什么如果{% mathjax %}y=1{% endmathjax %}的概率为`0.7`或`70%`，那么它为`0`的概率就必须是`0.3`或`30%`，则{% mathjax %}\mathbf{P}(y=1) + \mathbf{P}(y=0) = 1{% endmathjax %}。有时你会看到这种符号，即给定输入特征{% mathjax %}x{% endmathjax %}和参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}，{% mathjax %}f_{\vec{w},b}(\vec{x}) = \mathbf{P}(y = 1|\vec{x};\vec{w},b){% endmathjax %}。这里的分号只是用来表示{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}是影响计算的参数，即给定输入特征{% mathjax %}x{% endmathjax %}，{% mathjax %}y=1{% endmathjax %}的概率是多少？

##### 决策边界

回顾一下，逻辑回归模型输出的计算分为两个步骤。在第一步中，您将{% mathjax %}z{% endmathjax %}计算为{% mathjax %}z = \vec{w}\cdot\vec{x} + b{% endmathjax %}。然后将`Sigmoid`函数{% mathjax %}g{% endmathjax %}应用于{% mathjax %}z{% endmathjax %}。`Sigmoid`函数的公式：{% mathjax %}g(z) = \frac{1}{1+e^{-z}}{% endmathjax %}。另一种写法是，{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(\vec{w}\cdot\vec{x}+ b) = \frac{1}{1 + e^{-(\vec{w}\cdot\vec{x}+b)}}{% endmathjax %}。将其解释为给定{% mathjax %}x{% endmathjax %}并具有参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}时{% mathjax %}\hat{y}=1{% endmathjax %}的概率：{% mathjax %}f_{\vec{w},b}(\vec{x}) = \mathbf{P}(y=1|\vec{x};\vec{w},b){% endmathjax %}。这可能是一个概率值，比如`0.7`或`0.3`。现在，如果用学习算法来预测。可以设置一个阈值，高于这个阈值你预测{% mathjax %}\hat{y}{% endmathjax %}为`1`，低于这个阈值预测{% mathjax %}\hat{y}{% endmathjax %}为`0`。选择一个阈值为`0.5`，如果{% mathjax %}f_{\vec{w},b}(\vec{x}) \geq 0.5{% endmathjax %}，则预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。如果{% mathjax %}f_{\vec{w},b}(\vec{x}) \leq 0.5{% endmathjax %}，则预测{% mathjax %}\hat{y}{% endmathjax %}为`0`，{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z){% endmathjax %}。只要{% mathjax %}g(z) \geq 0.5{% endmathjax %}。但是，什么时候{% mathjax %}f_{\vec{w},b}(\vec{x}) \geq 0.5{% endmathjax %}？只要{% mathjax %}z geq 0{% endmathjax %}，{% mathjax %}g(z) \geq 0.5{% endmathjax %}。也就是说，只要{% mathjax %}z{% endmathjax %}位于该轴的右半部分。最后，什么时候{% mathjax %}z \geq 0{% endmathjax %}？{% mathjax %}z = \vec{w}\cdot\vec{x} + b{% endmathjax %}，只要{% mathjax %}\vec{w}\cdot\vec{x} + b \geq 0{% endmathjax %}，{% mathjax %}z \geq 0{% endmathjax %}。
{% asset_img ml_3.png %}

总结一下，只要{% mathjax %}\vec{w}\cdot\vec{x} + b \geq 0{% endmathjax %}，模型就会预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。当{% mathjax %}\vec{w}\cdot\vec{x} + b < 0{% endmathjax %}时，算法会预测 {% mathjax %}\hat{y}{% endmathjax %}为`0`。鉴于此，这里举一个分类问题的例子，其中有两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}，这不仅仅是一个特征。这是训练集，其中小红叉表示正例，小蓝圈表示负例。红叉对应{% mathjax %}\hat{y}= 1{% endmathjax %}，蓝圈对应{% mathjax %}y=0{% endmathjax %}。**逻辑回归模型**将使用{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1 + w_2x_2 + b){% endmathjax %}函数进行预测，其中{% mathjax %}z = w_1x_1 + w_2x_2 +b{% endmathjax %}是这里的表达式，因为有两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}。我们假设这个例子中的参数值为{% mathjax %}w_1 = 1,\;w_2 = 1,/;b=-3{% endmathjax %}。现在让我们看看逻辑回归如何进行预测。具体来说，弄清楚{% mathjax %}\vec{w}\cdot\vec{x} + b{% endmathjax %}何时大于等于 `0`，何时小于`0`。要弄清楚这一点，有一条非常有趣的线需要查看，即当{% mathjax %}\vec{w}\cdot\vec{x} + b = 0{% endmathjax %}时。这条线也称为**决策边界**，因为在这条线上，你几乎可以中立地判断{% mathjax %}\hat{y}=0{% endmathjax %}还是{% mathjax %}\hat{y}=1{% endmathjax %}。现在，对于上面的参数{% mathjax %}w_1,w_2,b{% endmathjax %}的值，这个**决策边界**就是{% mathjax %}x_1 + x_2 -3{% endmathjax %}。什么时候{% mathjax %}x_1 + x_2 -3 = 0{% endmathjax %}？这将对应于{% mathjax %}x_1 + x_2= 3{% endmathjax %}这条线。这条线就是**决策边界**，如果特征{% mathjax %}x{% endmathjax %}位于这条线的右侧，逻辑回归将预测`1`，而位于这条线的左侧，逻辑回归将预测`0`。换句话说，当参数{% mathjax %}w_1 =1,\;w_2 = 1,\;b=-3{% endmathjax %}时**逻辑回归**的**决策边界**。当然，如果您选择了不同的参数，**决策边界**将是一条不同的线。
{% asset_img ml_4.png %}

现在让我们看一个更复杂的例子，其中**决策边界**不再是直线。和以前一样，`十`字表示类{% mathjax %}\hat{y}=1{% endmathjax %}，小圆圈表示类{% mathjax %}\hat{y}=0{% endmathjax %}。您了解了如何在线性回归中使用**多项式**，您可以在逻辑回归中做同样的事情。将{% mathjax %}z{% endmathjax %}设置为{% mathjax %}z = w_1x_1^2 + w_2x_2^2 + b{% endmathjax %}。通过这种特征选择，多项式特征进入**逻辑回归**。{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + b){% endmathjax %}。假设我们最终选择{% mathjax %}w_1=1,\;w_2=1,\;b=-1{% endmathjax %}。{% mathjax %}z = x_1^2 + x_2^2 -1{% endmathjax %}。**决策边界**与前面一样，对应于{% mathjax %}z = 0{% endmathjax %}的情况。当{% mathjax %}x_1^2 + x_2^2 = 1{% endmathjax %}时，该表达式等于`0`。如果你在左侧的图表上绘制{% mathjax %}x_1^2 + x_2^2 = 1{% endmathjax %}对应的曲线，它就是一个圆圈。当{% mathjax %}x_1^2 + x_2^2 \geq 1{% endmathjax %}时，这就是圆圈外的区域，这时预测{% mathjax %}\hat{y}{% endmathjax %}为`1`。相反，当{% mathjax %}x_1^2 + x_2^2 < 1{% endmathjax %}时，这就是圆圈内的区域，这时你预测{% mathjax %}\hat{y}{% endmathjax %}为`0`。
{% asset_img ml_5.png %}

能想出比这更复杂的决策边界吗？是的。你可以通过使用更高阶的多项式项来实现这一点{% mathjax %}f_{\vec{w},b}(\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 + w_6x_1^3 + \ldots + b){% endmathjax %}。那么你就有可能得到更复杂的**决策边界**。模型可以定义**决策边界**，比如这个例子，像这样的椭圆，或者选择不同的参数。可以得到更复杂的**决策边界**。这种逻辑回归的实现将预测在这个形状内{% mathjax %}\hat{y}=1{% endmathjax %}，而在形状外将预测{% mathjax %}\hat{y}=0{% endmathjax %}。有了这些多项式特征，你可以得到非常复杂的**决策边界**。换句话说，**逻辑回归**可以学习拟合相当复杂的数据。但是，如果不包含这些高阶多项式，那么使用的特征只有{% mathjax %}x_1,x_2,x_3{% endmathjax %}等，**逻辑回归**的**决策边界**将始终是线性的（一条直线）。
{% asset_img ml_6.png %}

#### 逻辑回归的成本函数

**逻辑损失函数**可以帮助我们为**逻辑回归**选择更好的参数。**逻辑回归模型**的训练集可能如下图所示。这里的每一行可能对应于正在看病的患者，其中一位患者的一些诊断。我们将使用{% mathjax %}m{% endmathjax %}表示训练示例的数量。每个训练示例都有一个或多个特征，例如肿瘤大小、患者年龄等，总共有{% mathjax %}n{% endmathjax %}个特征。我们将这些特征称为{% mathjax %}x_1,x_2,\ldots,x_n{% endmathjax %}。由于这是一个**二元分类任务**，目标标签{% mathjax %}y{% endmathjax %}只采用两个值，即`0`或`1`。**逻辑回归模型**由该方程定义。你要回答的问题是，给定这个训练集，你如何选择参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}？回想一下**线性回归**，是**平方误差成本函数**：{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{m}\sum_{i=1}^m \frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2{% endmathjax %}。我唯一改变的是，我将一半放在求和内而不是求和外。你可能还记得，在**线性回归**的情况下，{% mathjax %}f_{\vec{x},b}(\vec{x}) = \vec{w}\cdot\vec{x} + b{% endmathjax %}。**成本函数**看起来是一个**凸函数**或碗形或锤形。**梯度下降**像收敛到**全局最小值**。现在你可以尝试对**逻辑回归**使用相同的**成本函数**。但事实证明，如果我将{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}绘制**成本函数**，那么成本函数将看起来像**非凸成本函数**，它不是凸的。如果你尝试使用**梯度下降**。有很多**局部最小值**会让你感到厌烦。对于**逻辑回归**，这个**平方误差成本函数**并不是一个好的选择。相反，会有一个不同的成本函数，可以使成本函数再次凸出。**梯度下降**可以保证收敛到**全局最小值**。唯一改变的是将一半放在总和内而不是总和外。
{% asset_img ml_7.png %}

为了构建一个新的成本函数。我将稍微改变{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的成本函数{% mathjax %}\mathbf{J}{% endmathjax %}的定义。具体来说，如果你看看这个总和，这个术语称为单个训练示例的损失。通过这个大写的{% mathjax %}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}){% endmathjax %}表示损失，并将其作为学习算法的**预测函数**。在这种情况下，给定{% mathjax %}x{% endmathjax %}的预测变量{% mathjax %}f{% endmathjax %}和真实标签{% mathjax %}y{% endmathjax %}的损失等于平方差的`1.5`。我们很快就会看到，为该损失函数选择不同的形式，将能够保持**总成本函数**为凸函数。如果标签{% mathjax %}y^{(i)} = 1{% endmathjax %}，则损失为{% mathjax %}-\log f(x){% endmathjax %}；如果标签{% mathjax %}y^{(i)} = 0{% endmathjax %}，则损失为{% mathjax %}-\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。为什么这个损失函数有意义。首先考虑{% mathjax %}y^{(i)} = 1{% endmathjax %}的情况，并绘制此函数的样子，以获得有关此损失函数正在做什么。请记住，**损失函数**衡量的是你在某一个训练样本上的表现，它是通过对所有训练样本的损失求和得到的，**成本函数**衡量的是你在整套训练样本上的表现。如果你绘制{% mathjax %}-\log(f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}，它看起来像这条曲线，其中{% mathjax %}f{% endmathjax %}位于横轴上。{% mathjax %}f{% endmathjax %}的对数的负数图看起来像这样，我们只需沿横轴翻转曲线即可。
{% asset_img ml_8.png %}

请注意，它在{% mathjax %}f(x) = 1{% endmathjax %}处与横轴相交，并从那里继续向下。现在，{% mathjax %}f(x) = 1{% endmathjax %}是**逻辑回归**的输出。因此，{% mathjax %}f{% endmathjax %}始终介于`0` 和`1`之间，因为**逻辑回归**的输出始终介于`0`和`1`之间。如果算法预测的概率接近`1`，而真实标签为`1`，则损失非常小。它几乎为`0`，因为你非常接近正确答案。现在继续使用真实标签{% mathjax %}y^{(i)} = 1{% endmathjax %}的例子，假设都是恶性肿瘤。如果算法预测为`0.5`，那么损失就是这里的这个点，这个点稍高一点，但不是那么高。相反，如果算法认为肿瘤恶性的可能性只有`10%`，但{% mathjax %}y^{(i)} = 1{% endmathjax %}，那么输出为`0.1`。如果真的是恶性的，那么损失就是就是很高。当{% mathjax %}y^{(i)} = 1{% endmathjax %}时，**损失函数**会激励，推动算法做出更准确的预测，因为当它预测的值接近`1`时，损失最低。当{% mathjax %}y^{(i)} = 1{% endmathjax %}时损失是多少。对应于{% mathjax %}y^{(i)} = 0{% endmathjax %}时。损失是{% mathjax %}-\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。当绘制此函数时。 {% mathjax %}f \in [0,1]{% endmathjax %}，因为逻辑回归只输出`0~1`之间的值。如果我们放大，它看起来就是这样。在此图中，对应于{% mathjax %}y^{(i)} = 0{% endmathjax %}，纵轴显示不同{% mathjax %}f(x){% endmathjax %}值时的损失值。当{% mathjax %}f(x) = 0{% endmathjax %}或非常接近`0`时，损失也会非常小，如果真实标签{% mathjax %}y^{(i)} = 0{% endmathjax %}且模型的预测非常接近`0`，那么，是正确的，因此损失非常接近`0`。{% mathjax %}f(x){% endmathjax %} 值越大，损失越大，因为预测离真实标签{% mathjax %}y^{(i)} = 0{% endmathjax %}越远。事实上，当预测接近`1`时，损失实际上接近无穷大。

回到肿瘤预测的例子，如果模型预测患者的肿瘤几乎是恶性的，比如说，恶性的概率为`99.9%`，但结果不是恶性的，所以{% mathjax %}y^{(i)} = 0{% endmathjax %}，那么我们就会用非常高的**损失惩罚模型**。{% mathjax %}x{% endmathjax %}的预测{% mathjax %}f{% endmathjax %}距离{% mathjax %}y^{(i)}{% endmathjax %}的真实值越远，损失就越高。事实上，如果{% mathjax %}f(x){% endmathjax %}接近`0`，这里的损失实际上会变得非常大，实际上接近无穷大。当真实标签为`1`时，算法会受到强烈的激励，不要预测太接近`0`的值。
{% asset_img ml_9.png %}

事实证明，选择这种**损失函数**后，总体成本函数将呈现凸形，因此您可以使用梯度下降法迭代到**全局最小值**。成本函数是整个训练集的函数，因此是单个训练示例的损失函数总和的平均值或{% mathjax %}\frac{1}{m}{% endmathjax %}倍。特定参数集{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的成本等于训练示例损失的所有训练示例总和的{% mathjax %}\frac{1}{m}{% endmathjax %}倍。
{% asset_img ml_10.png %}

<span style="color:#295F98;font-weight:900;">简化损失函数</span>，您可以按如下方式编写损失函数：{% mathjax %}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=-y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1 - y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。我们刚刚用一行写出的这个等式与上面这个更复杂的公式完全等价。{% mathjax %}y^{(i)}{% endmathjax %}只能取`1`或`0`的值。在第一种情况下，当{% mathjax %}y^{(i)} = 1{% endmathjax %}。损失为{% mathjax %}-\log(f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。第二种情况，当{% mathjax %}y^{(i)} = 0{% endmathjax %}时。损失为{% mathjax %}-\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。这里的损失为{% mathjax %}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=-y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1 - y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)})){% endmathjax %}。您会发现，无论{% mathjax %}y^{(i)}{% endmathjax %}是`1`还是`0`，这里的的表达式都等同于上面更复杂的表达式，这样提供了一种更简单的方法来定义**损失函数**，只需一个方程，而无需像上面那样将这两种情况分开。
{% asset_img ml_11.png %}

使用这个简化的**损失函数**，让我们回过头来写出**逻辑回归**的<span style="color:#295F98;font-weight:900;">成本函数</span>。这里是简化的**损失函数**。回想一下，成本{% mathjax %}\mathbf{J}{% endmathjax %}只是**平均损失**，即整个训练集的{% mathjax %}m{% endmathjax %}个示例的平均值。因此，成本函数为{% mathjax %}\mathbf{J}(\vec{w},b) = \frac{1}{m}\sum_{i=1}^m[L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})]{% endmathjax %}，从{% mathjax %}i = \{1,\ldots,m\}{% endmathjax %}。如果您代入上面简化损失的定义，那么它看起来就像这样，{% mathjax %}\mathbf{J}(\vec{w},b) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1 - y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)}))]{% endmathjax %}。最终会得到这样的表达式，这就是**成本函数**。几乎每个人都使用这个**成本函数**来训练**逻辑回归**。
{% asset_img ml_12.png %}

#### 逻辑回归的梯度下降

为了拟合**逻辑回归模型**的参数，我们将尝试找到最小化{% mathjax %}\vec{w}{% endmathjax %}和{% mathjax %}b{% endmathjax %}的成本函数{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1 + e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}的值，我们将再次应用**梯度下降**来实现这一点。如果为模型提供新的输入{% mathjax %}vec{x}{% endmathjax %}，例如医院的新患者，其肿瘤大小和年龄已知，这些都是诊断。然后模型可以进行预测，或者可以尝试估计标签{% mathjax %}y = 1{% endmathjax %}的概率：{% mathjax %}\mathbf{P}(y=1|\vec{x};\vec{w},b){% endmathjax %}。您可以使用**梯度下降**来最小化**成本函数**的平均值。再次使用**成本函数**。如果您想最小化{% mathjax %}\mathbf{J}(\vec{w},b) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1 - y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)}))]{% endmathjax %}，通常的**梯度下降算法**，其中您重复更新每个参数为{% mathjax %}w_j = w_j - \alpha\frac{\partial}{\partial w_j}\mathbf{J}(\vec{w},b),\; b = b - \alpha\frac{\partial}{\partial b}\mathbf{J}(\vec{w},b){% endmathjax %}的导数。通常，{% mathjax %}j=\{1,\ldots,n\}{% endmathjax %}，其中{% mathjax %}n{% endmathjax %}是特征的数量。你可以证明成本函数{% mathjax %}J{% endmathjax %}对{% mathjax %}w_j{% endmathjax %}的导数等于这里的这个表达式{% mathjax %}-\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1 - y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)}))]{% endmathjax %}。也就是{% mathjax %}\frac{\partial}{\partial w_j}\mathbf{J}(\vec{w}, b) = \frac{1}{m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x^{(i)}_j,\;\;\;\frac{\partial}{\partial b}\mathbf{J}(\vec{w}, b) = \frac{1}{m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)}){% endmathjax %}。它与上面的表达式非常相似，只是它在末尾没有乘以{% mathjax %}x^{(i)}_j{% endmathjax %}。与你在线性回归中看到的类似，执行这些更新的方法是同时更新，意味着首先计算所有这些更新的右侧，然后同时覆盖左侧的所有值。让我把这里的这些导数表达式代入这里的这些项中。这给出了**逻辑回归**的**梯度下降**。
{% asset_img ml_13.png %}

****

{% asset_img ml_14.png %}

现在，你可能会想，这两个方程看起来和之前为**线性回归**得出的平均值一模一样，**线性回归**和**逻辑回归**是一样的吗？即使这些方程看起来一样，但这不是**线性回归**的原因，是因为函数{% mathjax %}f_{\vec{w},b}(\vec{x}){% endmathjax %}的定义已经改变。在线性回归中，{% mathjax %}f_{\vec{w},b}(\vec{x}) = \vec{w}\cdot\vec{x} + b{% endmathjax %}。但在**逻辑回归**中，{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1+ e^{-(\vec{w}\cdot\vec{x} + b)}}{% endmathjax %}被定义为`S`型函数。虽然**线性回归**和**逻辑回归**的算法看起来相同，但实际上它们是两种非常不同的算法，因为{% mathjax %}f_{\vec{w},b}(\vec{x}){% endmathjax %}的定义不一样。之前讨论**线性回归**的**梯度下降**时，你看到了如何监控梯度下降以确保它**收敛**。你可以将相同的方法应用于**逻辑回归**以确保它也收敛。与线性回归的**矢量化**实现类似，您也可以使用矢量化来使**逻辑回归**的**梯度下降**操作。您可能还记得使用**线性回归**时的**特征缩放**。将所有特征缩放为相似的值范围，例如在{% mathjax %}(-1,1){% endmathjax %}之间，如何帮助**梯度下降**更快地收敛。应用**特征缩放**来缩放不同的特征以采用相似的值范围加快**逻辑回归**的**梯度下降**。

#### 过度拟合

现在有两种不同的学习算法，**线性回归**和**逻辑回归**。它们在许多任务上都表现良好。但有时会遇到<span style="color:#295F98;font-weight:900;">过度拟合的问题</span>，这会导致其性能不佳。与**过度拟合**几乎相反的问题，称为<span style="color:#295F98;font-weight:900;">欠拟合</span>。有一种称为**正则化**的方法。是非常有用的技术。**正则化**将帮助你最大限度地减少这种**过度拟合问题**，并使你的学习算法更好地工作。让我们看看什么是过度拟合？为了帮助我们理解什么是过度拟合。让我们看几个例子。我们回到我们最初用**线性回归**预测房价的例子。你想根据房子的大小来预测价格。假设您的数据集如下所示，输入特征{% mathjax %}x{% endmathjax %}是房屋大小，{% mathjax %}y{% endmathjax %}是您要预测的房屋价格。您可以对这些数据进行**线性函数拟合**。您会得到一条直线拟合数据。但这不是一个很好的模型。查看数据，似乎很明显，随着房屋面积的增加，过程趋于平稳。该算法与训练数据的**拟合度不高**。术语是模型对训练数据的**拟合不足**。在机器学习中，术语“偏见”有多重含义。检查学习算法是否存在基于性别或种族等特征的偏见是很关键的。但是术语“偏差”还有第二个技术含义，也就是我在这里使用的含义，即算法是否对数据**拟合不足**，意味着它甚至无法很好地拟合训练集。训练数据中有一个明显的模式，算法无法捕捉到。另一种思考这种偏差的方式是，学习算法有一个非常强烈的先入之见，或者我们说一个非常强烈的**偏见**，即房价将完全是线性函数，尽管数据恰恰相反。这种数据是线性的先入之见导致它拟合一条直线，而这条直线与数据拟合得不好，从而导致数据**拟合不足**。

现在，让我们看看模型的第二种变体，即如果你在数据中插入一个二次函数，有两个特征，{% mathjax %}x{% endmathjax %}和{% mathjax %}x^2{% endmathjax %}，那么当你拟合参数{% mathjax %}w_1{% endmathjax %}和{% mathjax %}w_2{% endmathjax %}时，你可以得到一条更适合数据的曲线。另外，如果你要买一栋新房子，它不在这组五个训练示例中。这个模型可能在那栋新房子上表现得相当好。如果你是房地产经纪人，你希望你的学习算法即使在训练集之外的样本上也能表现良好，这就是所谓的**泛化**。从技术上讲，我们说你希望你的学习算法具有很好的**泛化能力**，这意味着即使对从未见过的全新样本也能做出很好的预测。这些二次模型似乎并不完美地拟合训练集，但相当不错。我认为它可以很好地推广到新样本。现在让我们看看另一个极端。如果你要用**四阶多项式**拟合数据会怎样？你有{% mathjax %}x,x^2,x^3{% endmathjax %}和{% mathjax %}x^4{% endmathjax %}都是特征。有了多项式的这个四阶多项式，你实际上可以精确地拟合五个训练样本的曲线。一方面，这似乎可以很好地拟合训练数据，因为它完美地通过了所有的训练数据。事实上，你可以选择参数，使**成本函数**恰好等于零，因为所有五个训练样本的误差都是`0`。但这是一条非常弯曲的曲线，它到处都是上下起伏。如果你有房屋的整体尺寸，模型会预测这所房子比它小的房子便宜。我们认为这不是一个特别好的预测房价的模型。我们会说这个模型**过度拟合**了数据，或者这个模型存在**过度拟合问题**。因为即使它很好地拟合了训练集，但它过过度拟合了数据。看起来这个模型不会推广到以前从未见过的新样本。另一个术语是算法具有**高方差**。在机器学习中，许多人几乎会交替使用**过度拟合**和**高方差**这两个术语。我们几乎会交替使用**欠拟合**和**高偏差**这两个术语。**过度拟合**或**高方差**背后的感觉是算法会非常努力地拟合每一个训练示例。如果你的训练集有一点点不同，比如说一个房屋的价格稍微高一点，那么算法拟合函数最终可能会完全不同。如果两个不同的机器学习工程师将这个四阶多项式模型拟合到略有不同的数据集上，他们最终不会得到完全不同的预测或高度可变的预测。这就是我们说算法具有**高方差**的原因。将最右边的模型与同一栋房子中间的模型进行对比，似乎中间的模型给出了更合理的价格预测。对于这种处于中间的情况，其实没有专门的名称，我将其称为“**恰到好处**”，因为它既不是**欠拟合**也不是**过拟合**。机器学习的目标是找到一个既不是**欠拟合**也不是**过拟合**的模型。换句话说，希望这个模型既没有**高偏差**也没有**高方差**。如果特征太多，比如右边的四阶多项式，那么模型可能与训练集拟合得很好，但几乎过拟合，方差很大。另一方面，如果特征太少，那么在这个例子中，比如左边的例子，它**欠拟合**，**偏差很大**。
{% asset_img ml_16.png %}

在这个例子中，使用二次特征{% mathjax %}x{% endmathjax %}和{% mathjax %}x^2{% endmathjax %}，似乎正好合适。我们已经研究了**线性回归模型**的**欠拟合**和**过拟合**。同样，过拟合也适用于分类。这是一个有两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}的分类示例，其中{% mathjax %}x_1{% endmathjax %}可能是肿瘤大小，{% mathjax %}x_2{% endmathjax %}是患者的年龄。我们试图对肿瘤是恶性还是良性进行分类，如这些叉号和圆圈所示，你可以做的一件事就是**拟合逻辑回归模型**。就像这样一个简单的模型{% mathjax %}z = w_1x_1 + w_2x_2 + b,\;\;g(z) = f_{\vec{w},b}(\vec{x}){% endmathjax %}是`sigmoid`函数。如果你这样做，你最终会得到一条直线作为**决策边界**。这是一条{% mathjax %}z= w_1x_1 + w_2x_2 + b = 0{% endmathjax %}的线，它将正例和负例区分开来。这条直线看起来并不糟糕。但看起来也不太适合数据。这是一个**欠拟合**或**高偏差**的例子。让我们看另一个例子。如果你将这些二次项添加到特征中，那么{% mathjax %}z = w_1x_1 + w_2x_2 + w_3x^2_1 + w_4x_2^2 + w_5x_1x_2+ b{% endmathjax %}将成为中间的这个新项，**决策边界**，即{% mathjax %}z= w_1x_1 + w_2x_2 + w_3x^2_1 + w_4x_2^2 + w_5x_1x_2+ b = 0{% endmathjax %}的地方，看起来更像椭圆或椭圆的一部分。这很好地拟合了数据，即使它不能完美地对训练集中的每个训练示例进行分类。请注意其中一些交叉是如何在圆圈中分类的。但这个模型看起来相当不错。我会称它为**恰到好处**。它看起来对新患者有很好的**泛化**。最后，如果你要用许多这样的特征来拟合一个非常高阶的多项式{% mathjax %}z= w_1x_1 + w_2x_2 + w_3x^2_1x_2 + w_4x_1^2x_2^2 + w_5x_1^2x_2^3 + w_6x_1^3x_2 + \ldots + b{% endmathjax %}，那么模型可能会非常努力地勾勒轮廓以找到一个完美拟合训练数据的**决策边界**。拥有所有这些高阶多项式特征允许算法在复杂的**决策边界**上进行选择。如果特征是肿瘤大小和年龄，而你试图将肿瘤分类为恶性或良性，那么这看起来并不是一个很好的预测模型。这是一个**过度拟合**和**高方差**的例子，因为它的模型虽然在训练集上表现很好，但看起来它不能很好地推广到新的样本。现在你已经看到了算法如何**欠拟合**或具有**高偏差**或**过拟合**和**高方差**。你可能想知道如何得到一个恰到好处的模型。
{% asset_img ml_17.png %}

当已经发生**过度拟合**时，如何解决它？假设您拟合了一个模型，并且它具有很高的方差，即**过度拟合**。这是我们的**过度拟合**房价预测模型。解决这个问题的一种方法是收集更多的训练数据，这是一种选择。如果您能够获得更多数据，即更多关于房屋大小和价格的训练示例，那么使用更大的训练集，学习算法将学会拟合一个不那么摇摆的函数。您可以继续**拟合高阶多项式**或具有大量特征的某些函数，如果您有足够的训练示例，它仍然可以正常工作。总而言之，您可以用来对抗**过度拟合**的首要工具是获取更多的训练数据。现在，获取更多数据并不总是一种选择。也许这个位置只卖出了这么多房子，所以也许没有更多数据可以添加。但是当数据可用时，这种方法确实很有效。解决**过度拟合**的第二个选择是看看是否可以使用更少的特征。我们的模型特征包括房屋大小{% mathjax %}x{% endmathjax %}，以及{% mathjax %}x^2{% endmathjax %}，{% mathjax %}x^3{% endmathjax %}，{% mathjax %}x^4{% endmathjax %}等等。这些都是多项式特征。在这种情况下，减少过度拟合的一种方法就是不要使用这么多的多项式特征。但现在让我们看一个的例子。也许你有很多不同的房子特征来预测它的价格，从大小、卧室数量、楼层数量、年龄、社区平均收入等等，到最近的咖啡店的距离。如果你有很多这样的特征，但没有足够的训练数据，那么你的学习算法也可能会**过度拟合**你的训练集。现在，如果不使用这`100`个特征，而是只选择最有用的特征子集，例如房屋大小、卧室和房龄。如果您认为这些是最相关的特征，那么仅使用最小的**特征子集**，您可能会发现模型不再**过度拟合**。选择最合适的特征集有时也称为**特征选择**。一种方法是靠您的直觉来选择您认为最好的**特征集**，即与预测价格最相关的特征集。特征选择的一个缺点是，仅使用**特征子集**，算法会丢弃您拥有的有关房屋的一些信息。例如，也许所有这些特征，`100`个特征实际上都可用于预测房屋的价格。也许您不想通过丢弃某些特征而丢弃一些信息。后边会讲到一些算法。这让我们看到了减少**过度拟合**的第三个选项。它被称为**正则化**。如果你看一下**过度拟合模型**，这是一个使用多项式特征的模型：{% mathjax %}x,x^2,x^3{% endmathjax %}等等。你会发现参数通常相对较大。如果你要消除其中一些特征，比如说，如果你要消除特征{% mathjax %}x_4{% endmathjax %}，这相当于将此参数设置为`0`。因此，将参数设置为`0`相当于消除一个特征。**正则化**是一种更温和地减少某些特征影响的方法，而不需要采取像直接消除它那样严厉的措施。**正则化**的作用是鼓励学习算法缩小参数值，而不一定要求将参数设置为`0`。即使**拟合高阶多项式**，只要让算法使用较小的参数值：{% mathjax %}w_2,w_2,w_3,w_4{% endmathjax %}。最终，你会得到一条曲线，它会更好地拟合训练数据。因此，**正则化**的作用是，它让你保留所有的特征，但只是防止特征产生过大的影响，而这有时会导致**过度拟合**。按照惯例，我们通常只是减小{% mathjax %}w_j{% endmathjax %}参数的大小，即{% mathjax %}w_j,\ldots,w_n{% endmathjax %}。正则化参数{% mathjax %}b{% endmathjax %}并没有太大的区别。通常不这样做，将{% mathjax %}w_1,w_2,\ldots,w_n{% endmathjax %}一直正则化是可以的，但并不鼓励{% mathjax %}b{% endmathjax %}变得更小。在实践中，是否也正则化{% mathjax %}b{% endmathjax %}没有什么区别。
{% asset_img ml_18.png %}

总结一下，解决**过度拟合**的三种方法：一是收集更多数据，这确实有助于减少过度拟合；二是尝试选择并使用特征子集；三是对参数进行正则化。

#### 正则化

##### 正则化成本函数

如果您将二次函数拟合到这些数据中，它会给出相当好的**拟合**。但是，如果拟合一个非常高阶的多项式，最终会得到一条**过度拟合数据**的曲线。但现在考虑以下情况，假设您有办法使参数{% mathjax %}w_3{% endmathjax %}和 {% mathjax %}w_4{% endmathjax %}非常小且接近`0`。
{% asset_img ml_19.png %}

假设不是最小化这个目标函数，而是**线性回归**的**成本函数**：{% mathjax %}\underset{\vec{w},b}{\min}\frac{1}{2m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2{% endmathjax %}。假设您要修改**成本函数**，并将其{% mathjax %}1000w_3^2{% endmathjax %}倍和{% mathjax %}1000w_4^2{% endmathjax %}放入成本函数{% mathjax %}\underset{\vec{w},b}{\min}\frac{1}{2m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2 + 1000w_3^2 + 1000w_4^2{% endmathjax %}。这里选择{% mathjax %}1000{% endmathjax %}是因为这是一个很大的数值，其他的数值也都可以。使用这个修改后的**成本函数**，如果{% mathjax %}w_3{% endmathjax %}和{% mathjax %}w_4{% endmathjax %}很大，你可能会**惩罚模型**。如果你想最小化这个函数，让新的成本函数变小的唯一方法是 {% mathjax %}w_3{% endmathjax %}和{% mathjax %}w_4{% endmathjax %}都变很小，否则，{% mathjax %}w_3^2\times 1000{% endmathjax %}和{% mathjax %}w_4^2\times 1000{% endmathjax %}的项会非常大。因此，当你最小化这个函数时，你最终会得到接近于`0`的 {% mathjax %}w_3{% endmathjax %}和{% mathjax %}w_4{% endmathjax %}。所以几乎抵消了特征执行和额外的影响，并去掉了这两个项。如果这样做，最终会得到一个更接近二次函数的数据拟合结果，如果参数值较小，那么这拥有一个更简单且特征较少的模型，因此不太容易**过度拟合**。

比如`100`个特征，可能不知道哪些是最重要的特征以及哪些特征需要**惩罚**。因此，使用正则化是惩罚所有特征，或者更准确地说，惩罚所有{% mathjax %}w_j{% endmathjax %}参数，并且可能会使拟合更平滑、更简单，并且不易**过度拟合**。对于此示例，如果您拥有每栋房屋`100`个特征的数据，则很难选择要包含哪些特征以及要排除哪些特征。因此，让我们构建`100`个特征的模型。您有`100`个参数{% mathjax %}w_1,w_2,\ldots,w_100{% endmathjax %}和参数{% mathjax %}b{% endmathjax %}。由于我们不知道这些参数中的哪些是重要的。让我们对全部进行一点**惩罚**，并通过添加新的项{% mathjax %}\frac{\lambda}{2m}\sum_{j=1}^n w_j^2{% endmathjax %}来缩小它们。{% mathjax %}w_j{% endmathjax %}的特征数量平方，也称为**正则化参数**。与选择**学习率**{% mathjax %}\alpha{% endmathjax %}类似，您必须为{% mathjax %}\lambda{% endmathjax %}选择一个数值。更新后的成本函数为：{% mathjax %}\mathbf{J}(\vec{w},b) = [\underset{\vec{w},b}{\min}\frac{1}{2m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^n w_j^2]{% endmathjax %}，这里的第一项和第二项都按{% mathjax %}\frac{1}{2m}{% endmathjax %}缩放。事实证明，通过以方式缩放这两个项，选择一个好的{% mathjax %}\lambda{% endmathjax %}值会容易一些。即使训练集大小的增长，比如说你找到了更多的训练示例。所以训练集大小{% mathjax %}m{% endmathjax %}更大了。如果你有{% mathjax %}2m{% endmathjax %}缩放，你之前选择的{% mathjax %}\lambda{% endmathjax %}值现在也有可能满足要求。按照惯例，不会因为参数{% mathjax %}b{% endmathjax %}太大而惩罚它。总结一下，在这个修改后的**成本函数**中，希望最小化原始成本，即**均方误差成本**加上第二项，即**正则化项**。尝试最小化这个第一项会鼓励算法通过**最小化平方差**来很好地拟合训练数据预测值和实际值。并尝试最小化第二项。该算法还尝试保持参数{% mathjax %}w_j{% endmathjax %}较小，这将倾向于减少**过度拟合**。您选择的{% mathjax %}\lambda{% endmathjax %}值指定了相对权衡，或者您如何在这两个目标之间取得**平衡**。使用**线性回归**的房价预测示例。{% mathjax %}f_{\vec{w},b}(\vec{x}) = w_1x + w_2x^2 + w_3x^3 + w_4x^4 + b{% endmathjax %}是**线性回归**模型。如果{% mathjax %}\lambda = 0{% endmathjax %}，那么不会使用**正则化项**。因此，最终会拟合这个过于摇摆不定、过于复杂的曲线，并且它会**过度拟合**。所以这是{% mathjax %}\lambda = 0{% endmathjax %}时的一个极端。现在让我们看看另一个极端。如果您认为{% mathjax %}\lambda{% endmathjax %}是一个非常大的数字，比如说{% mathjax %}\lambda = 10^{10}{% endmathjax %}，将非常大的权重放在了右边的**正则化项**上。而最小化这个权重的唯一方法是确保{% mathjax %}w{% endmathjax %}的所有值都非常接近`0`。因此，如果{% mathjax %}\lambda{% endmathjax %}非常大，学习算法将选择非常接近`0`的{% mathjax %}w_1,w_2,w_3,w_4{% endmathjax %}，因此{% mathjax %}f_{\vec{w},b}(\vec{x}){% endmathjax %}基本上等于{% mathjax %}b{% endmathjax %}，因此学习算法将拟合一条水平直线并且**欠拟合**。

总结一下，如果{% mathjax %}\lambda = 0{% endmathjax %}，这个模型会**过拟合**。如果{% mathjax %}\lambda{% endmathjax %}非常大，比如{% mathjax %}{x}\lambda = 10^{10}{% endmathjax %}，这个模型会**欠拟合**。因此，想要的是介于两者之间的某个{% mathjax %}\lambda{% endmathjax %}值，适当地平衡第一和第二项，**最小化均方误差**并保持参数较小。当{% mathjax %}\lambda{% endmathjax %}的值不太小也不太大，而是恰到好处时，希望最终能够拟合一个`4`阶多项式，保留所有这些特征。这就是**正则化**的工作原理。

##### 正则化线性回归

如何让**梯度下降**与**正则化线性回归**配合使用？现在有了这个额外的**正则化项**{% mathjax %}\frac{\lambda}{2m}\sum_{j=1}^n w_j^2{% endmathjax %}，其中{% mathjax %}\lambda{% endmathjax %}是**正则化参数**，希望找到**最小正则化成本函数**({% mathjax %}\mathbf{J}(\vec{w},b) = [\underset{\vec{w},b}{\min}\frac{1}{2m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^n w_j^2]{% endmathjax %})的参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}。之前，对**原始成本函数**使用**梯度下降**，只是在添加第二个正则化项之前的第一个项，有以下**梯度下降算法**，即根据此公式({% mathjax %}w_j = w_j - \alpha\frac{\partial}{\partial w_j}\mathbf{J}(\vec{w},b),\;\;b = b - \alpha\frac{\partial}{\partial b}\mathbf{J}(\vec{w},b){% endmathjax %})反复更新参数{% mathjax %}w_j{% endmathjax %}和{% mathjax %}b{% endmathjax %}，其中{% mathjax %}j = \{1,\ldots,n\}{% endmathjax %}，并且{% mathjax %}b{% endmathjax %}也以类似的方式更新。同样，{% mathjax %}\alpha{% endmathjax %}是一个非常小的正数，称为**学习率**。**正则化线性回归**的更新看起来完全相同，只是成本函数{% mathjax %}\mathbf{J}{% endmathjax %}的定义略有不同。之前，{% mathjax %}\mathbf{J}{% endmathjax %}对{% mathjax %}w_j{% endmathjax %}和{% mathjax %}b{% endmathjax %}的**导数**由此处的这个表达式给出，现在添加了这个额外的**正则化项**，唯一改变的是，对{% mathjax %}w_j{% endmathjax %}的**导数表达式**添加了一个额外的项，即{% mathjax %}\frac{\lambda}{m}w_j{% endmathjax %}。这里没有对{% mathjax %}b{% endmathjax %}进行**正则化**，所以不会缩小{% mathjax %}b{% endmathjax %}。{% mathjax %}b{% endmathjax %}保持与之前相同，而更新后的{% mathjax %}\vec{w}{% endmathjax %}会发生变化。让我们把这些导数的定义放回到左边的表达式中，写出**正则化线性回归**的**梯度下降算法**。这是{% mathjax %}w_j{% endmathjax %}的更新，{% mathjax %}j=\{1,2,\ldots, n\}{% endmathjax %}。像往常一样，请记住同时更新所有这些参数。
{% asset_img ml_20.png %}

****

{% asset_img ml_21.png %}

让我们看看{% mathjax %}w_j{% endmathjax %}的更新规则，并以另一种方式重写它。我们将{% mathjax %}w_j{% endmathjax %}更新为{% mathjax %}w_j = 1w_j - \alpha\frac{\lambda}{m}w_j -\alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}{% endmathjax %}。通过简化{% mathjax %}w_j{% endmathjax %}更新为{% mathjax %}w_j = w_j(1- \alpha\frac{\lambda}{m}) -\alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}{% endmathjax %}。您可能会认出第二项是**非正则化线性回归**的**梯度下降更新**。这是进行正则化之前的线性回归更新，添加正则化的唯一变化是，{% mathjax %}w_j{% endmathjax %}不再等于{% mathjax %}w_j - \alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}{% endmathjax %}，而是{% mathjax %}w_j(1- \alpha\frac{\lambda}{m}) -\alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}{% endmathjax %}。{% mathjax %}\alpha{% endmathjax %}是一个非常小的正数，比如`0.01`。{% mathjax %}\lambda{% endmathjax %}是一个很小的数值，比如`1`或`10`。假设此示例中的{% mathjax %}\lambda = 1{% endmathjax %}，{% mathjax %}m{% endmathjax %}是训练集大小，比如`50`。当{% mathjax %}\alpha\frac{\lambda}{m} = 0.01\times \frac{1}{50} = 0.0002{% endmathjax %}时，因此，{% mathjax %}1-\alpha\frac{\lambda}{m}{% endmathjax %}会得到一个略小于`1`的数，在本例中为`0.9998`。该项的作用是，在梯度下降的每次迭代中，您都会取{% mathjax %}w_j{% endmathjax %}并将其乘以`0.9998`，即乘以一些略小于`1`的数，并进行常规更新。**正则化**在每次迭代中所做的就是将 {% mathjax %}\vec{w}{% endmathjax %}乘以一个略小于`1`的数字，这会使{% mathjax %}w_j{% endmathjax %}的值稍微缩小一点。这让我们从另一个角度了解了为什么**正则化**会在每次迭代中略小于参数{% mathjax %}w_j{% endmathjax %}，这就是**正则化**的工作原理。
{% asset_img ml_22.png %}
​
##### 正则化逻辑回归

如何实现**正则化逻辑回归**？正如**逻辑回归**的梯度更新与**线性回归**的梯度更新惊人地相似一样，您会发现**正则化逻辑回归**的梯度下降更新也与**正则化线性回归**的更新相似。我们之前看到，如果使用**高阶多项式特征**来**拟合逻辑回归**，则**逻辑回归**很容易**过度拟合**。{% mathjax %}z{% endmathjax %}是一个高阶多项式{% mathjax %}z = w_1x_1 + w_2x_2 + w_3x_1^2x_2 + w_4x_1^2x_2^2 + w_5x_1^2x_2^3+ \ldots + b{% endmathjax %}，它被传递到`S`型函数中，就像这样来计算{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-z}}{% endmathjax %}。您最终可能会得到一个过于复杂且**过度拟合**的**决策边界**作为训练集。当使用大量特征（无论是多项式特征还是其他特征）训练**逻辑回归**时，可能会有的**过度拟合风险**。这是**逻辑回归**的**成本函数**{% mathjax %}\mathbf{J}(\vec{w},b) = \underset{\vec{w},b}{\min}\mathbf{J}(\vec{w},b)= -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1-y^{(i)})\log(1 - f_{\vec{w},b}(\vec{x}^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n w_j^2{% endmathjax %}。如果修改它以使用正则化，需要向其中添加以下项{% mathjax %}w_j = w_j - \alpha\frac{\partial}{\partial w_j}\mathbf{J}(\vec{w},b),\;\;b = b -\alpha\frac{\partial}{\partial b}\mathbf{J}(\vec{w},b){% endmathjax %}。将{% mathjax %}\lambda{% endmathjax %}添加到**正则化参数**中。将此**成本函数**最小化为{% mathjax %}\vec{w}{% endmathjax %}和{% mathjax %}b{% endmathjax %}的函数时，它会惩罚参数{% mathjax %}w_1,w_2,\ldots,w_n{% endmathjax %}，并防止它们过大。如果这样做，那么即使您正在拟合具有大量参数的高阶多项式，您仍然会得到如下图所示的**决策边界**。看起来可以分离正例和负例，同时可以推广到训练集中没有的新示例。使用**正则化**时，您如何最小化包含**正则化项的成本函数**{% mathjax %}\mathbf{J}{% endmathjax %}？使用**梯度下降**({% mathjax %}w_j = w_j - \alpha\frac{1}{m}\sum_{i=1}^m[f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)})x_j^{(i)}] + \frac{\lambda}{m}w_j,\;\;b = b - \alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)} - y^{(i)}){% endmathjax %}))，对{% mathjax %}w_j{% endmathjax %}和{% mathjax %}b{% endmathjax %}同时更新。就像**正则化线性回归**一样，当您计算这些导数项的位置时，现在唯一改变的是，对{% mathjax %}w_j{% endmathjax %}的导数会有这个附加项，即{% mathjax %}\frac{\lambda}{m}w_j{% endmathjax %}。同样，它看起来很像**正则化线性回归**的更新。是完全相同的方程，只是{% mathjax %}f{% endmathjax %}的定义不再是**线性函数**，而是应用于{% mathjax %}z{% endmathjax %}的**逻辑函数**。与**线性回归**类似，仅对参数{% mathjax %}w_j{% endmathjax %}进行**正则化**，但不对参数{% mathjax %}b{% endmathjax %}进行**正则化**。
{% asset_img ml_23.png %}

****

{% asset_img ml_24.png %}