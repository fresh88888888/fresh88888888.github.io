---
title: 机器学习(ML)(四) — 探析
date: 2024-09-11 10:42:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

几十年前，**神经网络**刚被发明时，最初的动机是编写能够模仿人类大脑或生物大脑学习和思考方式的软件。尽管如今，**神经网络**（有时也称为**人工神经网络**）已经变得与我们对大脑实际看法大不相同。一些生物学动机仍然保留在我们今天对**人工神经网络**或**计算机神经网络**的看法中。让我们先来看看大脑是如何工作的，以及它与**神经网络**的关系。人类大脑，或者更广泛地说，生物大脑表现出更高水平或更强大的**智能**，**神经网络**的初衷是试图构建软件来模仿大脑。
<!-- more -->

**神经网络**的研究始于`20`世纪`50`年代，后来一度失宠。然后在`20`世纪`80`年代和`90`年代初期，它们再次受到欢迎，并在一些应用领域表现出巨大的吸引力，例如手写数字识别，当时甚至用于读取写邮件的邮政编码和手写支票上的美元数字。但随后在`20`世纪`90`年代末，它再次失宠。大约从`2005`年开始，它再次复苏，并随着深度学习而重新焕发活力。当时惊讶的一件事是**深度学习**和**神经网络**的含义非常相似。但当时可能没有得到充分重视，**深度学习**这个术语听起来好多了，因为它是深度的学习。所以，在过去的十年或十五年里，**深度学习**成为了一个迅速发展的品牌。从那时起，**神经网络**就彻底改变了一个又一个应用领域。现代**神经网络**或**深度学习**产生巨大影响的第一个应用领域可能是**语音识别**，我们开始看到现代深度学习带来的更好的语音识别系统，`[inaudible]`和`Geoff Hinton`等人对此起到了重要作用，然后它开始进入**计算机视觉**领域。人们仍然会谈论`2012`年的`ImageNet`时刻，那也许是一个更大的轰动，当时`[inaudible]`发挥了他们的想象力，对**计算机视觉**产生了巨大的影响。然后在接下来的几年里，它让我们进入了文本或自然语言处理领域等等。现在，**神经网络**被应用于从气候变化到医学成像到在线广告到产品推荐等各个领域，现在**机器学习**的许多应用领域都使用**神经网络**。尽管今天的**神经网络**与大脑的学习方式几乎没有任何关系，但早期的动机是尝试构建软件来模仿大脑。那么大脑是如何工作的呢？这是一张说明大脑中神经元的图表。
{% asset_img ml_1.png %}

人类的所有思维都来自大脑中的**神经元**，它们发出**电脉冲**，有时还会与其他神经元形成新的连接。给定一个像这样的**神经元**，它有许多输入，从其他**神经元**接收电脉冲，然后圈出的这个神经元进行一些计算，然后通过电脉冲将这些输出发送给其他神经元，这个**上层神经元**的输出反过来又成为**下层神经元**的输入，这个神经元再次聚合来自多个其他神经元的输入，然后可能将其自己的输出发送给其他**神经元**，这就是人类思维的组成部分。这是**生物神经元**的简化图。**神经元**由左侧显示的**细胞体**组成，如果你上过生物学课，你可能会认出这是**神经元**的核心。神经元有不同的输入。在**生物神经元**中，输入线称为**树突**，然后它偶尔会通过输出线（称为**轴突**）向其他神经元发送电脉冲。这些电脉冲会成为另一个神经元的输入。因此，**人工神经网络**使用一个非常简化的**数学模型**来描述**生物神经元**的功能。**神经元**的作用是接受一些输入，一个或多个输入，这些输入只是数字。它进行一些计算并输出其他数字，然后这些数字可以作为第二个神经元的输入，如右图所示。当你构建**人工神经网络**或**深度学习算法**时，你通常希望同时模拟许多这样的神经元，而不是一次构建一个神经元。在这个图中，画了三个神经元。这些神经元共同的作用是输入一些数字，进行一些计算，然后输出一些其他数字。尽管对**生物神经元**和**人工神经元**进行了松散的类比，但我今天几乎不知道人类大脑是如何工作的。事实上，每隔几年，神经科学家就会在大脑的工作原理上取得一些根本性的突破。大脑的实际工作原理还有许多突破尚未发现，因此，试图盲目模仿今天对人类大脑的了解（坦率地说，了解的很少），可能不会让我们在构建**原始智能**方面走得那么远。以目前在神经科学方面的知识水平，肯定不行。话虽如此，即使有了这些极其简化的**神经元模型**，也能够构建真正强大的**深度学习算法**。因此，当你深入研究**神经网络**和**深度学习**时，即使其起源是受生物驱动的，也不要太在意生物驱动。事实上，这些从事深度学习研究的人已经不再过多地关注生物驱动。相反，他们只是使用**工程原理**来弄清楚如何构建更有效的算法。但我认为，不时推测和思考生物神经元的工作原理仍然很有趣。**神经网络**的概念已经存在了几十年。为什么**神经网络**直到最近几年才真正流行起来？在横轴上画出你拥有的用于问题的**数据量**，在纵轴上画出应用于该问题的学习算法的**性能**或**准确性**。
{% asset_img ml_2.png %}

在过去的几十年里，随着互联网的兴起、手机的兴起、社会的数字化，我们为许多应用所拥有的数据量稳步向右发展。许多使用电子替代了纸质记录，例如，如果您订购了某样东西，而不是写在纸上，那么更有可能是数字记录。如果您去看医生，您的健康记录现在更有可能是数字的，而不是纸质的。因此，在许多应用领域，**数据量**呈爆炸式增长。我们看到，使用传统的**机器学习算法**，例如**逻辑回归**和**线性回归**，即使您为这些算法输入更多数据，也很难使其性能持续提高。因此，就像**线性回归**和**逻辑回归**等传统学习算法一样，它们无法随着我们现在可以输入的数据量而扩展，也无法有效地利用不同应用所拥有的这些数据。如果你在这个数据集上训练一个**小型神经网络**，那么它的性能可能看起来是这样的。如果你训练一个**中型神经网络**，也就是一个包含更多神经元的网络，它的性能可能看起来是那样的。如果你训练一个**大型神经网络**，也就是一个包含大量人工神经元的网络，那么对于某些应用来说，性能会不断提高。这意味着两件事，对于某些拥有大量数据的应用，有时你会听到大数据这个词，如果你能够训练一个非常大的神经网络来利用你拥有的大量数据，那么你就可以在**语音识别**、**图像识别**、**自然语言处理**应用等许多领域获得更好的性能，而这些是早期学习算法无法实现的。这导致**深度学习算法**的腾飞，这也是更快的计算机处理器（包括`GPU`）兴起的原因。这种硬件最初设计用于生成美观的计算机图形，但后来发现它对**深度学习**也非常有用。这也是**深度学习算法**成为今天的样子的主要力量。这就是**神经网络**的起源，也是它们在过去几年中如此迅速发展的原因。

为了说明**神经网络**的工作原理，让我们从一个例子开始。使用一个需求预测的例子，在这个例子中，查看产品并尝试预测该产品是否会成为畅销产品？你正在销售`T`恤，你想知道某件`T`恤是否会成为畅销产品，并且收集了以不同价格出售的不同`T`恤的数据，以及哪些`T`恤成为畅销产品。如今，零售商使用这种类型的应用程序来规划更好的库存水平以及营销活动。如果你知道什么可能成为畅销产品，你会计划提前购买更多库存。在这个例子中，输入特征{% mathjax %}x{% endmathjax %}是`T`恤的价格，这是学习算法的输入。如果使用**逻辑回归**来将`S`型函数拟合到这样的数据，那么预测输出看起来像这样{% mathjax %}f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-wx + b}}{% endmathjax %}，作为学习算法的输出。为了构建**神经网络**，使用字母{% mathjax %}a{% endmathjax %}来表示该**逻辑回归算法**的输出。{% mathjax %}a{% endmathjax %}代表激活，指的是**神经元**向下游其他**神经元**发送输出的程度。这个**逻辑回归单元**被认为是大脑中单个**神经元**的非常简化的模型。**神经元**的作用是输入价格{% mathjax %}x{% endmathjax %}，然后计算这个公式{% mathjax %}\vec{a} = f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-(wx + b)}}{% endmathjax %}，输出{% mathjax %}\vec{a}{% endmathjax %}由这个公式计算得出，输出这件`T`恤成为畅销品的概率。
{% asset_img ml_3.png %}

另一种理解**神经元**的方式是将其视为一台小型计算机，其唯一工作是输入一个或几个数字，例如价格，然后输出一个或几个其他数字，在本例中是`T`恤成为畅销品的**概率**。**逻辑回归算法**比大脑中的任何**生物神经元**都简单得多。这就是为什么**人工神经网络**是人类大脑的一个极其简化的模型。尽管在实践中，**深度学习算法**确实非常有效。鉴于对单个神经元的这种描述，现在构建**神经网络**只需要取出一堆这些**神经元**并将它们连接在一起或将它们放在一起。举一个例子，我们将使用四个特征来预测`T`恤是否是畅销品。这些特征包括`T`恤的价格、运费、特定`T`恤的营销量以及材料质量，这是高品质的厚棉布还是低质量的材料？现在，可能会怀疑`T`恤是否成为畅销品实际上取决于几个因素。首先，这件`T`恤的价格是否合理。其次，潜在买家对这件`T`恤的认知程度如何？第三是感知质量偏见或潜在偏见，认为这是一件高品质的`T`恤。创建一个**人工神经元**，估计这件`T`恤被认为非常实惠的概率。价格实惠主要取决于价格和运费，因为总付款金额是价格加上运费的一部分。在这里使用一个小神经元，一个逻辑回归单元来输入价格和运费，并预测人们是否认为这是实惠的？其次，我将在这里创建另一个人工神经元来估​​计，人们对这件 T 恤的认知程度是否很高？在这种情况下，认知度主要取决于 T 恤的营销。最后，我们将创建另一个神经元来估​​计人们是否认为这是高质量的，这可能主要取决于`T`恤的价格和材料质量。价格是一个因素，如果有一件价格非常高的`T`恤，人们会认为它是高质量的，因为它非常昂贵，也许人们认为它会是高质量的。根据这些对可负担性、认知度和感知质量的估计，将这三个**神经元**的输出连接到右边的另一个**神经元**，然后是另一个**逻辑回归单元**。它最终输入这三个数字并输出这件`T`恤成为畅销品的**概率**。在神经网络的术语中，把这三个**神经元**组合成一个**层**。**层是一组神经元**，它将相同或相似的特征作为输入，然后输出几个数字。左侧的三个神经元形成一层，这就是我将它们画在彼此之上的原因，而右侧的单个**神经元**也是一个层。左侧的层有三个神经元，因此一个层可以有多个神经元，也可以只有一个神经元，就像右侧这一层的情况一样。右侧这一层也称为**输出层**，因为这个最终神经元的输出是**神经网络**预测的**输出概率**。在神经网络术语中，把可负担性意识和感知质量称为**激活**。**激活**这个术语来自**生物神经元**，它指的是**生物神经元**向下游的其他**神经元**发送的输出值或电脉冲的程度。这些关于可负担性、意识和感知质量的数字是这一层中这三个**神经元**的**激活**，而且这个输出概率是右侧显示的这个神经元的**激活**。这个神经网络执行以下计算。它输入四个数字，然后**神经网络**的这一层使用这四个数字来计算新的数字，也称为**激活值**。然后最后一层，即**神经网络**的**输出层**使用这三个数字来计算一个数字。在**神经网络**中，这四个数字的列表也称为**输入层**，它只是一个四个数字的列表。现在，对这个**神经网络**做一个简化。按照目前为止必须一次检查一个**神经元**，并决定它要从前一层获取什么输入。例如，我们说可负担性只是价格和运费的函数，而知名度只是营销的函数，等等，但是如果你正在构建一个大型**神经网络**，那么手动决定哪些**神经元**应该将哪些特征作为输入将是一项艰巨的工作。**神经网络**在实践中的实现方式是某一层中的每个神经元；假设中间这一层可以访问前一层的每个**特征**和每个值，也就是**输入层**，如果试图预测可负担性，并且它知道价格、运费、营销和材料成本是多少，那么你可能会学会忽略营销和材料，只需通过适当设置参数来找出答案，只关注与可负担性最相关的特征子集。为了进一步简化这个**神经网络**的符号和描述，把这四个输入**特征**写成一个向量{% mathjax %}\vec{x}{% endmathjax %}，把**神经网络**视为具有四个特征，这些**特征**构成了这个**特征向量**{% mathjax %}\vec{x}{% endmathjax %}。这个**特征向量**被输入到中间这一层，然后计算三个激活值。这三个激活值又变成另一个向量，被输入到这个最终的**输出层**，最终输出这件`T`恤成为畅销品的概率。这就是神经网络的全部。

它有几个层，每个层输入一个向量并输出另一个向量。例如，中间的这个层输入四个数字{% mathjax %}\vec{x}{% endmathjax %}并输出三个数字，分别对应可负担性、知名度和感知质量。这`2`个层称为输出层，输入层。为了给中间的层起个名字，中间的这个层称为**隐藏层**。在训练集中，可以观察{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}。数据集告诉您{% mathjax %}x{% endmathjax %}是什么，{% mathjax %}y{% endmathjax %}是什么，但**数据集**不会告诉您可负担性、知名度和感知质量的正确值是什么。这些值的正确值是隐藏的。在训练集中看不到它们，这就是为什么中间的这个层被称为**隐藏层**。让我先把这个图的左半部分遮住，然后看看剩下的部分。你在这里看到的是，有一个**逻辑回归算法**或**逻辑回归单元**，它将`T`恤的可负担性、知名度和感知质量作为输入，并使用这三个特征来估计`T`恤成为畅销品的概率。这只是**逻辑回归**。但很酷的是，它不是使用原始特征，如价格、运费、营销等，而是使用可能更好的**特征集**，即可负担性、知名度和感知质量质量，希望这些特征能够更好地预测这件`T`恤是否会成为畅销品。理解这种神经网络的一种方式就是**逻辑回归**。但作为**逻辑回归**的一个版本，它们可以学习自己的特征，从而更容易做出准确的预测。如果你想预测房子的价格，你可以取地块的正面或宽度，然后将其乘以地块的深度，以构建一个更复杂的特征，{% mathjax %}x^{<1>}\times x^{<2>}{% endmathjax %}即草坪的大小。我们在那里进行手动特征工程，我们必须查看特征{% mathjax %}x^{<1>}x_2\times x^{<2>}{% endmathjax %}，并手动决定如何将它们组合在一起。**神经网络**所做的是不需要手动设计特征，它可以学习自己的特征。这就是*神经网络*成为当今世界上最强大的学习算法之一的原因。总而言之，输入层有一个**特征向量**，在这个例子中是四个数字，它被输入到**隐藏层**，**隐藏层**输出三个数字。我将使用一个向量来表示这个**隐藏层**输出的**激活向量**。然后**输出层**将其输入为三个数字并输出一个数字，这将是最终的**激活**，或者神经网络的最终预测。

{% note warning %}
**请注意**，尽管之前将这个神经网络描述为计算可负担性、意识和感知质量，但**神经网络**的一个非常好的特性是，当你从数据中训练它时，你不需要明确地决定**神经网络**应该计算哪些特征，比如可负担性等等，或者自己弄清楚它想要在这个**隐藏层**中使用哪些特征。这就是它如此强大的原因。这个神经网络有一个单层，即**隐藏层**。让我们看一些其他神经网络的例子。这个**神经网络**有一个**输入特征向量**`x`，它被馈送到一个**隐藏层**。将其称为第一个隐藏层。如果这个隐藏层有三个神经元，它将输出一个包含三个**激活值**的向量。这三个数字可以输入到第二个**隐藏层**。如果第二个**隐藏层**有两个**神经元**到**逻辑单元**，那么第二个**隐藏层**将输出另一个包含两个**激活值**的向量，向量接着进入**输出层**，然后输出**神经网络**的最终预测。在一些文献中，您会看到这种具有多个层的**神经网络**，称为**多层感知器**。
{% endnote %}
{% asset_img ml_4.png %}

如果您正在构建人脸识别应用程序，则可能需要训练一个**神经网络**，该**神经网络**将这样的图片作为输入，并输出图片中人物的身份。此图像为{% mathjax %}1,000 \times 1,000{% endmathjax %}像素。它在计算机中的表示实际上是{% mathjax %}1,000 \times 1,000{% endmathjax %}网格，或称为{% mathjax %}1,000 \times 1,000{% endmathjax %}**像素强度值矩阵**。在此示例中，**像素强度值**从{% mathjax %}0,1,\ldots,255{% endmathjax %}，这里的`197`是图像最左上角像素的亮度，`185`是像素的亮度，依此类推，直到`214`是该图像的右下角。如果要把这些像素强度值展开成一个向量，你最终会得到一个包含一百万个**像素强度值**的列表或向量。一百万是因为{% mathjax %}1,000 \times 1,000{% endmathjax %}的正方形会给你一百万个数字。人脸识别的问题是，你能否训练一个**神经网络**，具有一百万个像素亮度值的**特征向量**作为输入，并输出图片中人物的身份。这就是构建一个**神经网络**来执行此任务的方法。输入图像{% mathjax %}\vec{x}{% endmathjax %}被馈送到这一层**神经元**。这是第一个**隐藏层**，然后提取一些**特征**。第一个**隐藏层**的输出被馈送到第二个**隐藏层**，该输出被馈送到第三层，最后馈送到**输出层**，然后这是某个人的概率。有趣的是，如果查看一个在大量人脸图像上训练过的**神经网络**，并尝试可视化这些隐藏层，并尝试计算。当你在大量面部图片上训练这样的系统，并观察隐藏层中的不同神经元正在计算什么时，你可能会发现在第一个隐藏层中，会发现一个**神经元**正在寻找低垂直线或类似的垂直边缘。第二个神经元正在寻找有向线或有向边缘。第三个神经元寻找该方向的线，依此类推。在**神经网络**的最早层中，你可能会发现**神经元**正在寻找图像中非常短的线或非常短的边缘。如果你查看下一个隐藏层，你会发现这些**神经元**学会将许多小短线和小短边段组合在一起，以寻找面部的各个部分。例如，每个小方框都是该神经元试图检测的内容的可视化。第一个**神经元**看起来像是在试图检测图像中某个位置是否有眼睛。第二个**神经元**看起来好像在尝试检测鼻角，也许这里的**神经元**正在尝试检测耳朵的底部。然后，当您查看此示例中的下一个**隐藏层**时，**神经网络**会聚合面部的不同部分，然后尝试检测是否存在较大、较粗糙的面部形状。最后，检测面部与不同面部形状的对应程度会创建一组丰富的**特征**，然后帮助**输出层**尝试确定人物照片的身份。**神经网络**的一个了不起之处在于，可以自行学习不同隐藏层上的这些**特征检测器**。在此示例中，没有人告诉它在第一层中寻找短小的边缘，在第二层中寻找眼睛、鼻子和面部部分，然后在第三层中寻找更完整的面部形状。神经网络能够从数据中自行找出这些东西。
{% asset_img ml_5.png %}

{% note warning %}
**请注意**，在此可视化中，第一个**隐藏层**的**神经元**显示为查看相对较小的窗口以寻找这些边缘。第二个**隐藏层**查看更大的窗口，第三个**隐藏层**查看更大的窗口。这些小神经元可视化实际上对应于图像中不同大小的区域。只是为了好玩，让我们看看如果你在不同的数据集上训练这个**神经网络**会发生什么，比如在很多汽车图片上，侧面的图片。同样的学习算法被要求检测汽车，然后会在第一层学习边缘。非常相似，但它们将学习在第二**隐藏层**检测汽车的部分，第三**隐藏层**学习更完整的汽车形状。只需输入不同的数据，**神经网络**就会自动学习检测完全不同的**特征**，从而尝试做出汽车检测或人脸识别的预测，或者是否有特定的给定任务需要训练。这就是**神经网络**在计算机视觉应用的工作原理。
{% endnote %}

#### 神经网络模型

大多数**神经网络**的基本构建块是**神经元层**。让我们来看看**神经元层**是如何工作的。其中有四个输入特征，它们被设置为**隐藏层**中的三个**神经元**，然后将其输出发送到仅带有一个**神经元**的**输出层**。放大**隐藏层**以查看其计算。这个**隐藏层**输入四个数字，这四个数字是三个**神经元**中的的输入。这三个**神经元**中的每一个都只是实现一个小的**逻辑回归单元**。以第一个**神经元**为例。它有两个参数，{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}。为了表示这是第一个**隐藏单元**，把它下标为{% mathjax %}w_1{% endmathjax %},{% mathjax %}b_1{% endmathjax %}。它的作用是输出一些激活值{% mathjax %}a{% endmathjax %}，计算激活值{% mathjax %}a_1 = g(x) = \vec{w_1}\cdot\vec{x} + b_1 = 0.3{% endmathjax %}，{% mathjax %}z = \vec{w_1}\cdot\vec{x} + b_1 \; ,\; g(z) = \frac{1}{1+e^{-(z)}}{% endmathjax %} 。这就是第一个**神经元**的激活值{% mathjax %}a{% endmathjax %}。为了表示这是第一个**神经元**，给{% mathjax %}a{% endmathjax %}添加下标为{% mathjax %}a_1{% endmathjax %}。现在让我们看看第二个**神经元**。第二个**神经元**有参数{% mathjax %}w_2{% endmathjax %}和{% mathjax %}b_2{% endmathjax %}，{% mathjax %}w_2、b_2{% endmathjax %}是第二个逻辑单元的参数。计算激活值{% mathjax %}a_2 = g(x) = \vec{w_2}\cdot\vec{x} + b_2 = 0.7{% endmathjax %}。因为在这个例子中，我们认为潜在买家知道这件`T`恤的概率为`0.7`。第三个**神经元**有第三组参数{% mathjax %}w_3{% endmathjax %}和{% mathjax %}b_3{% endmathjax %}。计算激活值{% mathjax %}a_3 = g(x) =\vec{w_3}\cdot\vec{x} + b_3 = 0.2{% endmathjax %}。在这个例子中，这三个神经元输出`0.3、0.7`和`0.2`，这三个数字的向量变成激活值向量{% mathjax %}\vec{a}{% endmathjax %}，然后传递到这个**神经网络**的最终**输出层**。当构建具有多层的**神经网络**时，为每层赋予不同的数字会很有用。有些神经网络可以有几十层甚至几百层。但为了引入符号来帮助我们区分不同的层，我将使用上标方括号`1`来索引不同的层。具体来说，我将使用方括号 `1`中的上标，这是表示该**神经网络隐藏层**第`1`层的输出的符号，同样，这里的{% mathjax %}\hat{w}_1^{[1]},b_1^{[1]}{% endmathjax %}是**神经网络**第`1`层第一个单元的参数而 {% mathjax %}\hat{w}_2^{[1]},b_2^{[1]}{% endmathjax %}是第`1`层的第二个隐藏单元或第二个隐藏神经元的参数。同样，{% mathjax %}a_2^{[1]}{% endmathjax %}，表示这些是该神经网络第`1`层**隐藏单元**的**激活值**。如果你看到上标方括号`2`，它指的是与神经网络第`2`层相关的量，其他层也是如此。神经网络第`1`层的输出是这个激活向量{% mathjax %}a_1^{[1]}{% endmathjax %}，把它复制到这里，因为这个输出{% mathjax %}a_1^{[1]}{% endmathjax %}成为第`2`层的输入。这个神经网络第`2`层的计算是输出层。第`2`层的输入是第`1`层的输出，所以{% mathjax %}a^{[1]}{% endmathjax %}是这个向量{% mathjax %}\begin{bmatrix}0.3 \\0.7 \\0.2 \end{bmatrix}{% endmathjax %}。
{% asset_img ml_6.png %}

因为**输出层**只有一个**神经元**，所以它的工作就是计算{% mathjax %}a^{[1]}{% endmathjax %}，这是第一个也是唯一一个神经元的输出，作为{% mathjax %}g{% endmathjax %}，将`sigmoid`函数应用于{% mathjax %}\hat{w}_1{% endmathjax %}与{% mathjax %}\hat{a}^{[1]}{% endmathjax %}相乘，然后加上{% mathjax %}b_1{% endmathjax %}({% mathjax %}a_1 = g(\hat{w}_1\cdot \hat{a}^{[1]} + b_1) = 0.84{% endmathjax %})。和之前一样{% mathjax %}z = \hat{w}_1\cdot \hat{a}^{[1]} + b_1;,\; g(z) = \frac{1}{1+e^{-(z)}}{% endmathjax %}。在这个例子中，因为**输出层**只有一个**神经元**，所以这个输出只是一个标量，是一个数字，而不是一个数字向量。按照我们之前的符号惯例，使用方括号`2`中的上标来表示与该神经网络第`2`层相关的数量，{% mathjax %}a^{[2]}{% endmathjax %}是该层的输出。为了使符号一致，神经网络第`2`层相关的参数和激活值也可以定义为{% mathjax %}\hat{w}^{[2]},b^{[2]},a^{[2]}{% endmathjax %}。神经网络计算出{% mathjax %}a^{[2]}{% endmathjax %}。
{% asset_img ml_7.png %}

接下来是最后一个步骤，可以选择执行或不执行，即二进制预测，`1`或`0`，这是畅销书吗？是或否？激活值{% mathjax %}a^{[2]}_1 = 0.84{% endmathjax %}，并将其阈值设为`0.5`。如果{% mathjax %}a^{[2]}_1 > 0.5{% endmathjax %}，可以预测{% mathjax %}\hat{y} = 1{% endmathjax %}，如果{% mathjax %}a^{[2]}_1 < 0.5{% endmathjax %}，则预测{% mathjax %}\hat{y} = 0{% endmathjax %}。这就是**神经网络**的工作原理。
{% asset_img ml_8.png %}

每一层都输入一个数字向量，并对其应用一堆**逻辑回归单元**，然后计算另一个数字向量，然后将其从一层传递到另一层，直到您获得最终**输出层**的计算，即**神经网络的预测**。然后，可以将阈值设为`0.5`，也可以不设为`0.5`，以得出最终预测。

在复杂的**神经网络**示例中。该网络有四层，不包括**输入层**（也称为第`0`层），其中第`1、2`和`3`层是**隐藏层**，第`4`层是输出层，第`0`层通常为输入层。按照惯例，当神经网络有四层时，包括**输出层**中的所有**隐藏层**，但不计算**输入层**。第三层也是最后一层**隐藏层**，接下来看看该层的计算。第`3`层输入一个向量{% mathjax %}\vec{a}^{[2]}{% endmathjax %}，第`3`层从{% mathjax %}\vec{a}^{[2]}{% endmathjax %}到{% mathjax %}\vec{a}^{[3]}{% endmathjax %}做了哪些计算？如果它有三个**神经元**，那么它有参数{% mathjax %}\vec{w}_1,b_1,\vec{w}_2,b_2{% endmathjax %}和{% mathjax %}\vec{w}_3,b_3{% endmathjax %}，计算{% mathjax %}a_1^{[3]} = g(\vec{w}_1^{[3]}\cdot\vec{a}^{[2]} + b_1^{[3]}){% endmathjax %}。计算{% mathjax %}a_2^{[3]} = g(\vec{w}_2^{[3]}\cdot\vec{a}^{[2]} + b_2^{[3]}){% endmathjax %}，依此类推得到{% mathjax %}a_3^{[3]} = g(\vec{w}_3^{[3]}\cdot\vec{a}^{[2]} + b_3^{[3]}){% endmathjax %}。然后，该层的输出是一个向量{% mathjax %}\begin{bmatrix}a_1 \\a_2 \\a_3 \end{bmatrix}{% endmathjax %}。请注意，{% mathjax %}\vec{w}_1^{[3]}{% endmathjax %}表示与第`3`层相关的参数。第`3`层第二个神经元的激活用{% mathjax %}a_2^{[3]} = g(\vec{w}_2^{[3]}\cdot\vec{a}^{[2]} + b_2^{[3]}){% endmathjax %}表示。总结一下，{% mathjax %}a_j^{[l]} = g(\vec{w}_j^{[l]}\cdot\vec{a}^{[l-1]} + b_j^{[l]}){% endmathjax %}。现在，您知道如何根据上一层的激活来计算任何一层的激活。
{% asset_img ml_9.png %}

#### 推理（前向传播）

使用手写数字识别作为一个激励示例。只区分手写数字`0`和`1`。所以这只是一个**二元分类**问题，我们将输入一个图像并进行分类，使用一个{% mathjax %}8\times 8{% endmathjax %}的图像。`255`表示亮白色像素，`0`表示黑色像素。不同的数字是黑色和白色之间的不同灰度。给定`64`个输入特征，使用具有两个**隐藏层**的**神经网络**。第一层隐藏层有`25`个神经元。第二层隐藏层有`15`个神经元。最后是**输出层**，`1`或`0`的概率是多少？神经网络需要进行的一系列计算，从输入{% mathjax %}\vec{x}{% endmathjax %}（{% mathjax %}8\times 8{% endmathjax %}）到预测概率{% mathjax %}a^{[3]}{% endmathjax %}。第一个计算是从{% mathjax %}\vec{x}{% endmathjax %}到{% mathjax %}\vec{a}^{[1]}{% endmathjax %}，这就是第一层**隐藏层**计算。请注意，{% mathjax %}\vec{a}^{[1]}{% endmathjax %}有·个数字，因为这个隐藏层有`25`个单元。这就是为什么参数从{% mathjax %}w_1,w_2,\ldots,w_{25}{% endmathjax %}以及{% mathjax %}b_1,b_2,\ldots,b_25{% endmathjax %}。第`0`层的激活，即{% mathjax %}\vec{a}^{[0]} = \vec{x}{% endmathjax %}。所以只计算{% mathjax %}\vec{a}^{[1]}{% endmathjax %}。下一步是计算{% mathjax %}\vec{a}^{[2]}{% endmathjax %}。看看第二个隐藏层，它会执行此计算，其中{% mathjax %}\vec{a}^{[2]}{% endmathjax %}是{% mathjax %}\vec{a}^{[1]}{% endmathjax %}的函数，{% mathjax %}\vec{a}^{[1]} = g(\vec{w}^{[1]}\cdot\vec{a}^{[0]} + b^{[1]}){% endmathjax %}。第二层有`15`个神经元，这就是为什么这里的参数从{% mathjax %}w_1,w_2,\ldots,w_{15}{% endmathjax %}和{% mathjax %}b_1,b_2,\ldots,b_15{% endmathjax %}。现在计算{% mathjax %}\vec{a}^{[2]} = g(w^{[2]}\cdot\vec{a}^{[1]} + b^{[2]}){% endmathjax %}。最后一步是计算{% mathjax %}\vec{a}^{[3]} = g(w^{[3]}\cdot\vec{a}^{[2]} + b^{[3]}){% endmathjax %}，最后这个第三层，**输出层**只有一个单位，这就是为什么这里只有一个输出。所以{% mathjax %}\vec{a}^{[3]}{% endmathjax %}只是一个标量。最后，将其阈值设为`0.5`以得出二进制分类标签{% mathjax %}\hat{y}{% endmathjax %}。计算序列首先取 {% mathjax %}\vec{x}{% endmathjax %}，然后计算{% mathjax %}\vec{a}^{[1]}{% endmathjax %}，然后计算{% mathjax %}\vec{a}^{[2]}{% endmathjax %}，最后计算{% mathjax %}\vec{a}^{[3]}{% endmathjax %}，这也是**神经网络**的输出。您也可以将其写为{% mathjax %}f(x) = \vec{a}^{[3]}{% endmathjax %}。还记得**线性回归**和**逻辑回归**，使用{% mathjax %}f(x){% endmathjax %}表示线性回归或逻辑回归的输出。也可以使用{% mathjax %}f(x){% endmathjax %}表示**神经网络**计算的函数。因为这个计算是从左到右进行的，所以从{% mathjax %}\vec{x}{% endmathjax %}开始计算{% mathjax %}\vec{a}^{[1]}{% endmathjax %}，然后是{% mathjax %}\vec{a}^{[2]}{% endmathjax %}，最后是{% mathjax %}\vec{a}^{[3]}{% endmathjax %}。这​​个算法也被称为**前向传播**，因为正在传播**神经元**的激活。所以是从左到右向前进行计算。
{% asset_img ml_10.png %}

#### TensorFlow实现

##### 代码推理

`TensorFlow`是实现**深度学习算法**的领先框架之一。当构建项目时，`TensorFlow`实际上是我最常用的工具。另一个流行的工具是`PyTorch`。让我们看看如何使用`TensorFlow`实现推理代码。学习算法能否帮助优化烘焙过程中获得的咖啡豆的质量？烘焙咖啡时，您需要控制两个参数：加热生咖啡豆的温度和时间。我们创建了不同温度和不同持续时间的数据集，以及咖啡是否味道好的标签。这里{% mathjax %}\hat{y}=1{% endmathjax %}代表好咖啡，{% mathjax %}\hat{y}=0{% endmathjax %}代表坏咖啡。如果您在太低的温度下烘焙，它就不会被烘焙，最终会煮得不够熟。如果你煮的时间不够长，时间太短，它也不是一组烘焙得很好的咖啡豆。如果你把它煮得太久或温度太高，那么最终会得到过熟的咖啡豆。也不是好咖啡。只有这个小三角形内的点才代表好咖啡。任务中给定一个特征向量{% mathjax %}\vec{x}{% endmathjax %}，其中包含温度和持续时间，比如`200`摄氏度，持续`17`分钟，如何在神经网络中进行推理，这种温度和持续时间设置是否会制作出好咖啡？我们将{% mathjax %}x{% endmathjax %}设置为两个数的数组。输入特征为`200`摄氏度和`17`分钟。然后，将第`1`层创建为第一个**隐藏层**，**神经网络**在该层中有`3`个**隐藏单元**，使用激活函数，即`S`型函数。**密集**是神经网络层的另一个名称。这里称为**密集层**，接下来，通过取第`1`层（实际上是一个函数）并将此函数第`1`层应用于{% mathjax %}\vec{x}{% endmathjax %}的值来计算{% mathjax %}\vec{a}^{[1]}{% endmathjax %}。它将是三个数的列表，因为第`1`层有三个单元。因此，为了便于说明，这里的{% mathjax %}\vec{a}^{[1]}{% endmathjax %}可能为{% mathjax %}\begin{bmatrix}0.2 \\0.7 \\0.3 \end{bmatrix}{% endmathjax %}。接下来，对于第二个隐藏层，第`2`层将是`密集层`。现在它有一个单元，再次使用`S`型激活函数，然后您可以通过将第`2`层函数应用于从第`1`层到{% mathjax %}\vec{a}^{[1]}{% endmathjax %}的激活值来计算{% mathjax %}\vec{a}^{[2]}{% endmathjax %}。这将为您提供 {% mathjax %}\vec{a}^{[2]}{% endmathjax %}的值，为了便于说明，该值可能是`0.8`。最后，如果您希望将其阈值设为`0.5`，那么只需测试是否{% mathjax %}\vec{a}_1^{[2]} > 0.5{% endmathjax %}，并将{% mathjax %}\hat{y}{% endmathjax %}设置为`1`或`0`（正或负交叉）。这就是使用`TensorFlow`在神经网络中进行推理的方式。
{% asset_img ml_11.png %}

```python
x = np.array([[200.0,17.0]])
layer_1 = Dense(units = 3, activation = 'sigmoid')
a1 = layer_1(x) # [0.2,0.7,0.3]

layer_2 = Dense(units = 1, activation = 'sigmoid')
a2 = layer_2(a1) # 0.8

if a2 >= 0.5 :
    y = 1
else :
    y = 0
```

##### TensorFlow中的数据

让我们先看看`TensorFlow`如何表示数据。让我们从矩阵的示例开始。这是一个有{% mathjax %}2\times 3{% endmathjax %}的矩阵。在存储这个**矩阵**的代码中，您只需编写`x = np.array`，如下图所示。您会注意到`[1、2、3]`是这个矩阵的第一行，`[4、5、6]`是这个矩阵的第二行。然后这个左方括号将第一行和第二行组合在一起。这将{% mathjax %}x{% endmathjax %}设置为数值类型的数组。此矩阵只是一个二维数字数组。这里我写出了另一个矩阵。它是一个{% mathjax %}4\times 2{% endmathjax %}矩阵。要在代码中编写`x = np.array`，然后使用此处的语法将矩阵的这四行存储在变量{% mathjax %}x{% endmathjax %}中。因此，将创建一个包含这八个数字的二维数组。矩阵可以有不同的维度。您看到了{% mathjax %}2\times 3{% endmathjax %}矩阵和{% mathjax %}4\times 2{% endmathjax %}矩阵的示例。矩阵也可以是其他维度，例如{% mathjax %}1\times 2{% endmathjax %}或{% mathjax %}2\times 1{% endmathjax %}。我们之前将{% mathjax %}\vec{x}{% endmathjax %}设置为输入特征向量时，`x = np.array()`，即`[[200, 17]]`。这样做会创建一个{% mathjax %}1\times 2{% endmathjax %}矩阵，该矩阵只有一行和两列。如果你将`x = np.array()`，创建一个{% mathjax %}2\times 1{% endmathjax %}矩阵，该矩阵有两行和一列。即[[200],[17]]。上面这个例子也称为**行向量**，是只有一行的向量。下面这个例子也称为**列向量**，因为这个向量只有一列。使用像这样的双方括号和像这样的单方括号之间的区别在于，其中一个维度恰好是`1`。此示例生成一个一维向量。因此，这只是一个没有行或列的一维数组。从技术上讲，这不是{% mathjax %}1\times 2{% endmathjax %}或{% mathjax %}2\times 1{% endmathjax %}，只是一个没有行或没有列的线性数组，当我们处理**线性回归**和**逻辑回归**时，我们使用这些一维向量来表示**输入特征**{% mathjax %}\vec{x}{% endmathjax %}。`TensorFlow`，使用矩阵来表示数据。`TensorFlow`的设计初衷是处理非常大的数据集，通过用矩阵而不是一维数组来表示数据，`TensorFlow`的内部计算效率会更高一些。在这个数据集中，特征是`17`分钟内温度达到`200°C`。实际上是一个{% mathjax %}1\times 2{% endmathjax %}矩阵，恰好有一行和两列来存储数字{% mathjax %}\begin{bmatrix}200.0 \\17.0 \end{bmatrix}{% endmathjax %}。回到用于在神经网络中执行传播的代码。当您计算{% mathjax %}\vec{a}^{[1]}{% endmathjax %}等于第1 层应用于 x 时，{% mathjax %}\vec{a}^{[1]}{% endmathjax %}实际上是一个{% mathjax %}1\times 3{% endmathjax %}矩阵。即{% mathjax %}\begin{bmatrix}0.2 \\0.7 \\0.3 \end{bmatrix}{% endmathjax %}，这是一个{% mathjax %}1\times 3{% endmathjax %}矩阵。`TensorFlow`表示这是一个浮点数的方式，意味着它是一个可以使用计算机中`32`位内存表示的小数点的数字。

那么什么是**张量**呢？这里的**张量**是`TensorFlow`团队创建的一种数据类型，用于高效地存储和执行矩阵计算。从技术上讲，**张量**比**矩阵**更通用一些。`NumPy`和`TensorFlow`表示矩阵的方式已经融入这些系统中。如果您想将{% mathjax %}\vec{a}^{[1]}{% endmathjax %}（张量）转换为`NumPy`数组，您可以使用此函数`a1.numpy`来实现。它将获取相同的数据并以`NumPy`数组的形式返回，而不是`TensorFlow`数组或`TensorFlow`矩阵的形式。现在让我们看看第二层的激活输出是什么样子的。这是之前的代码，第`2`层是一个**密集层**，有一个单元和`S`形激活函数，`a2`是通过将第`2`层应用到{% mathjax %}\vec{a}^{[1]}{% endmathjax %}来计算的，那么{% mathjax %}\vec{a}^{[2]} = [[0.8]]{% endmathjax %}，从技术上讲，这是一个{% mathjax %}1\times 1{% endmathjax %}矩阵，是一个有一行和一列的二维数组，所以它等于这个数字`0.8`。如果打印出{% mathjax %}\vec{a}^{[2]}{% endmathjax %}，会看到它是一个`TensorFlow`**张量**，只有一个元素`0.8`。它是一个`float32`小数点数字，占用计算机内存中的`32`位。你可以使用`a2.numpy()`将`TensorFlow`**张量**转换为`NumPy`**矩阵**。

##### 构建神经网络

如果要进行**前向传播**，需要初始化数据{% mathjax %}x{% endmathjax %}，创建第一层并计算第一层，然后创建第二层并计算二层。这是一种显式的执行**前向传播**的方法，每次只计算一层。**张量流**有一种不同的实现**前向传播**的方法。在`TensorFlow`中构建**神经网络**的另一种方法，这与之前创建第一层和创建第二层的方法相同。但是不用手动获取数据并将其传递到第一层，然后从第一层获取激活并将其传递到第二层。我们可以告诉**张量流**，我们希望它获取第一层和第二层并将它们串联在一起以形成一个**神经网络**。这就是`TensorFlow`中的顺序函数所做的。使用顺序框架，**张量流**可以做很多工作。假设有一个像左边这样的训练集。这是咖啡示例。然后，可以将训练数据作为输入{% mathjax %}x{% endmathjax %}并将其放入`numpy`数组中。这是一个{% mathjax %}4\times 2{% endmathjax %}的矩阵和目标标签。然后可以按如下方式写出{% mathjax %}\hat{y}{% endmathjax %}。这只是长度为四的一维数组{% mathjax %}\hat{y}{% endmathjax %}，然后可以将这组目标存储为一维数组，例如`[1,0,0,1]`，对应于四个训练示例。给定存储在矩阵{% mathjax %}X{% endmathjax %}和数组{% mathjax %}y{% endmathjax %}。如果想训练这个**神经网络**，需要做的就是调用一些函数，使用一些参数调用`model.compile()`。然后调用`model.fit(x,y)`，告诉`TensorFlow`使用由第一层和第二层顺序串联在一起创建的**神经网络**，并在数据{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}上进行训练。如果你有一个新的示例，比如说{% mathjax %}x_{\text{new}}{% endmathjax %}，它是具有这两个特征的`NumPy`数组，那么如何进行**前向传播**呢？不必一层一层地进行**前向传播**，只需在{% mathjax %}x_{\text{new}}{% endmathjax %}上调用模型预测`model.predict()`。模型预测使用顺序函数编译的神经网络进行**前向传播**并进行**推理**。现在将这三行代码放在上面，进一步简化一下，也就是在`TensorFlow`中编码时。不会明确将两层分配给两个变量，第一层和第二层如下。模型是串联在一起的几个层的顺序模型时，通常会编写这样的代码。顺序地，第一层是具有三个单元和`S`型**激活函数**的**密集层**，第二层是具有一个单元的**密集层**，同样是一个`S`型激活函数。而不是对这些第一层和第二层变量进行显式分配。这几乎是在`TensorFlow`中**训练**和**推理神经网络**所需的代码。这就是在`TensorFlow`中构建**神经网络**的方式。
```python
layer_1 = Dense(units = 3, activation = 'sigmoid')
layer_2 = Dense(units = 1, activation = 'sigmoid')
model = Squential([layer_1,layer_2])

x = np.array([[200.0,17.0],[120.0,5.0],[425.0,20.0],[212.0,18.0]]) # 4 x 2 array
y = np.array([1,0,0,1])

model.compile(...)
model.fit(x,y)
model.predict(x_new)
```
