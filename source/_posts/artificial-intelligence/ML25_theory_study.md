---
title: 机器学习(ML)(二十五) — 强化学习探析
date: 2025-01-17 17:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 强化学习—结构

**强化学习**的结构大致分为`7`种**设计模式**：**抽象模式**(`Abstraction Pattern`)、**增强模式**(`Augmentation Pattern`)、**辅助优化模式**(`Auxiliary Optimization Pattern`)、**辅助模型模式**(`Auxiliary Model Pattern`)、**仓储模式**(`Portfolio Pattern`)、**环境生成模式**(`Environment Generation Pattern`)、**显式设计模式**(`Explicitly Designed`)。
<!-- more -->

##### 抽象模式

**抽象模式**利用结构信息在**强化学习**(`RL`)管道中创建**抽象实体**。对于任何实体{% mathjax %}\mathcal{X}{% endmathjax %}，**抽象模式**利用结构信息创建{% mathjax %}\mathcal{X}_{\text{abs}}{% endmathjax %}，其在学习过程中取代{% mathjax %}\mathcal{X}{% endmathjax %}的角色。在出租车示例中，**状态空间**可以抽象为出租车当前的网格单元、当前乘客的目的地网格单元，以及出租车是否正在载客。这显著简化了**状态空间**，相较于表示城市网格的完整细节。**动作空间**也可以抽象为向四个主要方向移动，以及接载和放下乘客。**行为抽象**与基于历史的抽象密切相关，因为它们解决类似的应用，即将**抽象状态**和历史转化为可以用于**强化学习**的低维表示。
{% asset_img ml_1.png "抽象模式" %}

找到合适的抽象本身可能是一项具有挑战性的任务。过多的抽象可能导致关键信息的丢失，而过少的抽象则可能无法显著降低**复杂性**。因此，联合学习抽象的学习方法将这种粒度纳入学习过程。早期工作涉及状态抽象的理论。最近的研究主要利用抽象来解决泛化问题。**泛化**是抽象最常见的应用案例。然而，前面提到的抽象优势通常与样本效率提升和安全性交织在一起。

**泛化**：**状态抽象**是提高**泛化**性能的标准选择，通过使用诸如**不变因果预测**、**相似性度量**、**自由能最小化**(`Free Energy Minimization`)和**解耦**(`disentanglement`)等方法，将共享动态捕捉到**抽象状态空间**中。**值函数**作为多任务环境中共享动态的时间抽象。**后继特征**(`Successor Features, SF`)利用**值函数**作为抽象，利用潜在**奖励**和**动态分解**。后续研究将其与广义策略迭代(`Generalized Policy Iteration`)和**通用值函数逼近器**(`Universal Value Function Approximators`)结合使用。另一方面，**值函数**的**因式分解**有助于提高样本效率和泛化能力。**关系抽象**通过将**符号空间**纳入**强化学习管道**来促进泛化。这些抽象有助于在分层框架中结合规划方法。此外，**关系抽象**可以帮助抽象出一组`MDP`的一般特征，从而使方法能够在**抽象状态**和**动作**上学习可泛化的`Q`值，这些`Q`值可以转移到新任务中。此外，**抽象**还可以通过**压缩状态空间**、**抽象自动机**、**跨任务动态的模板**，甚至与选项结合以保留**最优值**来实现分层设置中的**泛化**。

**样本效率**：**潜在变量模型**可以提高**强化学习**(`RL`)管道中的样本效率。潜在状态抽象可以提高基于模型的`RL`中的样本效率，并有助于提高策略学习的可操作性。在无模型任务中，潜在变量模型可以学习视觉特征或潜在空间控制的逆模型。**潜在过渡模型**通过捕获噪声环境中的相关信息、通过保留原始状态之间的双模拟距离或通过利用分解抽象来提升效率。学习的潜在抽象也有助于提升探索。**潜在动作模型**通过缩短随机场景（如对话生成）中的学习范围来加速**策略梯度**方法的收敛。另一方面，**动作嵌入**有助于降低**动作空间**的维度。**深度潜在变量模型**在基于模型的`RL`中取得了显著的成功经验，因为它们在建模方面具有表现力。它们可以促进学习、计划和探索，以提高`RL`的样本效率。**潜在变量模型**的**状态-动作值函数**的表示视图允许可处理的**变分学习算法**在面对不确定性时有效实施乐观/悲观原则进行探索。通过结合**潜在变量模型**的内核嵌入，可以实现具有`UCB`探索的规划算法。

**安全性与可解释性**：**关系抽象**是**可解释性**的绝佳选择，因为它们可以捕获交互复杂的分解。**对象中心表示**和**学习的抽象**的结合增加了透明度，而**符号插入**，例如跟踪对象之间的关系距离，有助于提高性能。**状态和奖励抽象**有助于安全性。潜在状态通过嵌入混杂因素来帮助学习安全的**因果推理模型**。另一方面，**网格**有助于对基准指标进行**基准测试**，例如学习策略的稳健性。

##### 增强模式

**增强模式**(`Augmentation Pattern`)将{% mathjax %}X{% endmathjax %}和{% mathjax %}z{% endmathjax %}视为动作选择机制的单独输入实体。组合范围可以从结构信息到状态或动作的简单连接，到在附加信息上调节策略或**价值函数**的更复杂方法。至关重要的是，结构信息既不直接影响优化过程，也不改变{% mathjax %}X{% endmathjax %}的性质，而只是增强了已经存在的实体。从这个角度来看，以辅助方式学习并连接到状态、动作或模型的抽象也可以被认为是**增强**，因为原始实体保持不变。对于出租车示例，应用**增强模式**的一种方法是根据附加信息（例如一天中的时间或一周中的一天）来调节策略。此信息可能很有用，因为交通状况和乘客需求会因这些因素而异。但是，增强会增加策略和学习过程的复杂性，需要注意确保策略不会**过度拟合**附加信息。因此，与抽象相比，这种模式的探索通常较少。
{% asset_img ml_2.png "增强模式" %}

**基于上下文的增强**：动态和目标相关信息的上下文表示通过向**智能体**公开最佳性所需的信息来帮助**泛化**和**样本效率**。**目标增强**还允许用于指定目标的可解释机制。将**元学习**的潜在空间增强到正常状态可以促进跨任务的**时间相干探索**。动作历史可以直接帮助提高样本效率，而动作关系有助于推广到大型动作集。

**语言增强**：语言明确地捕获世界中的**关系元数据**。潜在语言解释模型利用语言的组合性来实现更好的探索和泛化到不同的关系设置，如其语言描述所代表的。目标描述通过利用不同子任务之间的**语义关系**并为较低级别的策略产生更好的目标来帮助分层设置。增强还通过`Verma`等人提出的方法，通过指导搜索以人类可读格式编写的**近似策略**，帮助使现有方法更易于解释。

**控制增强**：增强有助于原始控制，例如分层设置中的多级控制。增强以原始技能为条件的内部潜在变量有助于解决分层设置中的样本效率问题。增强还通过将肢体建模为必须学习组合成形态以解决任务的单个**智能体**来帮助形态控制。

##### 辅助优化模式

**辅助优化模式**利用结构性辅助信息来优化过程，包括**对比损失**、**奖励塑造**、**并发优化**、**掩蔽策略**、**正则化**和**基线**等方法。由于优化中的更改通常与其他组件的更改相吻合，因此这种模式与其他许多模式共享方法；例如，**对比损失**可用于学习状态抽象。同样，学习的模型也可以用于**奖励塑造**。因此，属于此类别的方法同时利用了多种模式。在出租车的例子中，**奖励塑造**可以通过鼓励出租车在没有乘客时留在乘客频繁出没的区域附近，从而帮助策略在城市网格的轻微扰动中重复使用。确保修改后的优化与原始目标保持一致，这需要**正则化**来控制优化如何尊重原始目标。对于**奖励塑造**，这意味着**最优策略**在塑造奖励下保持不变。对于辅助目标，这表现为某种形式的**熵**或**散度正则化**。通过递归确保这一点，而**基线**控制更新**方差**。因此，**辅助优化模式**非常倾向于解决安全问题。
{% asset_img ml_3.png "辅助优化模式" %}

**奖励塑造**：**奖励塑造**是一种常见的技术，可将更多信息纳入优化过程中，通过利用任务描述中的**模块化**和**关系分解**，从而提高样本效率。**奖励**的历史记录有助于学习状态之间的**对称关系**，从而改进用于优化的批处理中状态的选择过程。将**状态**和**奖励分解**为内生和外生因素有助于通过奖励校正来提高安全性和样本效率。外在奖励还可以指导探索过程。具有关系表示的**符号规划器**通过分层设置中的外在奖励与原始学习策略交互，从而增加可解释性，同时通过外在奖励直接影响探索。额外的奖励来源有助于确定反事实轨迹的质量，从而有助于解释为什么**智能体**采取了某些类型的操作。此外，**奖励**的运行平均值有助于自适应地调整**异构动作空间**的探索参数。内在奖励有助于探索**稀疏奖励环境**。潜在分解通过直接影响探索来帮助改进这些方法。**语言抽象**用作单独用于探索的潜在分解。或者，几何结构可以比较状态嵌入并提供**情景奖励**。**奖励塑造**是一种将**领域知识**融入**强化学习**(`RL`)中的有效技术。探索引导**奖励塑造**(`ExploRS`)以完全自我监督的方式运行，甚至可以在**稀疏奖励**环境中加速**智能体**的学习. 一种新颖的方法将从历史经验中获得的成功率纳入**塑造奖励**中，以实现自适应和高效的**奖励塑造**。`ExploRS`通过结合基于探索的奖励来学习内在**奖励函数**，以最大限度地提高**智能体**相对于外在奖励的效用. 它可以解决现有**奖励塑造方法**的问题，这些方法要么需要外部领域知识，要么在奖励极其稀疏的环境中失败。`ExploRS`在奖励学习和策略优化之间交替进行，并在具有**稀疏/嘈杂奖励信号**的多个环境中展示了有效性。

**辅助学习目标**：通过学习不变子空间并使用这些子空间来创建辅助目标或基于熵的**策略正则化项**，从而在具有不同形态的**智能体**之间传递技能。在分层设置中发现适当的子任务是一个非常低效的过程，可以通过组合当前策略下的子轨迹的值来解决，他们随后将其用于**行为克隆**。当用于某种形式的**策略正则化**时，潜在分解还有助于提高鲁棒性和安全性。辅助损失通常有助于泛化，并且是类人归纳偏差的绝佳切入点。受潜在分解几何形状启发的指标有助于学习多任务设置中的最优值。

**约束和基线**：约束优化在安全`RL`中很常见，并且结合结构有助于提高此类方法的样本效率，同时使它们更易于解释。将状态分解为安全状态和不安全状态有助于开发持久的安全条件或语言抽象。**递归约束**有助于使用分解状态显式地将优化限制在安全操作的潜在子集上。将选项的探索限制在非风险状态也有助于在分层设置中纳入安全性。因子分解的动作还可以通过基线提高**策略梯度**方法的样本效率，通过直接值条件提高离线方法的样本效率，以及通过矩阵估计提高基于值的规划的效率。**动作选择机制**可以利用领域知识来实现安全性和可解释性，或者利用定向探索来提高样本效率。分层设置受益于通过修改终止条件而纳入的潜在状态分解。此外，**状态-动作**等价性有助于通过分解将`Q-Learning`扩展到大型空间。

**并发优化**：使用结构分解并行优化有助于提高样本效率。分解的`MDP`是对影响呈现给用户的内容进行建模并在并行方案中帮助集成方法的绝佳方法。同样，分层设置中分解的奖励有助于将多任务问题分解为单个任务`MDP`的线性组合。或者，**离散化多维动作空间**中的连续子动作有助于将每个子动作的`MDP`扩展到较低级别`MDP`，从而使用修改`Q`值的备份。**关系分解**还有助于分解**神经网络**的**掩蔽策略**。

##### 辅助模型模式

该模式表示在模型中使用结构信息。使用术语“模型”，特指利用**环境模型**来生成经验的方法，无论是完全生成还是部分生成。这个概念涵盖了一系列方法，从使用完整的**世界模型**来生成奖励和下一个状态转换，到使用这些方法来生成完整的经验序列。在分类中，特别关注如何将结构融入到这些模型中，以帮助生成学习经验的某些部分。例如，出租车**智能体**可以根据以往的经验学习城市交通的潜在模型。该模型可用于规划避开交通并更快到达目的地的路线。或者，**智能体**可以学习一种集成技术，将多个模型组合起来，每个模型都对交通动态的特定组成部分进行建模。使用模型时，通常需要在模型复杂性和准确性之间进行权衡，因此必须谨慎管理，以避免**过度拟合**并保持稳健性。为此，结合结构有助于提高模型学习阶段的效率，同时允许重复使用以进行**泛化**。如下图所示：
{% asset_img ml_4.png "辅助模型模式" %}

**具有结构化表示的模型**：`Young`等人利用分解的**状态空间**来证明基于模型的方法在组合复杂环境中的优势。类似地，`dreamer`模型利用基于像素环境的潜在表示。面向对象的状态表示有助于避免在`MBRL`中使用`CNN`学习潜在因素，或者将潜在因素作为可以使用`NN`细化的随机变量。**图**（卷积）**网络**捕获丰富的高阶交互数据，例如人群导航或不变性。**动作等价性**有助于学习用于规划和价值迭代的潜在模型（抽象`MDP`）。

**用于特定任务分解的模型**：在模型中利用分解的另一种方法是捕获特定于任务的分解。捕获某种形式的不相关性的模型，例如**因果强化学习**中的观察和干预数据，或者任务相关与不相关数据，有助于提高**泛化能力**和**样本效率**。潜在表示帮助模型捕获控制相关信息或子任务依赖关系。

用于安全性的模型通常包含一些**成本度量**来抽象安全状态，或者包含对状态和动作进行分解的无知。模型还可以通过潜在的**因果分解**和**状态子空间**直接指导探索机制，以提高样本效率。诸如CycleGAN之类的生成方法也是使用`MDP`不同组件的潜在模型来生成反事实轨迹的绝佳方法。

##### 仓储模式

**仓储模式**(`Portfolio Pattern`)使用**结构信息**来创建数据库或实体仓库，这些实体可以组合起来以实现特定目标。这些可以是**学习策略**和**价值函数**，甚至是模型。考虑到此类方法的在线性质，它们通常针对持续和终身学习问题。此类方法中固有的模块化使它们将知识重用作为中心主题。例如，出租车可以维护一个**价值函数**或**策略数据库**，用于城市的不同部分或一天中的不同时间。当出租车在城市中导航时，这些可以重复使用，从而提高学习效率。**仓储**通常可以提高**效率**和**泛化能力**，并且通过技能和选项框架来实现。此模式中一个重要的考虑因素是**管理仓储**的**规模**和**多样性**，以避免学习过程过多地偏向过去的经验。**仓储**主要应用于**样本效率**和**泛化**。然而，它们也与**可解释性**重叠，因为存储的数据可以轻松地用于分析**智能体**的行为并理解新场景的**策略**。如下图所示：
{% asset_img ml_5.png "投资组合模式" %}

**策略仓储**：**策略子空间**利用共享的潜在参数在策略中学习一个子空间，其线性组合有助于创建新策略。通过存储额外的策略来扩展这些子空间，自然地将其扩展到持续设置中。利用**目标**和**奖励**，任务分解赋予了多任务终身设置中的策略和`Q`值存储功能。从潜在空间生成的现有任务之间的关系图提供了一种建模终身多任务学习问题的方法。另一方面，`Devin`等人(`2017`)提出的方法将`MDP`分解为特定于**智能体**和特定于**任务**的变异程度，为此训练各个模块。使用**变分编码器-解码器**模型进行的解耦通过将动态分解为**共享因素**和特定于**智能体**的因素，有助于控制形态上不同的**智能体**。此外，将**智能体**的问题划分为相互连接的**子智能体**，这些**子智能体**学习**局部控制策略**。利用技能框架的方法有效地创建了一个学习原语的投资组合，类似于`HRL`中的选项。这些随后用于最大化低层次中的互信息，共同勾勒出一个**策略**，在持续设置中寻求**多样性先验**，或用于划分**状态空间**。类似地，`Gupta`等人(`2017`)在使用辅助优化学习的潜在嵌入空间上应用了**投资组合模式**。

**分解模型**：模型中固有存在的分解导致了通常集成多个模型的方法，这些模型各自反映问题的不同方面。**集成方法**，如**递归独立机制**，捕获个别模块中的动态，这些模块稀疏交互并使用**注意机制**。集成动态还帮助实现对未见 `MDP`的少量适应。结合**关系分解**的分解模型有助于将动作绑定到以对象为中心的表示上。

##### 环境生成模式

**环境生成模式**(`Environment Generation Pattern`)使用**结构信息**来创建任务、目标或动态分布，从中可以对`MDP`进行采样。这包含了**程序生成环境**的概念，同时结合了使用**辅助模型**在环境生成过程中引入结构的方法。分解体现在环境生成的各个方面，这些方面受到生成过程的影响，例如**动态、奖励结构、状态空间**等。考虑到环境生成的在线性质，此模式中的方法以某种方式解决了学习的问题。在出租车示例中，可以生成一个任务，从简单任务（如在空网格中导航）开始，逐渐引入复杂性（如添加交通和不同目的地的乘客）。确保生成的`MDP`能够良好覆盖问题空间对于避免对特定任务子集的**过度拟合**至关重要。这需要在环境生成过程中纳入额外的**多样性约束**。结构在环境生成过程中提供了额外的**可解释性**和**可控性**，因此使**基准测试**比使用**无监督技术**的方法更容易。规则基础的语法有助于建模学习问题的组合性质。`Kumar`等人(`2021`)利用这一点影响转移动态并生成环境。这使他们能够以隐式组合的方式训练**智能体**。`Kumar`等人(`2022`)在他们的辅助优化过程中使用了这些语法。捕获任务依赖性的另一种方法是通过潜在图形模型，这些模型生成**状态空间、奖励函数和转移动态**。潜在**动态模型**允许模拟任务分布，这有助于**泛化**。**聚类方法**，如探索性任务聚类，通过**探索策略**的**元学习**来研究任务相似性。在某种意义上，它们恢复了任务空间上的分解，其中各个聚类可以进一步用于**策略适应**。
{% asset_img ml_6.png "环境生成模式" %}

##### 显式设计模式

**显式设计模式**(`Explicitly Designed`)包含所有归纳偏差体现在特定架构或设置中的方法，这些架构或设置反映了它们旨在利用的问题的**可分解性**。这包括专门设计的神经架构，并扩展到其他方法，例如**顺序架构**，以捕获层次结构、关系等。结构信息的使用仅限于架构的**特异性**，而不限于管道的任何其他部分。在出租车的例子中，可以设计一种神经架构来将城市网格处理为图像并输出策略。诸如**卷积层**之类的技术可用于捕获城市网格的**空间结构**。不同的网络部分可以专门用于不同的子任务，例如识别乘客位置和规划路线。然而，这种模式涉及大量的手动调整和实验，并且要确保这些设计在不同的任务中能够很好地推广。设计特定的神经架构可以提供更好的**可解释性**，从而能够分解不同的组件并独立模拟它们。如下图所示：
{% asset_img ml_7.png "显式设计模式" %}

以下是一些研究和模型，它们结合了可分解性的概念，以提高**神经网络**和**强化学习**的**效率**和**准确性**：
- **结构化控制网络**(`SCN`)：`SCN`将通用**多层感知器**(`MLP`)分解为**非线性控制流**和**线性控制流**，并通过添加方式将它们组合起来，以提高**样本效率**和**泛化能力**。实验表明，与标准`MLP`基线相比，这种架构结合了**线性**和**非线性策略**的优点，同时使用更小的网络。**结构化控制网络**(`SCN`)已在`OpenAI MuJoCo、OpenAI Roboschool`和`Atari Games`等基准测试中进行了实验。
- **双线性值网络**：是一种用于**多目标强化学习**的**双线性值网络**，通过建筑方式将**动态分解**为**状态**和**目标条件分量**，以生成目标条件`Q`函数。
- **动作分支架构**：**动作分支架构**是一种新颖的**神经网络架构**，具有**共享网络模块**，后跟多个网络分支，每个动作维度一个。这种方法通过允许每个单独的**动作维度**具有一定程度的独立性，实现了网络输出数量随自由度数量的线性增加。动作分支**智能体**存储库提供了一组**深度强化学习智能体**，这些**智能体**基于将**动作分支架构**合并到现有**强化学习算法**中。分支架构`Q`网络(`BDQ`)是一种新颖的**智能体**，它基于将提议的**动作分支架构**合并到深度`Q`网络(`DQN`)算法中，以及采用其扩展的选择，双重`Q`学习，**分支网络架构**和**优先经验重放**。`HIQL`离线目标条件**强化学习**的分层隐式`Q-Learning`(`HIQL`)从**单个目标条件值函数**中提取所有必需的组件——**表示函数**、**高级策略**和**低级策略**。这些方法展示了将可分解性纳入**神经网络架构**如何为各种任务带来**更高效**、**可扩展**和**可解释**的模型。
- **分层强化学习**：**分层强化学习**提供了一种**分解复杂强化学习问题**的方法，将复杂性问题分解为更简单的子任务来执行。高层策略通常用于选择子任务或子目标。

**捕获架构中的不变性**：专门的架构有助于捕获问题中的**不变性**。**符号网络**通过首先将**关系马尔可夫决策过程**(`Relational MDPs`)转换为图形，然后使用**神经网络**捕获**节点嵌入**，从而训练一组**共享参数**。齐次网络在专门的**多层感知器**(`MLP`)和**卷积神经网络**(`CNN`)架构中捕获**对称性**。另一种捕获**对称性**的方法是通过**基函数**。注意机制明确帮助捕获实体分解场景。**关系网络**和**图网络**明确捕获额外的**关系归纳偏差**。**线性关系网络**提供了一种与**对象数量线性扩展的架构**。**图网络**也被用来显式建模**智能体**在具身控制中的形态。

**专门模块**：这些是结合了两者优点的方法，通过在**专门模块**中捕获**不变性**。这些模块捕获语义意义中的**关系结构**、**辅助模型的关系编码器**，或用于纳入领域知识的**专门架构**。

#### 总结

理解**深度强化学习**的复杂性和细微差别是具有挑战性的，这种挑战因不同问题领域采用的多样化方法而加剧。这种碎片化阻碍了统一原则和一致实践在**强化学习**中的发展。为了解决这一关键差距，提出了一种创新框架，以理解将学习问题的固有结构有效整合到**强化学习算法**中的不同方法。首先将结构概念化为关于**学习问题**及其相应解决方案**可分解性**的附加信息。将**可分解性**分为四种不同的原型——**潜在**、**分解**、**关系**和**模块化**。这一分类描绘了一种光谱，建立了与现有文献之间的深刻联系，阐明了**结构**在**强化学习**中的多样影响。接着，在对**强化学习**领域进行深入分析后，提出了七个关键模式——**抽象**、**增强**、**辅助优化**、**辅助模型**、**仓储**、**环境生成**和**显式设计**模式。这些模式代表了将结构知识纳入**强化学习**的战略方法。尽管框架提供了一个全面的起点，但这些模式并没有穷尽。希望这能激励研究人员进一步完善和开发新模式，从而扩展**强化学习**中的**设计模式库**。总之，这里提供了一种以**模式**为中心的视角，强调**结构分解**在塑造当前和未来范式中的关键作用。