---
title: 机器学习(ML)(二十五) — 强化学习探析
date: 2025-01-17 17:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 强化学习—结构

**强化学习**的结构大致分为`7`种**设计模式**：**抽象模式**(`Abstraction Pattern`)、**增强模式**(`Augmentation Pattern`)、**辅助优化模式**(`Auxiliary Optimization Pattern`)、**辅助模型模式**(`Auxiliary Model Pattern`)、**仓储模式**(`Portfolio Pattern`)、**环境生成模式**(`Environment Generation Pattern`)、**显式设计模式**(`Explicitly Designed`)。
<!-- more -->

##### 抽象模式

**抽象模式**利用结构信息在**强化学习**(`RL`)管道中创建**抽象实体**。对于任何实体{% mathjax %}\mathcal{X}{% endmathjax %}，**抽象模式**利用结构信息创建{% mathjax %}\mathcal{X}_{\text{abs}}{% endmathjax %}，其在学习过程中取代{% mathjax %}\mathcal{X}{% endmathjax %}的角色。在出租车示例中，**状态空间**可以抽象为出租车当前的网格单元、当前乘客的目的地网格单元，以及出租车是否正在载客。这显著简化了**状态空间**，相较于表示城市网格的完整细节。**动作空间**也可以抽象为向四个主要方向移动，以及接载和放下乘客。**行为抽象**与基于历史的抽象密切相关，因为它们解决类似的应用，即将**抽象状态**和历史转化为可以用于**强化学习**的低维表示。
{% asset_img ml_1.png "抽象模式" %}

找到合适的抽象本身可能是一项具有挑战性的任务。过多的抽象可能导致关键信息的丢失，而过少的抽象则可能无法显著降低**复杂性**。因此，联合学习抽象的学习方法将这种粒度纳入学习过程。早期工作涉及状态抽象的理论。最近的研究主要利用抽象来解决泛化问题。**泛化**是抽象最常见的应用案例。然而，前面提到的抽象优势通常与样本效率提升和安全性交织在一起。

**泛化**：**状态抽象**是提高**泛化**性能的标准选择，通过使用诸如**不变因果预测**、**相似性度量**、**自由能最小化**(`Free Energy Minimization`)和**解耦**(`disentanglement`)等方法，将共享动态捕捉到**抽象状态空间**中。**值函数**作为多任务环境中共享动态的时间抽象。**后继特征**(`Successor Features, SF`)利用**值函数**作为抽象，利用潜在**奖励**和**动态分解**。后续研究将其与广义策略迭代(`Generalized Policy Iteration`)和**通用值函数逼近器**(`Universal Value Function Approximators`)结合使用。另一方面，**值函数**的**因式分解**有助于提高样本效率和泛化能力。**关系抽象**通过将**符号空间**纳入**强化学习管道**来促进泛化。这些抽象有助于在分层框架中结合规划方法。此外，**关系抽象**可以帮助抽象出一组`MDP`的一般特征，从而使方法能够在**抽象状态**和**动作**上学习可泛化的`Q`值，这些`Q`值可以转移到新任务中。此外，**抽象**还可以通过**压缩状态空间**、**抽象自动机**、**跨任务动态的模板**，甚至与选项结合以保留**最优值**来实现分层设置中的**泛化**。

**样本效率**：**潜在变量模型**可以提高**强化学习**(`RL`)管道中的样本效率。潜在状态抽象可以提高基于模型的`RL`中的样本效率，并有助于提高策略学习的可操作性。在无模型任务中，潜在变量模型可以学习视觉特征或潜在空间控制的逆模型。**潜在过渡模型**通过捕获噪声环境中的相关信息、通过保留原始状态之间的双模拟距离或通过利用分解抽象来提升效率。学习的潜在抽象也有助于提升探索。**潜在动作模型**通过缩短随机场景（如对话生成）中的学习范围来加速**策略梯度**方法的收敛。另一方面，**动作嵌入**有助于降低**动作空间**的维度。**深度潜在变量模型**在基于模型的`RL`中取得了显著的成功经验，因为它们在建模方面具有表现力。它们可以促进学习、计划和探索，以提高`RL`的样本效率。**潜在变量模型**的**状态-动作值函数**的表示视图允许可处理的**变分学习算法**在面对不确定性时有效实施乐观/悲观原则进行探索。通过结合**潜在变量模型**的内核嵌入，可以实现具有`UCB`探索的规划算法。

**安全性与可解释性**：**关系抽象**是**可解释性**的绝佳选择，因为它们可以捕获交互复杂的分解。**对象中心表示**和**学习的抽象**的结合增加了透明度，而**符号插入**，例如跟踪对象之间的关系距离，有助于提高性能。**状态和奖励抽象**有助于安全性。潜在状态通过嵌入混杂因素来帮助学习安全的**因果推理模型**。另一方面，**网格**有助于对基准指标进行**基准测试**，例如学习策略的稳健性。

##### 增强模式

**增强模式**(`Augmentation Pattern`)将{% mathjax %}X{% endmathjax %}和{% mathjax %}z{% endmathjax %}视为动作选择机制的单独输入实体。组合范围可以从结构信息到状态或动作的简单连接，到在附加信息上调节策略或**价值函数**的更复杂方法。至关重要的是，结构信息既不直接影响优化过程，也不改变{% mathjax %}X{% endmathjax %}的性质，而只是增强了已经存在的实体。从这个角度来看，以辅助方式学习并连接到状态、动作或模型的抽象也可以被认为是**增强**，因为原始实体保持不变。对于出租车示例，应用**增强模式**的一种方法是根据附加信息（例如一天中的时间或一周中的一天）来调节策略。此信息可能很有用，因为交通状况和乘客需求会因这些因素而异。但是，增强会增加策略和学习过程的复杂性，需要注意确保策略不会**过度拟合**附加信息。因此，与抽象相比，这种模式的探索通常较少。
{% asset_img ml_2.png "增强模式" %}

**基于上下文的增强**：动态和目标相关信息的上下文表示通过向**智能体**公开最佳性所需的信息来帮助**泛化**和**样本效率**。**目标增强**还允许用于指定目标的可解释机制。将**元学习**的潜在空间增强到正常状态可以促进跨任务的**时间相干探索**。动作历史可以直接帮助提高样本效率，而动作关系有助于推广到大型动作集。

**语言增强**：语言明确地捕获世界中的**关系元数据**。潜在语言解释模型利用语言的组合性来实现更好的探索和泛化到不同的关系设置，如其语言描述所代表的。目标描述通过利用不同子任务之间的**语义关系**并为较低级别的策略产生更好的目标来帮助分层设置。增强还通过`Verma`等人提出的方法，通过指导搜索以人类可读格式编写的**近似策略**，帮助使现有方法更易于解释。

**控制增强**：增强有助于原始控制，例如分层设置中的多级控制。增强以原始技能为条件的内部潜在变量有助于解决分层设置中的样本效率问题。增强还通过将肢体建模为必须学习组合成形态以解决任务的单个**智能体**来帮助形态控制。

##### 辅助优化模式

**辅助优化模式**利用结构性辅助信息来优化过程，包括**对比损失**、**奖励塑造**、**并发优化**、**掩蔽策略**、**正则化**和**基线**等方法。由于优化中的更改通常与其他组件的更改相吻合，因此这种模式与其他许多模式共享方法；例如，**对比损失**可用于学习状态抽象。同样，学习的模型也可以用于**奖励塑造**。因此，属于此类别的方法同时利用了多种模式。在出租车的例子中，**奖励塑造**可以通过鼓励出租车在没有乘客时留在乘客频繁出没的区域附近，从而帮助策略在城市网格的轻微扰动中重复使用。确保修改后的优化与原始目标保持一致，这需要**正则化**来控制优化如何尊重原始目标。对于**奖励塑造**，这意味着**最优策略**在塑造奖励下保持不变。对于辅助目标，这表现为某种形式的**熵**或**散度正则化**。通过递归确保这一点，而**基线**控制更新**方差**。因此，**辅助优化模式**非常倾向于解决安全问题。
{% asset_img ml_3.png "辅助优化模式" %}

**奖励塑造**：**奖励塑造**是一种常见的技术，可将更多信息纳入优化过程中，通过利用任务描述中的**模块化**和**关系分解**，从而提高样本效率。**奖励**的历史记录有助于学习状态之间的**对称关系**，从而改进用于优化的批处理中状态的选择过程。将**状态**和**奖励分解**为内生和外生因素有助于通过奖励校正来提高安全性和样本效率。外在奖励还可以指导探索过程。具有关系表示的**符号规划器**通过分层设置中的外在奖励与原始学习策略交互，从而增加可解释性，同时通过外在奖励直接影响探索。额外的奖励来源有助于确定反事实轨迹的质量，从而有助于解释为什么**智能体**采取了某些类型的操作。此外，**奖励**的运行平均值有助于自适应地调整**异构动作空间**的探索参数。内在奖励有助于探索**稀疏奖励环境**。潜在分解通过直接影响探索来帮助改进这些方法。**语言抽象**用作单独用于探索的潜在分解。或者，几何结构可以比较状态嵌入并提供**情景奖励**。**奖励塑造**是一种将**领域知识**融入**强化学习**(`RL`)中的有效技术。探索引导**奖励塑造**(`ExploRS`)以完全自我监督的方式运行，甚至可以在**稀疏奖励**环境中加速**智能体**的学习. 一种新颖的方法将从历史经验中获得的成功率纳入**塑造奖励**中，以实现自适应和高效的**奖励塑造**。`ExploRS`通过结合基于探索的奖励来学习内在**奖励函数**，以最大限度地提高**智能体**相对于外在奖励的效用. 它可以解决现有**奖励塑造方法**的问题，这些方法要么需要外部领域知识，要么在奖励极其稀疏的环境中失败。`ExploRS`在奖励学习和策略优化之间交替进行，并在具有**稀疏/嘈杂奖励信号**的多个环境中展示了有效性。

**辅助学习目标**：通过学习不变子空间并使用这些子空间来创建辅助目标或基于熵的**策略正则化项**，从而在具有不同形态的**智能体**之间传递技能。在分层设置中发现适当的子任务是一个非常低效的过程，可以通过组合当前策略下的子轨迹的值来解决，他们随后将其用于**行为克隆**。当用于某种形式的**策略正则化**时，潜在分解还有助于提高鲁棒性和安全性。辅助损失通常有助于泛化，并且是类人归纳偏差的绝佳切入点。受潜在分解几何形状启发的指标有助于学习多任务设置中的最优值。

**约束和基线**：约束优化在安全`RL`中很常见，并且结合结构有助于提高此类方法的样本效率，同时使它们更易于解释。将状态分解为安全状态和不安全状态有助于开发持久的安全条件或语言抽象。**递归约束**有助于使用分解状态显式地将优化限制在安全操作的潜在子集上。将选项的探索限制在非风险状态也有助于在分层设置中纳入安全性。因子分解的动作还可以通过基线提高**策略梯度**方法的样本效率，通过直接值条件提高离线方法的样本效率，以及通过矩阵估计提高基于值的规划的效率。**动作选择机制**可以利用领域知识来实现安全性和可解释性，或者利用定向探索来提高样本效率。分层设置受益于通过修改终止条件而纳入的潜在状态分解。此外，**状态-动作**等价性有助于通过分解将`Q-Learning`扩展到大型空间。

**并发优化**：使用结构分解并行优化有助于提高样本效率。分解的`MDP`是对影响呈现给用户的内容进行建模并在并行方案中帮助集成方法的绝佳方法。同样，分层设置中分解的奖励有助于将多任务问题分解为单个任务`MDP`的线性组合。或者，**离散化多维动作空间**中的连续子动作有助于将每个子动作的`MDP`扩展到较低级别`MDP`，从而使用修改`Q`值的备份。**关系分解**还有助于分解**神经网络**的**掩蔽策略**。

##### 辅助模型模式

该模式表示在模型中使用结构信息。使用术语“模型”，特指利用**环境模型**来生成经验的方法，无论是完全生成还是部分生成。这个概念涵盖了一系列方法，从使用完整的**世界模型**来生成奖励和下一个状态转换，到使用这些方法来生成完整的经验序列。在分类中，特别关注如何将结构融入到这些模型中，以帮助生成学习经验的某些部分。例如，出租车**智能体**可以根据以往的经验学习城市交通的潜在模型。该模型可用于规划避开交通并更快到达目的地的路线。或者，**智能体**可以学习一种集成技术，将多个模型组合起来，每个模型都对交通动态的特定组成部分进行建模。使用模型时，通常需要在模型复杂性和准确性之间进行权衡，因此必须谨慎管理，以避免**过度拟合**并保持稳健性。为此，结合结构有助于提高模型学习阶段的效率，同时允许重复使用以进行**泛化**。如下图所示：
{% asset_img ml_4.png "辅助模型模式" %}

**具有结构化表示的模型**：`Young`等人利用分解的**状态空间**来证明基于模型的方法在组合复杂环境中的优势。类似地，`dreamer`模型利用基于像素环境的潜在表示。面向对象的状态表示有助于避免在`MBRL`中使用`CNN`学习潜在因素，或者将潜在因素作为可以使用`NN`细化的随机变量。**图**（卷积）**网络**捕获丰富的高阶交互数据，例如人群导航或不变性。**动作等价性**有助于学习用于规划和价值迭代的潜在模型（抽象`MDP`）。

**用于特定任务分解的模型**：在模型中利用分解的另一种方法是捕获特定于任务的分解。捕获某种形式的不相关性的模型，例如**因果强化学习**中的观察和干预数据，或者任务相关与不相关数据，有助于提高**泛化能力**和**样本效率**。潜在表示帮助模型捕获控制相关信息或子任务依赖关系。

用于安全性的模型通常包含一些**成本度量**来抽象安全状态，或者包含对状态和动作进行分解的无知。模型还可以通过潜在的**因果分解**和**状态子空间**直接指导探索机制，以提高样本效率。诸如CycleGAN之类的生成方法也是使用`MDP`不同组件的潜在模型来生成反事实轨迹的绝佳方法。

##### 仓储模式

**仓储模式**(`Portfolio Pattern`)使用**结构信息**来创建数据库或实体仓库，这些实体可以组合起来以实现特定目标。这些可以是**学习策略**和**价值函数**，甚至是模型。考虑到此类方法的在线性质，它们通常针对持续和终身学习问题。此类方法中固有的模块化使它们将知识重用作为中心主题。例如，出租车可以维护一个**价值函数**或**策略数据库**，用于城市的不同部分或一天中的不同时间。当出租车在城市中导航时，这些可以重复使用，从而提高学习效率。**仓储**通常可以提高**效率**和**泛化能力**，并且通过技能和选项框架来实现。此模式中一个重要的考虑因素是**管理仓储**的**规模**和**多样性**，以避免学习过程过多地偏向过去的经验。**仓储**主要应用于**样本效率**和**泛化**。然而，它们也与**可解释性**重叠，因为存储的数据可以轻松地用于分析**智能体**的行为并理解新场景的**策略**。如下图所示：
{% asset_img ml_5.png "投资组合模式" %}

**策略仓储**：**策略子空间**利用共享的潜在参数在策略中学习一个子空间，其线性组合有助于创建新策略。通过存储额外的策略来扩展这些子空间，自然地将其扩展到持续设置中。利用**目标**和**奖励**，任务分解赋予了多任务终身设置中的策略和`Q`值存储功能。从潜在空间生成的现有任务之间的关系图提供了一种建模终身多任务学习问题的方法。另一方面，`Devin`等人(`2017`)提出的方法将`MDP`分解为特定于**智能体**和特定于**任务**的变异程度，为此训练各个模块。使用**变分编码器-解码器**模型进行的解耦通过将动态分解为**共享因素**和特定于**智能体**的因素，有助于控制形态上不同的**智能体**。此外，将**智能体**的问题划分为相互连接的**子智能体**，这些**子智能体**学习**局部控制策略**。利用技能框架的方法有效地创建了一个学习原语的投资组合，类似于`HRL`中的选项。这些随后用于最大化低层次中的互信息，共同勾勒出一个**策略**，在持续设置中寻求**多样性先验**，或用于划分**状态空间**。类似地，`Gupta`等人(`2017`)在使用辅助优化学习的潜在嵌入空间上应用了**投资组合模式**。

**分解模型**：模型中固有存在的分解导致了通常集成多个模型的方法，这些模型各自反映问题的不同方面。**集成方法**，如**递归独立机制**，捕获个别模块中的动态，这些模块稀疏交互并使用**注意机制**。集成动态还帮助实现对未见 `MDP`的少量适应。结合**关系分解**的分解模型有助于将动作绑定到以对象为中心的表示上。

##### 环境生成模式

**环境生成模式**(`Environment Generation Pattern`)使用**结构信息**来创建任务、目标或动态分布，从中可以对`MDP`进行采样。这包含了**程序生成环境**的概念，同时结合了使用**辅助模型**在环境生成过程中引入结构的方法。分解体现在环境生成的各个方面，这些方面受到生成过程的影响，例如**动态、奖励结构、状态空间**等。考虑到环境生成的在线性质，此模式中的方法以某种方式解决了学习的问题。在出租车示例中，可以生成一个任务，从简单任务（如在空网格中导航）开始，逐渐引入复杂性（如添加交通和不同目的地的乘客）。确保生成的`MDP`能够良好覆盖问题空间对于避免对特定任务子集的**过度拟合**至关重要。这需要在环境生成过程中纳入额外的**多样性约束**。结构在环境生成过程中提供了额外的**可解释性**和**可控性**，因此使**基准测试**比使用**无监督技术**的方法更容易。规则基础的语法有助于建模学习问题的组合性质。`Kumar`等人(`2021`)利用这一点影响转移动态并生成环境。这使他们能够以隐式组合的方式训练**智能体**。`Kumar`等人(`2022`)在他们的辅助优化过程中使用了这些语法。捕获任务依赖性的另一种方法是通过潜在图形模型，这些模型生成**状态空间、奖励函数和转移动态**。潜在**动态模型**允许模拟任务分布，这有助于**泛化**。**聚类方法**，如探索性任务聚类，通过**探索策略**的**元学习**来研究任务相似性。在某种意义上，它们恢复了任务空间上的分解，其中各个聚类可以进一步用于**策略适应**。
{% asset_img ml_6.png "环境生成模式" %}

##### 显式设计模式

**显式设计模式**(`Explicitly Designed`)包含所有归纳偏差体现在特定架构或设置中的方法，这些架构或设置反映了它们旨在利用的问题的**可分解性**。这包括专门设计的神经架构，并扩展到其他方法，例如**顺序架构**，以捕获层次结构、关系等。结构信息的使用仅限于架构的**特异性**，而不限于管道的任何其他部分。在出租车的例子中，可以设计一种神经架构来将城市网格处理为图像并输出策略。诸如**卷积层**之类的技术可用于捕获城市网格的**空间结构**。不同的网络部分可以专门用于不同的子任务，例如识别乘客位置和规划路线。然而，这种模式涉及大量的手动调整和实验，并且要确保这些设计在不同的任务中能够很好地推广。设计特定的神经架构可以提供更好的**可解释性**，从而能够分解不同的组件并独立模拟它们。如下图所示：
{% asset_img ml_7.png "显式设计模式" %}

以下是一些研究和模型，它们结合了可分解性的概念，以提高**神经网络**和**强化学习**的**效率**和**准确性**：
- **结构化控制网络**(`SCN`)：`SCN`将通用**多层感知器**(`MLP`)分解为**非线性控制流**和**线性控制流**，并通过添加方式将它们组合起来，以提高**样本效率**和**泛化能力**。实验表明，与标准`MLP`基线相比，这种架构结合了**线性**和**非线性策略**的优点，同时使用更小的网络。**结构化控制网络**(`SCN`)已在`OpenAI MuJoCo、OpenAI Roboschool`和`Atari Games`等基准测试中进行了实验。
- **双线性值网络**：是一种用于**多目标强化学习**的**双线性值网络**，通过建筑方式将**动态分解**为**状态**和**目标条件分量**，以生成目标条件`Q`函数。
- **动作分支架构**：**动作分支架构**是一种新颖的**神经网络架构**，具有**共享网络模块**，后跟多个网络分支，每个动作维度一个。这种方法通过允许每个单独的**动作维度**具有一定程度的独立性，实现了网络输出数量随自由度数量的线性增加。动作分支**智能体**存储库提供了一组**深度强化学习智能体**，这些**智能体**基于将**动作分支架构**合并到现有**强化学习算法**中。分支架构`Q`网络(`BDQ`)是一种新颖的**智能体**，它基于将提议的**动作分支架构**合并到深度`Q`网络(`DQN`)算法中，以及采用其扩展的选择，双重`Q`学习，**分支网络架构**和**优先经验重放**。`HIQL`离线目标条件**强化学习**的分层隐式`Q-Learning`(`HIQL`)从**单个目标条件值函数**中提取所有必需的组件——**表示函数**、**高级策略**和**低级策略**。这些方法展示了将可分解性纳入**神经网络架构**如何为各种任务带来**更高效**、**可扩展**和**可解释**的模型。
- **分层强化学习**：**分层强化学习**提供了一种**分解复杂强化学习问题**的方法，将复杂性问题分解为更简单的子任务来执行。高层策略通常用于选择子任务或子目标。

**捕获架构中的不变性**：专门的架构有助于捕获问题中的**不变性**。**符号网络**通过首先将**关系马尔可夫决策过程**(`Relational MDPs`)转换为图形，然后使用**神经网络**捕获**节点嵌入**，从而训练一组**共享参数**。齐次网络在专门的**多层感知器**(`MLP`)和**卷积神经网络**(`CNN`)架构中捕获**对称性**。另一种捕获**对称性**的方法是通过**基函数**。注意机制明确帮助捕获实体分解场景。**关系网络**和**图网络**明确捕获额外的**关系归纳偏差**。**线性关系网络**提供了一种与**对象数量线性扩展的架构**。**图网络**也被用来显式建模**智能体**在具身控制中的形态。

**专门模块**：这些是结合了两者优点的方法，通过在**专门模块**中捕获**不变性**。这些模块捕获语义意义中的**关系结构**、**辅助模型的关系编码器**，或用于纳入领域知识的**专门架构**。

#### 离线强化学习

**离线强化学习**(`Offline Reinforcement Learning`，也称为**批量强化学习**)涉及从固定数据集中学习，而无需进一步与环境互动。这种方法在主动探索成本高、风险大或不可行时具有优势。因此，这类方法高度依赖于所收集的数据集，且由于预先收集数据的局限性，它们的**泛化能力**较差。**离线强化学习**中三种主要范式——**行为克隆**(`Behavior Cloning`)、`Q-Learning`和**序列建模**(`Sequence Modelling`)——在状态空间增大时性能普遍下降。**离线强化学习**面临的挑战包括有效克服分布变化和充分利用可用数据集。结构分解在以不同方式应对这些挑战方面可能至关重要，以下段落对此进行了总结。

**改善数据集的利用**：任务分解允许为不同子任务学习**个体策略**或**价值函数**，这可能更有效地利用可用数据。例如，使用相应数据子集的个体模块的策略组合进行模块化分解，可能比为整个任务学习单一策略更具样本效率。因此，任务分解为开发能够有效利用有限数据学习每个子任务的专门算法开辟了新途径，同时平衡不同子任务学习的影响。实践者可以利用这种分解，通过训练能够有效处理特定子任务的模型，从而最大化可用数据集的效用，潜在地提高整体系统在相同数据集上的性能。

**减轻分布变化**：结构信息可能有助于减轻分布变化的影响。例如，如果在一个分解中某些因素不易受到分布变化的影响，我们可以在学习过程中更加关注这些因素。这为深入理解**结构分解**、**任务分布**和**策略性能**之间复杂的相互作用提供了理论视角。另一方面，对于那些标准情况下存在分布变化的环境，实用方法可以利用**结构分解**来创建更**鲁棒**的**强化学习**系统。

**探索辅助任务**：**结构分解**可以用于定义**辅助任务**，以促进从数据集中学习。例如，在**关系分解**中，我们可以定义涉及预测不同实体之间关系的**辅助任务**，这有助于学习数据的**有价值表示**。通过所提出的框架，研究人员可以探讨如何定义有意义的**辅助任务**，以帮助**智能体**更好地理解环境。这可能导致新方法，通过学习这些**辅助任务**高效利用可用数据。实践者可以根据问题的具体分解设**计辅助任务**。例如，如果任务具有明确的关系结构，则预测不同实体之间关系的**辅助任务**可能会提高**智能体**对环境的理解及其整体性能。

#### 无监督强化学习

**无监督强化学习**(`Unsupervised RL`)是**强化学习**中的一个子领域，指的是**智能体**在没有明确**反馈**或**奖励**指导的情况下与环境进行交互的学习过程。该领域的方法可以根据用于内在评估性能的指标性质进行分类。知识驱动的方法通过对环境某些方面进行预测来定义**自监督任务**，数据驱动的方法则通过**最大化状态访问熵**来探索环境，而能力驱动的方法则**最大化轨迹**与**学习技能空间**之间的**互信息**。预训练阶段使得这些方法能够学习数据的潜在结构。然而，这一阶段也需要大量数据，因此影响了这些方法在**学习表示**不太有用的问题时的**可扩展性**。因此，目前这些方法主要处理中等复杂度的问题，而更好的可扩展性仍然是进一步研究的主题。**结构分解**可以通过改善预训练阶段的**可行性**和微调阶段的**普适性**来帮助这些方法。**潜在分解**可以帮助利用未标记数据中的结构，而**关系分解**则可以为学习到的表示增加**可解释性**。通过**增强**，针对**状态空间**特定部分的条件策略可以减少微调所需的数据。此外，理解**问题分解**可以将复杂问题简化为更易管理的子问题，有效降低感知问题的**复杂性**，同时将这种分解纳入外部知识以进行**微调**。结合由分解引导的能力驱动方法组合，可以提升学习技能的微调过程。

#### 大数据与基础模型在强化学习中的应用

**基础模型**是指一种**范式**，其中一个大模型在大量异构数据集上进行**预训练**，并针对特定任务进行**微调**。这些模型被称为“**基础性**”的，因为它们可以作为广泛任务的基础，从而减少为每个任务从头开始训练单独模型的需求。**在强化学习**(`RL`)领域，**基础模型**正逐渐接近现实。这类`RL`**模型**将遵循类似的概念，即在各种任务、环境和行为上训练一个大模型，以便为特定下游任务进行**微调**。`SMART`是当前此类模型的竞争者之一，它采用**自监督和控制中心的目标**，鼓励基于`Transformer`的模型捕捉与控制相关的表示，在用于**微调**时表现出优越的性能。`AdA`则在大量任务分布上训练一个**上下文学习智能体**，其中任务是从潜在规则集中生成的。鉴于**预训练范式**，这些方法在原则上高度依赖数据。然而，结合大量数据可以展示可扩展性优势，从而降低分布式应用中的**微调**成本。一个自然的问题是**结构化强化学习**在端到端学习和大数据领域中的作用。尽管此类方法遵循端到端范式，但**结构分解**可以以不同方式惠及它们。

**利用基础模型发现结构**：**基础模型**提供了以**无监督方式**发现结构的途径。这类方法在**无监督强化学习**中非常普遍，尤其是在**学习无监督技能**方面。整合**基础模型**使得可以学习不同管道部分的预测表示，并跨不同领域发现行为。因此，这些方法可以提供一种**隐式学习问题分解**的良好方式，从而利用上述模式。

**微调过程中的可解释性和选择**：研究人员可以通过对如何结合结构的方法进行分类，更好地理解**预训练模型**中的**可分解性**。因此，这可以指导根据任务选择**微调**方法的过程。从**预训练模型**中被动学习可以受益于更好的解释，了解**微调任务空间**中的哪些部分可能适合什么样的**热启动策略**。此外，结合面向**可解释性的分解**，如**关系表示**，可以帮助设计更具**可解释性**的**微调方法**。

**任务特定架构和算法**：通过更好地理解不同架构和算法如何结合结构信息，实践者可以更有效地调整现有方法或贡献于设计针对特定任务的新解决方案。例如，**行动分支架构**可能在下游任务中提供**模块化功能**，尤其适合多任务设置。另一方面，表示瓶颈可能适合那些因**上下文特征**的小变化而有所偏离的设置。

**改进微调和迁移学习**：通过理解如何分解任务并结合结构信息，可以更有效地对**基础模型**进行特定任务的**微调**。分解可以指导如何构建**微调**过程或将模型适应新任务。

**基准测试和评估**：具有不同分解级别的问题可能具有特定的基准测试要求。通过考虑这些属性，我们可以为利用**基础模型**的方法建立更好的**基准测试**和**评估协议**。研究人员可以利用这一框架设计更好的**评估协议**和**基础模型的基准测试**。对于实践者而言，这些**基准测试**和**评估协议**可以指导他们选择适合特定任务的模型和算法。

利用**基础模型**来发现问题中的分解，这些分解可以随后用于**微调**和**适应**：
- 在**预训练**的**基础模型**上使用**分解**或**关系抽象**进行**状态抽象**，以管理**高维状态空间**，从而降低**数据依赖性**。
- 根据额外的任务特定信息（如**目标信息**、**特定微调指令的表示**或**控制先验**）来**调节策略**，以提高**可扩展性**。
- 规范**微调**过程，以防止灾难性地遗忘在**预训练**期间学到的有用特征。
- 维护**微调策略**和**价值函数**的组合，以帮助重用先前学习的技能并将其适应于新任务，从而提高**学习效率**和**泛化能力**。
- 根据**智能体**的性能，采用日益复杂的**微调**环境，以帮助**智能体**逐步将**基础模型**的知识适应于特定的**强化学习任务**。
- 使用**显式架构**来**微调**不同的**强化学习**问题，如**感知**、**策略学习**和**价值估计**。

#### 部分可观测性与大世界

在**强化学习**(`RL`)中，**部分可观测性**是一个重要的挑战，尤其是在复杂的环境中。**部分可观测性**指的是**智能体**在做出决策时无法获得完整的环境状态信息。这种情况通常会导致决策过程缺乏**马尔可夫性质**，即当前观察无法完全描述未来状态的转移。

**部分可观测马尔可夫决策过程**(`POMDP`)是处理**部分可观测性**的一种模型。在`POMDP`中，**状态、观察**和**动作空间**被定义为一个**元组**，其中状态是**潜在的、不可观察的**，而观察则是基于这些状态生成的。由于观察往往不足以完全确定状态，**贝尔曼方程**不再成立，这使得**强化学习**中的**策略学习**变得更加复杂。

在**强化学习**(`RL`)领域，**大世界**的概念通常指的是具有**高维状态空间**和**复杂动态的环境**。在这样的环境中，**智能体**需要处理大量的信息和不确定性，以做出有效的决策。以下是一些在大世界中应用强化学习的关键领域：**自动驾驶**（在复杂的交通环境中，智能体必须实时处理来自周围环境的大量信息，例如其他车辆、行人和交通信号。）、**机器人探索与操作**、**能源管理**、**金融交易**、**个性化推荐系统**、**医疗保健**等。

在许多现实世界的情况下，**马尔可夫性质**可能无法完全捕捉环境的动态特性。这种情况可能发生在环境的状态或奖励不仅仅依赖于最近的状态和动作时，或者当智能体无法在每个时间步骤充分观察环境的状态时。在这种情况下，方法必须处理**非马尔可夫动态**和**部分可观测性**。

**抽象**：在这种情况下，**抽象**可以发挥关键作用，通过使用**结构分解**的**抽象模式**，使方法更加样本高效。通常在选项中使用的**时间抽象**允许**智能体**在较长时间内做出决策，从而封装这些扩展动作中的潜在时间依赖性。这可以有效地将**非马尔可夫问题**转化为**马尔可夫问题**。**状态抽象**则忽略状态中不相关的方面，因此有时可以忽略特定的时间依赖性，从而使抽象状态层面的过程变为**马尔可夫过程**。因此，关于分解在抽象中的作用的研究为理解**非马尔可夫模型**与它们用于解决不完整信息问题时所使用的抽象之间的依赖关系开辟了可能性。抽象还可以简化`POMDP`中的观察空间，从而减少信念更新过程的复杂性。**抽象**可能涉及对相似观察进行**分组**、识别更高层次特征或其他简化。**抽象**使我们能够将**部分可观测性**分解为不同类型，而不是总是假设最坏情况。利用这种对**部分可观测性**的限制假设，可以帮助我们构建更具体的算法，并为这些场景推导**收敛性**和**最优性**保证。

**增强**：任何额外的信息，如**信念状态**或**过去观察的记忆**，都可以作为**抽象**或**增强**。这也有助于更有效地学习过渡模型以进行规划。利用不同时间尺度进行优化的层次技术可以结合组合来重用跨不同抽象级别学习到的策略。**环境生成模式**还可以为**智能体**生成逐渐复杂化的任务课程，从简单的`MDP`开始，逐步引入**部分可观测性**或其他**非马尔可夫特征**。

**大世界**：随着我们将环境的信息内容扩展到极限，我们进入了**强化学习**中的**大世界假设**(`Javed, 2023`)，其中**智能体**的环境比**智能体**大多个数量级。在这种情况下，即使在无限数据极限下，**智能体**也无法表示**最优价值函数**和**策略**。在这样的场景中，**智能体**必须在重大不确定性下做出决策，这带来了几个挑战，包括**探索、泛化**和**高效学习**。尽管该假设表明，在这样的场景中，结合边际信息可能对**学习最优策略**和**价值**没有益处，但以不同方式对大环境进行**结构分解**可以允许沿不同轴进行**基准测试**，同时深入研究算法在**智能体**尚未经历过的环境部分上的表现。**模块化分解**可以指导**智能体**的探索过程，帮助**智能体**独立探索环境的不同部分。引入**模块化**为关于**任务分解**、**探索**和**学习效率**之间关系的新方法和理论见解打开了一扇大门。**关系分解**可以帮助**智能体**学习不同实体之间的关系，从而**增强**其对未见过环境部分的**泛化能力**。最后，**结构信息**可以用于促进更**高效的学习**。例如，在**辅助优化模式**中，**智能体**可以通过优化更易于学习或提供有关环境结构有用信息的**辅助任务**来加快学习速度。

利用**时间和状态抽象**来消除**时间依赖性**和**非马尔可夫状态**的方面。利用**模块化**将这些**抽象**与**学习到的原语**（如技能或选项）联系起来：
- 更有效地使用记忆作为学习过渡模型的**抽象**或**增强**。
- 创建**策略组合**，并利用它们在不同时间尺度上进行优化，例如在层次方法中，使其更易处理。
- 利用**模块化分解**指导对**状态空间**不同部分的**独立**和**并行探索机制**。利用**关系抽象**使这些知识更具**可解释性**。
- 利用**结构**进行**任务因子化**，以指导沿任务复杂性不同轴的**基准测试方法**。

#### 自动强化学习

**自动强化学习**(`AutoRL`)是**强化学习**(`RL`)领域的一个重要研究方向，旨在通过**自动化设计选择**和**算法优化**来提高`RL`的应用效率和效果。`AutoRL`的方法可以被放置在一个**自动化**的光谱上，一端是选择管道的方法，另一端是尝试从头开始以**数据驱动**方式**发现新算法的方法**。来自**自动机器学习**(`AutoML`)文献的技术被转移到**强化学习**环境中，包括**算法选择、超参数优化、动态配置**、**学习优化器**和**神经架构搜索**。同样，来自**进化优化**和**元学习**文献的技术也自然转移到这个环境中，旨在**元学习强化学习**管道的部分内容，如**更新规则、损失函数**、**算法的符号表示**或**概念漂移**。然而，在`AutoRL`领域仍然存在许多未解的问题，例如**强化学习**中的**超参数景观特性**、**合理的评估协议**、由于**非平稳学习任务**和**动态数据收集**而导致的**训练稳定性**。因此，大部分方法都面临着**可扩展性不足**的问题。

**算法选择与配置**：根据所处理问题的**可分解性**，不同的**强化学习**方法可能更为合适。**结构分解**可以通过根据问题特征建议适当的**分解类型**来指导`AutoRL`中的选择过程。理解不同**分解类型**如何影响**强化学习**方法的性能，可以弥合选择与配置之间的差距，帮助研究人员理解根据任务所需的**抽象级别**，从而有助于开发更高效和针对性的**搜索算法**。**可分解性**还可以指导排名程序，根据给定问题对不同**可分解性**的方法进行不同排名。

**超参数优化**：与**结构分解**相关的参数（例如，**模块化分解**中的子任务数量）可能是`AutoRL`中超参数优化过程的一部分。研究人员可以调查**超参数配置空间**与各种**结构分解**相关参数之间的相互作用。例如，**高可分解性**问题可能需要不同的**探索**或**学习率**，而**低可分解性**问题则可能需要不同的设置。这可能导致新的见解和方法，从而在`AutoRL`中实现更有效的**超参数优化**。实践者可以利用这种理解来指导其`AutoRL`系统中的**超参数优化**过程。通过调整与分解相关的参数，他们可以潜在地提高其**强化学习智能体**的性能。

#### 元强化学习

**元强化学习**(`Meta-RL`)是一种提升**智能体**在多任务环境中快速适应新任务能力的技术。其核心思想是让**智能体**学会如何学习，以便在面对新的任务或环境时能够迅速调整策略。**元强化学习**(`Meta-RL`)虽然与**自动强化学习**(`AutoRL`)有重叠之处，但它本身就是一个独立的领域，专注于训练**智能体**快速适应和学习新任务或环境。一般的`Meta-RL`设置涉及一个**双层优化**过程，其中**智能体**通过在一系列任务或环境的分布上进行训练来学习一组参数，这些参数帮助它适应并在新的、未见过的任务上表现良好，这些任务与训练任务共享某种形式的重叠。`Beck`等人(`2023`)根据在训练和适应阶段提供给**智能体**的**反馈类型**（**监督、无监督、奖励**）概述了`Meta-RL`中不同的问题设置。我们主要指的是**标准设置**，其中外部奖励在训练和适应阶段充当反馈。然而，分解对于其他部分中讨论的设置也可能有所帮助。

**任务分解**：不同的**任务分解方法**可以根据**元任务**的**可分解性**来指导**元学习**过程。因此，理解任务分解如何影响`Meta-RL`可以指导更有效的**元学习算法**的开发。它也可能导致对平衡不同子任务之间的学习的新见解。通过识别合适的**分解**，实践者可以设置他们的系统，使其以一种更符合任务结构的方式进行学习，从而可能提高性能。

**适应策略**：**分解**可以为`Meta-RL`**智能体**适应新任务的方式提供信息。例如，如果新任务是**高度可分解的**，那么**模块化适应策略**可能更合适，通过引导**智能体**到新任务的适当**潜在空间**。因此，结合现有的模式可以激发对**任务分解**如何指导`Meta-RL`中适应策略的新研究。这可能导致基于其结构更有效地适应新任务的新方法或理论。

使用**分解**来**抽象**任务分布，这可以融入适应过程：
- 将**学习过程**划分为**模块**，以应对**高度可分解**的问题。这些**模块**可以作为**元级别**的**抽象配置**，从而使外部循环更易处理。
- 学习并创建针对特定任务集群的**模型组合**，以指导适应阶段的**数据增强**，适用于不同的**可分解性类型**。
- 利用**可分解性**设计基于**任务抽象类型**的**学习课程**，以训练**热启动配置**。

#### 并行计算的三种范式

##### 数据并行

**数据并行**(`Data Parallelism`)是一种将数据集分割成多个部分，并在多个处理单元上同时执行相同操作的技术。在这种模式下，每个处理单元（如`CPU`或`GPU`）独立处理其分配的数据子集，通常适用于大规模数据集。**数据并行**(`Data Parallelism`)工作原理：
- **数据划分**：将整个数据集分成若干子集，每个处理单元（如`GPU`或`CPU`）独立处理一个子集。这些子集可以是样本的不同部分，通常是随机选择的。
- **模型复制**：每个处理单元上都有模型的完整副本。在每个周期中，所有处理单元对其本地数据进行前向传播和反向传播。
- **参数同步**：在每个训练周期结束时，所有处理单元会将其计算得到的梯度进行汇总（通常使用平均），然后更新模型参数。这个过程称为参数同步。

优点：
- **可扩展性**：**数据并行**可以通过增加更多的计算节点来处理更大的数据集，适应更复杂的模型。
- **性能提升**：通过并行计算，能够显著减少训练时间，尤其是在大规模数据集和复杂模型上。
- **实现简单**：由于每个处理单元执行相同的操作，代码实现相对简单，易于理解和维护。

**数据并行**特别适合以下情况：**大规模深度学习模型**：如**卷积神经网络**(`CNN`)和**循环神经网络**(`RNN`)，在**图像识别、自然语言处理**等领域广泛应用；**大规模数据集**：如**图像、文本**等，需要快速处理和训练的数据集。

##### 张量并行

**张量并行**(`Tensor Parallelism`)是一种用于**深度学习**模型训练的并行计算技术，旨在解决大规模模型在单个计算设备上内存不足的问题。它通过将模型中的张量（多维数组或矩阵）按照特定维度进行切分，并将这些切分后的部分分配到不同的计算设备上进行并行计算，从而提升训练效率。**张量并行**(`Tensor Parallelism`)工作原理：**张量切分**：**张量并行**主要通过两种方式进行切分：
- **行并行**(`Row Parallelism`)：将权重矩阵按行分割，同时将输入数据按列分割。例如，在矩阵乘法中，输入矩阵和权重矩阵的部分被分别分配到不同的`GPU`上进行计算。
- **列并行**（`Column Parallelism`）：与行并行相反，将权重矩阵按列切分，输入数据按行切分。

**计算过程**：每个设备只处理其分配到的张量部分，计算完成后，通过高效的数据通信机制（如`AllReduce`操作）将结果合并，以确保最终输出的正确性。

优势：
- **内存优化**：通过将大模型分割到多个设备上，显著降低了单个设备的内存需求，使得训练更大规模的模型成为可能。
- **加速训练**：多个设备并行处理计算任务，从而提高整体训练速度，尤其是在处理复杂模型时。

**张量并行**技术特别适用于以下情况：
- **大规模深度学习模型**：如基于`Transformer`架构的预训练模型（例如`BERT、GPT`等），这些模型通常具有数亿到数十亿个参数，单个`GPU`难以承载。
- **高内存需求的任务**：当模型参数或输入数据过大，导致单个计算设备无法有效处理时，**张量并行**提供了一个解决方案。

##### 流水线并行

**流水线并行**(`Pipeline Parallelism`)是一种用于**深度学习**模型训练的分布式计算技术，旨在通过将模型的不同部分分配到多个计算设备上，以提高训练效率和降低内存需求。随着**深度学习**模型规模的不断扩大，传统的单`GPU`训练方式已难以满足需求，**流水线并行**成为了一个重要的解决方案。

**流水线并行**(`Pipeline Parallelism`)工作原理：
- **模型分段**：**流水线并行**将**深度学习**模型垂直分割成多个阶段(`stage`)，每个阶段包含模型的一部分层。这些阶段可以部署在不同的`GPU`上。
- **数据流动**：在训练过程中，输入数据通过流水线依次流经各个阶段。每个阶段处理其接收到的数据，并将结果传递给下一个阶段。这种方式类似于工业生产中的流水线作业，每个工人(`GPU`)专注于完成自己的任务。
- **微批次处理**：为了充分利用计算资源，流水线并行通常将输入数据进一步划分为**微批次**(`micro-batch`)。每个阶段在处理一个**微批次**时，可以同时接收来自前一阶段的输出，并准备好下一个**微批次**的数据，从而实现连续的流水线作业。

优势：
- **内存效率**：通过将模型分割到多个设备上，每个设备只需存储其负责的部分，从而显著降低了单个设备的内存需求。
- **提高训练速度**：由于多个设备可以并行处理数据，流水线并行能够显著加快训练过程。尤其是在处理大模型时，能够有效减少训练时间。
- **减少通信量**：与数据并行相比，**流水线并行**的数据传输量较少，因为它仅涉及相邻阶段之间的**激活**和**梯度**传输。

**流水线并行**特别适用于层数较多、层间依赖关系较弱的**神经网络模型**，如`Transformer、BERT`等。这些模型通常具有较大的参数量和计算量，单机训练难以承受，而**流水线并行**则能有效缓解这一问题。**实现步骤**：
- **模型分割**：根据模型结构和计算需求，将**深度学习模型**划分为多个阶段。
- **数据预处理**：使用数据加载器对输入数据进行预处理，并分批次输入到模型中进行训练。
- **分布式训练**：通过分布式框架实现各个阶段在不同`GPU`上的部署，并使用高效的通信机制在`GPU`之间传输数据和梯度。
- **流水线调度**：合理调度数据和处理过程，确保数据在各个阶段之间顺畅流动，同时管理依赖关系和同步问题。

**流水线并行**是一种高效的大模型训练技术，通过合理设计**模型分割、数据预处理、分布式训练**和**流水线调度**，可以充分发挥其优势，提高训练速度和效率。随着深度学习技术的不断发展，**流水线并行**将在未来的人工智能领域发挥越来越重要的作用。

#### 总结

理解**深度强化学习**的复杂性和细微差别是具有挑战性的，这种挑战因不同问题领域采用的多样化方法而加剧。这种碎片化阻碍了统一原则和一致实践在**强化学习**中的发展。为了解决这一关键差距，提出了一种创新框架，以理解将学习问题的固有结构有效整合到**强化学习算法**中的不同方法。首先将结构概念化为关于**学习问题**及其相应解决方案**可分解性**的附加信息。将**可分解性**分为四种不同的原型——**潜在**、**分解**、**关系**和**模块化**。这一分类描绘了一种光谱，建立了与现有文献之间的深刻联系，阐明了**结构**在**强化学习**中的多样影响。接着，在对**强化学习**领域进行深入分析后，提出了七个关键模式——**抽象**、**增强**、**辅助优化**、**辅助模型**、**仓储**、**环境生成**和**显式设计**模式。这些模式代表了将结构知识纳入**强化学习**的战略方法。尽管框架提供了一个全面的起点，但这些模式并没有穷尽。希望这能激励研究人员进一步完善和开发新模式，从而扩展**强化学习**中的**设计模式库**。总之，这里提供了一种以**模式**为中心的视角，强调**结构分解**在塑造当前和未来范式中的关键作用。
