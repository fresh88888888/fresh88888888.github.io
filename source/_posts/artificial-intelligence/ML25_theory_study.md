---
title: 机器学习(ML)(二十五) — 强化学习探析
date: 2025-01-17 17:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 强化学习—结构

**强化学习**的结构大致分为`7`种**设计模式**：**抽象模式**(`Abstraction Pattern`)、**增强模式**(`Augmentation Pattern`)、**辅助优化模式**(`Auxiliary Optimization Pattern`)、**辅助模型模式**(`Auxiliary Model Pattern`)、**投资组合模式**(`Portfolio Pattern`)、**环境生成模式**(`Environment Generation Pattern`)、**明确设计模式**(`Explicitly Designed`)。
<!-- more -->

##### 抽象模式

**抽象模式**利用结构信息在**强化学习**(`RL`)管道中创建**抽象实体**。对于任何实体{% mathjax %}\mathcal{X}{% endmathjax %}，**抽象模式**利用结构信息创建{% mathjax %}\mathcal{X}_{\text{abs}}{% endmathjax %}，其在学习过程中取代{% mathjax %}\mathcal{X}{% endmathjax %}的角色。在出租车示例中，**状态空间**可以抽象为出租车当前的网格单元、当前乘客的目的地网格单元，以及出租车是否正在载客。这显著简化了**状态空间**，相较于表示城市网格的完整细节。**动作空间**也可以抽象为向四个主要方向移动，以及接载和放下乘客。**行为抽象**与基于历史的抽象密切相关，因为它们解决类似的应用，即将**抽象状态**和历史转化为可以用于**强化学习**的低维表示。
{% asset_img ml_1.png "抽象模式" %}

找到合适的抽象本身可能是一项具有挑战性的任务。过多的抽象可能导致关键信息的丢失，而过少的抽象则可能无法显著降低**复杂性**。因此，联合学习抽象的学习方法将这种粒度纳入学习过程。早期工作涉及状态抽象的理论。最近的研究主要利用抽象来解决泛化问题。**泛化**是抽象最常见的应用案例。然而，前面提到的抽象优势通常与样本效率提升和安全性交织在一起。

**泛化**：**状态抽象**是提高**泛化**性能的标准选择，通过使用诸如**不变因果预测**、**相似性度量**、**自由能最小化**(`Free Energy Minimization`)和**解耦**(`disentanglement`)等方法，将共享动态捕捉到**抽象状态空间**中。**值函数**作为多任务环境中共享动态的时间抽象。**后继特征**(`Successor Features, SF`)利用**值函数**作为抽象，利用潜在**奖励**和**动态分解**。后续研究将其与广义策略迭代(`Generalized Policy Iteration`)和**通用值函数逼近器**(`Universal Value Function Approximators`)结合使用。另一方面，**值函数**的**因式分解**有助于提高样本效率和泛化能力。**关系抽象**通过将**符号空间**纳入**强化学习管道**来促进泛化。这些抽象有助于在分层框架中结合规划方法。此外，**关系抽象**可以帮助抽象出一组`MDP`的一般特征，从而使方法能够在**抽象状态**和**动作**上学习可泛化的`Q`值，这些`Q`值可以转移到新任务中。此外，**抽象**还可以通过**压缩状态空间**、**抽象自动机**、**跨任务动态的模板**，甚至与选项结合以保留**最优值**来实现分层设置中的**泛化**。

**样本效率**：**潜在变量模型**可以提高**强化学习**(`RL`)管道中的样本效率。潜在状态抽象可以提高基于模型的`RL`中的样本效率，并有助于提高策略学习的可操作性。在无模型任务中，潜在变量模型可以学习视觉特征或潜在空间控制的逆模型。**潜在过渡模型**通过捕获噪声环境中的相关信息、通过保留原始状态之间的双模拟距离或通过利用分解抽象来提升效率。学习的潜在抽象也有助于提升探索。**潜在动作模型**通过缩短随机场景（如对话生成）中的学习范围来加速**策略梯度**方法的收敛。另一方面，**动作嵌入**有助于降低**动作空间**的维度。**深度潜在变量模型**在基于模型的`RL`中取得了显著的成功经验，因为它们在建模方面具有表现力。它们可以促进学习、计划和探索，以提高`RL`的样本效率。**潜在变量模型**的**状态-动作值函数**的表示视图允许可处理的**变分学习算法**在面对不确定性时有效实施乐观/悲观原则进行探索。通过结合**潜在变量模型**的内核嵌入，可以实现具有`UCB`探索的规划算法。

**安全性与可解释性**：**关系抽象**是**可解释性**的绝佳选择，因为它们可以捕获交互复杂的分解。**对象中心表示**和**学习的抽象**的结合增加了透明度，而**符号插入**，例如跟踪对象之间的关系距离，有助于提高性能。**状态和奖励抽象**有助于安全性。潜在状态通过嵌入混杂因素来帮助学习安全的**因果推理模型**。另一方面，**网格**有助于对基准指标进行**基准测试**，例如学习策略的稳健性。

##### 增强模式

**增强模式**(`Augmentation Pattern`)将{% mathjax %}X{% endmathjax %}和{% mathjax %}z{% endmathjax %}视为动作选择机制的单独输入实体。组合范围可以从结构信息到状态或动作的简单连接，到在附加信息上调节策略或**价值函数**的更复杂方法。至关重要的是，结构信息既不直接影响优化过程，也不改变{% mathjax %}X{% endmathjax %}的性质，而只是增强了已经存在的实体。从这个角度来看，以辅助方式学习并连接到状态、动作或模型的抽象也可以被认为是**增强**，因为原始实体保持不变。对于出租车示例，应用**增强模式**的一种方法是根据附加信息（例如一天中的时间或一周中的一天）来调节策略。此信息可能很有用，因为交通状况和乘客需求会因这些因素而异。但是，增强会增加策略和学习过程的复杂性，需要注意确保策略不会**过度拟合**附加信息。因此，与抽象相比，这种模式的探索通常较少。
{% asset_img ml_2.png "增强模式" %}

**基于上下文的增强**：动态和目标相关信息的上下文表示通过向**智能体**公开最佳性所需的信息来帮助**泛化**和**样本效率**。**目标增强**还允许用于指定目标的可解释机制。将**元学习**的潜在空间增强到正常状态可以促进跨任务的**时间相干探索**。动作历史可以直接帮助提高样本效率，而动作关系有助于推广到大型动作集。

**语言增强**：语言明确地捕获世界中的**关系元数据**。潜在语言解释模型利用语言的组合性来实现更好的探索和泛化到不同的关系设置，如其语言描述所代表的。目标描述通过利用不同子任务之间的**语义关系**并为较低级别的策略产生更好的目标来帮助分层设置。增强还通过`Verma`等人提出的方法，通过指导搜索以人类可读格式编写的**近似策略**，帮助使现有方法更易于解释。

**控制增强**：增强有助于原始控制，例如分层设置中的多级控制。增强以原始技能为条件的内部潜在变量有助于解决分层设置中的样本效率问题。增强还通过将肢体建模为必须学习组合成形态以解决任务的单个**智能体**来帮助形态控制。

##### 辅助优化模式

**辅助优化模式**利用结构性辅助信息来优化过程，包括**对比损失**、**奖励塑造**、**并发优化**、**掩蔽策略**、**正则化**和**基线**等方法。由于优化中的更改通常与其他组件的更改相吻合，因此这种模式与其他许多模式共享方法；例如，**对比损失**可用于学习状态抽象。同样，学习的模型也可以用于**奖励塑造**。因此，属于此类别的方法同时利用了多种模式。在出租车的例子中，**奖励塑造**可以通过鼓励出租车在没有乘客时留在乘客频繁出没的区域附近，从而帮助策略在城市网格的轻微扰动中重复使用。确保修改后的优化与原始目标保持一致，这需要**正则化**来控制优化如何尊重原始目标。对于**奖励塑造**，这意味着**最优策略**在塑造奖励下保持不变。对于辅助目标，这表现为某种形式的**熵**或**散度正则化**。通过递归确保这一点，而**基线**控制更新**方差**。因此，**辅助优化模式**非常倾向于解决安全问题。
{% asset_img ml_2.png "辅助优化模式" %}

**奖励塑造**：**奖励塑造**是一种常见的技术，可将更多信息纳入优化过程中，通过利用任务描述中的**模块化**和**关系分解**，从而提高样本效率。**奖励**的历史记录有助于学习状态之间的**对称关系**，从而改进用于优化的批处理中状态的选择过程。将**状态**和**奖励分解**为内生和外生因素有助于通过奖励校正来提高安全性和样本效率。外在奖励还可以指导探索过程。具有关系表示的**符号规划器**通过分层设置中的外在奖励与原始学习策略交互，从而增加可解释性，同时通过外在奖励直接影响探索。额外的奖励来源有助于确定反事实轨迹的质量，从而有助于解释为什么**智能体**采取了某些类型的操作。此外，**奖励**的运行平均值有助于自适应地调整**异构动作空间**的探索参数。内在奖励有助于探索**稀疏奖励环境**。潜在分解通过直接影响探索来帮助改进这些方法。**语言抽象**用作单独用于探索的潜在分解。或者，几何结构可以比较状态嵌入并提供**情景奖励**。**奖励塑造**是一种将**领域知识**融入**强化学习**(`RL`)中的有效技术。探索引导**奖励塑造**(`ExploRS`)以完全自我监督的方式运行，甚至可以在**稀疏奖励**环境中加速**智能体**的学习. 一种新颖的方法将从历史经验中获得的成功率纳入**塑造奖励**中，以实现自适应和高效的**奖励塑造**。`ExploRS`通过结合基于探索的奖励来学习内在**奖励函数**，以最大限度地提高**智能体**相对于外在奖励的效用. 它可以解决现有**奖励塑造方法**的问题，这些方法要么需要外部领域知识，要么在奖励极其稀疏的环境中失败。`ExploRS`在奖励学习和策略优化之间交替进行，并在具有**稀疏/嘈杂奖励信号**的多个环境中展示了有效性。

**辅助学习目标**：通过学习不变子空间并使用这些子空间来创建辅助目标或基于熵的**策略正则化项**，从而在具有不同形态的**智能体**之间传递技能。在分层设置中发现适当的子任务是一个非常低效的过程，可以通过组合当前策略下的子轨迹的值来解决，他们随后将其用于**行为克隆**。当用于某种形式的**策略正则化**时，潜在分解还有助于提高鲁棒性和安全性。辅助损失通常有助于泛化，并且是类人归纳偏差的绝佳切入点。受潜在分解几何形状启发的指标有助于学习多任务设置中的最优值。

**约束和基线**：约束优化在安全`RL`中很常见，并且结合结构有助于提高此类方法的样本效率，同时使它们更易于解释。将状态分解为安全状态和不安全状态有助于开发持久的安全条件或语言抽象。**递归约束**有助于使用分解状态显式地将优化限制在安全操作的潜在子集上。将选项的探索限制在非风险状态也有助于在分层设置中纳入安全性。因子分解的动作还可以通过基线提高**策略梯度**方法的样本效率，通过直接值条件提高离线方法的样本效率，以及通过矩阵估计提高基于值的规划的效率。**动作选择机制**可以利用领域知识来实现安全性和可解释性，或者利用定向探索来提高样本效率。分层设置受益于通过修改终止条件而纳入的潜在状态分解。此外，**状态-动作**等价性有助于通过分解将`Q-Learning`扩展到大型空间。

**并发优化**：使用结构分解并行优化有助于提高样本效率。分解的`MDP`是对影响呈现给用户的内容进行建模并在并行方案中帮助集成方法的绝佳方法。同样，分层设置中分解的奖励有助于将多任务问题分解为单个任务`MDP`的线性组合。或者，**离散化多维动作空间**中的连续子动作有助于将每个子动作的`MDP`扩展到较低级别`MDP`，从而使用修改`Q`值的备份。**关系分解**还有助于分解**神经网络**的**掩蔽策略**。

##### 辅助模型模式

