---
title: 数据科学 — 数学（机器学习）
date: 2024-08-06 15:35:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**数据科学**是一门跨学科的领域，结合了**统计学**、**计算机科学**和领域知识，以从数据中提取有价值的信息。**数学**在数据科学中起着至关重要的作用，以下是数据科学中一些关键的数学基础。
- **线性代数**：**矩阵和向量**—线性代数是数据科学的基础，特别是在机器学习和数据分析中。矩阵和向量用于表示和操作数据集；**矩阵分解**—如**特征值分解**和**奇异值分解**(`SVD`)，这些技术在**降维**和**数据压缩**中非常重要。
- **微积分**：**导数和积分**—**微积分**用于优化算法，尤其是**梯度下降法**，这是训练机器学习模型的核心技术；**偏导数和多变量微积分**—在复杂模型中，涉及多个变量的优化问题需要用到这些概念。
- **概率与统计**：**基本概率**—包括**概率分布**、**期望值**和**方差**，这些是理解随机过程和不确定性的重要工具；**统计推断**—如**假设检验**、**置信区间**和**贝叶斯统计**，用于从样本数据中推断总体特征。
- **最优化**：**线性规划和非线性规划**—用于解决**资源分配**和**决策**问题；**凸优化**—许多机器学习算法的基础，通过优化**目标函数**来找到最佳参数。
<!-- more -->

##### 数学—符号注释

|符号|解释|
|:---|:---|
|{% mathjax %}A,B,C{% endmathjax %}|大写字母代表矩阵(`Matrix`)|
|{% mathjax %}u,v,w{% endmathjax %}|小写字母代表矢量|
|{% mathjax %}A, m\times n{% endmathjax %}|矩阵(`Matrix`){% mathjax %}A{% endmathjax %}有{% mathjax %}m{% endmathjax %}行和{% mathjax %}n{% endmathjax %}列|
|{% mathjax %}A^{\mathsf{T}}{% endmathjax %}|矩阵(`Matrix`){% mathjax %}A{% endmathjax %}的转置|
|{% mathjax %}v^{\mathsf{T}}{% endmathjax %}|向量{% mathjax %}v{% endmathjax %}的转置|
|{% mathjax %}A^{-1}{% endmathjax %}|矩阵(`Matrix`){% mathjax %}A{% endmathjax %}的逆矩阵|
|{% mathjax %}\text{det}(A){% endmathjax %}|矩阵(`Matrix`){% mathjax %}A{% endmathjax %}的行列式|
|{% mathjax %}AB{% endmathjax %}|矩阵(`Matrix`){% mathjax %}A{% endmathjax %}和矩阵{% mathjax %}B{% endmathjax %}的矩阵乘法|
|{% mathjax %}u\cdot v{% endmathjax %}|向量{% mathjax %}u{% endmathjax %}和向量{% mathjax %}v{% endmathjax %}的点积|
|{% mathjax %}\mathbb{R}{% endmathjax %}|实数集，例如`0,−0.642,2,3.456`|
|{% mathjax %}\mathbb{R}^2{% endmathjax %}|二维向量集|
|{% mathjax %}\mathbb{R}^n{% endmathjax %}|`n`维向量集|
|{% mathjax %}v\in \mathbb{R}^2{% endmathjax %}|向量{% mathjax %}v{% endmathjax %}是{% mathjax %}\mathbb{R}^2{% endmathjax %}中的一个元素|
|{% mathjax %}|v|_1{% endmathjax %}|向量的`L1-norm`|
|{% mathjax %}|v|_2{% endmathjax %}|向量的`L2-norm`|
|{% mathjax %}T:\mathbb{R}^2 \rightarrow \mathbb{R}^3; T(v) = w{% endmathjax %}|将向量{% mathjax %}v\in \mathbb{R}^2{% endmathjax %}转化为向量{% mathjax %}w \in \mathbb{R}^3{% endmathjax %}|

#### 方程组

##### 线性代数

一种常用的系统建模机器学习方法称为：**“线性回归”**。**线性回归**是一种监督式的机器学习方法，假如你已经收集了许多输入、输出数据，你的目标是发现它们之间的关系。例如，你想预测风力涡轮机的电力输出。如果你只有一个特征，`X`轴上显示的是风速，`Y`轴上显示的是输出功率，这里的数据点代表风速和功率输出的实际测量值。显然，线性回归的目标是找到最接近这些数据点的线。对于这样的模型，假设这种关系是线性的，它可以用一条线建模。例如，风速以每秒`5`米的速度吹来，那么我预测风力涡轮机的功率输出降为`1500`千瓦。现在这个模型并不完美，你可以看到实际数据分散在模型的线条上，但它做的不错，这里的模型是线性方程：{% mathjax %}y = mx + b{% endmathjax %}，其中{% mathjax %}y{% endmathjax %}是功率输出，{% mathjax %}x{% endmathjax %}是风速。你的目标是找出适合数据的{% mathjax %}m{% endmathjax %}和{% mathjax %}b{% endmathjax %}的最佳值，在机器学习中你经常会看到该模型的方程会写成：{% mathjax %}y = wx + b{% endmathjax %}，因为数字{% mathjax %}w{% endmathjax %}乘以{% mathjax %}x{% endmathjax %}中的{% mathjax %}w{% endmathjax %}被称为权重，{% mathjax %}b{% endmathjax %}称为偏差，只有一个特征的线性回归很容易可视化。
{% asset_img m_1.png  %}

但在机器学习问题中，你需要考虑更多的特征。在预测涡轮机的功率输出时，包括风速、温度，为了包括新的输入，需要更改方程：{% mathjax %}y = w_1x_1 + w_2x_2 + b{% endmathjax %}，为第二个变量添加了新的权重，这个方程将不是一条直线，而是三维空间中的平面。但是如果你想考虑更多的特征，比如压力、湿度等，这时候该怎么办？这时你只需要为每个特征添加一个新的权重。
{% asset_img m_2.png  %}

以此类推，你有{% mathjax %}w_nx_n{% endmathjax %}任意数量的特征。然后你添加{% mathjax %}b{% endmathjax %}，并将其全部设置为{% mathjax %}y{% endmathjax %}，如果你将方程想象为数据集中的一行，那你已经知道了{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}的值，你的目标是找到{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的值来适合这个方程，由于数据集有很多条数据记录，所以你可以写下更多个方程。写下第一个方程中的所有{% mathjax %}x_n{% endmathjax %}和{% mathjax %}y{% endmathjax %}的上面添加一个带括号的标号`1`，则依此类推，包含{% mathjax %}m{% endmathjax %}条记录的数据集，最后一个方程的标号为{% mathjax %}m{% endmathjax %}。
{% asset_img m_3.png  %}

同时求解所有这些方程的权重{% mathjax %}w_n{% endmathjax %}和偏差值{% mathjax %}b{% endmathjax %}，而这些方程被统称为：**“线性方程组”**。数据集包含了一系列特征，例如风速、温度、大气压力、湿度等。我们用{% mathjax %}x_1,x_2,x_3,x_4,\ldots,x_n{% endmathjax %}来表示。然后向数据集添加一个上标来表示这一组特征是属于哪一行数据。用模型权重乘可以表示为{% mathjax %}w_1,w_2,\ldots,w_n{% endmathjax %}，最后添加了一个偏差项{% mathjax %}b{% endmathjax %}。
{% asset_img m_4.png  %}

这里有一个权重向量{% mathjax %}w{% endmathjax %}，它由{% mathjax %}w_1,w_2,\ldots,w_n{% endmathjax %}组成。还有一个特征矩阵用{% mathjax %}X{% endmathjax %}表示。
{% asset_img m_5.png  %}

##### 句子系统

 **组合句子**为你提供信息的方式与**组合方程**为你提供信息的方式非常相似。换句话说，**句子系统**的行为很像方程组，让我们从一些句子系统的例子开始。假设你只有一只狗和一只猫，而且 它们都只有一种颜色。你会得到一些信息，你的目标是尝试弄清楚每种动物的颜色。因此，这是带有句子的系统`1`：“狗是黑色的，猫是橙色的”。带有句子的系统`2`：“狗是黑色的，狗是黑色的”。最后，带有句子的系统`3`：“狗是黑色的， 狗是白色的”。每个句子都有一个信息。因此，诸如狗是黑色的，狗是白色或不允许的句子，它们分别包含两条信息。系统的目标是用这些简单的句子尽可能多地传达信息。请注意这些系统有很大的不同。特别是，第一个句子系统包含两个句子和两条信息。这意味着该系统包含的信息与句子一样多，这就是所谓的**完整系统**。 第二个句子系统的信息量要少一些，因为它有两个句子，但它们完全一样。 因此，尽管该系统包含两个句子，但它只携带一条信息，这些句子 它是重复的，因此该系统被称为**冗余**。最后一个句子系统，句子相互矛盾的。这是因为狗不可能同时是黑白的，记住我们有一条狗，它只能有一种颜色。因此，该系统被称为**矛盾系统**。 系统携带的信息越多，对您就越有用。当系统冗余或相互矛盾时，它被称为**单一系统**。当一个系统完整时，它被称为非单一系统。简而言之，非单一系统是一个承载与句子一样多的信息的系统。因此，它是信息量最大的系统，而**单一系统**的信息量不如非单一系统。句子系统可以包含两个以上的句子。实际上，它们可以随心所欲地携带。以下是一些包含三个句子的系统的示例。 在这个新示例中，您有三只动物，并且再次尝试确定它们的颜色。第一个系统有句子：“狗是黑色的，胡萝卜是橙色的，鸟是红色的”。第二个系统句子：“狗是黑色的，狗是黑色的，鸟是红色的”。第三个系统句子：“说狗是黑色的，狗是黑色的，狗是黑色的”。“狗是黑色的，狗是白色的，鸟是红色的”。因此，第一个句子系统是完整的，因为它使用三个句子传递三条不同的信息，因此它是**完整且非单一**的。 请注意，第三个系统比第二个系统更冗余。是否可以衡量系统的冗余程度？答案是肯定的，它叫做**等级**。

 ##### 线性方程组

 例如，如何将方程{% mathjax %}a + b = 10{% endmathjax %}可视化为一条直线？首先，让我们绘制一个网格，其中横轴代表{% mathjax %}a{% endmathjax %}，即苹果的价格，纵轴代表{% mathjax %}b{% endmathjax %}，也就是香蕉的价格。现在让我们来看看这个方程{% mathjax %}a + b =10{% endmathjax %}的解。两个显而易见的解是点{% mathjax %}(10,0){% endmathjax %}，因此{% mathjax %}a{% endmathjax %}坐标，苹果的价格为`10`。 而且{% mathjax %}b{% endmathjax %}坐标，即香蕉的价格为`0`，因为{% mathjax %}10 + 0{% endmathjax %}等于 `10`。另一个解是{% mathjax %}a{% endmathjax %}为`0`且{% mathjax %}b{% endmathjax %}等于`10`的点{% mathjax %}(0,10){% endmathjax %}。其他解{% mathjax %}(4,6){% endmathjax %}，因为{% mathjax %}4+6 = 10{% endmathjax %}。这时{% mathjax %}a{% endmathjax %}等于`4`且{% mathjax %}b{% endmathjax %}等于`6`；{% mathjax %}(8,2){% endmathjax %}其中{% mathjax %}a =8{% endmathjax %}且{% mathjax %}b =2{% endmathjax %}。请注意，您也可以使用负解，例如{% mathjax %}(-4,14){% endmathjax %}。现在注意所有这些点形成一条线。实际上，这条线中的每一个点都是方程的解。因此，您可以将方程{% mathjax %}a + b =10{% endmathjax %}与该直线相关联。现在让我们再做一个方程式。假设方程{% mathjax %}a + 2b =12{% endmathjax %}，水平坐标加上两倍垂直坐标的点加起来等于`12`。因此，这个方程的解包含点{% mathjax %}(0,6){% endmathjax %}。再一次，这些点形成一条线，线上的每个点都是这个方程的解。因此，这条直线与方程{% mathjax %}a + 2b =12{% endmathjax %}相关联。
 {% asset_img m_6.png  %}

 请注意，直线方程{% mathjax %}a + b =10{% endmathjax %}的穿过点{% mathjax %}(10,0), (0,10){% endmathjax %}和{% mathjax %}2a + 2b = 24{% endmathjax %}的穿过点{% mathjax %}(12,0),(0,12){% endmathjax %}。这两个方程非常相似，他们只不过是偏移了`2`个单位。这两个方程组与坐标平面平行的两条直线相关联，平行线永远不会相遇，所以这两个方程没有解。

 ##### 奇异/非奇异矩阵

 线性代数中最重要和最基本的对象之一—“矩阵”。矩阵有许多非常重要的属性。由下图所示，如果你取{% mathjax %}a,b{% endmathjax %}的系数，把它们放在一个{% mathjax %}2\times 2{% endmathjax %}的方框内，这个方框被称为矩阵，这就是系统对应的矩阵。与第一个系统对应的矩阵是{% mathjax %}\begin{bmatrix} 1 & 1\\ 1 & 2 \end{bmatrix}{% endmathjax %}。在矩阵中，每行对应每个方程，每列对应每个变量的系数，所以{% mathjax %}a{% endmathjax %}代表第一列，{% mathjax %}b{% endmathjax %}代表第二列。同理，第二个系统的矩阵也一样。矩阵就像线性方程组一样，也可以是奇异或非奇异的。第一个系统是非奇异的，因为他有一个独特的解；第二个系统是奇异的，因为它有无限多个解，所以它对应的矩阵是奇异矩阵。
 {% asset_img m_7.png  %}

##### 线性依赖性和独立性

无需解线性方程组即可分辨矩阵是否是奇异/非奇异矩阵，首先让我们回顾一下**句子系统**，如果第二句与第一句包含相同的信息，则句子系统是单一的。同理第二个方程与第一个方程包含相同的信息，则方程式奇异的。这是**线性依赖**的概念。如下图所示，看到两个对应的线性方程组。重点关注一下右边的方程组，第二个方程是第一个方程的倍数，所以这个方程组是奇异的，你看一下相应的矩阵，第二行是第一行的两倍，意味着第二行可以从第一行获得，因此第二行依赖于第一行，这种依赖，我们把它称为**线性依赖**。
{% asset_img m_8.png  %}

相比之下，在左边的非奇异系统中，第二个方程不是第一个方程的倍数。没有常数可以将第一个方程相乘得到第二个方程。这就是为什么系统是**非奇异**的。每个方程告诉你完全不同的东西。因此，在相应的矩阵 中，也会发生同样的事情。没有一行是另一行的倍数。我无法取一个数字然后将一行完全乘以该数字得出另一行。意味着行是**线性独立**的。你可以想象，同样的事情发生在列和行上，人们可以定义行之间和列之间的**线性依赖**，它们决定了矩阵的**奇异/非奇异性**。

例如，下图四个矩阵：
{% asset_img m_9.png  %}

- 第一个矩阵：第一行乘以三加上第二行乘以二等于第三行，因此行是线性相关的，该矩阵是奇异矩阵。
- 第二个矩阵：第一行中减去第二行，则得到第三行，因此这些行是线性依赖关系，该矩阵是奇异矩阵。
- 第三个矩阵：所有行没有关系，因此行是线性独立的，该矩阵是非奇异矩阵。
- 第四个矩阵：第一行乘以二等于第三行，因此行是线性相关的，该矩阵是奇异矩阵。

##### 行列式

让我们看一下这么两个矩阵：{% mathjax %}\begin{bmatrix} 1 & 1\\ 1 & 2 \end{bmatrix}{% endmathjax %}，{% mathjax %}\begin{bmatrix} 1 & 1\\ 2 & 2 \end{bmatrix}{% endmathjax %}，右边的矩阵，第一行乘以二获得第二行因此这些行时线性相关的，该矩阵是奇异矩阵；左边的矩阵，这些行是线性独立的，所以该矩阵是非奇异矩阵。假设矩阵的元素是`a,b,c,d`，假设存在一个常数`k`，其第一行乘以常数`k`等于第二行，则该矩阵是奇异的。则{% mathjax %}ak = c, bk = d{% endmathjax %}，则{% mathjax %}\frac{c}{a} = \frac{d}{b} = k{% endmathjax %}，去除`k`，则得出{% mathjax %}ad = bc{% endmathjax %}或{% mathjax %}ad - bc = 0{% endmathjax %}，我们将其称为矩阵的**行列式**({% mathjax %}ad - bc{% endmathjax %})。根据构造，如果矩阵是奇异矩阵，则该行列式为零；如果是非奇异矩阵，则该行列式为非零。{% mathjax %}ad{% endmathjax %}是主对角线中数字的乘积，{% mathjax %}bc{% endmathjax %}是反对角线中数字的乘积。
{% asset_img m_10.png  %}

非奇异矩阵具有非零行列式；它们是非零的，奇异矩阵的行列式为零。总而言之，元素为`a、b、c`和`d`的矩阵的行列式为{% mathjax %}ad - bc{% endmathjax %}，当矩阵为奇异矩阵时，行列式恰好为零，当矩阵为非奇异矩阵时行列式为非零。

#### 接线性方程组行梯形和秩

##### 矩阵的秩

跟着我一起看看如何定义和计算**秩**。**秩**是机器学习中的一个很好的应用（图像压缩）。看看下面这张图片，它非常清晰，但它也占用了大量存储空间，因为每个像素都必须存储为一个数字。你能否使用更少的空间存储这张图片或者稍微模糊一点的版本。答案是肯定的。即**矩阵的秩**。事实证明，像素化图像是矩阵，矩阵的**等级**与存储相应图像所需的**空间量**有关。这张特别的图片秩为`200`，相当高。有一种非常强大的**奇异值分解**(`SVD`)技术， 它可以降低矩阵的秩，同时尽可能少地对其进行更改。你可以看到此处，将排名第`200`的图像**缩减**秩为`1、2、5、15`和`50`的图片。请注意，秩为`15`和`50`的图像与原始图像非常相似，但占用的存储空间要少得多。
{% asset_img m_11.png  %}

所以回想一下，在句子系统中，有一个关于系统携带多少信息的概念，让我们看一看三种句子系统。系统`1`：“狗是黑色的，猫是橙色的”；系统`2`：“狗是黑人，狗是黑人”；系统`3`：“狗和狗会注意到”。系统`1`有两个句子，它们携带两条信息；系统`2`也有两个句子，但它们是一样的。因此，这个系统只携带一条信息；系统`3`有两个句子，但它没有关于动物颜色的信息。因此，它携带的信息为零。回想一下，你的目标是确定颜色！因此，句子系统携带的信息量被定义为系统的**秩**。系统`1`的秩为`2`、系统`2`的秩为`1`、系统`3`的秩为`0`。现在，让我们回到前面的三个方程组。正如你已经看到的，第一个系统有两个方程，每个方程都为表格带来了一些新的信息。这就是为什么你可以将解缩小到一个点。第一个方程缩小到一条直线，第二个方程将它们缩小到系统有两条信息的点。系统的秩就是这样定义的。所以秩是`2`，第二个系统有两个方程，第二个方程与第一个方程相同。因此，该系统实际上只携带一条信息，即第一个方程。这就是为什么你可以将解范围缩小到一条线的原因。因此，第二个系统的秩定义为`1`，最后第三个系统有两个方程。但是它们不携带任何信息，系统携带`0`条信息，其秩定义为`0`。现在来定义**矩阵的秩**，而矩阵的秩被定义为相应方程组的秩。因此，与第一个系统对应的矩阵秩为`2`。第二个系统对应的矩阵秩为`1`，第三个系统对应的矩阵秩为`0`。现在，矩阵的**秩**与其**解空间**之间存在一种特殊的关系。
{% asset_img m_12.png  %}

回想一下，当常数为零时，每个矩阵的**解空间**是方程组的解集。首先回想一下解只有{% mathjax %}a=0{% endmathjax %}和{% mathjax %}b=0{% endmathjax %}。这是一个点，维解空间为`0`，因为一个点的维度为`0`。对于第二个解，一组解是某条线，一条线的维度为`1`。解空间的维度为`1`。对于第三个来说，每个{% mathjax %}a{% endmathjax %}和{% mathjax %}b{% endmathjax %}在这里都有效。解空间是一个平面，其维度为`2`。请注意，第一个矩阵是非奇异矩阵，另外两个矩阵是奇异矩阵。**当且仅当矩阵具有满等级，即秩等于行数时，矩阵才是非奇异的**。这等于说，**如果一个方程组携带的信息与其所拥有的方程一样多，则方程组是非奇异的**，因此，第一个解空间的维度为`0`，因此秩为`2`，而第二个的解空间为维度`1`，因此秩为`1`。第一个矩阵是**非奇异矩阵**， 第二个矩阵是**奇异矩阵**。

##### 行阶梯形形式

**行阶梯形形式**：这为您提供了有关矩阵的大量信息，并且可以通过简单的行运算获得。如下图所示，这三个矩阵的行阶梯形形式是右边的。为了计算这个矩阵的**行阶梯形形式**，其中矩阵包含元素`5、1、4`和`-3`，您需要执行以下操作。想法是去掉左下角的`4`。首先将每一行除以最左边的非零系数，得到包含元素`1、0.2、1`和`-0.75`的矩阵。现在，为了删除左下角的`4`，保持第一行不变，但从第二行中减去第一行，得到包含`0`和`-0.95`项的行。我们成功地让左下角得到了一个零，这正是我们想要的。现在，作为最后一步，用第二行除以最左边的非零系数，以便在右下角得到一个`1`。现在矩阵是行阶梯形形式。如果你对奇异矩阵执行此操作会怎样？如果你尝试对包含`5、1、10`和`2`项的矩阵执行此操作，让我们看看会发生什么。第一步是将每行除以最左边的系数，这样你就会得到包含`1、0.2、1`和`0.2`项的矩阵。现在，为了消除左下角的那个`1`，你可以做的是取底行，减去顶行，得到的是`0，0`。这是新的底行。现在让我们看看第二行除以最左边的非零系数时会发生什么。这是不可能的，因为你会用`0`除以`0`，这是未定义的。你要做的是让这个成为行阶梯形形式。最后，对于非奇异的矩阵元素`0、0、0，0`你能做的不多，因为你不能用最左边的系数除每一行，那些系数是零。相反，你只能说这是**行阶梯形形式**。总之，这是矩阵，这是行阶梯形形式。这是一个与**秩**非常有趣的联系。看第一个。它的对角线上有两个 `1`，它的秩是`2`。下一个在对角线上有`1、1`，它的秩是`1`。第三个在对角线上有零个`1`，它的秩是`0`。这实际上就是联系。矩阵的秩是行阶梯形形式对角线上`1`的数量。这是一种计算秩的简单方法。此外，请注意，第一个是非奇异的，第二个是奇异的，第三个是奇异的。当且仅当行阶梯形形式只有`1`而没有`0`时，矩阵才是非奇异的。
{% asset_img m_13.png  %}

现在你已经了解了{% mathjax %}2\times 2{% endmathjax %}矩阵的**行阶梯形式**。回想一下这个方程组，其中第一个方程有变量`a、b`和`c`，第二个方程只有变量`b`和`c`，第三个方程只有变量`c`。可以在与系统相对应的矩阵中执行完全相同的行操作，以获得右侧的矩阵，现在对角线上有`1`，对角线下方有`0`。这是矩阵的**行阶梯形式**。
{% asset_img m_14.png  %}

这是行阶梯形式矩阵的一般形式，这里有两个不同的例子。在这里，星号代表数字，可以是零也可以是非零，这并不重要。矩阵可能有也可能没有全是零的行。但是，如果有，它们需要放在底部。此外，请注意，每个非零行都有一个最左边的非零项。这些被称为**枢轴**。每一行都有一个特定的**主元**，并且有一个关于主元的**规则**，即每个主元都必须严格位于上一行主元的右侧。换句话说，如果你站在一个主元处并查看上面的所有主元，它们都必须严格位于其左侧。现在，正如你之前所见，**行阶梯形式**对于矩阵的**秩**非常有用。它实际上是主元的数量。左侧矩阵的秩为`5`，右侧矩阵的秩为`3`。在左侧，你看到一个行阶梯形式矩阵。现在你可以做一些修饰，将第一行除以三，将第二行除以一，或者它们的含义相同，将第三行除以`-4`，得到一个如下图所示的矩阵。显然，星号现在是不同的值，但重要的是，枢轴现在是`1`，并且它们位于相同的位置，因为除以`1`会将零变成非零，或将非零变成零。一般来说，允许使用不同于`1`的枢轴。我们将采取额外的步骤，除以枢轴中的首项，以使枢轴为`1`。这在**秩**方面没有数学差异。它是相同的秩，并且与我们通过除以首项系数来求解方程组的方式更一致。
{% asset_img m_15.png  %}

简化的**行阶梯形式**，这只是一个步骤。假设您正在解决这个系统{% mathjax %}5a + b = 17,\;\;\;4a - 3b = 6{% endmathjax %}。回想一下，您要做的是从底部方程中删除变量{% mathjax %}a{% endmathjax %}，以计算{% mathjax %}b{% endmathjax %}的值。然后您要逐步替换第一个方程中的{% mathjax %}b{% endmathjax %}值以获得{% mathjax %}a{% endmathjax %}的值。然后{% mathjax %}a = 3\;\;\;b = 2{% endmathjax %}。现在按照相同的步骤，但使用相应的系数矩阵，忘记常数{% mathjax %}17{% endmathjax %}和{% mathjax %}6{% endmathjax %}，您可以使用行操作通过**行阶梯形式**，其中元素为`1、0.2`和`1`。最后，再进行一些操作，我们将得到度量，对角线上为`1`，其他地方为`0`。为什么这是与上述系统相对应的矩阵？因为你可以把方程组{% mathjax %}a = 3\;\;\;b = 2{% endmathjax %}看作一个方程组，{% mathjax %}1a + 0b = 3\;\;\;0a + 1b = 2{% endmathjax %}从中可以得出这个包含 `1001`个元素的矩阵。回想一下，中间矩阵称为**行阶梯形式**。最后一个称为**简化行阶梯形式**，是与求解系统相对应的形式。从行梯形矩阵到简化行梯形形式的方法很简单，就是使用对角线中的每个元素来消除上方所有非零元素。例如，在这里你想去掉右上角那个讨厌的`0.2`。在这种情况下，你可以保持底行不变，从第一行中减去第二行的`0.2`，这样你就得到了`0`，从第一行中减去`0.2`得到第`[1,0]`行，这就是你的新行，矩阵的一行，这就是简化行阶梯形式。所简化的行梯形矩阵通常看起来是这样的。
{% asset_img m_16.png  %}

这里有两个例子。第一条规则是它必须是行梯形。此外，每个主元必须是`1`，主元上方的任何数字都必须是`0`，这是主要区别。它与行梯形属性相同，**即矩阵的秩实际上是主元的数量**。左边的这个矩阵的秩为`5`，右边的矩阵的秩为`3`。这是从行梯形矩阵转换为简化行梯形矩阵的一般方法。假设你有一个矩阵，这些是主元，它们可以不是`1`，第一个`1`乘以`3`，第二个`1`乘以`2`，第三个`1`减`4`，得到右边这个主元为`1`的矩阵。现在，得到简化**行梯形矩阵**，你所要做的就是用每一个`1`清除它上面的任何数字。例如，如果在`1`上面有一个`5`，你所要做的就是将`1`所在行乘以`5`，然后从前一个数字中减去`1`。这里有一个小例子。假设这是矩阵的行梯形矩阵，我们要把它变成简化行梯形矩阵。首先要去掉第一行中的`2`，为此我们可以从第一行中减去第二行的`2`来去掉它，这会改变一些数字，但至少会把它变成零。现在，我们去掉负`5`，将第三行乘以`5`并将其添加到第一行，这样就去掉了`-5`。现在要去掉那个`4`，将第三行乘以`4`并从第一行中减去。乘以`4`，再从第一行中减去，这样就得到了矩阵的简化行阶梯形式。

##### 高斯消除算法

如下图所示，假设有以下方程组：
{% asset_img m_17.png  %}

回想一下，当你了解矩阵的奇异性时，你会忽略方程右侧的常数值。这些方程的常数为：{% mathjax %}1,-2,-1{% endmathjax %}，你可以将它们视为`0`。
|`Step`|`Description`|`Computing Graph`|
|:---|:---|:---|
|步骤一|首先，根据方程组的系数创建一个矩阵。矩阵的右侧添加一列，该列包含常数值：`1、-2、-1`，称为**增广矩阵**，这里可以用增广矩阵来求解方程组。回想一下，为了完成消元法，您需要在矩阵的对角线上反复找到一个称为**枢轴**的元素。|{% asset_img m_18.png %}|
|步骤二|首先，您需要将第一行的枢轴变为`1`。由于枢轴当前为`2`，因此将行乘以{% mathjax %}\frac{1}{2}{% endmathjax %}。经过行操作后，枢轴现在为`1`，并且行中的其他每个值也除以`2`。将{% mathjax %}R_1{% endmathjax %}替换为您刚刚计算的行。|{% asset_img m_19.png %}<br>{% asset_img m_20.png %}|
|步骤三|接下来您要使用行运算将主元以下的所有值设置为`0`。我将从第`2`行中的`2`开始。主元是`1`，您要取消的值是`2`。如果从第`2`行中减去`2`倍第`1`行，您将得到主元下方的`0`。完成此行运算后，您将得到一个新的行：`0、3、3、-3`，然后使用它来更新第`2`行。|{% asset_img m_21.png %}|
|步骤四|同样，由于枢轴是`1`，因此选择行操作。您只需从第`3`行中减去第`1`行的`4`倍即可抵消枢轴。完成此行操作将为第`3`行生成一组新的值。|{% asset_img m_22.png %}<br>{% asset_img m_23.png %}|
|步骤五|第一列是`1`，后面跟着两个`0`。您可以继续处理第二列和第二个基准点。沿对角线移动，新的基准点是`3`。与之前一样，您需要将基准点设置为`1`，然后将基准点下方的值设置为`0`。由于新的基准点是`3`，因此将行乘以`1/3`即可将基准点设置为1。再次，更新方程组以匹配矩阵中的新值。|{% asset_img m_24.png %}<br>{% asset_img m_25.png %}|
|步骤六|现在你需要将第三行的第二个元素变为`0`。由于你的枢轴已经是`1`，从第3行中减去第`2`行的`3`倍将抵消`3`并得到`0`。完成此操作将得到新行，即`0，0，-5，0`。再次，更新方程组以匹配矩阵中的新值。|{% asset_img m_26.png %}<br>{% asset_img m_27.png %}|
|步骤七|将对角线上的`-5`作为最终的枢轴。与之前一样，您需要将枢轴设置为`1`，因此将第三行除以`-5`。因此，第`3`行的最终值将是`0、0、1、0`。再次更新方程组。请注意，矩阵为行阶梯形。对角线全为`1`，对角线下方只有`0`。|{% asset_img m_28.png %}<br>{% asset_img m_29.png %}|
|步骤八|从最后一行的枢轴开始，然后取消其上方的`1`。枢轴是`1`，您需要取消`1`，因此行操作将是第二行减去第三行。经过此行操作后，第二行的新值将为`0、1、0、-1`。|{% asset_img m_30.png %}|
|步骤九|接下来，在第一行中取消{% mathjax %}\frac{1}{2}{% endmathjax %}。这次你需要的行运算是第一行减去第三行的{% mathjax %}\frac{1}{2}{% endmathjax %}。完成这一行运算后，第一行的新值为：`1/2、0、1/2`。现在，你将得到以下矩阵，其中包含更新后的方程组|{% asset_img m_31.png %}<br>{% asset_img m_32.png %}|
|步骤十|最后重复该过程，在这种情况下，您将第`2`行的{% mathjax %}\frac{1}{2}{% endmathjax %}添加到第`1`行。您就完成了这个矩阵。请注意，其对角线上全为`1`，而其他所有位置上全为`0`。再次更新方程组以表示矩阵，得到了方程的解，其中{% mathjax %}a=0,\;b=-1,\;c=0{% endmathjax %}。|{% asset_img m_33.png %}<br>{% asset_img m_34.png %}|

{% note warning %}
**请注意**，**增广矩阵**的平方部分在对角线上只有1。这样的矩阵称为**单位矩阵**，通过使用**高斯消元法**将矩阵简化为这种形式，您就解了原始方程组。现在让我们讨论奇异情况。如果矩阵是**奇异**的，高斯消元法会起作用吗？您已经知道，如果矩阵是奇异的，那么在**简化行阶梯形式**中，您将得到一行全为0。**高斯消元法**的整个目的是找到方程组的解。但是，如果你找到一行0，你就知道你的矩阵是奇异的，没有解。也就是说，你仍然可以确定你的矩阵是矛盾的，没有解，还是有无数个解。
{% endnote %}

要做到这一点，你只需要查看常数列。如果零行中的常数值也是`0`，则该行只是表示：{% mathjax %}0a + 0b + oc = 0{% endmathjax %}。无论你为{% mathjax %}a,b,c{% endmathjax %}选择什么值，左边总是等于`0`，这个等式就是正确的。所以这个系统有无数个解。
{% asset_img m_35.png %}

如果方程组稍有变化,如下图所示，第三个方程等于`10`，结果会怎样？在行减少之后，您将得到以下矩阵，其中第三行中的常数值变为`4`。现在最后一行表示：{% mathjax %}0a + 0b + oc = 4{% endmathjax %}。无论你为{% mathjax %}a,b,c{% endmathjax %}选择什么值，该等式的左边都将等于`0`，但右边等于`4`。这意味着系统没有解。
{% asset_img m_36.png %}

回顾一下，如果你在行轴形式中找到一整行`0`，并且该行中的常数为`0`，则系统有无穷多个解。但是，如果该常数不为`0`，则系统没有解。高斯消元法的回顾，首先，通过将常数添加到右侧的新列来创建**增广矩阵**。接下来，获取**简化行阶梯形式**的矩阵。最后，完成回代以找到右侧的值。完成回代以找到变量的值。

#### 向量代数

在很多情况下，**特征和目标之间的关系是非线性的**。**神经网络**是用于表示非线性的强大的机器学习模型之一。神经网络最令人惊奇的一点是，在底层，它们实际上只是一大堆线性模型。你可能见过像这样表示的神经网络，它们有垂直方向的层，即**神经元**，它们都通过这样的线连接起来。你可以这样想这个图所表示的内容：左边的这一层代表你对网络的输入，也就是你的**特征**。{% mathjax %}x_1,x_2,\ldots,x_n{% endmathjax %}。然后这些线表示你将把所有这些{% mathjax %}x{% endmathjax %}值发送到下一层中的每个单独的神经元。例如，所有这些{% mathjax %}x{% endmathjax %}都发送到第一个神经元，该神经元有一个{% mathjax %}w{% endmathjax %}值向量和一个与之关联的{% mathjax %}b{% endmathjax %}值。你可以把第一个神经元中发生的内容表示为：{% mathjax %}wx + b{% endmathjax %}。你有一个和之前一样的线性模型。事实上，你有一组线性方程。但现在它全部包含在这个小神经元中。可以把这个神经元中的线性方程表示为：{% mathjax %}wx + b{% endmathjax %}。这个神经元中真正发生的内容是：“你将整个{% mathjax %}wx + b{% endmathjax %}塞入**激活函数**中”。该激活函数生成一些输出，我在这里将其称为{% mathjax %}a{% endmathjax %}，它是另一个向量。然后，层中的每个神经元都执行完全相同的操作，但在每种情况下都有一组不同的**权重**和**偏差**，并生成自己的输出（一个向量）。为了明确每个神经元都是独一无二的，我将为所有的{% mathjax %}w,b,a{% endmathjax %}添加下标，其中下标`1`表示您位于顶部的第一个神经元中，同样，向下移动也是如此。当然，您通过每个神经元传递的{% mathjax %}x{% endmathjax %}值矩阵在每种情况下都是相同的。这不需要任何下标。我将在这里的方括号中添加一个小上标`1`，以表示这些操作发生在网络的第一层。这是因为接下来发生的事情是将这些{% mathjax %}a{% endmathjax %}值传递到下一层，然后这个过程再次产生。现在在第二层中，输入是来自第一层的{% mathjax %}a{% endmathjax %}值。这里第二层的线性模型看起来相同，只是您有{% mathjax %}a{% endmathjax %}而不是{% mathjax %}x{% endmathjax %}。需要明确的是，这些上标`1`现在是包含前一层的所有`a`向量的矩阵，即{% mathjax %}a_1,a_2,\ldots, a_n{% endmathjax %}。将这些向量乘以一组全新的**权重**，并在每种情况下添加不同的**偏差值**。{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}以及生成的新{% mathjax %}a{% endmathjax %}值都得到上标括号`2`，以指示它们属于第二层。然后，当您向前传播通过网络的每一层时，所有这些都会重复，将前一层的输出乘以一组权重，添加偏差项，并应用激活函数，直到获得最终输出。您无需写下无数个小线性方程来表示神经网络，而是将每层的输入和输出表示为**向量、矩阵**和**张量**进行操作，并计算您的机器学习结果。
{% asset_img m_37.png %}

