---
title: 机器学习(ML)(十三) — 推荐系统探析
date: 2024-10-29 10:44:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 召回 - 双塔模型

训练双塔模型需要正样本和负样本，选对正、负样本大于改进模型结构。**选择正样本**：如果物品给用户曝光之后，会有点击行为，就说明用户对物品感兴趣。把用户和物品二元组作为作为正样本，但是选取正样本有个问题需要解决，就是少部分物品占据了大部分点击，正样本是有点击的物品，导致正样本属于热门物品。拿过多的热门物品作为正样本，会对冷门物品不公平，这样会使热门物品更热，冷门物品更冷。解决方案是：对冷门物品过采样，或降采样热门物品。**过采样**(`up-sampling`)：一个样本出现多次；降采样(`down-sampling`)：一些样本被抛弃，以一定概率抛弃一些样本。抛弃的概率与样本的点击次数正相关。
<!-- more -->

**选择负样本**：无样本就是用户不感兴趣的物品，没有被召回的物品是负样本；召回了，但是没有被选中和曝光是负样本；曝光了，但是没有被用户点击的也是负样本。负样本分类：
- **简单负样本**：未被召回的物品，大概率是用户不感兴趣的，几亿个物品中只有几千个物品被召回，所以从全体物品中做抽样就可以了，抽到的物品作为负样本。问题在于怎样做抽样？均匀抽样还是非均匀抽样？均匀抽样的坏处是对冷门物品不公平，如果在全体物品中做均匀抽样产生负样本，负样本大多是冷门物品。总拿热门物品做正样本，冷门物品做负样本，这会使热门物品更热，冷门物品更冷。所以负样本采用**非均匀采样**，目的是打压热门物品，负样本抽样的概率与热门物品（点击次数）正相关。热门物品成为负样本的概率大，物品的热门程度可以用点击次数来衡量。可以这样做抽样：每个物品的抽样概率正比于点击次数的`0.75`次方，`0.75`是一个经验值。
- `Batch`**内负样本**：设一个`batch`内有{% mathjax %}n{% endmathjax %}个正样本，那么一个用户可以跟{% mathjax %}n-1{% endmathjax %}个样品组成负样本。那么这个`batch`内有{% mathjax %}n(n-1){% endmathjax %}个负样本，这些都是简单负样本（因为第一个用户不喜欢第二个物品）。对于第一个用户来说第二个物品相当于从全体物品随机抽样的，第一个用户大概率不会喜欢第二个物品，`batch`负样本存在一个问题，(用户, 物品)这个二元组都是通过点击行为选取的，第一个用户和第一个物品之所以成为正样本，原因是用户点击了物品，所以一个物品出现在`batch`内的概率正比于点击次数。也就是它的热门程度，物品成为负样本的概率应该是正比于点击次数的0.75次方，但这里却是正比于点击次数1次方，也就是说热门物品成为负样本的概率过大，这样会造成偏差。修正偏差方案是参考论文：[`Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations`](https://storage.googleapis.com/gweb-research2023-media/pubtools/5716.pdf)，假设物品{% mathjax %}i{% endmathjax %}被抽样到的概率记作{% mathjax %}p_i{% endmathjax %}，则{% mathjax %}p_i\varpropto \text{点击次数}{% endmathjax %}，反映出物品的热门程度，双塔模型通常用于计算相似度，预估用户对物品{% mathjax %}i{% endmathjax %}的兴趣分数：{% mathjax %}\cos(a,b_i){% endmathjax %}，其中{% mathjax %}a{% endmathjax %}是用户的特征向量，{% mathjax %}b_i{% endmathjax %}是物品的特征向量，训练的时候要尽量鼓励正样本的余弦相似度大，鼓励负样本的余弦相似度小。根据上面论文中的建议：训练双塔模型的时候应该把{% mathjax %}\cos(a,b_i){% endmathjax %}调整为{% mathjax %}\cos(a,b_i) - \log p_i{% endmathjax %}，这样可以纠偏。避免过分打压热门的物品，训练结束之后，在线上做召回时还是使用{% mathjax %}\cos(a,b_i){% endmathjax %}，线上做召回不用做调整。
- **困难负样本**：是被排序淘汰的样本，比如物品被召回，但是被粗排淘汰，例如召回`5000`个物品进行粗排，粗排按照分数做截断，只保留前`500`个物品，那么被淘汰的`4500`个物品都可以被视作负样本，为什么被粗排淘汰的负样本叫做困难负样本呢？这些物品被召回，说明它们跟用户的兴趣多少有些关系，被粗排淘汰，说明用户对物品的兴趣不够强烈，所以被分为了负样本，这些正、负样本做二元分类的话，这些困难负样本容易被分错，容易被错误的判定为正样本，更困难的负样本是通过了粗排，但是精排分数靠后的物品，比方说精排给`500`个物品打分，排名在后`300`个物品都是负样本，能够通过粗排进入精排，说明物品已经比较符合用户兴趣了，但未必是用户最感兴趣的，所以在精排中排名靠后的物品视为负样本。训练双塔模型其实是一个二元分类任务，让模型区分正负样本。把全体物品作为简单负样本，则分类准确率会很高。因为它们明显跟用户兴趣不符，被粗排淘汰的物品也是负样本，但它们多少跟用户兴趣有些相关，所以分类比较困难，分类准确率会稍微低一些。精排分类靠后的物品也是负样本，这些物品跟正样本有些相似，所以它们很容易判定为正样本，对它们做分类非常困难，比较常用的做法是把简单负样本和困难负样本混合起来作为训练数据。比如`50%`（全体物品中随机非均匀抽样）是简单负样本，另外`50%`（从粗排和精排淘汰的物品中随机抽样）是困难负样本。
{% note warning %}
**常见的错误**：可以把曝光但没有点击的物品作为负样本，这是错误的。用双塔模型去召回训练，效果肯定会变差。训练召回模型不能用这样的负样本，训练排序模型会用这类负样本。
{% endnote %}

**选择负样本的原理**：召回的目的是快速找到用户可能感兴趣的物品。凡是用户感兴趣全部取回来，然后再交给后面的排序模型逐一做甄别，召回模型的任务是区分用感兴趣的物品和不感兴趣的物品，而不是区分比较感兴趣的物品和非常感兴趣的物品。这就是选择负样本的基本思路。
- **全体物品**(`easy`)：可以把全体物品当做负样本，把它们叫做**简单负样本**，这些物品绝大多数都是用户不感兴趣的，双塔模型很容易区分这些负样本；
- **被排序淘汰**(`hard`)：被召回但被粗排或精排淘汰的叫做**困难负样本**，这些物品能被召回说明它们跟用户的兴趣有一定的相关性，被排序模型过滤掉，说明它们跟用户的兴趣不够强烈，他们可以作负样本，这样的负样本跟正样本有点相似，做分类的时候难以区分，所以算是**困难负样本**。
- **有曝光没点击**：看起来可以作为负样本，但其实不能，只要用了这样的负样本，双塔模型的效果肯定会变差，一个物品可以通过精排模型的甄别并曝光给用户，说明物品已经非常匹配用户的兴趣点，每次给用户展示几十个物品，用户不可能每个都点击，没有点击不代表不感兴趣，所以不应该把**有曝光没点击**的物品作为召回的负样本，他只适合训练排序模型，不适合训练召回模型。

如下图所示，这是训练好的两个塔，它们分别提取用户、物品特征，在开始双塔训练之后，线上服务之前，先用右边物品的塔提取物品的特征，把物品特征向量记作{% mathjax %}\vec{b}{% endmathjax %}，这里有多少个物品就有多少个向量（几亿），把<特征向量{% mathjax %}\vec{b}{% endmathjax %}, 物品`ID`>保存到向量数据库，向量数据库存储物品向量和物品`ID`的二元组，用作最近邻查找，左边的用户塔不要事先计算并存储向量，而是当用户线上发起推荐请求的时候，调用神经网络在线上计算一个向量{% mathjax %}\vec{a}{% endmathjax %}，然后把向量作为`query`去数据库中做检索，查找最近邻，也就是跟向量{% mathjax %}\vec{a}{% endmathjax %}相似度最高的{% mathjax %}k{% endmathjax %}个红色向量，每个红色向量对应一个物品，`k`-近邻查找一共召回了{% mathjax %}k{% endmathjax %}个物品，作为这条召回通道的结果返回。
{% asset_img ml_1.png "左边是物品向量b离线存储，右边用户向量a线上召回" %}

**双塔模型的召回**：离线存储，把物品向量{% mathjax %}\vec{b}{% endmathjax %}存入向量数据库；线上召回，查找用户最感兴趣的{% mathjax %}k{% endmathjax %}个物品，给定用户ID和画像，线上用神经网络算用户向量{% mathjax %}\vec{a}{% endmathjax %}，然后最近邻查找（把向量{% mathjax %}\vec{a}{% endmathjax %}作为`query`，调用向量数据库做最近邻查找，数据库返回余弦相似度最大的{% mathjax %}k{% endmathjax %}个物品，作为召回结果。）。

事先存储物品向量{% mathjax %}\vec{b}{% endmathjax %}，线上现算用户向量{% mathjax %}\vec{a}{% endmathjax %}，为什么要区别对待物品向量和用户向量呢？没做一次召回只用到一个向量{% mathjax %}\vec{a}{% endmathjax %}，而要用到几亿物品向量{% mathjax %}\vec{b}{% endmathjax %}，拿神经网络现算一个用户向量，计算量不大，但是几亿个物品向量显然是算不起的。所以不得不离线算好物品向量，那能不能把几亿个用户向量提前算好并存储起来，进一步减少线上负担呢？这样做当然可以，早期的时候就是这样做的。这样工程实现很简单，但这样不利于推荐的效果。用户的兴趣点是动态变化的，所以应该线上实时计算用户向量，而不是事先计算好存储起来。但是事先存储物品向量是可以的，是因为物品特征相对稳定，短期内不会发生变化。

模型的更新包括模型的**全量更新**和模型的**增量更新**：
 - **全量更新**：每天凌晨，用昨天全天的数据训练模型，而不是随机初始化。把昨天`1`天的数据去打包成`TFRecord`文件，在昨天模型参数的基础上做训练。把昨天的数据过一遍，每条数据只用`1`次，也就是训练只做`1 epoch`。训练完成之后，发布新的用户塔（是在线上实时计算用户向量，作为召回的`query`）神经网络和物品向量（几亿个向量存入向量数据库，向量数据库会重新建索引，在线上可以做最近邻查找），全量更新的实现相对比较容易，对数据流、整个系统的要求不高，全量更新不需要实时的数据流，对训练生成的速度没有要求。只需要把每天的数据落表，在凌晨做个批处理，把数据打包成`TFRecord`文件就可以，**全量更新**对系统的要求也很低，每天做一次**全量更新**，所以只需要把神经网络和物品向量每天发布一次就够了。
 - **增量更新**：做`online learning`更新模型参数，每隔几十分钟就把新的模型参数发布出去。为什么要做**增量更新**呢？这是因为用户的兴趣会随时发生变化，**增量更新**对数据流的要求比较高，要实时收集线上数据做流式处理，实时生成训练模型需要的`TFRecord`文件，然后对模型做`online learning`，做**梯度下降**`ID Embedding`的参数，也就是从早到晚，训练数据文件不断生成，不断做梯度下降更新模型的`Embedding`层，注意`online learning`不更新神经网络其他部分的参数，全连接层的参数都是锁住的，不做**增量更新**，只更新`Embedding`层的参数，只有做**全量更新**的时候，才会更新全连接层。模型更新之后，再把算出的用户`ID Embedding`发布出去，用户的`ID Embedding`是一个哈希表的形式，给定用户`ID`，可以查出`ID Embedding`向量。发布用户`ID Embedding`的目的是为了线上计算用户向量。最新的用户`ID Embedding`能够捕捉到用户最新的兴趣点，对推荐很有帮助。发布用户`ID Embedding`的过程会有延迟，通过对系统做优化，可以将延迟降低到几十分钟或者更短。如下图所示：
 {% asset_img ml_2.png "全量更新 vs 增量更新" %}
 
 为什么只做**增量更新**不好呢？如果你只看一个小时的数据它是有偏的，分钟级数据差别更大。在不同的时间段用户的行为是不一样的，比如中午和傍晚的数据明显不一致，如果你只看`5`分钟的数据，那么偏差就更大了，它跟全天数据的统计值差别更大，做全量更新数据的时候，要随机排列数据，也就是`random shuffle`，这样就是为了消除偏差，全量更新是在`random shuffle`数据上做，而增量更新是按照数据从早到晚的顺序上做训练。同样使用一天的数据，这两种排列顺序的方式会导致训练的效果有差别，把数据按照从早到晚的顺序进行排列，效果不如把数据随机打乱，所以全量训练的效果比增量训练更好，这就是为什么我既要做全量训练又要做增量训练。全量训练的模型更好，增量训练能够实时捕捉用户的兴趣。

**双塔模型**，如下图所示，左边的用户塔，右边是物品塔：
{% asset_img ml_3.png "双塔模型结构" %}

**自监督学习**的目的是把物品塔训练的更好，为什么要做**自监督学习**呢？在实际的推荐系统模型中，数据上的问题会影响双塔模型的表现，推荐系统都有**头部效应**：少部分的物品占据了大部分的点击，大部分的物品点击次数不高。训练双塔模型的时候，用点击数据作为正样本，模型学习物品的表征，靠的就是点击行为。如果一个物品给几千个用户曝光，有好几百个用户点击，那么物品的表征就会学的比较好，反过来，长尾物品的表征就学的不够好，这是因为长尾物品的点击和曝光次数太少，训练的样本数量不够，一种比较好的方法就是**自监督学习**。对物品做`data augmentation`，可以更好的学习长尾物品的**向量表征**。请参考文献：[`Self-supervised Learning for Large-scale Item Recommendations`](https://arxiv.org/pdf/2007.12865)。

推导**损失函数**：现在考虑`batch`内第{% mathjax %}i{% endmathjax %}个用户，第{% mathjax %}n{% endmathjax %}个物品，这{% mathjax %}n{% endmathjax %}个数值分别是{% mathjax %}\cos(\vec{a}_i,\vec{b}_1),\cos(\vec{a}_i,\vec{b}_2),\ldots,\cos(\vec{a}_i,\vec{b}_i),\ldots,\cos(\vec{a}_i,\vec{b}_n){% endmathjax %}，把这{% mathjax %}n{% endmathjax %}个数值输入`softmax`激活函数，得到{% mathjax %}n{% endmathjax %}个概率值，记作{% mathjax %}p_{i,1},p_{i,2},\ldots,p_{i,i},\ldots,p_{i,n}{% endmathjax %}。这里的{% mathjax %}\vec{a}_i,\vec{b}_i{% endmathjax %}组成一对正样本，如果双塔模型的预估足够准确，那么{% mathjax %}\cos(\vec{a}_i,\vec{b}_i){% endmathjax %}应该比其它{% mathjax %}n-1{% endmathjax %}个**余弦相似度**大很多，`softmax`的输出值{% mathjax %}p_{i,i}{% endmathjax %}应该接近于`1`，把这{% mathjax %}n{% endmathjax %}个概率值记作向量{% mathjax %}\vec{p}_i{% endmathjax %}，上面的{% mathjax %}n{% endmathjax %}个数值是标签，全都是`0`，只有{% mathjax %}p_{i,i}{% endmathjax %}对应的是标签是`1`，它对应正样本，把这{% mathjax %}n{% endmathjax %}个标签记作向量{% mathjax %}\vec{y}_i{% endmathjax %}，{% mathjax %}\vec{y}_i{% endmathjax %}只有第{% mathjax %}i{% endmathjax %}个元素是`1`，其它都是`0`。做训练的时候我们希望向量{% mathjax %}\vec{p}_i{% endmathjax %}尽量接近{% mathjax %}\vec{y}_i{% endmathjax %}，{% mathjax %}\vec{p}_i{% endmathjax %}越接近{% mathjax %}\vec{y}_i{% endmathjax %}说明双塔模型的预估越准确，用{% mathjax %}\vec{y}_i{% endmathjax %}和{% mathjax %}\vec{p}_i{% endmathjax %}的交叉熵作为损失函数({% mathjax %}\text{CrossEntropyLoss}(\vec{y}_i,\vec{p}_i) = -\log(p_{i,i}){% endmathjax %})。把{% mathjax %}p_{i,i}{% endmathjax %}替换成`softmax`函数输出的第{% mathjax %}i{% endmathjax %}个数值，就得到{% mathjax %}\text{CrossEntropyLoss}(\vec{y}_i,\vec{p}_i) = -\log(p_{i,i}) = -\log(\frac{\exp(\cos(a_i,b_j))}{\sum_{j=1}^n \exp(\cos(a_i,b_j))}){% endmathjax %}，这就是`listwise`训练双塔模型的损失函数。训练的时候要最小化损失函数。如下图所示：
{% asset_img ml_4.png "listwise训练双塔模型的损失函数" %}

`Batch`**内负样本**会过度打压热门物品，造成偏差，如果使用`Batch`**内负样本**就需要做**纠偏**。物品{% mathjax %}j{% endmathjax %}被抽样到的概率为：{% mathjax %}p_j \varpropto \text{点击次数}{% endmathjax %}，预估用户{% mathjax %}i{% endmathjax %}对物品{% mathjax %}j{% endmathjax %}的兴趣为：{% mathjax %}\cos(a_i,b_j){% endmathjax %}，做训练的时候，把{% mathjax %}\cos(a_i,b_j){% endmathjax %}替换为{% mathjax %}\cos(a_i,b_j) - \log p_i{% endmathjax %}，这样起到纠偏的作用，热门物品不至于过分打压，训练结束，线上做召回的时候还是用原本的余弦相似度({% mathjax %}\cos(a_i,b_j){% endmathjax %})，训练双塔模型，每次要用一个`batch`的样本，从点击数据中随机抽取{% mathjax %}n{% endmathjax %}个用户-物品二元组，组成一个`batch`，则`batch`训练双塔模型的损失函数为{% mathjax %}L_{\text{main}}[i] = -\log(\frac{\exp(\cos(a_i,b_j)) - \log p_i}{\sum_{j=1}^n \exp(\cos(a_i,b_j)) - \log p_j}){% endmathjax %}，它对应`batch`内第{% mathjax %}i{% endmathjax %}个用户，双塔模型预测的越准，损失函数也就越小。训练双塔模型的时候要做梯度下降，减小损失函数({% mathjax %}\frac{1}{n}\sum_{i=1}^n L_{\text{main}}[i]{% endmathjax %})是{% mathjax %}n{% endmathjax %}项的平均，序号{% mathjax %}i{% endmathjax %}指的是第{% mathjax %}i{% endmathjax %}个用户，{% mathjax %}n{% endmathjax %}的意思是`batch`中一共有{% mathjax %}n{% endmathjax %}个用户。

**自监督学习**：下面是两个物品{% mathjax %}i{% endmathjax %}和物品{% mathjax %}j{% endmathjax %}，对两个物品做随机变换，得到物品的特征{% mathjax %}i'{% endmathjax %}和{% mathjax %}j'{% endmathjax %}，对两个物品做另一种变换，得到特征为{% mathjax %}i''{% endmathjax %}和{% mathjax %}j''{% endmathjax %}。把这些特征输入物品塔，模型一共只有一个物品塔，物品塔输出的向量分别是{% mathjax %}b^'_i, b^{''}_i{% endmathjax %}，它们都是物品{% mathjax %}i{% endmathjax %}的向量表征，但是由于下边的随机变换导致这两个向量不完全相等，两个向量是同一个物品的表征，如果物品塔足够好，两个向量应该有很高的相似度，训练的时候会鼓励两个向量的余弦相似度尽量的大。右边的物品塔输出的向量分别是{% mathjax %}b^'_j, b^{''}_j{% endmathjax %}，他们都是物品{% mathjax %}j{% endmathjax %}的向量表征，同样的道理，两个向量应该有很高的相似度。但是不同物品的向量表征应该离的尽量远，也就是这些向量应该分散开，而不是集中在一小块区域。向量{% mathjax %}b^'_i, b^'_j{% endmathjax %}对应两个不同的物品，它们的相似度应该比较低才对。做训练的时候，应该降低向量{% mathjax %}b^'_i, b^'_j{% endmathjax %}的相似度。如下图所示：
{% asset_img ml_5.png %}

**自监督学习**会使用多种**特征变换**，它们分别是：
- `Random Mask`：随机选择一些离散特征（比如类目），把它们遮住。比方说选出类目这个特征，例如某物品的类目特征是向量{% mathjax %}u = \{\text{数码},\text{摄影}\}{% endmathjax %}。一个物品可以有多个类目，如果不做`Random Mask`，正常的特征处理方法是对数码和摄影分别做`Embedding`得到两个向量，再取加和或者平均，最终输出一个向量，表征物品的类目。如果对类目特征做`Mask`后，这物品的类目特征就变成了{% mathjax %}u^' = \{\text{defult}\}{% endmathjax %}，意思是默认的缺失值，然后对{% mathjax %}\text{default}{% endmathjax %}做`Embedding`，得到一个向量表征类目，也就是说做`Mask`后，物品的类目特征直接丢弃，数码和摄影都没了。
- `Dropout`(仅对多值离散特征生效)：