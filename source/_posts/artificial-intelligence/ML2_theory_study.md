---
title: 机器学习(ML)(二) — 探析
date: 2024-08-25 17:45:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 梯度下降

我们看到了**成本函数**{% mathjax %}\mathbf{J}{% endmathjax %}的可视化，以及如何尝试选择不同的参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}。如果我们有一种更系统的方法来找到{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的值，从而得到{% mathjax %}w、b{% endmathjax %}的最小成本{% mathjax %}\mathbf{J}{% endmathjax %}。事实证明，有一种称为**梯度下降**的算法可实现这一点。**梯度下降**在机器学习中随处可见，不仅用于**线性回归**，还用于训练一些最先进的神经网络模型，也称为**深度学习模型**。
<!-- more -->

此函数不是**平方误差成本函数**。对于具有平方误差成本函数的**线性回归**，您总是会得到弓形或吊床形。但如果您训练神经网络模型，您可能会得到这种类型的成本函数。注意轴，即底部轴上的 {% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}。对于不同的{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}值，你会得到这个曲面上的不同点，{% mathjax %}w,b{% endmathjax %}的{% mathjax %}\mathbf{J}{% endmathjax %}，其中曲面在某个点的高度是**成本函数**的值。现在，让我们想象一下，这个曲面图实际上是一个略微丘陵的户外公园，其中高点是山丘，低点是山谷，就像这样。想象一下，你现在正站在山上的这个点上。如果这能帮助你放松，想象一下有很多非常漂亮的绿草、蝴蝶和鲜花，这是一座非常漂亮的山。你的目标是从这里出发，尽可能高效地到达其中一个山谷的底部。**梯度下降算法**的作用是，你要旋转`360`度，环顾四周，问自己，如果我要朝一个方向迈出一小步，我想尽快下坡到其中一个山谷。我会选择朝哪个方向迈出这一小步？如果你想尽可能高效地走下这座山，那么如果你站在山上的这个点上环顾四周，你会发现，你下一步下山的最佳方向大致就是那个方向。从数学上讲，这是下降速度最快的方向。意味着，当你迈出一小步时，这会比你朝其他方向迈出一小步的速度更快。迈出第一步后，你现在就站在山上的这个点上。现在让我们重复这个过程。站在这个新的点上，你将再次旋转`360`度，问自己，下一步我要朝哪个方向迈出一小步才能下山？如果你这样做，再迈出一步，你最终会朝那个方向移动一点，然后你就可以继续走下去了。从这个新的点开始，你可以再次环顾四周，决定哪个方向可以让你最快地下山。再走一步，再走一步，等等，直到你发现自己到了山谷的底部，到了这个局部最小值，就在这里。你刚才做的是经过**多步梯度下降**。**梯度下降**有一个有趣的特性。你可以通过选择参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}的起始值来选择表面的起点。刚才执行**梯度下降**时，你是从这里的这个点开始的。现在，想象一下，如果你再次尝试梯度下降，但这次你选择一个不同的起点，通过选择将你的起点放在这里右边几步的参数。如果你重复梯度下降过程，这意味着你环顾四周，朝着最陡峭的上升方向迈出一小步，你就会到达这里。然后你再次环顾四周，再迈出一步，依此类推。如果你第二次运行**梯度下降**，从我们第一次执行的位置的右​​边几步开始，那么你最终会到达一个完全不同的山谷。右边这个不同的最小值。第一个和第二个山谷的底部都称为**局部最小值**。因为如果你开始沿着第一个山谷向下走，梯度下降不会带你到第二个山谷，同样，如果你开始沿着第二个山谷向下走，你会停留在第二个最小值，而不会找到进入第一个局部最小值的路。
{% asset_img ml_1.png %}
