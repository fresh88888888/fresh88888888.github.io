---
title: 机器学习(ML)(十六) — 推荐系统探析
date: 2024-11-11 16:10:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 重排 - 多样性算法(DPP)

**行列式点过程**(`determinantal point process, DPP`)是一种**概率模型**，最早引入于**量子物理学**中，用于描述**费米子系统**的分布。`DPP`的核心思想是能够有效地从一个全集中抽取出具有高相关性和**多样性**的子集，广泛应用于推荐系统、机器学习等领域，`DPP`是目前推荐系统重排多样性公认最好的多样性算法。
<!-- more -->

在`2`维空间中，**超平行体**就是平行四边形。如下图所示，向量{% mathjax %}\vec{v}_1,\vec{v}_2{% endmathjax %}是平行四边形的两条边，它们唯一确定了这个平行四边形，平行四边形中的点都可以表示为：{% mathjax %}x = \alpha_1\vec{v}_1 + \alpha_2\vec{v}_2{% endmathjax %}，其中{% mathjax %}\alpha_1{% endmathjax %}和{% mathjax %}\alpha_2{% endmathjax %}是系数，取值范围是`[0,1]`，举例这里有一个平行四边形中的点{% mathjax %}x{% endmathjax %}，记作{% mathjax %}x = \frac{1}{2}\vec{v}_1 + \frac{1}{2}\vec{v}_2{% endmathjax %}，这个点落在平行四边形的中心；还有一个点刚好落在品行四边形的边界上，记作{% mathjax %}x = \vec{v}_1 + \vec{v}_2{% endmathjax %}，也就是说系数{% mathjax %}\alpha_1,\alpha_2{% endmathjax %}的值都是`1`。
{% asset_img ml_1.png %}

3维空间的超平行体为**平行六面体**。如下图所示，向量{% mathjax %}\vec{v}_1,\vec{v}_2,\vec{v}_3{% endmathjax %}是平行六面体的`3`条边，它们唯一确定了一个平行六面体，平行六面体中的点都可以表示为：{% mathjax %}x = \alpha_1\vec{v}_1 + \alpha_2\vec{v}_2 + \alpha_3\vec{v}_3{% endmathjax %}，其中系数{% mathjax %}\alpha_1,\alpha_2,\alpha_3{% endmathjax %}的取值范围是`[0,1]`。
{% asset_img ml_2.png %}

**超平行体**：一组向量{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k\in \mathbb{R}^d{% endmathjax %}可以确定一个{% mathjax %}k{% endmathjax %}维超平形体：{% mathjax %}\mathcal{P}(\vec{v}_1,\ldots,\vec{v}_k) = \{\alpha_1\vec{v}_1 + \ldots + \alpha_k\vec{v}_k| 0\leq \alpha_1,\ldots,\alpha_k\leq 1\}{% endmathjax %}。超平行体的边{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k\in \mathbb{R}^d{% endmathjax %}都是{% mathjax %}d{% endmathjax %}维向量，{% mathjax %}k{% endmathjax %}是向量的数量，{% mathjax %}d{% endmathjax %}是向量的维度，要求{% mathjax %}k\leq d{% endmathjax %}，比如{% mathjax %}d = 3{% endmathjax %}空间中有{% mathjax %}k=2{% endmathjax %}维平行四边形。如果想让超平行体有意义，那么{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k{% endmathjax %}就必须线性独立，如果{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k{% endmathjax %}线性相关（某个向量可以表示为其余向量的加权和），则体积{% mathjax %}\text{vol}(\mathcal{P}) = 0{% endmathjax %}（例：有{% mathjax %}k=3{% endmathjax %}个向量，落在一个平面上，则平行六面体的体积为`0`）。

**平行四边形的面积**：{% mathjax %}\text{面积} = ||\text{底}||_2 \times ||\text{高}||_2{% endmathjax %}，如下图所示，向量{% mathjax %}\vec{v}_1{% endmathjax %}为底，计算高{% mathjax %}\vec{q}_2{% endmathjax %}，这两个向量必须正交，两个向量长度的乘积就是平行四边形的面积。假如给定向量{% mathjax %}\vec{v}_1,\vec{v}_2{% endmathjax %}，把{% mathjax %}\vec{v}_1{% endmathjax %}作为底，该如何计算高{% mathjax %}\vec{q}_2{% endmathjax %}？可以计算向量{% mathjax %}\vec{v}_2{% endmathjax %}在{% mathjax %}\vec{v}_1{% endmathjax %}方向上的投影：{% mathjax %}\text{Proj}_{\vec{v}_1}(\vec{v}_2) = \frac{\vec{v}_1^{\mathsf{T}}\vec{v}_2}{||\vec{v}_1||^2_2}\cdot \vec{v}_1{% endmathjax %}。用{% mathjax %}\vec{v}_2{% endmathjax %}减去刚才得到的投影，就得到{% mathjax %}\vec{q}_2 = \vec{v}_2 - \text{Proj}_{\vec{v}_1}(\vec{v}_2){% endmathjax %}。计算的结果满足于：底{% mathjax %}\vec{v}_1{% endmathjax %}与高{% mathjax %}\vec{q}_2{% endmathjax %}正交。同样可以以{% mathjax %}\vec{v}_2{% endmathjax %}为底，计算{% mathjax %}\vec{q}_1{% endmathjax %}，首先计算向量{% mathjax %}\vec{v}_1{% endmathjax %}在{% mathjax %}\vec{v}_2{% endmathjax %}方向上的投影{% mathjax %}\text{Proj}_{\vec{v}_2}(\vec{v}_1) = \frac{\vec{v}_2^{\mathsf{T}}\vec{v}_1}{||\vec{v}_2||^2_2}\cdot \vec{v}_2{% endmathjax %}。计算出向量与{% mathjax %}\vec{v}_2{% endmathjax %}的方向相同，用{% mathjax %}\vec{v}_1{% endmathjax %}减去刚才得到的投影，就得到{% mathjax %}\vec{q}_1 = \vec{v}_1 - \text{Proj}_{\vec{v}_2}(\vec{v}_1){% endmathjax %}。计算的结果满足于：底{% mathjax %}\vec{v}_2{% endmathjax %}与高{% mathjax %}\vec{q}_1{% endmathjax %}正交。不管以谁为底，计算出的平行四边形的面积都是相同的，
{% asset_img ml_3.png %}

**平行六面体的体积**：{% mathjax %}\text{体积} = \text{底面积} \times ||\text{高}||_2{% endmathjax %}，以{% mathjax %}\vec{v}_1,\vec{v}_2{% endmathjax %}为边，组成平行四边形，这个平行四边形{% mathjax %}\mathcal{P}(\vec{v}_1,\vec{v}_2){% endmathjax %}是平行六面体{% mathjax %}\mathcal{P}(\vec{v}_1,\vec{v}_2,\vec{v}_3){% endmathjax %}的底，向量{% mathjax %}q_3{% endmathjax %}平行六面体的高，高{% mathjax %}q_3{% endmathjax %}垂直于{% mathjax %}\mathcal{P}(\vec{v}_1,\vec{v}_2){% endmathjax %}，用底面积乘以{% mathjax %}q_3{% endmathjax %}的长度，就得到平行六面体的体积。假设固定向量{% mathjax %}\vec{v}_1,\vec{v}_2,\vec{v}_3{% endmathjax %}的长度，那么平行六面体的体积何时最大化、最小化？设{% mathjax %}\vec{v}_1,\vec{v}_2,\vec{v}_3{% endmathjax %}都是**单位向量**，也就是说它们的二范数都等于`1`，当`3`个向量正交时，平行六面体是正方体，此时的体积最大，{% mathjax %}\text{vol} = 1{% endmathjax %}。如果3个向量线性相关，也就是说某个向量可以表示为其他向量的加权和，那么此时的体积就最小，{% mathjax %}\text{vol} = 0{% endmathjax %}。
{% asset_img ml_4.png %}

这里给定{% mathjax %}k{% endmathjax %}个物品，把它们表征为单位向量{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k\in \mathbb{R}^d{% endmathjax %}，它们的二范数都等于`1`，之前说过向量最好用`CLIP`学习出的图文内容表征，这些向量都是{% mathjax %}d{% endmathjax %}维的，必须{% mathjax %}d\geq k{% endmathjax %}，可以用超平行体的体积来衡量物品的多样性，体积介于`0~1`之间。如果向量{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k{% endmathjax %}两两正交（多样性好），此时的体积最大，{% mathjax %}\text{vol} = 1{% endmathjax %}；如果向量{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k{% endmathjax %}线性相关（多样性差），此时的体积最小，{% mathjax %}\text{vol} = 0{% endmathjax %}。给定{% mathjax %}k{% endmathjax %}个物品单位向量的表征{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k\in \mathbb{R}^d{% endmathjax %}，把它们作为矩阵{% mathjax %}\mathbf{V} = \in \mathbb{R}^{d\times k}{% endmathjax %}的列，设{% mathjax %}d\geq k{% endmathjax %}，行列式与体积满足：{% mathjax %}\text{det}(\mathbf{V}^{\mathsf{T}}\mathbf{V}) = \text{vol}(\mathcal{P}(\vec{v}_1,\ldots,\vec{v}_k))^2{% endmathjax %}。因此可以用行列式{% mathjax %}\text{det}(\mathbf{V}^{\mathsf{T}}\mathbf{V}){% endmathjax %}衡量向量{% mathjax %}\vec{v}_1,\ldots,\vec{v}_k{% endmathjax %}的**多样性**。多样性越好，体积和行列式都会越大。矩阵{% mathjax %}\mathbf{V}{% endmathjax %}如下图所示：
{% asset_img ml_5.png %}

**精排**给{% mathjax %}n{% endmathjax %}个候选物品打分，把融合之后的分数记为：{% mathjax %}\text{reward}_1,\ldots,\text{reward}_n{% endmathjax %}，它们表示物品的价值。每个物品还有一个向量表征，通常是基于图文内容计算出的向量，把这{% mathjax %}n{% endmathjax %}个向量记作{% mathjax %}v_1,\ldots,v_n\in \mathbb{R}^d{% endmathjax %}，在精排的后处理阶段，也就是重排阶段，需要从{% mathjax %}n{% endmathjax %}个物品中选出{% mathjax %}k{% endmathjax %}个，组成集合{% mathjax %}\mathcal{S}{% endmathjax %}，做选择要考虑两个因素：一个是物品价值大，分数之和{% mathjax %}\sum_{j\in\mathcal{S}}\text{reward}_j{% endmathjax %}越大越好。第二个是物品的多样性好，把集合{% mathjax %}\mathcal{S}{% endmathjax %}中的{% mathjax %}k{% endmathjax %}个向量组成的超平行体{% mathjax %}\mathcal{P}(\mathcal{S}){% endmathjax %}的体积越大越好，体积越大就说明多样性越好。集合{% mathjax %}\mathcal{S}{% endmathjax %}中的{% mathjax %}k{% endmathjax %}个向量作为列，组成矩阵{% mathjax %}\mathbf{V}_{\mathcal{S}}\in \mathbb{R}^{d\times k}{% endmathjax %}，矩阵{% mathjax %}\mathbf{V}_{\mathcal{S}}{% endmathjax %}的形状是{% mathjax %}d\times k{% endmathjax %}，以这{% mathjax %}k{% endmathjax %}个向量为边，组成超平行体{% mathjax %}\mathcal{P}(\mathcal{S}){% endmathjax %}，超平行体的体积记作{% mathjax %}\text{vol}(\mathcal{P}(\mathcal{S})){% endmathjax %}，它可以衡量集合{% mathjax %}mathcal{S}{% endmathjax %}中物品的**多样性**。多样性越好，则体积越大。如果物品的数量{% mathjax %}k\leq d{% endmathjax %}，则行列式与体积满足：{% mathjax %}\text{det}(\mathbf{V}^{\mathsf{T}}_{\mathcal{S}}\mathbf{V}_{\mathcal{S}}) = \text{vol}(\mathcal{P}(\mathcal{S}))^2{% endmathjax %}。这说明行列式和体积是等价的，因此可以用行列式衡量向量的多样性多样性越好，则行列式也就越大。

**行列式点过程**(`determinantal point process, DPP`)是一种传统的统计机器学习方法：{% mathjax %}\underset{\mathcal{S}:|\mathcal{S}| = k}{\text{argmax}}\;\log\text{det}(\mathbf{V}^{\mathsf{T}}_{\mathcal{S}}\mathbf{V}_{\mathcal{S}}){% endmathjax %}。它可以度量集合{% mathjax %}\mathcal{S}{% endmathjax %}中向量的多样性，`DPP`要求从{% mathjax %}n{% endmathjax %}个候选物品中选出{% mathjax %}k{% endmathjax %}个，组成集合{% mathjax %}\mathcal{S}{% endmathjax %}，使得行列式的对数最大化，`Hulu`的论文[`Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity`](https://arxiv.org/pdf/1709.05135)将`DPP`应用在**推荐系统**，记作：{% mathjax %}\underset{\mathcal{S}:|\mathcal{S}| = k}{\text{argmax}}\ \theta\cdot (\sum_{j\in\mathcal{S}}\text{reward}_j) + (1 -\theta)\cdot\text{det}(\mathbf{V}^{\mathsf{T}}_{\mathcal{S}}\mathbf{V}_{\mathcal{S}}){% endmathjax %}。从{% mathjax %}n{% endmathjax %}个物品选出{% mathjax %}k{% endmathjax %}个，组成集合{% mathjax %}\mathcal{S}{% endmathjax %}，这一项{% mathjax %}\mathbf{V}^{\mathsf{T}}_{\mathcal{S}}\mathbf{V}_{\mathcal{S}}{% endmathjax %}，用行列式的对数度量集合{% mathjax %}\mathcal{S}{% endmathjax %}的多样性，把这一项记作矩阵{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}，形状为{% mathjax %}k\times k{% endmathjax %}，设矩阵{% mathjax %}\mathbf{A}{% endmathjax %}为{% mathjax %}n\times n{% endmathjax %}的矩阵，它的第{% mathjax %}i{% endmathjax %}行第{% mathjax %}j{% endmathjax %}列的元素为{% mathjax %}a_{ij} = v_i^{\mathsf{T}}v_j{% endmathjax %}（内积），给定向量{% mathjax %}v_1,\ldots,v_n\in \mathbb{R}^d{% endmathjax %}，它们的大小都是{% mathjax %}d{% endmathjax %}维，计算{% mathjax %}\mathbf{A}{% endmathjax %}的时间复杂度为{% mathjax %}\mathcal{O}(n^2d){% endmathjax %}，上面定义了{% mathjax %}\mathbf{A}_{\mathcal{S}} = \mathbf{V}^{\mathsf{T}}_{\mathcal{S}}\mathbf{V}_{\mathcal{S}}{% endmathjax %}，{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}是{% mathjax %}\mathbf{A}{% endmathjax %}的一个{% mathjax %}k\times k{% endmathjax %}的子矩阵。如果物品{% mathjax %}i,j\in \mathcal{S}{% endmathjax %}，{% mathjax %}a_{ij}{% endmathjax %}是{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}的一个元素。`DPP`的公式可以等价于写成：{% mathjax %}\underset{\mathcal{S}:|\mathcal{S}| = k}{\text{argmax}}\ \theta\cdot (\sum_{j\in\mathcal{S}}\text{reward}_j) + (1 -\theta)\cdot\log\text{det}(\mathbf{A}_{\mathcal{S}}){% endmathjax %}。`DPP`是个组合优化问题，要求从集合{% mathjax %}\{1,\ldots,n\}{% endmathjax %}中选出一个大小为{% mathjax %}k{% endmathjax %}的子集{% mathjax %}\mathcal{S}{% endmathjax %}，精确求解`DPP`是不可能的，因为`DPP`是一个`NP-hard`问题，用集合{% mathjax %}\mathcal{S}{% endmathjax %}表示已选中的物品，用集合{% mathjax %}\mathcal{R}{% endmathjax %}表示未选中的物品，通常会用贪心算法近似求解：{% mathjax %}\underset{i\in\mathcal{R}}{\text{argmax}}\ \theta\cdot\text{reward}_i + (1 -\theta)\cdot\log\text{det}(\mathbf{A}_{\mathcal{S}\cup\{i\}}){% endmathjax %}。{% mathjax %}\text{reward}_i {% endmathjax %}是物品{% mathjax %}i{% endmathjax %}的价值，{% mathjax %}\log\text{det}(\mathbf{A}_{\mathcal{S}\cup\{i\}}){% endmathjax %}是行列式的对数，{% mathjax %}\mathbf{A}_{\mathcal{S}\cup\{i\}}{% endmathjax %}是{% mathjax %}\mathbf{A}{% endmathjax %}的一个子矩阵，这个子矩阵比{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}多了一行和一列。每一轮选择一个物品{% mathjax %}i{% endmathjax %}，这个物品既要有较高的价值，也不能跟已经选中的物品相似。

求解`DPP`的方法：
- **贪心算法求解**：{% mathjax %}\underset{i\in\mathcal{R}}{\text{argmax}}\ \theta\cdot\text{reward}_i + (1 -\theta)\cdot\log\text{det}(\mathbf{A}_{\mathcal{S}\cup\{i\}}){% endmathjax %}，从集合{% mathjax %}\mathcal{R}{% endmathjax %}中选出一个物品，对于集合{% mathjax %}\mathcal{R}{% endmathjax %}中所有的物品{% mathjax %}i{% endmathjax %}需要计算这样的行列式({% mathjax %}\log\text{det}(\mathbf{A}_{\mathcal{S}\cup\{i\}}){% endmathjax %})，算法的难点就在于计算行列式，简单粗暴计算行列式，则计算量非常大。对于单个物品{% mathjax %}i{% endmathjax %}，计算{% mathjax %}\log\text{det}(\mathbf{A}_{\mathcal{S}\cup\{i\}}){% endmathjax %}的时间复杂度为{% mathjax %}\mathcal{O}(|\mathcal{S}^3|){% endmathjax %}。想要求解上边的公式，需要对集合{% mathjax %}\mathcal{R}{% endmathjax %}中所有物品{% mathjax %}i{% endmathjax %}求行列式，因此计算行列式的时间复杂度为{% mathjax %}\mathcal{O}(|\mathcal{S}^3|\cdot |\mathcal{R}|){% endmathjax %}，我们想要从{% mathjax %}n{% endmathjax %}个物品中选出{% mathjax %}k{% endmathjax %}个物品，所以要重复求解上面的公式{% mathjax %}k{% endmathjax %}次，如果暴力计算行列式，那么总的时间复杂度为：{% mathjax %}\mathcal{O}(|\mathcal{S}^3|\cdot |\mathcal{R}|\cdot k) = \mathcal{O}(nk^4){% endmathjax %}，暴力算法的总时间复杂度为：{% mathjax %}\mathcal{O}(dn^2 + nk^4){% endmathjax %}。{% mathjax %}dn^2{% endmathjax %}是计算矩阵{% mathjax %}\mathbf{A}{% endmathjax %}的时间，{% mathjax %}nk^4{% endmathjax %}是计算行列式的时间。{% mathjax %}n{% endmathjax %}的量级是几百，{% mathjax %}k,d{% endmathjax %}的量级都是几十。系统留给算法的时间也就是`10`毫秒左右，这个算法他慢了。
- `Hulu`**的快速算法**：`Hulu`的论文设计了一种数值算法，仅需{% mathjax %}\mathcal{O}(dn^2 + nk^2){% endmathjax %}的时间从{% mathjax %}n{% endmathjax %}个物品中选出{% mathjax %}k{% endmathjax %}个物品。给定向量{% mathjax %}v_1,\ldots,v_n\in \mathbb{R}^d{% endmathjax %}，需要{% mathjax %}\mathcal{O}(dn^2){% endmathjax %}计算矩阵{% mathjax %}\mathbf{A}{% endmathjax %}，因为算法运行过程中，需要不断用到{% mathjax %}\mathbf{A}{% endmathjax %}的子矩阵，还需要{% mathjax %}\mathcal{O}(nk^2){% endmathjax %}的时间计算所有行列式（利用了`Cholesky`分解）。`Hulu`的算法比暴力算法快。矩阵的`Cholesky`分解：给定一个对称的矩阵{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}，可以把它分解为{% mathjax %}\mathbf{A}_{\mathcal{S}} = \mathbf{L}\mathbf{L}^{\mathsf{T}}{% endmathjax %}，其中的{% mathjax %}\mathbf{L}{% endmathjax %}是一个下三角矩阵（对角线以上的元素全都等于`0`），有了`Cholesky`分解就可以计算矩阵{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}的行列式，下三角矩阵{% mathjax %}\mathbf{L}{% endmathjax %}的行列式{% mathjax %}\text{det}(\mathbf{L}){% endmathjax %}等于{% mathjax %}\mathbf{L}{% endmathjax %}对角线元素乘积。矩阵{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}的行列式等于{% mathjax %}\text{det}(\mathbf{A}_{\mathcal{S}}) = \text{det}(\mathbf{L})^2 = \prod_i l_{ii}^2{% endmathjax %}。基本思想是这样的：已知矩阵{% mathjax %}\mathbf{A}_{\mathcal{S}} = \mathbf{L}\mathbf{L}^{\mathsf{T}}{% endmathjax %}的`Cholesky`分解，给{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}添加一行或者一列不需要重新计算`Cholesky`分解，否则代价很大，给{% mathjax %}\mathbf{A}_{\mathcal{S}}{% endmathjax %}添加一行或者一列，`Cholesky`分解的变化非常小，有办法快速算出变化的地方，这样就不用完整算一遍`Cholesky`分解，那么就可以快速算出所有{% mathjax %}\mathbf{A}_{\mathcal{S}\cup\{i\}}{% endmathjax %}的`Cholesky`分解，因此就可以快速算出所有{% mathjax %}\mathbf{A}_{\mathcal{S}\cup\{i\}}{% endmathjax %}的行列式，这样时间复杂度就很低。

#### 物品冷启动 - 评价指标

什么是**物品冷启动**？例如小红书上用户新发布的笔记、`B`站或`YouTube`上用户新上传的视频、今日头条上作者新发布的文章，这些都属于**物品冷启动**。这里只考虑`UGC`(`User-Generated Content`)的冷启，内容都是用户自己上传的。与`UGC`相对应的`PGC`(`Plant-Generated Content`)，主要内容是像腾讯视频、爱奇艺平台采购的，`UGC`比`PGC`的冷启更难，这是因为用户自己上传的内容良莠不齐，而且量很大，很难用人工去评判，很难让运营人员去做流量调控。

**新物品冷启动**：新物品缺少与用户的交互，很难根据用户的行为做推荐，这会导致推荐的难度大、效果差。如果用正常的推荐链路，新物品很难得到曝光。即使得到曝光，效果也不好，消费指标也会很差。特殊对待新物品的另一个原因是促进发布，大量的实验表明，扶持新发布、低曝光的物品，可以增强作者的发布意愿。出现首次曝光的交互越快，有利于作者的积极性，新物品获得的曝光越多，也有利于作者的积极性。

**优化冷启的目标**：
- **精准推荐**：把新物品推荐给合适的用户，不引起用户的反感。
- **激励发布**：流量向新物品低曝光倾斜，激励作者发布，丰富内容池。
- **挖掘高潜**：通过初期小流量的试探，找到高质量的物品，给与流量倾斜。

**冷启动评价指标**：作者侧指标：发布渗透率(`penetration rate`)、人均发布量；用户侧指标：新物品的点击率、交互率、消费时长、日活、月活；内容侧指标：高热物品占比。{% mathjax %}\text{发布渗透率} = \text{当日发布人数} / \text{日活人数}{% endmathjax %}，{% mathjax %}\text{人均发布量} = \text{当日发布物品数} / \text{日活人数}{% endmathjax %}，发布渗透率(`penetration rate`)、人均发布量这两个指标反映出了作者的发布积极性。**物品冷启动的优化**包括：**优化全链路**（包括召回和排序）；**流量调控**（流量怎样在新物品、老物品中分配）。

#### 物品冷启动 - 简单召回通道

物品召回的依据包括：自带文字、图片、地点，算法或人工标注的标签，用户点击、点赞等信息，但是新物品缺少用户点击、点赞等信息。用户对物品的点击、点赞等统计数据可以反映出物品的质量，以及什么样的用户会喜欢这个物品，这对精准推荐的帮助会很大。可惜新物品没有这些信息，而且`ItemCF、UserCF`需要知道之类的召回通道需要知道物品跟哪些用户有些交互。如果一个物品还没有跟用户交互，就走不了`ItemCF`这种召回通道，物品冷启动缺少的另外一个关键信息是物品`ID Embedding`，召回和排序模型都有`Embedding`层，把每个物品`ID`映射到一个向量，这个向量是从用户跟物品的交互行为中学习出来的，物品冷启的时候这个向量是刚刚初始化的，还没有用反向传播更新，也就是说，新物品的`ID Embedding`啥都不是，反映不出物品的特点，缺少这个特征会是召回和排序很不准。

**冷启动改造双塔模型**：物品ID是物品塔中最重要的特征，每个物品都有一个`ID Embedding`向量，需要从用户和物品的交互中学习，可是新物品还没有跟用户交互过，所以它的`ID Embedding`向量还没有背学好，如果用双塔模型直接做新物品的召回，效果会不太好。介绍一下，改造`ID Embedding`的`2`种方案：
- **方案一**：新物品使用`default embedding`，也就是说，物品塔做`ID Embedding`的时候，让所有新物品共享一个`ID`，而不是用自己真正的`ID`。新物品发布之后，到下次模型训练的时候，新物品才具有自己`ID Embedding`向量。
- **方案二**：利用相似物品的`ID Embedding`向量，当新物品发布之后，查找`Top-K`内容最相似（图片、文字）的高爆物品，把{% mathjax %}k{% endmathjax %}个高爆物品的`ID Embedding`向量取平均，作为新物品的`ID Embedding`。之所以选择高曝光的物品，通常是因为高曝光的物品的`ID Embedding`通常学的比较好。

在实践中，通常会用多个向量召回池，用多个召回池可以让新物品有更多的曝光机会，所有这些召回池会共享一个双塔模型，那么多个召回池不增加训练模型的代价。**基于类目的召回通道**：一般推荐系统会维护一份从**类目->物品**的索引，索引上的key是类目，比如美食、旅游、摄影等。每个类目后边是一个物品的列表，按发布时间倒排，最新发布的物品排在最前面。系统要维护这样一个类目索引，系统用类目索引做召回：用户画像-> 类目-> 物品列表。取回物品列表上前{% mathjax %}k{% endmathjax %}个物品；**基于关键词召回**：原理跟基于类目的召回通道是一样的，系统需要维护一个关键词索引：**关键词-> 物品列表**(按时间倒排)，给用户做推荐的时候，根据用户画像上的**关键词**做召回。跟类目召回唯一的区别是用关键词代替类目，这两种召回通道都有很明显的缺点：1、只对刚刚发布的新物品有效，基于类目的索引、关键词索引都是根据发布时间倒排，发布的新物品排在最前面，并取回某类目或关键词下最新的{% mathjax %}k{% endmathjax %}个物品。发布之后时间比较长的物品，就再也没有机会被召回了；2、弱个性化、不够精准。

#### 物品冷启动 - 聚类召回

