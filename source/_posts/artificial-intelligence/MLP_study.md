---
title: 多层感知机 (非线性神经网络) (PyTorch)
date: 2024-05-08 12:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 隐藏层

回想一下`softmax`回归的模型架构。该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行`softmax`操作。如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。但是，仿射变换中的线性是一个很强的假设。例如，线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正），或者导致模型输出的减小（如果对应的权重为负）。例如，如果我们试图预测一个人是否会偿还贷款。我们可以认为，在其它条件不变的情况下，收入较高的申请人比收入较低的申请人更有可能偿还贷款。但是，虽然收入与还款概率存在单调性，但是它们不是线性相关的。收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。处理这一问题的一种方法是对我们的数据进行预处理，使线性变得更合理，如使用收入的对数作为我们的特征。然而我们可以很容易找出违反单调性的例子。
<!-- more -->
例如，我们想要根据体温预测死亡率。对于体温高于37摄氏度的人来说，温度越高风险越大。然而对于体温低于37摄氏度的人来说，温度越高风险就越低。在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。例如，我们可以使用与37摄氏度的距离作为特征。但是，如何对猫和狗的图像进行分类呢？增加位置`(13,17)`处像素的强度是否总是增加（或降低）图像描绘狗的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。与我们前面的例子相比，这里的线性很荒谬，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。
##### 网络架构

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能够处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起，每一层都输出到上面的层，知道生成最后的输出。我们可以把钱{% mathjax %}L-1{% endmathjax %}层看做表示，把最后一层看作是线性预测器。这种架构通常称为**多层感知机**(`multilayer perceptron`)，通常缩写为`MLP`。下面，我们以图的方式描述了多层感知机：
{% asset_img mlp_1.png "一个单隐藏层的多层感知机，具有5个隐藏单元" %}

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2.注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。具有全连接层的多层感知机的参数开销可能会高的令人望而却步。即使不改变输入或输出大小的情况下，可能在参数节约和模型有效性之间进行平衡。

同之前一样，我们通过矩阵{% mathjax %}\mathbf{X}\in \mathbb{R}^{n\times d}{% endmathjax %}来表示{% mathjax %}n{% endmathjax %}个样本的小批量，其中每个样本具有{% mathjax %}d{% endmathjax %}个输入特征。对于具有{% mathjax %}h{% endmathjax %}个隐藏单元的但隐藏层多层感知机，用{% mathjax %}\mathbf{H}\in \mathbb{R}^{n\times h}{% endmathjax %}表示隐藏层的输出，称为隐藏表示(`hidden representations`)。在数学或代码中，{% mathjax %}\mathbf{H}{% endmathjax %}也被称为隐藏层变量(`hidden-layer variable`)因为隐藏层和输出层都是全连接的，所以我们有隐藏层权重{% mathjax %}\mathbf{W}^{(1)}\in \mathbb{R}^{d\times h}{% endmathjax %}和隐藏层偏置{% mathjax %}\mathbf{b}^{(1)}\in \mathbb{R}^{1\times h}{% endmathjax %}以及输出层权重{% mathjax %}\mathbf{W}^{(2)}\in \mathbb{R}^{1\times q}{% endmathjax %}和输出层偏置{% mathjax %}\mathbf{b}^{(2)}\in \mathbb{R}^{1\times q}{% endmathjax %}。在形式上，我们按如下方式计算但隐藏层多层感知机的输出{% mathjax %}\mathbf{O}\in \mathbb{R}^{n\times q}{% endmathjax %}：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{align}
& \mathbf{H} = \mathbf{XW}^{(1)} + \mathbf{b}^{(1)} \\
& \mathbf{O} = \mathbf{HW}^{(2)} + \mathbf{b}^{(2)} \\
\end{align}
{% endmathjax %}
注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！原因很简单：上面的隐藏单元由输入的仿射函数给出，而输出（`softmax`操作前）只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数，但是我们之前的线性模型已经能够表示任何仿射函数。我们可以证明这一等价性，即对于任意权重值，我们只需合并隐藏层，便可产生具有参数{% mathjax %}\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}{% endmathjax %}和{% mathjax %}\mathbf{b}=\mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}{% endmathjax %}的等价单层模型。
{% mathjax '{"conversion":{"em":14}}' %}
\mathbf{O} = (\mathbf{XW}^{(1)}+b^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{XW}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{XW} + \mathbf{b}
{% endmathjax %}
为了f发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性激活函数(`activation function`){% mathjax %}\sigma{% endmathjax %}。激活函数的输出（例如，{% mathjax %}\sigma(\cdot){% endmathjax %}）被称为活性值(`activations`)。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{align}
& \mathbf{H} = \sigma(\mathbf{XW}^{(1)} + \mathbf{b}^{(1)}) \\
& \mathbf{O} = \mathbf{HW}^{(2)} + \mathbf{b}^{(2)} \\
\end{align}
{% endmathjax %}
由于{% mathjax %}\mathbf{X}{% endmathjax %}中的每一行对应于小批量中的每一个样本，出于记号习惯的考量，我们定义非线性函数{% mathjax %}\sigma{% endmathjax %}也以按行的方式作用于其输入，记一次计算一个样本。以相同的方式使用`softmax`符号来表示按行操作。但是应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。这意味着，在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如：{% mathjax %}\mathbf{H}^{(1)} = \sigma_1(\mathbf{XW}^{(1)} + \mathbf{b}^{(1)}){% endmathjax %}和{% mathjax %}\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}){% endmathjax %}一层叠一层，从而产生更有表达能力的模型。

多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的。神经网络有点像`C`语言。`C`语言和任何其他现代编程语言一样，能够表达任何可计算的程序。但实际上，想出一个符合规范的程序才是最困难的部分。而且，虽然一个单隐层网络能学习任何函数，但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。

##### 激活函数

激活函数(`activation function`)通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。

###### ReLU函数

最受欢迎的激活函数是修正线性单元（`Rectified linear unit，ReLU`），因为它实现简单，同时在各种预测任务中表现良好。`ReLU`提供了一种非常简单的非线性变换。给定元素{% mathjax %}x{% endmathjax %}，`ReLU`函数被定义为该元素与{% mathjax %}0{% endmathjax %}的最大值：
{% mathjax '{"conversion":{"em":14}}' %}
ReLU(x) = \text{max}(x,0)
{% endmathjax %}
通俗地说，`ReLU`函数通过将相应的活性值设为`0`，仅保留正元素并丢弃所有负元素。为了直观感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。
```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
plt.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```
{% asset_img mlp_2.png %}

当输入为负时，`ReLU`函数的导数为`0`，而当输入为正时，`ReLU`函数的导数为`1`。 注意，当输入值精确等于`0`时，`ReLU`函数不可导。在此时，我们默认使用左侧的导数，即当输入为`0`时导数为`0`。我们可以忽略这种情况，因为输入可能永远都不会是`0`。这里引用一句古老的谚语，“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”，这个观点正好适用于这里。下面我们绘制`ReLU`函数的导数。
```python
y.backward(torch.ones_like(x), retain_graph=True)
plt.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```
{% asset_img mlp_3.png %}

使用`ReLU`的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且`ReLU`减轻了困扰以往神经网络的梯度消失问题。注意，`ReLU`函数有许多变体，包括参数化`ReLU（Parameterized ReLU，pReLU）`函数，该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：
{% mathjax '{"conversion":{"em":14}}' %}
p\mathbf{ReLU}(x) = \text{max}(x,0) + \alpha\text{min}(0,x)
{% endmathjax %}
###### sigmoid函数

对于一个定义域在{% mathjax %}\mathbb{R}{% endmathjax %}中的输入，`sigmoid`函数将输入变换为区间(0, 1)上的输出。因此，sigmoid通常称为挤压函数（`squashing function`）：它将范围(`-inf, inf`)中的任意输入压缩到区间(`0, 1`)中的某个值：
{% mathjax '{"conversion":{"em":14}}' %}
\text{sigmoid}(x) = \frac{1}{1+\text{exp}(-x)}
{% endmathjax %}
在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。阈值单元在其输入低于某个阈值时取值`0`，当输入超过阈值时取值`1`。当人们逐渐关注到到基于梯度的学习时，`sigmoid`函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二元分类问题的概率时，`sigmoid`仍然被广泛用作输出单元上的激活函数（`sigmoid`可以视为`softmax`的特例）。然而，`sigmoid`在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的`ReLU`所取代。下面，我们绘制`sigmoid`函数。注意，当输入接近0时，`sigmoid`函数接近线性变换。
```python
y = torch.sigmoid(x)
plt.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```
{% asset_img mlp_4.png %}

`sigmoid`函数的导数为下面的公式：
{% mathjax '{"conversion":{"em":14}}' %}
\frac{d}{dx}\text{sigmoid}(x) = \frac{\text{exp}(-x)}{(1 + \text{exp}(-x))^2} = \text{sigmoid}(x)(1-\text{sigmoid}(x))
{% endmathjax %}
`sigmoid`函数的导数图像如下所示。注意，当输入为0时，`sigmoid`函数的导数达到最大值`0.25`；而输入在任一方向上越远离`0`点时，导数越接近`0`。
```python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
plt.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```
{% asset_img mlp_5.png %}

###### tanh函数

与`sigmoid`函数类似，`tanh`(双曲正切)函数也能将其输入压缩转换到区间(`-1, 1`)上。`tanh`函数的公式如下：
{% mathjax '{"conversion":{"em":14}}' %}
\text{tanh}(x) = \frac{1-\text{exp}(-2x)}{1+\text{exp}(-2x)}
{% endmathjax %}
下面我们绘制`tanh`函数。注意，当输入在`0`附近时，`tanh`函数接近线性变换。函数的形状类似于`sigmoid`函数，不同的是`tanh`函数关于坐标系原点中心对称。
```python
y = torch.tanh(x)
plt.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```
{% asset_img mlp_6.png %}

`tanh`函数的导数是：
{% mathjax '{"conversion":{"em":14}}' %}
\frac{d}{dx}\text{tanh}(x) = 1-\text{tanh}^2(x)
{% endmathjax %}
`tanh`函数的导数图像如下所示。当输入接近`0`时，`tanh`函数的导数接近最大值`1`。与我们在`sigmoid`函数图像中看到的类似，输入在任一方向上越远离`0`点，导数越接近`0`。
```python
# 清除以前的梯度
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
plt.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```
{% asset_img mlp_7.png %}

##### 总结

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。常用的激活函数包括`ReLU`函数、`sigmoid`函数和`tanh`函数。

#### 模型选择、欠拟合和过拟合

我们的目标是发现**模式**(`pattern`)。但是，我们如何才能确定模型是真正发现了一种泛化模式，而不是简单的记住了数据呢？例如，我们想要在患者的基因数据与痴呆状态之间寻找模式，其中标签是从集合{痴呆,轻度认知障碍,健康}中提取的。因为基因可以唯一确定每个个体（不考虑双胞胎），所以在这个任务中是有可能记住这个数据集。我们不想让模型只会做这样的事情：“那是鲍勃！我记得他！他有痴呆症！”。原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。只有当模型真正发现了一种泛化模式时，才会作出有效的预测。

更正式地说，我们的目标是发现某些模式，这些模式捕捉到了我们训练集潜在总体的规律。如果成功做到了这点，即使是对以前从未遇到过的个体，模型也可以成功地评估风险。如何发现可以泛化的模式是机器学习的根本问题。困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。最大的公开图像数据集包含大约一百万张图像。而在大部分时候，我们只能从数千或数万个数据样本中学习。在大型医院系统中，我们可能会访问数十万份医疗记录。当我们使用有限的样本时，可能会遇到这样的问题：当收集到更多的数据时，会发现之前找到的明显关系并不成立。将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合(`overfitting`)，用于对抗过拟合的技术称为正则化(`regularization`)。

##### 训练误差和泛化误差

为了进一步讨论这一现象，我们需要了解**训练误差**和**泛化误差**。**训练误差**(`training error`)是指，模型在训练数据集上计算得到的误差。**泛化误差**(`generalization error`)是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。问题是，我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。

下面的三个思维实验将有助于更好地说明这种情况。假设一个大学生正在努力准备期末考试。一个勤奋的学生会努力做好练习，并利用往年的考试题目来测试自己的能力。尽管如此，在过去的考试题目上取得好成绩并不能保证他会在真正考试时发挥出色。例如，学生可能试图通过死记硬背考题的答案来做准备。他甚至可以完全记住过去考试的答案。另一名学生可能会通过试图理解给出某些答案的原因来做准备。在大多数情况下，后者会考得更好。

类似地，考虑一个简单地使用查表法来回答问题的模型。如果允许的输入集合是离散的并且相当小，那么也许在查看许多训练样本后，该方法将执行得很好。但当这个模型面对从未见过的例子时，它表现的可能比随机猜测好不到哪去。这是因为输入空间太大了，远远不可能记住每一个可能的输入所对应的答案。例如，考虑{% mathjax %}28\times 28{% endmathjax %}的灰度图像。如果每个像素可以取{% mathjax %}256{% endmathjax %}个灰度值中的一个，则有{% mathjax %}256^784{% endmathjax %}个可能的图像。这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子要多得多。即使我们可能遇到这样的数据，我们也不可能存储整个查找表。

最后，考虑对掷硬币的结果（类别0：正面，类别1：反面）进行分类的问题。假设硬币是公平的，无论我们想出什么算法，泛化误差始终是{% mathjax %}\frac{1}{2}{% endmathjax %}。然而，对于大多数算法，我们应该期望训练误差会更低（取决于运气）。考虑数据集{`0，1，1，1，0，1`}。我们的算法不需要额外的特征，将倾向于总是预测多数类，从我们有限的样本来看，它似乎是1占主流。在这种情况下，总是预测类1的模型将产生{% mathjax %}\frac{1}{3}{% endmathjax %}的误差，这比我们的泛化误差要好得多。当我们逐渐增加数据量，正面比例明显偏离{% mathjax %}\frac{1}{2}{% endmathjax %}的可能性将会降低，我们的训练误差将与泛化误差相匹配。

由于泛化是机器学习中的基本问题，许多数学家和理论家毕生致力于研究描述这一现象的形式理论。在同名定理(`eponymous theorem`)中，格里文科和坎特利推导出了训练误差收敛到泛化误差的速率。在一系列开创性的论文中，`Vapnik`和`Chervonenkis`将这一理论扩展到更一般种类的函数。这项工作为统计学习理论奠定了基础。在我们目前已探讨、并将在之后继续探讨的监督学习情景中，我们假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为**独立同分布假设**，这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第`2`个样本和第`3`个样本的相关性，并不比抽取的第`2`个样本和第`200`万个样本的相关性更强。

有时候我们即使轻微违背独立同分布假设，模型仍将继续运行得非常好。比如，我们有许多有用的工具已经应用于现实，如人脸识别、语音识别和语言翻译。毕竟，几乎所有现实的应用都至少涉及到一些违背独立同分布假设的情况。有些违背独立同分布假设的行为肯定会带来麻烦。比如，我们试图只用来自大学生的人脸数据来训练一个人脸识别系统，然后想要用它来监测疗养院中的老人。这不太可能有效，因为大学生看起来往往与老年人有很大的不同。目前，即使认为独立同分布假设是理所当然的，理解泛化性也是一个困难的问题。此外，能够解释深层神经网络泛化性能的理论基础，也仍在继续困扰着学习理论领域最伟大的学者们。当我们训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。这种情况正是我们想要避免或控制的。深度学习中有许多启发式的技术旨在防止过拟合。

当我们有简单的模型和大量的数据时，我们期望**泛化误差与训练误差相近**。当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大。模型复杂性由什么构成是一个复杂的问题。一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要**早停**(`early stopping`)的模型（即较少训练迭代周期）就不那么复杂。

我们很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。就目前而言，一条简单的经验法则相当有用：统计学家认为，能够轻松解释任意事实的模型是复杂的，而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。在哲学上，这与波普尔的科学理论的可证伪性标准密切相关：如果一个理论能拟合数据，且有具体的测试可以用来证明它是错误的，那么它就是好的。这一点很重要，因为所有的统计估计都是事后归纳。也就是说，我们在观察事实之后进行估计，因此容易受到相关谬误的影响。目前，我们将把哲学放在一边，坚持更切实的问题。

影响模型泛化的因素：
- 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
- 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
- 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

##### 模型选择

在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做模型选择。有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。又有时，我们需要比较不同的超参数设置下的同一类模型。例如，训练多层感知机模型时，我们可能希望比较具有 不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用验证集。

原则上，在我们确定所有的超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们过拟合了测试数据，我们又该怎么知道呢？因此，我们决不能依靠测试数据进行模型选择。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。在实际应用中，情况变得更加复杂。虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。解决此问题的常见做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加一个验证数据集(`validation dataset`)，也叫验证集(`validation set`)。

当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用{% mathjax %}K{% endmathjax %}折交叉验证。这里，原始训练数据被分成{% mathjax %}K{% endmathjax %}个不重叠的子集。然后执行{% mathjax %}K{% endmathjax %}次模型训练和验证，每次在{% mathjax %}K-1{% endmathjax %}个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对{% mathjax %}K{% endmathjax %}次实验的结果取平均来估计训练和验证误差。

##### 欠拟合还是过拟合?

当我们比较训练和验证误差时，我们要注意两种常见的情况。首先，我们要注意这样的情况：训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合(`underfitting`)。另一方面，当我们的训练误差明显低于验证误差时要小心，这表明严重的过拟合(`overfitting`)。注意，过拟合并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。

为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出一个多项式的例子。给定由单个特征{% mathjax %}x{% endmathjax %}和对应实数标签{% mathjax %}y{% endmathjax %}组成的训练数据，我们试图找到下面的{% mathjax %}d{% endmathjax %}阶多项式来估计标签{% mathjax %}y{% endmathjax %}。
{% mathjax '{"conversion":{"em":14}}' %}
\hat{y} = \sum_{i=0}^{d} x^i w_i
{% endmathjax %}
这只是一个线性回归问题，我们的特征是{% mathjax %}x{% endmathjax %}的幂给出的，模型的权重是{% mathjax %}w_i{% endmathjax %}给出的，偏置是{% mathjax %}w_0{% endmathjax %}给出的（因为对于所有的{% mathjax %}x{% endmathjax %}都有{% mathjax %}x^0 = 1{% endmathjax %}）。由于这只是一个线性回归问题，我们可以使用平方误差作为我们的损失函数。

高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了{% mathjax %}x{% endmathjax %}的不同值时，函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。在下图中直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。
{% asset_img mlp_8.png "模型复杂度对欠拟合和过拟合的影响" %}

另一个重要因素是数据集的大小。训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小。此外，一般来说，更多的数据不会有什么坏处。对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。给出更多的数据，我们可能会尝试拟合一个更复杂的模型。能够拟合更复杂的模型可能是有益的。如果没有足够的数据，简单的模型可能更有用。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。
##### 多项式回归

###### 生成数据集

给定{% mathjax %}x{% endmathjax %}，我们将使用以下三阶多项式来生成训练和测试数据的标签：
{% mathjax '{"conversion":{"em":14}}' %}
y = 5 + 1.2x- 3.4\frac{x^2}{2!} + 5.6\frac{x^3}{3!} + \epsilon\;\text{where}\;\epsilon \sim\mathcal{N}(0,0.1^2)
{% endmathjax %}
噪声项{% mathjax %}\epsilon{% endmathjax %}服从均值为`0`且标准差为`0.1`正态分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从{% mathjax %}x^i{% endmathjax %}调整为{% mathjax %}\frac{x^i}{i!}{% endmathjax %}的原因，这样可以避免很大的{% mathjax %}i{% endmathjax %}带来的特别大的指数值。我们降为训练集和测试集各生成`100`个样本。
