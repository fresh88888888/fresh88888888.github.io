---
title: 多层感知机 (非线性神经网络) (PyTorch)
date: 2024-05-08 12:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 隐藏层

回想一下`softmax`回归的模型架构。该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行`softmax`操作。如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。但是，仿射变换中的线性是一个很强的假设。例如，线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正），或者导致模型输出的减小（如果对应的权重为负）。例如，如果我们试图预测一个人是否会偿还贷款。我们可以认为，在其它条件不变的情况下，收入较高的申请人比收入较低的申请人更有可能偿还贷款。但是，虽然收入与还款概率存在单调性，但是它们不是线性相关的。收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。处理这一问题的一种方法是对我们的数据进行预处理，使线性变得更合理，如使用收入的对数作为我们的特征。然而我们可以很容易找出违反单调性的例子。例如，我们想要根据体温预测死亡率。对于体温高于37摄氏度的人来说，温度越高风险越大。然而对于体温低于37摄氏度的人来说，温度越高风险就越低。在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。例如，我们可以使用与37摄氏度的距离作为特征。但是，如何对猫和狗的图像进行分类呢？增加位置`(13,17)`处像素的强度是否总是增加（或降低）图像描绘狗的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。与我们前面的例子相比，这里的线性很荒谬，而且我们难以通过简单的预处理来解决这个问题。这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。
<!-- more -->
##### 网络架构

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能够处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起，每一层都输出到上面的层，知道生成最后的输出。我们可以把钱{% mathjax %}L-1{% endmathjax %}层看做表示，把最后一层看作是线性预测器。这种架构通常称为**多层感知机**(`multilayer perceptron`)，通常缩写为`MLP`。下面，我们以图的方式描述了多层感知机：
{% asset_img mlp_1.png "一个单隐藏层的多层感知机，具有5个隐藏单元" %}

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2.注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。具有全连接层的多层感知机的参数开销可能会高的令人望而却步。即使不改变输入或输出大小的情况下，可能在参数节约和模型有效性之间进行平衡。

同之前一样，我们通过矩阵{% mathjax %}\mathbf{X}\in \mathbb{R}^{n\times d}{% endmathjax %}来表示{% mathjax %}n{% endmathjax %}个样本的小批量，其中每个样本具有{% mathjax %}d{% endmathjax %}个输入特征。对于具有{% mathjax %}h{% endmathjax %}个隐藏单元的但隐藏层多层感知机，用{% mathjax %}\mathbf{H}\in \mathbb{R}^{n\times h}{% endmathjax %}表示隐藏层的输出，称为隐藏表示(`hidden representations`)。在数学或代码中，{% mathjax %}mathbf{H}{% endmathjax %}也被称为隐藏层变量(`hidden-layer variable`)因为隐藏层和输出层都是全连接的，所以我们有隐藏层权重{% mathjax %}\mathbf{W}^{(1)}\in \mathbb{R}^{d\times h}{% endmathjax %}和隐藏层偏置{% mathjax %}\mathbf{b}^{(1)}\in \mathbb{R}^{1\times h}{% endmathjax %}以及输出层权重{% mathjax %}\mathbf{W}^{(2)}\in \mathbb{R}^{1\times q}{% endmathjax %}和输出层偏置{% mathjax %}\mathbf{b}^{(2)}\in \mathbb{R}^{1\times q}{% endmathjax %}。在形式上，我们按如下方式计算但隐藏层多层感知机的输出{% mathjax %}\mathbf{O}\in \mathbb{R}^{n\times q}{% endmathjax %}：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{align}
& \mathbf{H} = \mathbf{XW}^{(1)} + \mathbf{b}^{(1)} \\
& \mathbf{O} = \mathbf{HW}^{(2)} + \mathbf{b}^{(2)} \\
\end{align}
{% endmathjax %}
注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！原因很简单：上面的隐藏单元由输入的仿射函数给出，而输出（`softmax`操作前）只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数，但是我们之前的线性模型已经能够表示任何仿射函数。我们可以证明这一等价性，即对于任意权重值，我们只需合并隐藏层，便可产生具有参数{% mathjax %}\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}{% endmathjax %}和{% mathjax %}mathbf{b}=\mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}{% endmathjax %}的等价单层模型。