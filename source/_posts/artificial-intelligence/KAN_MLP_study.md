---
title: 多层感知器(MLP) vs 科尔莫戈罗夫-阿诺德网络(KAN)（机器学习）
date: 2024-06-25 15:20:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 多层感知器(MLP)

多层感知器(`MLP`)是如何工作的？多层感知器(`MLP`)是由多个神经元层组成的神经网路，每个神经元层以前馈方式组织，这意味着一层的输出称为下一层的输入，通常在每一层，我们会放置一些非线性激活函数，例如`RelU`，在这种情况下，会形成一个非常简单的网络，如下图所示：
{% asset_img km_1.png %}
<!-- more -->

上图中有包含三个特征的输入向量，接下来有`5`个神经元组成的第一层，`5`个输出特征的网络，这里的网络采用`5`个特征作为输入，并产生`5`个输出特征，它们具有相同的结构，通常我们将激活函数放在各层中。让我们看看它在`pytorch`中是如何工作的：
{% asset_img km_2.png %}

在文档中你将看到线性层执行这个非常简单的操作，它接受输入，你可以将其视为由特征组成的向量，或者你可以将其视为由许多项目组成的矩阵，每个项目都具有输入特征，让后我们将它乘以权重矩阵，我们它称之为{% mathjax %}A{% endmathjax %}，让我们分析一下它的结构。假设我们有`10`个输入，每个输入向量有`3`个特征组成，在这里称之为{% mathjax %}f_1、f_2{% endmathjax %}和{% mathjax %}f_3{% endmathjax %}，这三个输入特征时线性层。将执行以下操作，即输入乘以某个权重矩阵，该矩阵由权重组成，可以通过网络学习获得，加上偏置权重矩阵。在这里我们讨论的是线性层，`3`个特征作为输入并产生`5`个输出特征，因此，权重矩阵转置后的权重矩阵将是{% mathjax %}(3,5){% endmathjax %},你可以将每个神经元视为一列权重。如果{% mathjax %}n{% endmathjax %}是输入特征的数量，则每个神经元将具有{% mathjax %}n{% endmathjax %}个权重，每个输入特征都有一个权重，并且它将产生一些输出，因此当我们将{% mathjax %}X\times W^\top{% endmathjax %}执行这个矩阵乘法，将产生以下结果。你可以认为这个矩阵是一批`10`个项目，因为在输入中我们有`10`个项目(`items`)，每个项目有`5`个特征，因为线性层从`3`个特征变为`5`个特征输出。第`1`个特征是由第一个项目与第`1`个神经元的点积生成的。这里的值是第`2`个输出第一项的特征是第一项的特征值乘以第`2`个神经元，因此第`2`个神经元的权重在执行此乘法之后负责该线性层中的一个输出特征，我们添加一个偏差项，在本例中是一个向量，每个神经元都有一个值，我们将它广播到这个矩阵。每个项目的每个输出特征将有一个附加项，即与该特定神经元相关的偏差，因此第一个神经元将添加第一个特征的值加上{% mathjax %}b_1{% endmathjax %}，第二个特征将具有第二个特征的值加上{% mathjax %}b_2{% endmathjax %}等等。这将产生线性层的输出，我们得到`10`个项目，每个项目有`5`个特征。线性层是从`3`变换到`5`。所以基本上它是每个项目的输入特征乘以相应的权重加上偏差。
{% asset_img km_3.png %}

##### 为什么线性层需要激活函数？

主要有以下几个原因：
- 引入非线性：非线性激活函数能够为神经网络引入非线性特性，使网络能够学习和表示复杂的非线性关系。如果没有非线性激活函数，多层神经网络就等同于单层线性网络，无法建模复杂的函数。
- 增强表达能力：非线性激活函数使神经网络能够近似任意复杂的函数。根据通用近似定理，具有非线性激活函数的前馈网络可以近似任何连续函数。
- 解决梯度消失问题：某些非线性激活函数（如`ReLU`）可以帮助缓解深度网络中的梯度消失问题，使得深层网络更容易训练。
- 实现决策边界：非线性激活函数使得神经网络能够学习非线性决策边界，这对于解决复杂的分类问题至关重要。
- 映射到特定范围：某些激活函数（如`Sigmoid`和`Tanh`）可以将输入映射到特定的范围，这对于某些任务（如概率预测）非常有用。
- 促进反向传播：非线性激活函数的可微性质使得反向传播算法能够有效地调整网络权重。
- 增加网络深度的意义：使用非线性激活函数使得增加网络深度变得有意义，因为每一层都可以学习更复杂的特征表示。

总之，非线性激活函数是使神经网络能够学习复杂模式和关系的关键组成部分，它们使得神经网络成为强大的通用函数逼近器，能够解决各种复杂的机器学习任务。
##### 数据拟合

想象以下我们正在创建一个`2D`游戏，其中我们有一个角色，想要通过由点组成来制作动画。当然，制作角色动画的方法是：用这些点并在它们之间画直线，这样就可以为角色设置动画。但看起来不太好，这个动作不太平滑，你可以看到这个角色在刚性运动。所以有一种更好的办法是制作一条弯曲的多项式曲线。它穿过这些点并生成更平滑的路径，这看起来更好。创建一条多项式，穿过这些点的线是弯曲的，如何做到这一点？当你有两个点时，你只能在它们之间画一条线，当你有三个点时，则意味着它们不再是直线，你可以画一条二次曲线，想抛物线一样；如果你有四个点，你需要一个3次方程来画一条穿过它们的多项式线。如何计算这条线的方程而正好穿过所有这些点？我们想象有`4`个点{% mathjax %}(x,y)\in {(0,5),(1,1),(2,3),(5,2)}{% endmathjax %}，我们最终要写出{% mathjax %}n-1{% endmathjax %}次的多项式曲线方程：
{% mathjax '{"conversion":{"em":14}}' %}
y = ax^3+ bx^2 + cx + d
{% endmathjax %}
我们创一个方程组，在该方程组中我们加一个条件，即该曲线必须从所有这些点上通过，一次我们通过代入来写出该曲线方程：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{aligned}
5 & = a(0)^3 + b(0)^2 + c(0) + d \\
1 & = a(1)^3 + b(1)^2 + c(1) + d \\
3 & = a(2)^3 + b(2)^2 + c(2) + d \\
2 & = a(5)^3 + b(5)^2 + c(5) + d 
\end{aligned}
{% endmathjax %}
{% asset_img km_4.png %}

如果现在我们有数百个点并且我们想生成一条穿过它们的平滑曲线，我们可以这样做。但是会存在两个问题：1.我们需要求解非常复杂的方程组，随着点数的增加，这条多项式曲线会以越来越奇怪的方式呈现，我们需要一种方法来控制这条曲线的平滑度，并且不让它在极端情况下变得如此疯狂，在这里需要研究`B`曲线，看看它是如何工作的。
###### B曲线

B曲线是一条参数化的曲线，因为该差值曲线上的点坐标取决于一个称为{% mathjax %}T{% endmathjax %}的自变量，在这种情况下，你可以将其视为从{% mathjax %}[0,1]{% endmathjax %}的时间，例如，我们只有两个点，两个点只能画一条直线，假设你想在{% mathjax %}p_0{% endmathjax %}和{% mathjax %}p_1{% endmathjax %}之间画一条线。我们{% mathjax %}p_0{% endmathjax %}开始，走向{% mathjax %}p_1{% endmathjax %}，这是我们的差值线，正如你所看到的，随着时间的移动，该点越来越接近{% mathjax %}p_1{% endmathjax %}而远离{% mathjax %}p_0{% endmathjax %}，因此，你可以将变量{% mathjax %}t{% endmathjax %}视为时间变量，方程如下：
{% mathjax '{"conversion":{"em":14}}' %}
\mathbf{B}(t) = \mathbf{P}_0 + t(\mathbf{p}_1 - \mathbf{P}_0) = (1 - t) + t\mathbf{P}_1
{% endmathjax %}
现在这条曲线是参数化，这也是你所看到的{% mathjax %}B{% endmathjax %}是粗体，而{% mathjax %}t{% endmathjax %}不是粗体的原因，因为它可以是一个向量，意味着这个点位于{% mathjax %}X{% endmathjax %}和{% mathjax %}Y{% endmathjax %}坐标中，所有这些坐标都取决于{% mathjax %}t{% endmathjax %}变量，因此在每个时间步，它都会告诉我们相对于时间的位置。当然我们可以将其扩展到3个点，我们可以绘制一条平滑的曲线，对它们进行差值，在B曲线的情况下，它仅从第一个点和最后一个点穿过，并在中间点之间进行差值。因此它不会接触中间点，而是接近中间点。如何计算这条差值曲线的方程呢？（即红色曲线），在这种情况下我们要做的就是进行递归计算。首先在{% mathjax %}p_0{% endmathjax %}和{% mathjax %}p_1{% endmathjax %}之间进行线性差值，你可以使用这个方程{% mathjax %}(1 - t) + t\mathbf{P}_1{% endmathjax %}，{% mathjax %}p_0{% endmathjax %}和{% mathjax %}p_1{% endmathjax %}差值计算的点为{% mathjax %}q_0{% endmathjax %},随着时间的移动，我们从{% mathjax %}p_0{% endmathjax %}移动到{% mathjax %}p_1{% endmathjax %}；然后我们在{% mathjax %}p_1{% endmathjax %}和{% mathjax %}p_2{% endmathjax %}之间进行线性差值，从而产生一个新的点{% mathjax %}q_1{% endmathjax %}，随着时间的推移，该点将从{% mathjax %}p_1{% endmathjax %}移动到{% mathjax %}p_2{% endmathjax %}，接下来我们在{% mathjax %}q_0{% endmathjax %}和{% mathjax %}q_1{% endmathjax %}之间创建另一个线性插值。这将为我们提供差值3个点的曲线的差值点的坐标。这将为我们提供三个点差值的最终曲线的方程，得到：
{% mathjax '{"conversion":{"em":14}}' %}
\begin{aligned}
\mathbf{Q}_0(t) & = (1-t)\mathbf{P}_0 + t\mathbf{P}_1 \\
\mathbf{Q}_1(t) & = (1-t)\mathbf{P}_1 + t\mathbf{P}_2 \\
\mathbf{B}(t) & = (1-t)\mathbf{Q}_0 + t\mathbf{Q}_1 \\
& = (1-t)[(1-t)\mathbf{P}_0 + t\mathbf{P}_1] + t[(1-t)\mathbf{P}_1 + t\mathbf{P}_2] \\
& = (1 - t)^2\mathbf{P}_0 + 2(1-t)t\mathbf{P}_1 + t^2\mathbf{P}_2 
\end{aligned}
{% endmathjax %}

