---
title: 机器学习-可解释性（特征可视化 & 对抗性示例 & 差值）
date: 2024-07-26 14:10:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**可解释机器学习**是指使机器学习系统的行为和预测可以被人类理解的方法和模型。**数据集**是包含机器学习数据的表格。数据集包含特征和要预测的目标。当用于建立模型时，数据集称为**训练数据**。**实例**是数据集中的一行。“**实例**”的其他名称是：（数据）点、示例。**实例**由特征值组成以及目标结果。**特征**是用于预测或分类的输入。特征是数据集中的一列。**特征**被认为是可解释的，这意味着很容易理解它们的含义，例如某一天的温度或一个人的身高。特征的可解释性是一个很大的假设。但如果很难理解输入特征，那么理解模型的作用就更难了。**目标**是机器学习预测的信息。在数学公式中，对于单个实例来说，**目标**通常称为{% mathjax %}y{% endmathjax %}或者{% mathjax %}y_i{% endmathjax %}。**机器学习任务**是具有特征的数据集和目标的组合。根据目标的类型，**任务**可以是分类、回归、聚类或异常值检测等。**预测**是机器学习模型根据给定的特征“猜测”目标值应该是什么。模型预测表示为{% mathjax %}\hat{F}(x^{(i)}){% endmathjax %}或者{% mathjax %}\hat{y}{% endmathjax %}。
<!-- more -->

#### 可解释性的重要性

**可解释性**是人类理解决策原因的程度。另一个定义是：**可解释性**是人类能够一致预测模型结果的程度。机器学习模型的**可解释性**越高，人们就越容易理解为什么做出某些决策或预测。如果一个模型的决策比其他模型的决策更容易被人类理解，那么该模型的可解释性就比另一个模型更好。

**可解释性**的需求源于**问题形式化的不完整性**(`Doshi-Velez`和`Kim 2017`)，这意味着对于某些问题或任务，仅仅获得预测（是什么）是不够的。模型还必须解释它是如何得出预测的（为什么），因为正确的预测只能部分解决您的原始问题。机器学习模型只有在可解释时才能进行调试和审核。即使在电影推荐等低风险环境中，解释能力在研发阶段以及部署后都很有价值。之后，当模型用于产品时，可能会出错。对错误预测的解释有助于了解错误的原因。它为如何修复系统提供了方向。考虑一个哈士奇与狼分类器的例子，它将一些哈士奇误归类为狼。使用可解释的机器学习方法，您会发现错误分类是由于图像上的雪造成的。如果你能确保机器学习模型能够解释决策,那你可以轻松检查以下特征：
- 公平性：确保预测不带偏见，不会隐性或显性地歧视代表性不足的群体。可解释的模型可以告诉你为什么它决定某个人不应该获得贷款，而人类更容易判断该决定是否基于习得的人口统计学（例如种族）偏见。
- 隐私：确保数据中的敏感信息受到保护。
- 可靠性或稳健性：确保输入的微小变化不会导致预测的重大变化。
- 因果关系：检查是否只存在因果关系。
- 信任：与黑匣子相比，人类更容易信任能够解释其决策的系统。

#### 可解释性方法的分类

- **内在可解释性**是指由于结构简单而被认为是可解释的机器学习模型，例如短决策树或稀疏线性模型。
- **事后可解释性**是指在模型训练后应用解释方法。例如，排列特征重要性是一种事后解释方法。

事后方法也可以应用于内在可解释的模型。例如，可以计算决策树的排列特征重要性。解释方法大致可以区分为：
- **特征汇总统计**：许多解释方法都提供每个特征的汇总统计信息。有些方法会为每个特征返回一个数字，例如特征重要性，或更复杂的结果，例如成对特征交互强度，它由每个特征对的数字组成。
- **特征摘要可视化**：大多数特征摘要统计数据也可以可视化。一些特征摘要实际上只有在可视化的情况下才有意义，而表格则是一个错误的选择。特征的部分依赖性就是这样一种情况。部分依赖性图是显示特征和平均预测结果的曲线。呈现部分依赖性的最佳方式是实际绘制曲线，而不是打印坐标。
- **模型内部结构**（例如学习到的权重）：本质上可解释的模型的解释属于这一类。例如，线性模型中的权重或决策树的学习到的树结构（用于分割的特征和阈值）。例如，在线性模型中，模型内部结构和特征汇总统计之间的界限很模糊，因为权重既是模型内部结构，又是特征的汇总统计。输出模型内部结构的另一种方法是可视化卷积神经网络中学习到的特征检测器。输出模型内部结构的可解释性方法根据定义是特定于模型的。
- **数据点**(实例)：此类别包括所有返回数据点（已存在或新创建）以使模型可解释的方法。其中一种方法称为**反事实解释**。为了解释数据实例的预测，该方法通过改变一些特征来找到类似的数据点，这些特征会导致预测结果以相关的方式发生变化（例如，预测类中的翻转）。另一个示例是识别预测类的原型。要发挥作用，输出新数据点的解释方法要求数据点本身可以​​被解释。这对图像和文本很有效，但对具有数百个特征的表格数据用处不大。
- **内在可解释模型**：解释黑盒模型的一种解决方案是使用可解释模型来近似它们（无论是全局还是局部）。可解释模型本身是通过查看内部模型参数或特征汇总统计数据来解释的。

**特定于模型还是与模型无关**？特定于模型的解释工具仅限于特定的模型类别。线性模型中回归权重的解释是特定于模型的解释，因为 - 根据定义 - 本质上可解释的模型的解释始终是特定于模型的。仅适用于解释神经网络等的工具是特定于模型的。与模型无关的工具可用于任何机器学习模型，并在模型训练后应用（事后）。这些不可知论方法通常通过分析特征输入和输出对来工作。根据定义，这些方法无法访问模型内​​部信息，例如权重或结构信息。

#### 可解释性的范围

算法训练产生预测的模型。每个步骤都可以根据透明度或可解释性进行评估。

##### 算法透明度

**算法透明度**是关于算法如何从数据中学习模型以及它可以学习什么样的关系。如果你使用卷积神经网络对图像进行分类，你可以解释算法在最低层学习边缘检测器和过滤器。这是对算法如何工作的理解，但不是对最终学习到的特定模型的理解，也不是如何进行个别预测的理解。算法透明度只需要了解算法，而不是数据或学习到的模型。诸如线性模型的最小二乘法之类的算法已经得到了充分的研究和理解。它们的特点是透明度高。深度学习方法（通过具有数百万个权重的网络推动梯度）的理解较少，其内部工作原理是正在进行的研究的重点。它们被认为透明度较低。

##### 全局、整体模型的可解释性

训练好的模型如何做出预测？如果您可以一次性理解整个模型，则可以将模型描述为可解释的。要解释全局模型输出，您需要训练好的模型、算法知识和数据。这种级别的可解释性是关于理解模型如何基于其特征和每个学习到的组件（例如权重、其他参数和结构）的整体视图做出决策。哪些特征很重要，它们之间会发生什么样的相互作用？全局模型可解释性有助于理解基于特征的目标结果分布。全局模型可解释性在实践中很难实现。任何超过少数参数或权重的模型都不太可能适合普通人的短期记忆。我认为你无法真正想象一个具有`5`个特征的线性模型，因为这意味着在`5`维空间中在脑海中绘制的超平面。任何超过`3`维的特征空间对人类来说都是不可想象的。通常，当人们试图理解一个模型时，他们只考虑它的一部分，例如线性模型中的权重。

##### 模块化层面的全局模型可解释性

模型的各个部分如何影响预测？具有数百个特征的朴素贝叶斯模型对于我和你来说都太大了，无法保存在我们的工作记忆中。而且，即使我们设法记住所有权重，我们也无法快速对新数据点做出预测。此外，你需要在脑海中了解所有特征的联合分布，预测每个特征的重要性以及这些特征平均如何影响预测。这是一项不可能完成的任务。但你可以轻松理解单个权重。虽然全局模型的可解释性通常遥不可及，但很有可能在模块化层面上理解至少一些模型。并非所有模型都能够在参数级别上解释。对于线性模型，可解释的部分是权重，对于树，则是分割（选定的特征加上截止点）和叶节点预测。

##### 单个预测的局部可解释性

为什么模型会对某个实例做出某个预测？从局部来看，预测可能仅线性或单调地依赖于某些特征，而不是对它们具有复杂的依赖性。例如，房屋的价值可能非线性地取决于其大小。但如果您只看一套特定的 100 平方米的房子，那么对于该数据子集，您的模型预测可能线性依赖于房屋大小。您可以通过模拟将房屋面积增加或减少 10 平方米时预测价格的变化来发现这一点。因此，局部解释可能比全局解释更准确。

##### 一组预测的局部可解释性

为什么模型针对一组实例做出特定的预测？可以使用全局模型解释方法（在模块级别）或单个实例的解释来解释多个实例的模型预测。可以通过采用实例组来应用全局方法，将它们视为完整数据集，然后对此子集使用全局方法。可以对每个实例使用单个解释方法，然后针对整个组列出或汇总单个解释方法。

#### 可解释性评估

`Doshi-Velez`和`Kim(2017)`提出了可解释性的评估三个主要层次：
- **应用级评估**（实际任务）：将解释放入产品中并让最终用户进行测试。想象一下带有机器学习组件的骨折检测软件，该组件可以在`X`射线中定位和标记骨折。在应用层面，放射科医生将直接测试骨折检测软件以评估模型。这需要良好的实验设置和对如何评估质量的理解。一个好的基准始终是人类在解释相同决定时有多好。
- **人类层面的评估**（简单任务）：是一种简化的应用程序层面的评估。不同之处在于，这些实验不是由领域专家进行的，而是由外行人进行的。这使得实验更便宜（特别是如果领域专家是放射科医生），并且更容易找到更多的测试人员。一个例子是向用户展示不同的解释，用户会选择最好的一个。
- **功能级别评估**（代理任务）：不需要人工。当所使用的模型类别已由其他人在人工级别评估中评估过时，这种方法效果最佳。例如，可能知道最终用户了解决策树。在这种情况下，解释质量的代理可能是树的深度。较短的树会获得更好的可解释性分数。添加约束是有意义的，即树的预测性能保持良好并且与较大的树相比不会下降太多。

#### 解释的性质

- **表达能力**是指该方法能够生成的解释的“语言”或结构。解释方法可以生成`IF-THEN`规则、决策树、加权和、自然语言或其他内容。
- **透明度**描述了解释方法对机器学习模型（如其参数）的依赖程度。例如，依赖于线性回归模型（特定于模型）等本质上可解释的模型的解释方法是高度透明的。仅依赖于操纵输入和观察预测的方法的透明度为零。根据场景的不同，可能需要不同级别的透明度。高透明度的优点是该方法可以依赖更多信息来生成解释。低透明度的优点是解释方法更具可移植性。
- **可移植性**描述了解释方法可以与哪些机器学习模型一起使用。透明度低的方法具有更高的可移植性，因为它们将机器学习模型视为黑匣子。替代模型可能是可移植性最高的解释方法。仅适用于循环神经网络等的方法可移植性较低。
- **算法复杂度**描述了生成解释的方法的计算复杂度。当计算时间成为生成解释的瓶颈时，此属性很重要。

#### 人性化的解释

- **解释是对比性的**：人类通常不会问为什么会做出某个预测，而是问为什么会做出这个预测而不是另一个预测。我们倾向于在反事实的情况下思考，即“如果输入`X`不同，预测会怎样？”。人类并不想对预测有一个完整的解释，而是想比较与另一个实例的预测（可能是人为的）之间的差异。创建对比解释依赖于应用程序，因为它需要一个比较的参考点。这可能取决于要解释的数据点，也取决于接收解释的用户。
- **解释是经过选择的**：人们并不期望解释能够涵盖事件的实际和完整原因列表。我们习惯于从各种可能的原因中选择一两个原因作为解释。即使世界更加复杂，解释也要非常简短，只给出`1`到`3`个理由。
- **解释是社会性的**：它们是解释者和解释接收者之间对话或互动的一部分。社会背景决定了解释的内容和性质。关注你的机器学习应用程序的社会环境和目标受众。正确处理机器学习模型的社交部分完全取决于你的具体应用。
- **解释侧重于异常**：人们更关注用异常原因来解释事件。这些原因发生的概率很小，但还是发生了。消除这些异常原因将极大地改变结果（反事实解释）。人类认为这类“异常”原因是一种很好的解释。如果预测的输入特征之一在任何意义上是异常的（例如分类特征的罕见类别）并且该特征影响了预测，则应将其包含在解释中，即使其他“正常”特征对预测的影响与异常特征相同。
- **解释是真实的**：好的解释在现实中（即在其他情况下）被证明是正确的。但令人不安的是，这并不是“好的”解释最重要的因素。例如，选择性似乎比真实性更重要。仅选择一两个可能原因的解释很少能涵盖所有相关原因。选择性会忽略部分事实。例如，只有一两个因素导致股市崩盘的说法并不正确，但事实是，有数百万个原因影响着数百万人的行为，最终导致股市崩盘。解释应该尽可能真实地预测事件，在机器学习中有时被称为保真度。

#### 深度学习(计算图)

假设我们有一个非常简单的神经网络，它使用房屋的卧室数量{% mathjax %}x_1{% endmathjax %}和浴室数量{% mathjax %}x_2{% endmathjax %}来计算房屋的价格。隐藏层中的每个神经元{% mathjax %}j{% endmathjax %}将计算以下内容：{% mathjax %}(\sum_1^2 x_i w_{ji}) + b_j{% endmathjax %}让我们可视化该神经网络的计算图。
{% asset_img mI_1.png  %}

{% asset_img mI_2.png  %}

##### 反向传播(Backpropagation)

通常，我们有一个由（输入，输出）对组成的数据集，我们的目标是训练神经网络以最小化选择的某个损失函数。例如，对于回归任务（根据房屋特征估算房屋价格），我们希望神经网络能够以最小的误差预测输出。我们希望神经网络不仅学会训练期间看到的输入来预测输出，而且还能推广到看不见的输入。
- 我们选择一个损失函数（例如`MSE`）。
- 通过网络运行输入，计算相对于目标的损失。
- 计算相对于神经网络参数的损失梯度。
- 更新网络参数以逆着梯度方向移动。

{% asset_img mI_3.png  %}

**前向传递**：进行训练循环之前的第一步是根据训练输入评估模型的输出，这通常称为前向传递（从左到右）。
{% asset_img mI_4.png  %}

**反向传递**：在反向传递过程中，我们计算损失函数对模型每个参数的导数，然后更新权重以逆着导数的方向移动。例如，要计算损失函数对参数{% mathjax %}w{% endmathjax %}的导数，我们需要评估以下梯度：{% mathjax %}\frac{dl}{dw_11} = \frac{dl}{da_6}\frac{da_6}{da_5}\frac{da_5}{da_3}\frac{da_3}{da_1}\frac{da_1}{dw_11}{% endmathjax %}(基于**链式法则**)。

计算出相对于每个权重的损失梯度后，我们可以更新权重，使其沿梯度方向移动，使用通常的更新规则如下：
{% mathjax '{"conversion":{"em":14}}' %}
w_{11_{\text{NEW}}} = w_{11_{\text{OLD}}} - \alpha\ast\frac{dl}{w_{11}}
{% endmathjax %}

为什么我们要有**学习率**？因为我们正在运行随机梯度下降，梯度方向不是真正的梯度，而是一个近似值，所以我们不能完全信任它。
{% asset_img mI_5.png  %}

##### 分类器

**分类器**是一种特殊类型的神经网络，经过训练可以将输出分类到可用的类别之一中。例如，`ResNet`神经网络可以将输入分类到`1000`个类别中。分类器的输出称为`logit`，每个`logit`都表示模型为每个类别分配的分数。我们通常采用得分最高的`logit`（应用`softmax`后）来选择预测类别和模型的置信度。

假如给定一张鱼的图片，如果训练正确，分类器将以很高的概率预测“鱼”类。如果我们能用相同的参数制作相同的分类器，将输入的鱼预测为火山，结果会怎样？
{% asset_img mI_6.png  %}

我们的目标是：给图片添加一点噪声。图片看起来仍然像一条鱼，但我们的分类器会把它归类为一座火山。
{% asset_img mI_7.png  %}

为什么会这样？**分类器**的训练如下：我们提供一系列输入图像及其标签，并使用我们之前看到的反向传播算法让模型学习输入和标签之间的映射：我们计算损失函数对模型每个权重的导数，并不断更新权重，直到模型能够很好地从训练、验证和测试数据集中对任何给定的图像进行分类。
{% asset_img mI_8.png  %}

在深度学习中，我们计算损失函数相对于模型参数的梯度，以便我们能够以最小化损失函数的方式更新模型参数。损失相对于输入的梯度表明了我们应该朝哪个方向改变输入以增加损失。因此，我们可以朝这个方向“改变”我们的输入，这样特定的`logit`就会增加。例如，我们可以计算针对输入图像的损失的梯度（针对目标类“火山”），然后逆着梯度指示的方向移动输入图像，以减少所选目标类的损失。
{% asset_img mI_9.png  %}

##### 特征可视化

特征可视化让我们了解神经网络中特定单元学习到了哪些特征：例如，卷积层关注哪些特征？这个神经元关注哪些特征？为了进行特征可视化，我们利用了神经网络的可微分性。假设我们有一个卷积神经网络，我们想要了解层的一些特征。由于神经网络通常相对于其输入是**可微分的**，因此我们可以使用层的激活作为目标并优化输入以最大化该目标。
{% asset_img mI_10.png  %}

想象一下，我们想要了解什么样的图像会激活某个输出：我们可以创建一个类似于用于生成对抗性示例的损失，并优化初始噪声图像以最大化特定的输出类。不幸的是，当从随机噪声进行优化时，我们最终会得到看起来不太自然的高频特征。
{% asset_img mI_11.png  %}

我们可以在目标中添加更多“约束”，使梯度朝表现出更多特定模式或更少其他模式的方向移动。
{% asset_img mI_12.png  %}

