---
title: 离散去噪扩散模型(DDMs) — 数据隐私探析（深度学习）
date: 2024-08-16 18:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**离散去噪扩散模型**(`Discrete Denoising Diffusion Models, DDMs`)是一种用于**生成合成数据**的**深度学习模型**，近年来因其在**隐私保护**方面的潜力而受到关注。随着对数据隐私的日益重视，研究人员开始探索这些模型在生成合成数据时的隐私保护能力。在生成合成数据的过程中，传统的隐私保护方法往往无法有效应对数据泄露的风险。**离散去噪扩散模型**通过逐步引入噪声并在后续步骤中去噪，生成与原始数据分布相似的合成数据。尽管已有实证研究评估了这些模型的性能，但对其隐私保护能力的数学表征仍存在较大缺口。
<!-- more -->

**离散去噪扩散模型**在隐私保护方面的研究为合成数据生成提供了新的视角。通过理论分析和实证验证，研究者们不仅揭示了这些模型的隐私泄露机制，还为未来的隐私保护技术提供了理论基础。这一研究方向将有助于在数据生成和使用中更好地平衡隐私保护与数据实用性之间的关系。

[`“On the Inherent Privacy Properties of Discrete Denoising Diffusion Models”`](https://openreview.net/pdf?id=UuU6C6CUoF)，这篇论文主要介绍了，隐私问题导致合成数据集创建的激增，而扩散模型则成为一种很有前途的技术手段。尽管先前的研究已经对这些模型进行了实际的评估，但在提供其隐私保护能力的数学表征方面仍存在差距。为了解决这个问题，作者提出了用于离散数据集生成的**离散去噪扩散模型**(`DDMs`)固有的隐私属性的开创性理论探索。作者的框架专注于每个实例的差异隐私(`pDP`)，阐明了给定训练数据集中每个数据点的潜在隐私泄露，并深入了解了每个点的隐私损失如何与数据集的分布相关联。结果表明，使用`s-sized`的数据点进行训练会导致(`DDMs`)从纯噪声阶段过渡到合成清洗数据阶段时从{% mathjax %}(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))\text{-pDP}{% endmathjax %}到{% mathjax %}(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))\text{-pDP}{% endmathjax %}的隐私泄漏激增，而扩散系数的更快衰减会增强隐私保证。最后，作者在合成数据集和真实数据集上进行了理论验证。

具有分类属性的离散表格或图形数据集在许多隐私敏感领域中很普遍，包括金融、电商和医学。例如，医学研究人员经常以离散表格形式收集患者数据，例如种族、性别和就医状况。然而，在这些领域使用和共享数据存在泄露个人信息的风险。为了解决这类问题，有人提出发布具有隐私保护的合成数据集，作为保护敏感信息和降低隐私泄露风险的一种方式。
{% asset_img d_1.png  "离散扩散模型(DDMs)的原理" %}

在论文中，作者分析了固定训练数据集的DDM隐私保护。利用了数据相关的隐私框架，称为**每个实例差异隐私**(`pDP`)，该框架是根据固定训练数据集中的实例定义的。`pDP`的分析允许对训练集中每个数据点的潜在隐私泄露进行细粒度的表征。这让数据管理员能够更好地了解训练数据的敏感性。作者的分析考虑了一个在`s`个样本上训练的`DDM`并生成`m`个样本，跟踪每个生成步骤中的隐私泄漏。实验证明，随着数据生成步骤从`t = T`（噪声状态）过渡到`t = 0`（无噪声状态），隐私泄漏从{% mathjax %}(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon(1-e^{}-\epsilon)}))\text{-pDP}{% endmathjax %}增加到{% mathjax %}(\epsilon, \mathcal{O}(\frac{1}{s\epsilon(1 - e^{-\epsilon})}))\text{-pDP}{% endmathjax %}，其中数据相关项隐藏在{% mathjax %}\text{big-}\mathcal{O}{% endmathjax %}符号中。因此，最后几个生成步骤主导了`DDM`中的主要隐私泄漏。此外，分析表明，当`m = 1`时，隐私边界{% mathjax %}\mathcal{O}(1/s){% endmathjax %}很紧，并强调了`DDM`固有的弱隐私保护。此外，**扩散系数衰减越快，隐私保护效果越好**。对于数据部分，作者开发了一种算法，根据`pDP`边界估计真实数据集中每个数据点的隐私泄漏。通过从数据集中删除最敏感的数据点（根据数据相关隐私参数）来训练`DDM`，然后评估基于`DDM`生成的合成数据集训练的`ML`模型，从而评估数据部分。有趣的是，作者观察到，在删除部分数据后获得的`ML`模型甚至超过没有删除此类数据的其他模型。作者将其归因于这样一个事实，即删除的数据点可能是异常值，这可能实际上不利于`ML`模型学习。

为了避免混淆，作者提供了几个重要的解释。最坏情况并与数据集无关的`DP`（`Wang，2019`）相比，针对训练集量身定制的`pDP`为数据管理员提供了对每个数据点潜在隐私泄漏的更准确、更细粒度的估计。然而，重要的是要理解`pDP`。直接为数据添加噪声是不允许的，因为添加的噪声可能会因其数据依赖性而泄露隐私信息。可以使用其他方法，例如**平滑灵敏度**（`Nissim`等人，`2007`）和**提议测试发布**（`Dwork & Lei，2009`）。作者的分析旨在深入了解 `DDM`所提供的固有隐私，并指导数据管理员评估与数据集不同部分的隐私泄露风险。这里并非以开发一种匹配特定隐私评估的算法为目标。鉴于此目的，`pDP`是比`DP`更合适。在实践中，`pDP`评估应该保密，并由数据管理员了解数据集并使用 `DDM`生成合成数据集时的潜在隐私泄露。