---
title: 基于人类反馈的强化学习(RLHF) — 推导（深度学习）
date: 2024-06-28 12:20:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

基于人类反馈的强化学习(`Reinforcement Learning Human Feedback, RLHF`)是一种结合强化学习技术和人类反馈来训练人工智能(`AI`)模型的方法。`RLHF`是一种机器学习方法，通过人类反馈来优化`AI`模型的行为，使其更符合人类的期望和偏好。这种方法特别适用于自然语言处理(`NLP`)任务，如对话系统、文本生成和摘要生成等。`RLHF`的训练过程通常分为三个主要阶段：
<!-- more -->
- **预训练语言模型**：首先，使用传统的预训练目标对语言模型进行预训练。这一步通常使用大量的文本数据来训练模型，使其具备基本的语言理解和生成能力。例如，`OpenAI`的`InstructGPT`就是在一个较小版本的`GPT-3`模型上进行预训练的。
- **训练奖励模型**：接下来，收集人类反馈数据并训练一个奖励模型。奖励模型的作用是预测人类对模型生成文本的评分。具体步骤如下：人类评估者对模型生成的文本进行评分或排序；使用这些评分数据训练一个监督学习模型，使其能够预测给定文本的评分。
- **使用强化学习微调语言模型**：最后，使用奖励模型对语言模型进行微调。通过强化学习算法（如近端策略优化`PPO`），模型根据奖励模型的评分来优化其生成的文本，使其更符合人类偏好。

在实践中为单词序列分配概率，例如：`“Shanghai is a city in”`，如下图所示：
{% asset_img d_1.png %}

这里简化为：一个`token`是一个单词，在大多数语言模型中实际上并不是这样的，但用于解释它很有帮助。给定特定提示的情况下，下一个`token`是`china`、`Beijing`、`Cat`、`Pizza`的概率是`85%、10%、2.5%、...`，语言模型给了我们这些概率。我们如何使用语言模型来生成文本呢？首先我们会给出一个提示，如：`“Where is Shanghai?”`，我们把它提交给语言模型，语言模型会给出下一个单词或`token`的概率列表，假设我们选择概率分数最高的`token`，假设它是`“Shanghai”`，然后我们将其(`token`)放回到提示中：`“Where is Shanghai? Shanghai”`，再次交给语言模型，然后语言模型再次给出一个单词或`token`的概率列表，我们选择相关性最重要的一个。然后将其放回到提示后边，然后再次提交给语言模型等，直到完成了句子标记的结尾。这种情况下，这是一个特殊的`token`。
{% asset_img d_2.png %}

#### 强化学习

强化学习(`Reinforcement Learning, RL`)是机器学习的一个重要分支,主要关注如何让智能体(`agent`)在与环境的交互中学习最优策略。智能体通过试错的方式,在环境中采取行动,获得奖励或惩罚,从而学习如何最大化长期累积奖励。举个简单的例子：
|`example_1`|`example_2`|
|:---|:---|
|`Agent`：猫|`Agent`：语言模型|
|状态：猫在网格中的位置`(x,y)`|状态：提示（输入的`tokens`）|
|行为：在每个位置，猫可以移动到`4`个方向连接的单元格之一，如果移动无效，则单元格将不会移动并保持在同一位置。每次猫移动时，都会产生新的状态和奖励。|行为：哪个`token`被选为下一个 `token`|
|奖励模式：<br> 1.移至另一个空单元格将导致奖励`0`。<br> 2.移向扫帚将导致奖励`-1`。<br> 3.移向浴缸将导致奖励`-10`，猫会晕倒（剧集结束）。猫会再次重生在初始位置。<br> 4.移向肉将导致奖励`+100`|奖励模式：语言模型应该因产生“好的反应”而获得奖励，而不应该因产生“坏的反应”而获得任何奖励。|
|策略：策略规定代理如何在其所处的状态下选择要执行的操作：{% mathjax %}a_t\sim \pi(\cdot | s_t){% endmathjax %}|策略：对于语言模型来说，策略就是语言模型本身！因为它根据代理的当前状态模拟动作空间的概率：{% mathjax %}a_t\sim \pi(\cdot | s_t){% endmathjax %}|
|{% asset_img d_3.png %}|{% asset_img d_4.png %}|

`RL`中的目标是选择一种策略，当代理按照该策略采取行动时，该策略可以最大化预期回报。

#### 奖励模型架构

当我们将一串`token`作为语言模型（通常是`Transformer`模型）的输入时，它会生成一个隐藏状态列表，每个隐藏状态对应一个输入`token`，这是一个“捕获”其之前所有`token`信息的嵌入。然后，隐藏状态通过线性层转换为逻辑，然后使用`softmax`函数转换为概率。要生成响应的奖励，我们只需使用响应的最后一个`token`的隐藏状态，将其发送到线性层（只有一个输出特征），并将其用作与输入相关的奖励值。
{% asset_img r_1.png %}

#### 奖励模型损失

为了使用强化学习(`RL`)优化语言模型的行为，我们需要一个评分模型，该模型为语言模型生成的每个响应提供数值。现在我们有了一个数据集，可以根据查询（提示）定义我们喜欢哪个答案，我们可以构建一个神经网络，为每个响应提供数值分数。
{% asset_img r_2.png %}

{% mathjax '{"conversion":{"em":14}}' %}
Loss = -\log\sigma(r(x,y_w) - r(x,y_l))
{% endmathjax %}
在这里我们分析一下这个损失函数：有两种可能性：
- 当{% mathjax %}r(x,y_w) > r(x,y_l){% endmathjax %}时，`Sigmoid`返回一个大于`0.5`的值，损失将返回一个很小的负值（顺序正确的情况下损失值将会很小）。
- 当{% mathjax %}r(x,y_w) < r(x,y_l){% endmathjax %}时，`Sigmoid`返回一个小于`0.5`的值，损失将返回一个很大的负值（顺序错误的情况下损失值将会很大）。

这种损失将导致模型对“好”的答案给予高奖励，对“坏”的答案给予低奖励，因为这是模型最小化损失的唯一方法。
