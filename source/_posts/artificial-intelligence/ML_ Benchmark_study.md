---
title: 模型(LLM)基准-探析
date: 2024-07-28 17:25:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

`LLM`基准如何运作？从本质上讲，`LLM`基准测试遵循一个相当简单的原则：给模型一个任务，看看它表现如何，然后测量评估结果。但是，在评估可靠性方面存在一些细微差别。运行基准测试有以下几种方法：
- **零样本**：模型在没有任何先前示例或提示的情况下接受任务。这展示了其理解和适应新情况的原始能力。
- **少量样本**：在要求`LLM`解决类似任务之前，会先给其一些如何完成任务的示例。这揭示了其从少量数据中学习的能力。
- **微调**：在这种情况下，`LLM`专门针对与基准任务相关的数据进行训练，目的是最大限度地提高其在该特定领域的熟练程度。如果微调有效，它将展示模型在任务中的最佳性能。
<!-- more -->

区分一个`LLM`的表现优于另一个`LLM`是纯粹出于运气还是实际技能差异非常重要。因此，坚持严格的**统计完整性**很重要。鉴于此，在将模型与竞争对手的性能进行基准测试时，必须明确模型是针对**特定任务**部署在**零样本**、**少量样本**还是**微调**能力下。可以使用哪些指标来比较LLM的表现？
- **准确性**(`Accuracy`)：许多基准的基石，这只是大语言模型(`LLM`)完全正确答案的百分比。
- **BLEU分数**(`BLEU Score`)：衡量`LLM`生成的文本与人工编写的参考文献的匹配程度。这对于翻译和创意写作等任务很重要。
- **困惑度**(`Perplexity`)：`LLM`面对任务时表现出的惊讶或困惑程度。困惑度越低，理解能力越好。
- **人工评估**(`Human Evaluation`)：基准非常有用，但有时细微的任务需要专家对`LLM`输出的**质量、相关性**或**连贯性**进行判断。

#### LLM基准

语言很复杂，这意味着需要进行各种测试才能确定大语言模型(`LLM`)的真正能力。以下是一些最常见的基准，用于评估大语言模型(`LLM`)在人工智能的普遍应用中的表现，以及它们的工作原理和用途。
<table>
<caption>LLM基准(Benchmarks)</caption>
<tr><th>Category</th><th>Benchmark</th><th>Descrption</th></tr>
<tr>
    <td rowspan="3">通用</td>
    <td>MMLU Chat(0-shot, CoT)</td>
    <td rowspan="2">大规模多任务语言理解(Massive Multitask Language Understanding, MMLU)是一个用于评估语言模型多任务处理能力的基准测试。由UC Berkeley的研究人员于2020年推出，MMLU测试旨在通过零样本(0-shot：模型在没有任何示例的情况下直接进行任务)和少样本(few-shot：模型在提供少量示例后进行任务)设置评估模型在预训练期间获得的知识和能力。这种测试方法使得基准测试更具挑战性，更接近于人类评估的方式。MMLU涵盖了57个任务，涉及多个领域，包括：基础数学、美国历史、计算机科学、法律、道德和伦理学。这些任务要求模型展示广泛的知识基础和卓越的问题解决能力，评估其在不同领域的表现。</td>
</tr>
<tr>
    <td>MMLU PRO(5-shot, CoT)</td>
</tr>
<tr>
    <td>IFEval</td>
    <td>IFEval(Instruction-Following Evaluation)是一个用于评估大型语言模型(LLMs)遵循自然语言指令能力的基准测试。IFEval由Zhou等人于2023年提出，旨在通过一组“可验证的指令”来客观地评估模型的指令跟随能力。可验证指令：IFEval专注于一组可以客观验证的指令（例如，“写超过400个单词”，“至少提到3次AI关键词”）。这些指令的设计使得评估过程更加清晰和客观，避免了人工评估的主观性和基于LLM的自动评估可能存在的偏见。广泛的指令覆盖：IFEval确定了25种可验证指令，并构建了约500个提示，每个提示包含一个或多个可验证指令。这些指令涵盖了多种任务，确保评估的全面性。</td>
</tr>
<tr>
    <td rowspan="2">Code</td>
    <td>HumanEval(0-shot)</td>
    <td>HumanEval(0-shot)是HumanEval基准测试的一种特定设置，用于评估大型语言模型在没有任何示例或额外上下文的情况下生成代码的能力。零样本设置：在0-shot设置中，模型仅基于问题描述和函数签名生成代码，没有任何示例或额外的上下文信息。这种设置更能测试模型的真实能力和泛化性。<br><b>主要特点</b>：<br>pass@1：模型第一次生成的代码通过所有测试用例的概率；<br>pass@10：在10次生成中至少有一次通过所有测试用例的概率。</td>
</tr>
<tr>
    <td>MBPP EvalPlus(base) (0-shot)</td>
    <td>MBPP EvalPlus(base) (0-shot)是对原始MBPP(Mostly Basic Python Problems)基准测试的扩展和改进版本,用于评估大型语言模型在零样本设置下的Python代码生成能力。MBPP EvalPlus基于原始MBPP数据集,但进行了重要的改进和扩展。原始MBPP包含约1,000个Python编程问题,而MBPP EvalPlus选取了其中经过手动验证的高质量子集。</td>
</tr>

<tr>
    <td rowspan="2">Math</td>
    <td>GSM8K(8-shot, CoT)</td>
    <td>GSM8K(8-shot, CoT)是一种用于评估大型语言模型数学推理能力的基准测试方法。GSM8K(Grade School Math 8K)是一个包含约8,000个小学数学应用题的数据集。这些问题涵盖了基础算术、代数、几何等多个领域，难度适合小学生水平。在8-shot设置中，模型会先看到8个示例问题及其解答过程，然后再回答新的测试问题。这种少样本学习方法旨在测试模型的快速适应能力。CoT是一种提示方法，鼓励模型生成详细的推理步骤，而不仅仅是直接给出最终答案。在GSM8K任务中，CoT通常包括以下步骤：理解问题，分解问题为子步骤，逐步计算，得出最终答案。GSM8K(8-shot, CoT)主要使用准确率作为评估指标。模型需要通过正确的推理步骤得出准确的最终答案。</td>
</tr>
<tr>
    <td>MATH(0-sho, CoT)</td>
    <td>MATH(0-shot, CoT)是一种用于评估大型语言模型数学推理能力的基准测试方法。<br><b>主要特点</b>：<br>零样本设置(0-shot):模型在没有看到任何示例的情况下直接回答问题,测试其内在的数学推理能力。<br>思维链(Chain-of-Thought, CoT)推理:鼓励模型生成详细的推理步骤,而不仅仅是给出最终答案。通常通过在问题后添加"Let's think step by step"等提示来实现。<br>数据集:通常使用MATH数据集,这是一个包含高中和大学水平数学问题的数据集,涵盖代数、几何、微积分等多个领域。<br><b>评估方法</b>：<br>主要使用准确率作为评估指标。<br>模型需要通过正确的推理步骤得出准确的最终答案。<br>有时还会评估模型生成的推理步骤的质量和合理性。</td>
</tr>

<tr>
    <td rowspan="3">推理</td>
    <td>ARC Challenge(0-shot)</td>
    <td>ARC Challenge(0-shot)是一种用于评估人工智能模型抽象推理能力的基准测试。<br><b>主要特点</b>：<br>零样本设置(0-shot)：模型在没有看到任何示例的情况下直接解决问题，测试其内在的抽象推理能力。<br>数据集：使用Abstraction and Reasoning Corpus(ARC) 数据集，包含一系列抽象视觉推理任务。<br>任务类型：每个任务包含输入-输出对的训练示例和一个测试输入，模型需要推断规则并生成正确的测试输出。<br><b>评估方法<b>：<br>1.主要使用准确率作为评估指标。<br>2.模型需要正确推断规则并生成与人类标注一致的测试输出。</td>
</tr>
<tr>
    <td>GPQA(0-shot, CoT)</td>
    <td>GPQA(0-shot, CoT)是一种用于评估大型语言模型在高难度问答任务中的推理能力的基准测试方法。GPQA(Graduate-Level Google-Proof Q&A)是一个包含高难度问答任务的数据集，旨在评估模型在面对复杂问题时的推理和回答能力。问题涵盖多个学科领域，包括科学、技术、工程和数学(STEM)。在零样本设置中，模型在没有任何示例或额外上下文的情况下直接回答问题。这种设置测试模型的内在知识和推理能力，而不依赖于特定的训练示例。<br><b>评估方法</b>：<br>GPQA(0-shot, CoT)主要使用准确率作为评估指标。模型需要通过正确的推理步骤得出准确的最终答案。</td>
</tr>
<tr>
    <td>DROP(F1)</td>
    <td>DROP(Discrete Reasoning Over Paragraphs)是一种用于评估自然语言处理模型在段落级别进行离散推理能力的基准测试。该任务要求模型从英文段落中提取相关信息，并对其进行离散推理，例如排序或计数，以得出正确答案。评估指标：DROP使用自定义的F1分数和精确匹配分数来评估模型的表现。F1分数衡量模型预测与正确答案之间的重合度，而精确匹配分数则评估模型预测是否与正确答案完全一致。</td>
</tr>

<tr>
    <td rowspan="2">工具使用</td>
    <td>BFCL</td>
    <td>BFCL(Benchmark for Few-shot Compositional Learning)是一个用于评估大型语言模型在少样本学习和组合学习任务中表现的基准测试。<br><b>主要特点</b>：<br>少样本学习：BFCL旨在测试模型在只有少量示例的情况下学习和应用新知识的能力。<br>组合学习：评估模型将已知概念组合成新概念的能力，这是人类智能的一个关键特征。<br>工具使用：BFCL包含需要模型使用外部工具或 API来解决问题的任务，测试模型的工具使用能力。<br>多样化任务：包括各种类型的任务，如自然语言处理、代码生成、数学推理等。<br><b>评估方法</b>：<br>BFCL使用多种指标来评估模型性能，包括准确率、完成率等。<br>对于工具使用任务，还会评估模型正确调用和使用工具的能力。</td>
</tr>
<tr>
    <td>Nexus(0-shot)</td>
    <td>Nexus(0-shot)是一个用于评估大型语言模型在零样本设置下执行复杂任务能力的基准测试。<br><b>主要特点</b>：<br>零样本设置：模型在没有任何示例或额外上下文的情况下直接回答问题，测试其内在知识和推理能力。<br>复杂任务：Nexus包含一系列需要多步推理和综合能力的复杂任务。<br>多领域覆盖：任务涵盖多个领域，可能包括科学、技术、工程、数学等多个学科。<br><b>评估方法<b>：<br>Nexus(0-shot)主要使用准确率作为评估指标。模型需要通过正确的推理步骤得出准确的最终答案。</td>
</tr>

<tr>
    <td rowspan="3">长上下文</td>
    <td>ZeroSCROLLS/QuALITY</td>
    <td>ZeroSCROLLS是一个用于评估大型语言模型在长文本理解任务中的零样本表现的基准测试。<br><b>主要特点</b>：<br>零样本设置(0-shot)：ZeroSCROLLS仅包含测试集和小规模验证集，没有训练数据。这种设置测试模型在没有示例的情况下直接处理任务的能力。<br>多任务覆盖：ZeroSCROLLS包含十个自然语言处理任务，涉及多个领域，包括摘要生成、问答、情感分类和信息重排序等。<br>长文本处理：所有任务都要求模型处理和理解长文本，这对模型的记忆和推理能力提出了更高的要求。<br><b>评估方法</b>：<br>ZeroSCROLLS使用多种评估指标来衡量模型的表现，包括ROUGE分数（用于摘要任务）、F1分数（用于问答任务）和准确率（用于多项选择问答任务）。</td>
</tr>
<tr>
    <td>InfiniteBench/En.MC</td>
    <td>InfiniteBench/En.MC是一个用于评估大型语言模型在多项选择问答任务中的零样本表现的基准测试。<br><b>主要特点</b>：<br>零样本设置(0-shot)：模型在没有任何示例或额外上下文的情况下直接回答问题，测试其内在知识和推理能力。<br>多项选择问答(Multiple Choice Question Answering)：任务要求模型从多个选项中选择一个正确答案，这种设置能够有效评估模型的知识覆盖和推理能力。<br>多领域覆盖：问题涵盖多个领域，包括科学、技术、工程、数学(STEM)以及人文社科等多个学科，确保评估的全面性。<br><b>评估方法</b>：<br>准确率(Accuracy)：主要使用准确率作为评估指标，模型需要选择正确的答案以获得高分。<br>自动化评估：评估过程完全自动化，确保结果的客观性和一致性。</td>
</tr>
<tr>
    <td>NIH/Multi-needle</td>
    <td>-</td>
</tr>

<tr>
    <td>多语言</td>
    <td>Multilingual MGSM(0-shot)</td>
    <td>Multilingual MGSM(0-shot)是一个用于评估大型语言模型在多语言数学问题解决能力方面的基准测试。<br><b>主要特点</b>：：<br>多语言设置：这个基准测试包含多种语言版本的数学问题，旨在评估模型在不同语言环境下的数学推理能力。<br>零样本设置(0-shot)：模型在没有任何特定于任务的训练或微调的情况下直接回答问题，测试其内在的数学推理能力。<br>基于MGSM数据集：MGSM(Multilingual Grade School Math)是一个包含小学数学应用题的多语言数据集，涵盖了基础算术、代数、几何等多个领域。<br><b>评估方法</b>：<br>准确率：主要使用准确率作为评估指标，模型需要给出正确的最终答案。<br>跨语言表现：评估模型在不同语言版本的相同问题上的表现一致性。</td>
</tr>

<tr>
    <td>特殊</td>
    <td>Big-Bench Hard(BBH)</td>
    <td>Big-Bench Hard (BBH)是一个从BIG-Bench评估套件中精选出来的子集,包含23个特别具有挑战性的任务。这些任务在之前的语言模型评估中未能超过平均人类评分者的表现。BBH旨在评估当前语言模型在特别困难任务上的能力,并探索如何提高模型在这些任务上的表现。<br><b>主要特点：</b><br>多数BBH任务需要多步推理。<br>涵盖了各种复杂的认知和推理能力。<br><b>评估方法</b>：<br>传统方法:使用少样本提示(Few-Shot Prompting)。<br>改进方法:应用思维链(Chain-of-Thought, CoT)提示。</td>
</tr>
</table>
