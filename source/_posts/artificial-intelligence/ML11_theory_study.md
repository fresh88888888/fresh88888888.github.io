---
title: 机器学习(ML)(十一) — 推荐系统探析
date: 2024-10-16 15:50:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**推荐系统**(`Recommendation system`)的商业影响和实际使用案例数量甚至远远超过学术界的关注程度。每次你访问京东`app`、淘宝`app`、美团`app`等或腾讯视频等电影流媒体网站，或者访问提供短视频(抖音、快手)应用时，这类应用都会向你推荐他们认为你可能想买的东西、他们认为你可能想看的电影或他们认为你可能想尝试的餐馆。对于许多公司来说，很大一部分销售额是由他们的**推荐系统**(`Recommendation system`)推动的。因此，对于许多公司来说，**推荐系统**(`Recommendation system`)带来的经济效益或价值非常大。因此，我们很有必要深入了解一下什么是**推荐系统**(`Recommendation system`)。
<!-- more -->

我将使用预测电影评分的应用作为示例。假设您经营一家大型电影流媒体网站，您的用户使用一到五颗星对电影进行评分。因此，在典型的**推荐系统**(`Recommendation system`)中，您有一组用户，这里有四个用户`Alice、Bob Carol`和`Dave`。用户编号为`1、2、3、4`。以及一组电影《爱在最后》、《浪漫永恒》、《可爱的小狗》、《不停歇的汽车追逐》和《剑与空手道》。用户所做的就是将这些电影评为一到五颗星。假设`Alice`给《爱在最后》评了五颗星，给《浪漫永恒》评了五颗星。也许她还没有看过《可爱的小狗》，所以没有对这部电影进行评分。则通过问号来表示，她认为《不停歇的汽车追逐》和《剑与空手道》应该得零颗星等。在**推荐系统**(`Recommendation system`)中，你有一定数量的用户和一定数量的项目。在这种情况下，**项目**是您想要推荐给用户的电影。尽管在这个例子中使用的是电影，但同样的逻辑或同样的东西也适用于任何东西，从产品或网站到餐馆，甚至推荐哪些媒体的文章、要展示的社交媒体文章，对用户感到更有趣的东西。这里使用的符号是{% mathjax %}n_u{% endmathjax %}来表示用户数量。所以在这个例子中，{% mathjax %}n_u = 4{% endmathjax %}，因为你有四个用户{% mathjax %}n_m{% endmathjax %}表示电影数量或实际上是项目数量。所以在这个例子中，{% mathjax %}n_m = 5{% endmathjax %}，因为我们有五部电影。如果用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}进行了评分，将设置{% mathjax %}r(i,j) = 1{% endmathjax %}。假设`Dallas Alice`对电影`1`进行了评分，但尚未对电影`3`进行评分，因此{% mathjax %}r(1,1) = 1{% endmathjax %}，因为她对电影`1`进行了评分，但{% mathjax %}r(3,1) = 0{% endmathjax %}，因为她尚未对电影`3`进行评分。最后使用{% mathjax %}y^{(i,j)}{% endmathjax %}表示用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}给出的评分。例如，此处的评分将是用户`2`对电影`3`的评分等于{% mathjax %}y^{(3,2)} = 4{% endmathjax %}。请注意，并非每个用户都会对每部电影进行评分，因此系统需要知道哪些用户对哪些电影进行了评分。这就是为什么用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}进行了评分，将定义为{% mathjax %}r(i,j) = 1{% endmathjax %}，如果用户{% mathjax %}j{% endmathjax %}尚未对电影{% mathjax %}i{% endmathjax %}进行评分，则{% mathjax %}r(i,j) = 0{% endmathjax %}。使用此**推荐系统**(`Recommendation system`)框架，解决问题的一种方法是查看用户尚未评分的电影。并尝试预测用户对这些电影的评分，因为这样我们就可以尝试向用户推荐他们更有可能评为五星的电影。

如果我们有每件商品的特征或每部电影的特征，我们该如何开发一个推荐系统？这里四个用户对五部电影中的部分电影进行了评分。在这里添加了两个特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}，它们分别表示每一部电影在多大程度上是爱情电影或动作电影。例如，《爱在最后》是一部非常浪漫的电影，所以这个特征取`0.9`，并且它不是一部动作电影。所以这个特征取`0`。《不停歇的汽车追逐》中只有一点点浪漫。所以它的值为`0.1`，但它有大量的动作。所以这个特征取值为`1.0`。这时的用户数量表示为：{% mathjax %}n_u = 4{% endmathjax %}，电影数量表示为{% mathjax %}n_m = 5{% endmathjax %}。最后引入{% mathjax %}n{% endmathjax %}来表示特征数量。因此{% mathjax %}n = 2{% endmathjax %}。有了这些特征，则第一部电影（即电影《爱在最后》）的特征为{% mathjax %}x^{(1)} = \begin{bmatrix}0.9 \\0 \end{bmatrix}{% endmathjax %}。第三部电影《可爱的小狗》的特征为{% mathjax %}x^{(3)} =  \begin{bmatrix}0.99 \\0 \end{bmatrix}{% endmathjax %}。如何预测`Alice`的电影评分。假设预测电影{% mathjax %}i{% endmathjax %}的评分为{% mathjax %}w\cdot x^{(i)} + b{% endmathjax %}。这个很像线性回归。例如，如果最终选择参数{% mathjax %}w^{(1)} =  \begin{bmatrix}5 \\0 \end{bmatrix}{% endmathjax %}并假设{% mathjax %}b^{(1)} = 0{% endmathjax %}，那么对于电影三的预测，其特征为{% mathjax %}x^{(3)} = \begin{bmatrix}0.99 \\0 \end{bmatrix}{% endmathjax %}，第一个特征为`0.99`，第二个特征为`0`。我们的预测将是{% mathjax %}w^{(1)}\cdot x^{(3)} + b^{(1)} = 4.95{% endmathjax %}。这个评分似乎很合理。看起来`Alice`对《爱在最后》和《永远的浪漫》这两部非常浪漫的电影给出了高评分，但对动作片《不停歇的汽车追逐》和《剑与空手道》给出了低评分。所以对于《可爱的小狗的爱》预测给出`4.95`分似乎很合理。因此，参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}对于`Alice`来说似乎是很合理的模型。对于多个用户来说，只需添加一些符号。在这里添加上标`1`，表示用户`1`的参数{% mathjax %}w^{(1)},\;b^{(1)}{% endmathjax %}，这样为数据集上的`4`个用户中的每一个设置不同的参数。将用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}的评分预测为{% mathjax %}w^{(j)}\cdot x^{(i)} + b^{(j)}{% endmathjax %}。这里的参数{% mathjax %}w^{(j)}{% endmathjax %}和{% mathjax %}b^{(j)}{% endmathjax %}是用于预测用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}评分的参数，{% mathjax %}x^{(i)}{% endmathjax %}是电影{% mathjax %}i{% endmathjax %}的特征。这很像**线性回归**，只是为数据集中的`4`个用户中的每一个拟合不同的线性回归模型。让我们看看如何为该算法制定成本函数。这里注意，如果用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}进行了评分，则{% mathjax %}r^{(i,j)} = 1/0{% endmathjax %}。{% mathjax %}y^{(i,j)}{% endmathjax %}用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}的评分。这里引入一种新的符号，使用{% mathjax %}m^{(j)}{% endmathjax %}表示用户{% mathjax %}j{% endmathjax %}评分的电影数量。如果用户评分了`4`部电影，则{% mathjax %}m^{(j)} = 4{% endmathjax %}。如果用户评分了`3`部电影，则{% mathjax %}m^{(j)} = 3{% endmathjax %}。预测的**成本函数**为{% mathjax %}\frac{1}{2m^{(j)}}\sum\limits_{i:r(i,j) = 1}(w^{(j)}\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2{% endmathjax %}。选择参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}来最小化预测评分和观察实际评分之间的**平方误差**。但是用户还没有对所有电影进行评分，如果要对此求和，我们将仅对{% mathjax %}r^{(i,j)} = 1{% endmathjax %}的{% mathjax %}i{% endmathjax %}值求和。意味着用户{% mathjax %}j{% endmathjax %}对该电影{% mathjax %}i{% endmathjax %}进行了评分。最后，对{% mathjax %}m^{(j)}{% endmathjax %}进行归一化为`1`。这类似于用于**线性回归**的**成本函数**。这就是{% mathjax %}w^{(j)},b^{(j)}{% endmathjax %}的成本函数{% mathjax %}\mathbf{J}{% endmathjax %}。如果我们将其最小化，会得到一组相当不错的参数{% mathjax %}w^{(j)}{% endmathjax %}和{% mathjax %}b^{(j)}{% endmathjax %}用于预测用户{% mathjax %}j{% endmathjax %}的评分。再给这个成本函数添加一个项，即防止**过度拟合**的正则化项。即{% mathjax %}\frac{\lambda}{2m^{(j)}}{% endmathjax %}。对于**推荐系统**(`Recommendation system`)来说，{% mathjax %}m^{(j)}{% endmathjax %}只是一个常数。因此，即使把它取出来，也应该得到相同的{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}值。我们将**最小化成本函数**作为{% mathjax %}w^{(j)}{% endmathjax %}和{% mathjax %}b^{(j)}{% endmathjax %}的函数{% mathjax %}\underset{w^{(j)},b^{(j)}}{\min}\mathbf{J}(w^{(j)},b^{(j)}) = \frac{1}{2m^{(j)}}\sum\limits_{i:r(i,j) = 1}(w^{(j)}\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum\limits_{k = 1}^n (w_k^{(j)})^2{% endmathjax %}。为了学习所有用户的参数{% mathjax %}w^{(1)}、b^{(1)}、w^{(2)}、b^{(2)}、\ldots、w^{(n_u)}、b^{(n_u)}{% endmathjax %}，则成本函数的计算公式为{% mathjax %}\mathbf{J}\left(\begin{array}{ccc} w^{(1)} ,& \ldots &, w^{(n_u)}\\ b^{(1)} ,& \ldots &, b^{(n_u)}\end{array}\right) = \frac{1}{2m^{(j)}}\sum\limits_{j=1}^{n_u}\sum\limits_{i:r(i,j) = 1}(w^{(j)}\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum\limits_{j=1}^{n_u}\sum\limits_{k = 1}^n (w_k^{(j)})^2{% endmathjax %}。这成为学习所有用户的所有参数的成本。如果我们使用**梯度下降法**或其他优化算法将其最小化{% mathjax %}w^{(1)}、b^{(1)}、w^{(2)}、b^{(2)}、\ldots、w^{(n_u)}、b^{(n_u)}{% endmathjax %}的**成本函数**为{% mathjax %}\underset{w^{(1)},b^{(1)},w^{(2)},b^{(2)},\ldots,w^{(n_u)},b^{(n_u)}}{\min}\;\frac{1}{2m^{(j)}}\sum\limits_{j=1}^{n_u}\sum\limits_{i:r(i,j) = 1}(w^{(j)}\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum\limits_{j=1}^{n_u}\sum\limits_{k = 1}^n (w_k^{(j)})^2{% endmathjax %}，那么你将拥有一组非常好的参数来预测所有用户的电影评分。请注意，这个算法很像**线性回归**，它的作用类似于**线性回归**的输出{% mathjax %}f(x){% endmathjax %}。只是现在需要为每个用户训练一个不同的**线性回归模型**。将这些特征{% mathjax %}x_1,x_2{% endmathjax %}输入模型，就可以学习参数并预测电影评分。

#### 协同过滤

如果每部电影都有特征，例如特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}可以表明这是一部爱情片还是一部动作片。那么可以使用**线性回归**来预测电影评级。但是，如果没有这些特征该怎么办？如何从数据找到这些特征{% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}。如下图所示，假设我们学习了用户`1`的参数{% mathjax %}w^{(1)}= \begin{bmatrix}5 \\0 \end{bmatrix}{% endmathjax %}，{% mathjax %}b^{(1)} = 0{% endmathjax %}。{% mathjax %}w^{(2)}= \begin{bmatrix}5 \\0 \end{bmatrix}{% endmathjax %}，{% mathjax %}b^{(2)} = 0{% endmathjax %}。{% mathjax %}w^{(3)}= \begin{bmatrix}0 \\5 \end{bmatrix}{% endmathjax %}，{% mathjax %}b^{(3)} = 0{% endmathjax %}，对于用户`4`，{% mathjax %}w^{(4)}= \begin{bmatrix}0 \\5 \end{bmatrix}{% endmathjax %}，{% mathjax %}b^{(4)} = 0{% endmathjax %}。要预测用户{% mathjax %}j{% endmathjax %}对电影{% mathjax %}i{% endmathjax %}的评分，则{% mathjax %}w^{(j)}\cdot x^{(i)} + b^{(j)}{% endmathjax %}。
{% asset_img ml_1.png %}

为了简化这个例子，我们将设置{% mathjax %}b = 0{% endmathjax %}，{% mathjax %}b{% endmathjax %}的值忽略。考虑到`Alice`给第一部电影的评分是`5`，则{% mathjax %}w^{(1)}\cdot x^{(1)} = 5{% endmathjax %}，因为`Bob`给它评分也是`5`，所以{% mathjax %}w^{(2)}\cdot x^{(2)} = 5{% endmathjax %}。{% mathjax %}w^{(3)}\cdot x^{(1)} \approx 0{% endmathjax %}，{% mathjax %}w^{(4)}\cdot x^{(1)} \approx 0{% endmathjax %}。如果第一部电影的特征是`1、0`，在这种情况下，{% mathjax %}w^{(1)}\cdot x^{(1)} = 5{% endmathjax %}，{% mathjax %}w^{(2)}\cdot x^{(1)} = 5{% endmathjax %}，类似地，{% mathjax %}w^{(3)}\cdot x^{(1)} = 0{% endmathjax %}，{% mathjax %}w^{(4)}\cdot x^{(1)} = 0{% endmathjax %}。如果你有这些参数向量，你也可以尝试为第二部电影提出特征向量{% mathjax %}x^{(2)}{% endmathjax %}，为第三部电影提出特征向量{% mathjax %}x^{(3)}{% endmathjax %}，依此类推，让算法对这些电影的预测接近用户给出的实际评分。让我们设计出一个**成本函数**来学习{% mathjax %}x^{(1)}{% endmathjax %}和{% mathjax %}x^{(2)}{% endmathjax %}的值。但在协同过滤中，这是因为您有来自同一部电影的同一项目的多个用户的评分。这使我们能够尝试猜测这些特征的可能值。给定{% mathjax %}w^{(1)}、b^{(1)}、w^{(2)}、b^{(2)}、\ldots、w^{(n_u)}、b^{(n_u)}{% endmathjax %}等。如果您想了解特定电影的特征{% mathjax %}x^{(i)}{% endmathjax %}，使用的成本函数为{% mathjax %}\mathbf{J}(x^{(i)}) = \frac{1}{2}\sum\limits_{j:r(i,j) = 1} (w^{(j)}\cdot x^{(i)} + b^{(j)} -y^{(i,j)})^2 + \frac{\lambda}{2}\sum\limits_{k=1}^n (x_k^{(i)})^2{% endmathjax %}。最后，为了学习所有特征{% mathjax %}x^{(1)},\ldots,x^{(n_m)}{% endmathjax %}（有{% mathjax %}n_m{% endmathjax %}部电影），在则成本函数调整为：{% mathjax %}\mathbf{J}(x^{(1)},x^{(2)},\ldots,x^{(n_m)}) = \frac{1}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{j:r(i,j) = 1} (w^{(j)}\cdot x^{(i)} + b^{(j)} -y^{(i,j)})^2 + \frac{\lambda}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^n (x_k^{(i)})^2{% endmathjax %}，这就是学习数据集中所有电影特征的**成本函数**。如果您有参数{% mathjax %}w{% endmathjax %}和{% mathjax %}b{% endmathjax %}，那么使用**梯度最小化**，将其作为{% mathjax %}x^{(1)},x^{(2)},\ldots,x^{(n_m)}{% endmathjax %}的**成本函数**{% mathjax %}\underset{x^{(1)},x^{(2)},\ldots,x^{(n_m)}}{\min}\;\frac{1}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{j:r(i,j) = 1} (w^{(j)}\cdot x^{(i)} + b^{(j)} -y^{(i,j)})^2 + \frac{\lambda}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^n (x_k^{(i)})^2{% endmathjax %}，对于**协同过滤**，通过学习参数{% mathjax %}w^{(1)}、b^{(1)}、w^{(2)}、b^{(2)}、\ldots、w^{(n_u)}、b^{(n_u)}{% endmathjax %}最小化**成本函数**为{% mathjax %}\underset{w^{(1)},b^{(1)},w^{(2)},b^{(2)},\ldots,w^{(n_u)},b^{(n_u)}}{\min}\;\frac{1}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{i:r(i,j) = 1}(w^{(j)}\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{k = 1}^n (w_k^{(j)})^2{% endmathjax %}，将上下两个成本函数合并为：{% mathjax %}\mathbf{J}(w,b,x) =  \frac{1}{2}\sum\limits_{(i,j):r(i,j) = 1} (w^{(j)}\cdot x^{(i)} + b^{(j)} -y^{(i,j)})^2 + \frac{\lambda}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{k = 1}^n (w_k^{(j)})^2 + \frac{\lambda}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^n (x_k^{(i)})^2{% endmathjax %}。最后计算参数梯度下降的值为{% mathjax %}w_{i}^{(j)} = w_i^{(j)} - \alpha\frac{\partial}{\partial w_i^{(j)}}\mathbf{J}(w,b,x),\; b^{(j)} = b^{(j)} - \alpha\frac{\partial}{\partial b^{(j)}}\mathbf{J}(w,b,x),\; x_{k}^{(i)} = x_k^{(i)} - \alpha\frac{\partial}{\partial x_k^{(i)}}\mathbf{J}(w,b,x){% endmathjax %}。这里的算法称为**协同过滤算法**，**协同过滤**指的是多个用户协同评价了同一部电影，让你对这部电影有所了解，让你可以猜测这部电影的特征是什么，反过来可以预测其他尚未评价一部电影的用户如何评价它。这种**协同过滤**是从多个用户收集数据。用户之间的这种**协作**可以帮助您预测未来其他用户的评分。

**推荐系统**(`Recommendation system`)或**协同过滤**算法都涉及到了**二元标签**，如何从**线性回归**到**逻辑回归**，再到预测数字，再到预测**二元标签**，这是一个带有**元标签**的**协同过滤**数据集的示例。标签`1`指出用户喜欢某部电影。意味着`Alice`从头到尾看完了电影《爱在最后》和《浪漫永恒》。问号通常表示用户尚未看该商品，因此无法决定是否对该特定商品点赞或收藏。如何将**协同过滤算法**应用于此数据集。在具有**二元标签**的协同过滤中，有很多方法可以定义标签`1`和标签`0`。在一个在线购物网站中，标签可以表示用户{% mathjax %}j{% endmathjax %}在看到商品后是否选择购买商品。标签`1`表示购买了商品，标签`0`表示没有购买商品。问号表示没有看到商品。或者在社交媒体环境中，标签`1`或`0`可以表示用户在看到某项商品后是否喜欢或不喜欢该商品。问号表示用户尚未看到该商品，或者许多网站不要求用户明确评分，而是使用用户行为来猜测用户是否喜欢该商品。例如，您可以测量用户是否在某项商品上花费了至少`30`秒。如果是，则将其标记为`1`；如果用户看到了某项商品但没有花费至少`30`秒，则将其标记为`0`。如果用户尚未看到该商品，则将其标记为问号。另一种根据用户行为隐式生成评分的方法是查看用户是否点击了某项商品。这通常在在线广告中完成，如果用户看到了广告，如果他们点击了该广告，则将其标记为`1`，如果他们没有点击，则将其标记为`0`；如果用户甚至没有看到该广告，则标记为问号。这些`二元标签`通常具有以下粗略含义。`1`表示用户在看到某件商品后参与其中，而参与可能意味着他们点击或花费`30`秒或明确喜欢或想购买该商品。`0`表示用户在看到该商品后没有参与，问号表示该商品尚未显示给用户。给定这些**二元标签**，该算法与**线性回归**非常相似，可以预测这些二元输出。之前，我们预测标签{% mathjax %}y^{(i,j)} = w^{(j)}\cdot x^{(i)} + b^{(j)}{% endmathjax %}。这很像**线性回归模型**。对于**二元标签**，预测{% mathjax %}y^{(i,j)}{% endmathjax %}的概率{% mathjax %}w^{(j)}\cdot x^{(i)} + b^{(j)}{% endmathjax %}。但它由公式{% mathjax %}g(w^{(j)}\cdot x^{(i)} + b^{(j)}){% endmathjax %}表示，其中{% mathjax %}g(z) = \frac{1}{1 + e^{-z}}{% endmathjax %}。这就是逻辑函数，就像在逻辑回归中看到的一样。我们要做的是将与**线性回归模型**转变为与**逻辑回归模型**，该模型将预测{% mathjax %}y^{(i,j)} = 1{% endmathjax %}的概率。为了构建此算法，我们还必须将成本函数从**平方误差成本函数**修改为更适合**二元标签**的成本函数，以用于**逻辑回归**模型。二元标签损失为{% mathjax %}y^{(i,j)}:\;f_{(w,b,x)}(x) = g(w^{(j)}\cdot x^{(i)} + b^{(j)}),\; L(f_{(w,b,x)}(x),y^{(i,j)}) = -y^{(i,j)}\log(f_{(w,b,x)}(x)) - (1-y^{(i,j)})\log(1- f_{(w,b,x)}(x)){% endmathjax %}，则二元标签协同过滤的成本函数为{% mathjax %}\mathbf{J}(w,b,x) = \sum\limits_{(i,j):r(i,j) = 1} L(f_{w,b,x}(x), y^{(i,j)}){% endmathjax %}。这也称为**二元交叉熵成本函数**。这就是采用**线性回归**（如**协同过滤算法**）并将其推广到**二元标签**的方法。