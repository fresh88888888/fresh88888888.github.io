---
title: 理解自注意力&多头注意力&交叉注意力&因果注意力（深度学习）
date: 2024-06-17 15:50:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 自注意力机制

自注意力等相关机制是LLM的核心组成部分。深度学习中的“注意力”概念源于改进循环神经网络(`RNN`)以处理较长的序列或句子所做的努力。例如，考虑将一个句子从一种语言翻译成另一种语言。逐字翻译一个句子通常不是一种选择，因为它忽略了每种语言独有的复杂语法结构和惯用表达，导致翻译不准确或无意义。
{% asset_img a_1.png  %}
<!-- more -->
为了解决这个问题，我们引入了注意力机制，以便在每个时间步骤中访问所有序列元素。关键是要有选择性，并确定哪些词在特定上下文中是最重要的。`2017`年，`Transformer`架构引入了独立的自注意力机制，完全消除了对`RNN`的需要。我们可以将自注意力机制视为一种通过包含有关输入上下文的信息来增强输入嵌入信息内容的机制。换句话说，自注意力机制使模型能够衡量输入序列中不同元素的重要性，并动态调整它们对输出的影响。这对于语言处理任务尤为重要，因为单词的含义可以根据句子或文档中的上下文而改变。

{% note warring %}
**注意**，自注意力机制有很多变体。其中特别关注的是提高自注意力机制的效率。然而，大多数论文仍然采用**缩放点积注意力机制**，因为它通常能带来更高的准确率，而且对于大多数训练大规模 `Transformer`的公司来说，自注意力机制很少成为计算瓶颈。
{% endnote %}
我们重点介绍原始的**缩放点积注意力机制**（称为自注意力机制），它仍然是实践中最流行、应用最广泛的注意力机制。

#### 嵌入输入句子

在开始之前，我们先考虑一个输入句子“`Life is short, eat dessert first`”，我们想将其放入自注意力机制中。与其他类型的文本处理建模方法（例如，使用循环神经网络或卷积神经网络）类似，我们首先创建一个句子嵌入。为简单起见，此处我们的词典`dc`仅限于输入句子中出现的单词。在实际应用中，我们会考虑训练数据集中的所有单词（典型词汇量在`30k~50k`之间）。
输入：
```python
sentence = 'Life is short, eat dessert first'
dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}
print(dc)
```
输出结果为：
```bash
{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}
```
