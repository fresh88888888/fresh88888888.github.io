---
title: 基于LoRA构建稀疏混合专家模型(MoE)的方法(MixLoRA)-探析(微调)
date: 2024-07-29 17:25:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

[`MixLoRA`](https://arxiv.org/pdf/2404.15159v2)是一种用于优化大规模语言模型(`LLMs`)微调的新方法，结合了`LoRA`(`Low-Rank Adaptation`)和专家混合(`Mixture of Experts, MoE`)技术。大规模语言模型的微调通常需要大量的计算资源和显存。`LoRA`通过引入低秩适配器，显著减少了微调时的参数数量和显存需求。然而，`LoRA`在多任务学习场景中的性能仍有提升空间。专家混合模型(`MoE`)在多任务学习中表现出色，但其资源需求对普通消费者级`GPU`来说是一个挑战。
<!-- more -->

**主要特点**：
- **架构设计**：1.`MixLoRA`在冻结的预训练密集模型的前馈网络块中插入多个`LoRA`专家模块；2.使用常见的`top-k`路由器（如`Mixtral`）或`top-1`开关路由器（如`Switch Transformers`）来动态选择合适的专家。
- **性能提升**：1.`MixLoRA`在多任务学习场景下的准确率比现有的参数高效微调(`PEFT`)方法提高了约`9%`；2.通过独立的注意力层`LoRA`适配器增强模型性能。
- **资源效率**：1.引入辅助负载平衡损失来解决路由器的不平衡问题；2.新的高吞吐框架在训练和推理过程中减少了`40%`的`GPU`显存消耗和`30%`的计算延迟。

大型语言模型(`LLM`)的**指令微调**已在自然语言处理(`NLP`)领域中取得了很好的成绩。随着参数规模的增加，`LLM`已被证明能够识别复杂的语言模式，从而实现强大的跨任务泛化能力。然而指令微调将会使其在计算资源和下游任务的性能之间做出权衡。为了减少全参数微调过程所需的计算和内存资源，引入了参数高效微调(`PEFT`)方法，其中，低秩自适应是流行的`PEFT`方法中的一种。`MixLoRA`通过结合`LoRA`和专家混合技术，提供了一种高效的微调大规模语言模型的方法，显著提升了多任务学习的性能，同时大幅减少了计算资源和显存的需求。
{% asset_img ml_1.png  "LoRA-MoE方法发布日期的时间表，包括有关集成位置的详细模型信息、如何使用LoRA-MoE方法进行训练以及它们旨在解决的问题" %}

`PEFT`，**大型语言模型**(`LLM`)在`NLP`任务中表现出了卓越的能力。在此之后，**指令微调**进一步使`LLM`能够理解人类意图并遵循指令，成为聊天系统的基础。但是，随着LLM规模的扩大，对其进行微调成为一个耗时且占用大量内存的过程。为了缓解这个问题，研究探索了不同的方法：**参数高效微调**(`PEFT`)、**蒸馏**、**量化**、**修剪**等。`LoRA`利用**低秩矩阵**分解线性层权重，是最流行的`PEFT`方法之一，不仅提高了模型性能，而且不会在推理过程中引入任何额外的计算开销。例如，`VeRA`结合可学习的缩放向量来调整跨层的共享冻结随机矩阵对。此外，`FedPara`专注于**联邦学习**场景的低秩`Hadamard`积。`Tied-Lora`实现了权重绑定以进一步减少可训练参数的数量。`AdaLoRA`使用**奇异值分解**(`SVD`)来**分解矩阵**并且修剪不太重要的奇异值以简化更新。`DoRA`将预训练权重分解为两个分量，即**幅度**和**方向**，并在微调期间利用`LoRA`进行方向更新，从而有效减少可训练参数的数量。

**专家混合**：专家混合(`MoE`)的概念可以追溯到`1991`年，它引入了一种新颖的监督学习方法，涉及多个网络（专家），每个网络专门处理一组训练示例。`MoE`的现代版本通过合并**稀疏激活的专家**来修改`Transformer`块内的前馈子层，从而能够在不增加计算量的情况下大幅增加**模型宽度**。`LLaVA-MoLE`有效地将`token`路由到`Transformer`层内的特定领域专家，从而缓解数据冲突并实现与普通`LoRA`基线一致的性能提升。对于其他基于`MoE`的架构，`MoRAL`解决了将`LLM`适应新领域/任务的挑战。`LoRAMoE`使用路由器网络集成`LoRA`，以缓解**知识遗忘**。`PESC`使用`MoE`架构将密集模型转换为稀疏模型，从而降低了计算成本和`GPU`内存要求。`MoE-LoRA`提出了一种新颖的参数高效的`MoE`方法，该方法具有分层专家分配(`MoLA`)，适用于`Transformer`的模型。`MoCLE`可根据指令集群激活定制任务的模型参数。`MixLoRA`将`LoRA`作为随机专家进行集成，从而降低了计算成本，同时扩展了模型容量并增强了`LLM`的泛化能力。
{% asset_img ml_2.png  "MixLoRA架构。MixLoRA由n位专家组成，由原始FFN子层与不同的LoRA组合而成，其中FFN子层的权重由所有专家共享" %}

