---
title: 机器学习(ML)(十九) — 强化学习探析
date: 2024-12-02 17:30:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**强化学习**(`RL`)背后的想法是**智能体**(`Agent`)通过与**环境**(`Environment`)交互（通过反复试验），并从环境中接收**奖励**(`Rewards`)作为执行**动作**(`Action`)的反馈来学习。从环境的互动中学习，源自于经验。这就是人类与动物通过互动进行学习的方式，**强化学习**(`RL`)是一个**解决控制任务**（也称**决策问题**）的框架，通过构建**智能体**(`Agent`)，通过反复试验与环境交互从环境中学习并获得奖励（正面或负面）作为独特反馈。**强化学习**(`RL`)只是一种从行动中学习的计算方法。
<!-- more -->

**任务**是**强化学习**(`RL`)问题的一个实例，这里有两种类型的任务：**情景式任务**和**持续式任务**。
- **情景式任务**：这种情况下，会有一个起点和终点（称为**终端状态**），这将创建一个情节：状态、动作、奖励和新状态的列表。
- **持续式任务**：这些任务会永远持续下去（没有终止状态），在这种情况下，**智能体**(`Agent`)必须学习如何选择最佳**动作**(`Action`)，并同时与环境进行交互，例如，一个执行自动股票交易的代理。对于此任务，没有起点和终点。

**探索**：是通过尝试随机动作来探索环境以便找到有关环境的更多信息。**利用权衡**：是利用已知信息来最大化回报。**强化学习**(`RL`)**智能体**(`Agent`)的目标是最大化预期累积奖励。我们需要平衡对环境的**探索程度**和**环境的利用程度**。因此，我们必须定义一个有助于处理这种权衡的规则。例如，选择餐厅的问题，**利用权衡**：你每天都去同一家你知道不错的餐厅，但却有可能错过另一家更好的餐厅。**探索**：尝试从未去过的餐厅，可能会有不愉快的经历，但也有可能获得美妙的体验。**策略**{% mathjax %}\pi{% endmathjax %}可以视为**智能体**(`Agent`)的大脑，它是一种函数，可以告知在指定状态下采取什么动作。
{% asset_img ml_1.png %}

**策略**{% mathjax %}\pi{% endmathjax %}就是我们要学习的函数，目标就是找到最优的**策略**{% mathjax %}\pi^{*}{% endmathjax %}，也就是当**智能体**(`Agent`)按照这个策略行动时，能够**最大化预期回报**的策略。通过训练找到{% mathjax %}\pi^{*}{% endmathjax %}。有2种方法通过训练**智能体**(`Agent`)来找到最佳策略{% mathjax %}\pi^{*}{% endmathjax %}：基于策略的和基于价值的方法。
- **基于策略的方法**：在**基于策略的方法**中，学习策略函数。此函数定义从每个状态到最佳动作的映射。或者定义该状态下可能动作集的**概率分布**。这里的策略分为：**确定性策略**，给定状态下的策略将始终返回相同的动作，记作{% mathjax %}a = \pi(s){% endmathjax %}。**随机性策略**：输出**动作**的概率分布，记作{% mathjax %}\pi(a|s) = P[A|s]{% endmathjax %}。
- **基于价值的方法**：在**基于价值的方法**中，不是学习策略函数，而是学习将状态映射到它的预期值的**价值函数**，记作{% mathjax %}v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s]{% endmathjax %}，这里可以看到**价值函数**为每一个可能得状态定义了值。


