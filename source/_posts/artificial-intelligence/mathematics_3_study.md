---
title: 数据科学 — 数学(三)（机器学习）
date: 2024-08-13 12:05:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 线性变换

##### 线性代数—张成

一组向量的**张成**(`span`)就是沿着这些向量的方向以任意组合移动到达的点的集合。例如，您已经看到这两个向量的**张成**(`span`)是平面，因为您可以通过沿着这两个方向移动到达平面上的任何点。同样，这两个向量的**张成**(`span`)也是平面。到达这些点可能需要一段时间，但可以只使用这两个方向。然而，这两个向量并不跨越平面。因为正如您之前所看到的，并非每个点都可以通过沿着这两个方向移动到达，它们是同一个方向。它们跨越哪一侧？那么这条线上的任何一点都可以通过沿着向量的方向移动到达，因此这两个向量的**张成**(`span`)就是那条线。
<!-- more -->

两个相隔`180`度的向量呢？那么**张成**(`span`)包含它们的线，因为您可以通过沿着这两个方向移动到达该线上的每个点。这个向量的**张成**(`span`)是多少？**张成**(`span`)就是包含它并通向原点的线，因为那是你沿着那个方向行走可以到达的点的集合。这里有一个问题。这两个向量是否构成这条线的基？你怎么看？答案实际上是否定的。原因是基需要是一个最小生成集。这里有太多的向量。看看这两个向量中的任何一个都跨越这条线，所以这两个向量太多了。**基**是一个最小生成集，所以它们中的每一个都是基。但是它们两个不是基。它们是**生成集**，那是这条线的基。事实上，任何从原点开始并朝着同一方向的向量也是这条线的基。
{% asset_img m_1.png  %}
{% asset_img m_2.png  %}

总之，**基是一个最小生成集**，所以左边的向量构成了这条线的基。但这两个向量不是，因为它们太多了。这种情况也发生在高维空间中。我们来看看这个集合。这个集合构成了**平面的基**，因为它跨越了平面，因为我们可以通过沿着这两个方向行走到达任何一点。但是如果我们移除其中任何一个，它们就不再跨越平面，现在它们只跨越一条线。然而，这组向量并不构成平面的基。它们跨越平面，因为平面上的任何一点都可以通过沿着这三个方向的组合移动到达。但它们太大了，不能成为基。这三个向量中的两个子集都是基，但第三个是多余的，所以它不是基。现在注意一些有趣的事情，**基的长度是平面维度的空间。空间基的长度就是该空间的维度**。左边的线有一个长度为`1`的基，它的维度为`1`，因为线的维度为`1`。右边的平面有一个长度为`2`的基，由两个向量组成，平面的维度为`2`。意味着任何空间的任何基实际上都具有与其他基相同数量的元素。例如三维空间，想象一下那里的基会是什么样子。现在你已经对基有了很好的直觉，让我们看看正式的定义。
{% asset_img m_3.png  %}

我们需要引入**线性无关**和**线性相关**向量的概念。如果一组向量中没有一个向量可以作为其他向量的线性组合获得，则该组向量被称为**线性无关**的。如果你只考虑平面中的一个向量，它总是线性无关的。现在让我们添加另一个向量。请注意，你不可能将新向量作为第一个向量的线性组合，因为它们指向不同的方向。这两个向量仍然是线性无关的。如果你添加另一个向量会怎么样？在这种情况下，红色的新向量指向与绿色向量相同的方向，但它的长度只是原始向量的`2`倍。由于一个向量可以通过其他向量的线性组合获得，因此这组向量称为**线性相关**。还要注意，即使我们在集合中添加了一个新向量，这些向量的**张成**(`span`)也不会改变，它仍然是一条直线。
{% asset_img m_4.png  %}
{% asset_img m_5.png  %}
{% asset_img m_6.png  %}
{% asset_img m_7.png  %}

现在让我们看一个不同的例子。这次我们保留线性无关的橙色和绿色向量，并添加第三个红色向量。虽然这两个向量都不是其他向量的倍数，但它们不再独立。这是因为你可以通过将绿色向量的`1`倍与橙色向量的`3`倍相加来获得第三个向量。可以肯定的是，我们再次向集合中添加了一个新向量，但是向量的跨度没有改变，它们仍然只跨越平面。事实证明，您添加的任何其他向量都可以写成前两个向量的线性组合。这个结果一般成立。如果您拥有更多向量并且您尝试跨越的空间维数，那么您将始终拥有一个线性相关的组。这意味着在平面中有三个或更多向量，或者在三维空间中有四个或更多向量，等等。让我们看看如何检查集合是否线性相关。我们将橙色向量称为{% mathjax %}v_1{% endmathjax %}，坐标为`(-1,1)`。{% mathjax %}v_2{% endmathjax %}将是青色向量，其坐标为`(2,1)`。最后，{% mathjax %}v_3{% endmathjax %}将是红色向量，坐标为`(-5,3)`。要看出向量集是线性相关的，您需要找到满足{% mathjax %}\alpha\times v_1 + \beta\times v_2 = v_3{% endmathjax %}的常数和{% mathjax %}\beta{% endmathjax %}。您正在给出{% mathjax %}v_3{% endmathjax %}的线性组合的系数。{% mathjax %}\alpha{% endmathjax %}和{% mathjax %}\beta{% endmathjax %}实际上是数字。
{% asset_img m_9.png  %}

这产生了有两个未知数的两个方程，{% mathjax %}-\alpha + 2\beta = -5{% endmathjax %}，{% mathjax %}\alpha + \beta = 3{% endmathjax %}。我们有一个方程组，分别称它们为方程`1`和`2`。如果将方程`1`和`2`相加，则会得到{% mathjax %}3\beta = -2{% endmathjax %}。这意味着{% mathjax %}\beta = -\frac{2}{3}{% endmathjax %}。在方程`2`中使用这个结果，您会得到{% mathjax %}\alpha - \frac{2}{3} = 3{% endmathjax %}。{% mathjax %}\alpha = \frac{11}{3}{% endmathjax %}。你可以找到方程组的解，所以{% mathjax %}v_3{% endmathjax %}是{% mathjax %}v_1{% endmathjax %}和{% mathjax %}v_2{% endmathjax %}的线性组合，并且这个集合是线性相关的。如果你发现系统没有解，那么这意味着该集合是线性独立的。
{% asset_img m_10.png  %}

现在我们来做个测验。我会给你三个向量，你告诉我这些向量是否是线性无关的。实际上它们不是，因为第一个向量乘以`1`加上第二个向量乘以`-1`等于第三个向量。事实上它们是线性相关的。但是如果你移除这三个向量中的任何一个，那么你就会得到一个线性独立的集合。但是由于每个线性独立集合只有两个向量，并且它们存在于三维空间中，因为它们有三个坐标，所以这些都不是三维空间的基。从几何学上讲，意味着如果你在三维空间中绘制这三个向量，它们都将位于同一平面内。现在你已经看到了一些线性独立的例子，让我们回到基的定义。基是满足两个条件的一组向量。该集合必须跨越一个向量空间，并且该集合中的向量必须线性独立。回到您之前看到的示例，前两个跨越一条线或一维空间，以及一个平面或二维空间，它们是线性独立的，因此它们构成一个基。后两个是线性相关的，它们不构成基。正如我们在这两个示例中看到的，并非所有`n`个向量的集合都会构成`n`维空间的**基**。
{% asset_img m_11.png  %}

##### 特征

有一种**基**(`basis`)可以统领所有**基**(`basis`)，称为**特征基**。**特征基**非常有用，特别是在机器学习应用中，例如我们之前提到的**主成分分析**(`PCA`)。**特征基**的工作原理如下。让我们看一下矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}的**线性变换**。这个基有两个向量，向量`(1,0)`，当您将其乘以矩阵时，您会看到它得到向量`(2,0)`和向量`(0,1)`，当您将其乘以矩阵时，您会得到向量`(1,3)`。如下图所示，左边的这个正方形变成右边的这个平行四边形，平面的其余部分也随之改变。这也被称为**坐标变换**或**基变换**。从左侧的正方形坐标移动到右侧的平行四边形坐标。
{% asset_img m_12.png  %}

正方形的选择非常随意。实际上选择一个不同的基，看看会发生什么。再次选择向量`(1,0)`，它指向向量`(2,0)`。作为该基的第二个元素，选择这个向量`(1, 1)`。它指向向量`(3, 3)`。这个平行四边形指向这个平行四边形。这里有什么特别之处？请注意，两个平行四边形的边与另一个基中相应的边平行。平面的其余部分也随之平行。因为这些点是平行的，所以我们对平面所做的就是在这个方向（水平方向）拉伸`2`倍，在这个对角线方向拉伸`3`倍。这是一个非常特殊的基。它只包含两个拉伸。这就是所谓的**特征基**。这是观察线性变换的一种非常特殊的方式，相对于一个基，将一个平行四边形移动到到另一个平行四边形，其边与原始平行。
{% asset_img m_13.png  %}

为什么这很有用？假设你想找到点`(3,2)`的图像。你可以将其乘以矩阵并得到`(8, 6)`。但你也可以将该点表示为基中元素的组合。这是什么意思？我们利用拥有的两个方向找到一条到达该点的路径。现在线性变换对应于将水平向量拉伸`2`倍，将垂直向量拉伸`3`倍。这确实简化了线性变换。它只包含两次拉伸。基中的两个向量将被称为**特征向量**，拉伸因子`2`和`3`将被称为**特征值**。**特征值**和**特征向量**在线性代数中非常重要，因为它确实简化了计算。
{% asset_img m_14.png  %}

##### 特征值和特征向量

为什么**特征值**和**特征向量**如此特殊。我将把矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}乘以两个已经显示为**特征向量**的向量和一个不是**特征向量**的向量。第一个向量`(1,0)`是**特征向量**。变换后，它变为`(2,0)`。请注意，向量仍然指向同一方向。原始向量`(1,0)`的`2`倍。第二个向量`(1,1)`也是特征向量。变换后，它变为`(3、3)`。同样，它指向同一方向，因此可以将结果写为原始向量的`3`倍。最后一个向量`(-1,2)`不是**特征向量**。变换后，它变成了向量`(0,6)`。注意，它不再指向同一方向，而且没有常数可以与原始向量相乘来得到最终的向量`(0,6)`。
{% asset_img m_15.png  %}

将这里的矩阵称为{% mathjax %}A{% endmathjax %}，将第一个特征向量称为{% mathjax %}v_1{% endmathjax %}。对于这个特殊向量{% mathjax %}v_1{% endmathjax %}，{% mathjax %}A\times v_1 = v_1\times \labda_1{% endmathjax %}。使用更正式的符号，但在这种情况下，您已经知道{% mathjax %}v_1{% endmathjax %}是向量`(1,0)`，而{% mathjax %}\lambda_1{% endmathjax %}的标量为`2`。第二个方程定义为{% mathjax %}A\times v_2 = \lambda_2\times v_2{% endmathjax %}。您已经知道向量{% mathjax %}v_2{% endmathjax %}为`(1,1)`，标量为`3`。{% mathjax %}v_1{% endmathjax %}和 {% mathjax %}v_2{% endmathjax %}是矩阵{% mathjax %}A{% endmathjax %}的特征向量，{% mathjax %}\lambda_1{% endmathjax %}和{% mathjax %}\lambda_2{% endmathjax %}是矩阵{% mathjax %}A{% endmathjax %}的特征值。请注意，**特征向量和特征值是成对出现的**。因此，{% mathjax %}v_1{% endmathjax %}和{% mathjax %}\lambda_1{% endmathjax %}形成一对，{% mathjax %}v_2{% endmathjax %}和{% mathjax %}\lambda_2{% endmathjax %}形成第二对。您可能仍在想为什么特征向量如此特殊。在这些等式的左边有一个矩阵乘法，在右边有一个标量乘法。一个很大的区别是，矩阵乘法的工作量要大得多。即使对于这个小小的{% mathjax %}2\times 2{% endmathjax %}的例子，计算等式的左边也需要`8`次乘法，而右边只需要`2`次乘法。对于具有数百或数千列的矩阵，差异将变得非常大。这个等式的意思是，至少沿着**矩阵特征向量**，你可以将一个大计算变成一个小计算。
{% asset_img m_16.png  %}

如果使用**基**，实际上可以在任何地方使用这个快捷方式。如下图所示，红色向量不是特征向量。你只需要像平常一样完成矩阵乘法。但是，这两个特征向量`(1,0)`和`(1,1)`是线性无关的，并且跨越平面，因此它们形成一个**基**。这实际上是矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}的**特征基**。由于向量`(1,0)`和`(1,1)`形成一个基，那么你可以将向量`(-1,2)`写成这些基向量的线性组合。在这种情况下，它是{% mathjax %}-3\times (1,0) + 2\times (1,1){% endmathjax %}。您可以将`(-1, 2)`替换为{% mathjax %}-3\times (1,0) + 2\times (1,1){% endmathjax %}。向量`(-1,2)`，重写为**基向量**的线性组合。将矩阵乘法移到括号内，并将`-3`和`2`向前移动。此时，您就可以使用特征向量快捷方式了。如下图所示，您已经计算了这些矩阵乘法，可以像这样替换标量乘法。{% mathjax %}-3\times 2 \times (1,0) + 2 \times 3 \times (1,1) = (0,6){% endmathjax %}。您可以快速将这些标量相乘，最后解这个方程以得到向量`(0,6)`。即使红色向量是特征向量，你也能够解决这个线性变换，而无需进行任何**矩阵乘法**。你只是在整个过程中使用了**标量乘法**。还记得之前，红色向量相对于特征基`(-3,2)` 的坐标。但一般来说，获取这些坐标实际上需要大量的工作。但要做到这一点，对于平面上的每个点，你都需要计算特征基的**逆**，然后将该逆乘以红色向量。或者换句话说，这需要做很多工作，甚至需要进行矩阵乘法。
{% asset_img m_17.png  %}

****

{% asset_img m_18.png  %}

****

{% asset_img m_19.png  %}

****

{% asset_img m_20.png  %}

****

{% asset_img m_21.png  %}

在某些机器学习应用中，提前计算这些坐标是值得的，这样当需要应用转换时，你可以更快地完成。它们的特征在于，对于每对**特征向量**和**特征值**，方程{% mathjax %}A_v = \lambda_v{% endmathjax %}。沿着该向量，**矩阵乘法**就变成了**标量乘法**。从视觉上讲，你可以将**特征向量视为线性变换只是拉伸的方向，而特征值则是拉伸的程度**。你可以从**特征向量**创建一个称为**特征基**的基。你经常会看到**特征基**表示为一个矩阵，每列都有一个**特征向量**。从这个特征基的角度来看，线性变换{% mathjax %}A{% endmathjax %}只是拉伸的集合。**特征向量可以节省很多工作量并且帮助表征线性变换**。

##### 计算特征值和特征向量

