---
title: 数据科学 — 数学(三)（机器学习）
date: 2024-08-13 12:05:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 线性变换

##### 线性代数—张成

一组向量的**张成**(`span`)就是沿着这些向量的方向以任意组合移动到达的点的集合。例如，您已经看到这两个向量的**张成**(`span`)是平面，因为您可以通过沿着这两个方向移动到达平面上的任何点。同样，这两个向量的**张成**(`span`)也是平面。到达这些点可能需要一段时间，但可以只使用这两个方向。然而，这两个向量并不跨越平面。因为正如您之前所看到的，并非每个点都可以通过沿着这两个方向移动到达，它们是同一个方向。它们跨越哪一侧？那么这条线上的任何一点都可以通过沿着向量的方向移动到达，因此这两个向量的**张成**(`span`)就是那条线。
<!-- more -->

两个相隔`180`度的向量呢？那么**张成**(`span`)包含它们的线，因为您可以通过沿着这两个方向移动到达该线上的每个点。这个向量的**张成**(`span`)是多少？**张成**(`span`)就是包含它并通向原点的线，因为那是你沿着那个方向行走可以到达的点的集合。这里有一个问题。这两个向量是否构成这条线的基？你怎么看？答案实际上是否定的。原因是基需要是一个最小生成集。这里有太多的向量。看看这两个向量中的任何一个都跨越这条线，所以这两个向量太多了。**基**是一个最小生成集，所以它们中的每一个都是基。但是它们两个不是基。它们是**生成集**，那是这条线的基。事实上，任何从原点开始并朝着同一方向的向量也是这条线的基。
{% asset_img m_1.png  %}
{% asset_img m_2.png  %}

总之，**基是一个最小生成集**，所以左边的向量构成了这条线的基。但这两个向量不是，因为它们太多了。这种情况也发生在高维空间中。我们来看看这个集合。这个集合构成了**平面的基**，因为它跨越了平面，因为我们可以通过沿着这两个方向行走到达任何一点。但是如果我们移除其中任何一个，它们就不再跨越平面，现在它们只跨越一条线。然而，这组向量并不构成平面的基。它们跨越平面，因为平面上的任何一点都可以通过沿着这三个方向的组合移动到达。但它们太大了，不能成为基。这三个向量中的两个子集都是基，但第三个是多余的，所以它不是基。现在注意一些有趣的事情，**基的长度是平面维度的空间。空间基的长度就是该空间的维度**。左边的线有一个长度为`1`的基，它的维度为`1`，因为线的维度为`1`。右边的平面有一个长度为`2`的基，由两个向量组成，平面的维度为`2`。意味着任何空间的任何基实际上都具有与其他基相同数量的元素。例如三维空间，想象一下那里的基会是什么样子。现在你已经对基有了很好的直觉，让我们看看正式的定义。
{% asset_img m_3.png  %}

我们需要引入**线性无关**和**线性相关**向量的概念。如果一组向量中没有一个向量可以作为其他向量的线性组合获得，则该组向量被称为**线性无关**的。如果你只考虑平面中的一个向量，它总是线性无关的。现在让我们添加另一个向量。请注意，你不可能将新向量作为第一个向量的线性组合，因为它们指向不同的方向。这两个向量仍然是线性无关的。如果你添加另一个向量会怎么样？在这种情况下，红色的新向量指向与绿色向量相同的方向，但它的长度只是原始向量的`2`倍。由于一个向量可以通过其他向量的线性组合获得，因此这组向量称为**线性相关**。还要注意，即使我们在集合中添加了一个新向量，这些向量的**张成**(`span`)也不会改变，它仍然是一条直线。
{% asset_img m_4.png  %}
{% asset_img m_5.png  %}
{% asset_img m_6.png  %}
{% asset_img m_7.png  %}

现在让我们看一个不同的例子。这次我们保留线性无关的橙色和绿色向量，并添加第三个红色向量。虽然这两个向量都不是其他向量的倍数，但它们不再独立。这是因为你可以通过将绿色向量的`1`倍与橙色向量的`3`倍相加来获得第三个向量。可以肯定的是，我们再次向集合中添加了一个新向量，但是向量的跨度没有改变，它们仍然只跨越平面。事实证明，您添加的任何其他向量都可以写成前两个向量的线性组合。这个结果一般成立。如果您拥有更多向量并且您尝试跨越的空间维数，那么您将始终拥有一个线性相关的组。这意味着在平面中有三个或更多向量，或者在三维空间中有四个或更多向量，等等。让我们看看如何检查集合是否线性相关。我们将橙色向量称为{% mathjax %}v_1{% endmathjax %}，坐标为`(-1,1)`。{% mathjax %}v_2{% endmathjax %}将是青色向量，其坐标为`(2,1)`。最后，{% mathjax %}v_3{% endmathjax %}将是红色向量，坐标为`(-5,3)`。要看出向量集是线性相关的，您需要找到满足{% mathjax %}\alpha\times v_1 + \beta\times v_2 = v_3{% endmathjax %}的常数和{% mathjax %}\beta{% endmathjax %}。您正在给出{% mathjax %}v_3{% endmathjax %}的线性组合的系数。{% mathjax %}\alpha{% endmathjax %}和{% mathjax %}\beta{% endmathjax %}实际上是数字。
{% asset_img m_9.png  %}

这产生了有两个未知数的两个方程，{% mathjax %}-\alpha + 2\beta = -5{% endmathjax %}，{% mathjax %}\alpha + \beta = 3{% endmathjax %}。我们有一个方程组，分别称它们为方程`1`和`2`。如果将方程`1`和`2`相加，则会得到{% mathjax %}3\beta = -2{% endmathjax %}。这意味着{% mathjax %}\beta = -\frac{2}{3}{% endmathjax %}。在方程`2`中使用这个结果，您会得到{% mathjax %}\alpha - \frac{2}{3} = 3{% endmathjax %}。{% mathjax %}\alpha = \frac{11}{3}{% endmathjax %}。你可以找到方程组的解，所以{% mathjax %}v_3{% endmathjax %}是{% mathjax %}v_1{% endmathjax %}和{% mathjax %}v_2{% endmathjax %}的线性组合，并且这个集合是线性相关的。如果你发现系统没有解，那么这意味着该集合是线性独立的。
{% asset_img m_10.png  %}

现在我们来做个测验。我会给你三个向量，你告诉我这些向量是否是线性无关的。实际上它们不是，因为第一个向量乘以`1`加上第二个向量乘以`-1`等于第三个向量。事实上它们是线性相关的。但是如果你移除这三个向量中的任何一个，那么你就会得到一个线性独立的集合。但是由于每个线性独立集合只有两个向量，并且它们存在于三维空间中，因为它们有三个坐标，所以这些都不是三维空间的基。从几何学上讲，意味着如果你在三维空间中绘制这三个向量，它们都将位于同一平面内。现在你已经看到了一些线性独立的例子，让我们回到基的定义。基是满足两个条件的一组向量。该集合必须跨越一个向量空间，并且该集合中的向量必须线性独立。回到您之前看到的示例，前两个跨越一条线或一维空间，以及一个平面或二维空间，它们是线性独立的，因此它们构成一个基。后两个是线性相关的，它们不构成基。正如我们在这两个示例中看到的，并非所有`n`个向量的集合都会构成`n`维空间的**基**。
{% asset_img m_11.png  %}

##### 特征概述

有一种**基**(`basis`)可以统领所有**基**(`basis`)，称为**特征基**。**特征基**非常有用，特别是在机器学习应用中，例如我们之前提到的**主成分分析**(`PCA`)。**特征基**的工作原理如下。让我们看一下矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}的**线性变换**。这个基有两个向量，向量`(1,0)`，当您将其乘以矩阵时，您会看到它得到向量`(2,0)`和向量`(0,1)`，当您将其乘以矩阵时，您会得到向量`(1,3)`。如下图所示，左边的这个正方形变成右边的这个平行四边形，平面的其余部分也随之改变。这也被称为**坐标变换**或**基变换**。从左侧的正方形坐标移动到右侧的平行四边形坐标。
{% asset_img m_12.png  %}

正方形的选择非常随意。实际上选择一个不同的基，看看会发生什么。再次选择向量`(1,0)`，它指向向量`(2,0)`。作为该基的第二个元素，选择这个向量`(1, 1)`。它指向向量`(3, 3)`。这个平行四边形指向这个平行四边形。这里有什么特别之处？请注意，两个平行四边形的边与另一个基中相应的边平行。平面的其余部分也随之平行。因为这些点是平行的，所以我们对平面所做的就是在这个方向（水平方向）拉伸`2`倍，在这个对角线方向拉伸`3`倍。这是一个非常特殊的基。它只包含两个拉伸。这就是所谓的**特征基**。这是观察线性变换的一种非常特殊的方式，相对于一个基，将一个平行四边形移动到到另一个平行四边形，其边与原始平行。
{% asset_img m_13.png  %}

为什么这很有用？假设你想找到点`(3,2)`的图像。你可以将其乘以矩阵并得到`(8, 6)`。但你也可以将该点表示为基中元素的组合。这是什么意思？我们利用拥有的两个方向找到一条到达该点的路径。现在线性变换对应于将水平向量拉伸`2`倍，将垂直向量拉伸`3`倍。这确实简化了线性变换。它只包含两次拉伸。基中的两个向量将被称为**特征向量**，拉伸因子`2`和`3`将被称为**特征值**。**特征值**和**特征向量**在线性代数中非常重要，因为它确实简化了计算。
{% asset_img m_14.png  %}

##### 特征值和特征向量

为什么**特征值**和**特征向量**如此特殊。我将把矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}乘以两个已经显示为**特征向量**的向量和一个不是**特征向量**的向量。第一个向量`(1,0)`是**特征向量**。变换后，它变为`(2,0)`。请注意，向量仍然指向同一方向。原始向量`(1,0)`的`2`倍。第二个向量`(1,1)`也是特征向量。变换后，它变为`(3、3)`。同样，它指向同一方向，因此可以将结果写为原始向量的`3`倍。最后一个向量`(-1,2)`不是**特征向量**。变换后，它变成了向量`(0,6)`。注意，它不再指向同一方向，而且没有常数可以与原始向量相乘来得到最终的向量`(0,6)`。
{% asset_img m_15.png  %}

将这里的矩阵称为{% mathjax %}A{% endmathjax %}，将第一个特征向量称为{% mathjax %}v_1{% endmathjax %}。对于这个特殊向量{% mathjax %}v_1{% endmathjax %}，{% mathjax %}A\times v_1 = v_1\times \labda_1{% endmathjax %}。使用更正式的符号，但在这种情况下，您已经知道{% mathjax %}v_1{% endmathjax %}是向量`(1,0)`，而{% mathjax %}\lambda_1{% endmathjax %}的标量为`2`。第二个方程定义为{% mathjax %}A\times v_2 = \lambda_2\times v_2{% endmathjax %}。您已经知道向量{% mathjax %}v_2{% endmathjax %}为`(1,1)`，标量为`3`。{% mathjax %}v_1{% endmathjax %}和 {% mathjax %}v_2{% endmathjax %}是矩阵{% mathjax %}A{% endmathjax %}的特征向量，{% mathjax %}\lambda_1{% endmathjax %}和{% mathjax %}\lambda_2{% endmathjax %}是矩阵{% mathjax %}A{% endmathjax %}的特征值。请注意，**特征向量和特征值是成对出现的**。因此，{% mathjax %}v_1{% endmathjax %}和{% mathjax %}\lambda_1{% endmathjax %}形成一对，{% mathjax %}v_2{% endmathjax %}和{% mathjax %}\lambda_2{% endmathjax %}形成第二对。您可能仍在想为什么特征向量如此特殊。在这些等式的左边有一个矩阵乘法，在右边有一个标量乘法。一个很大的区别是，矩阵乘法的工作量要大得多。即使对于这个小小的{% mathjax %}2\times 2{% endmathjax %}的例子，计算等式的左边也需要`8`次乘法，而右边只需要`2`次乘法。对于具有数百或数千列的矩阵，差异将变得非常大。这个等式的意思是，至少沿着**矩阵特征向量**，你可以将一个大计算变成一个小计算。
{% asset_img m_16.png  %}

如果使用**基**，实际上可以在任何地方使用这个快捷方式。如下图所示，红色向量不是特征向量。你只需要像平常一样完成矩阵乘法。但是，这两个特征向量`(1,0)`和`(1,1)`是线性无关的，并且跨越平面，因此它们形成一个**基**。这实际上是矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}的**特征基**。由于向量`(1,0)`和`(1,1)`形成一个基，那么你可以将向量`(-1,2)`写成这些基向量的线性组合。在这种情况下，它是{% mathjax %}-3\times (1,0) + 2\times (1,1){% endmathjax %}。您可以将`(-1, 2)`替换为{% mathjax %}-3\times (1,0) + 2\times (1,1){% endmathjax %}。向量`(-1,2)`，重写为**基向量**的线性组合。将矩阵乘法移到括号内，并将`-3`和`2`向前移动。此时，您就可以使用特征向量快捷方式了。如下图所示，您已经计算了这些矩阵乘法，可以像这样替换标量乘法。{% mathjax %}-3\times 2 \times (1,0) + 2 \times 3 \times (1,1) = (0,6){% endmathjax %}。您可以快速将这些标量相乘，最后解这个方程以得到向量`(0,6)`。即使红色向量是特征向量，你也能够解决这个线性变换，而无需进行任何**矩阵乘法**。你只是在整个过程中使用了**标量乘法**。还记得之前，红色向量相对于特征基`(-3,2)` 的坐标。但一般来说，获取这些坐标实际上需要大量的工作。但要做到这一点，对于平面上的每个点，你都需要计算特征基的**逆**，然后将该逆乘以红色向量。或者换句话说，这需要做很多工作，甚至需要进行矩阵乘法。
{% asset_img m_17.png  %}

****

{% asset_img m_18.png  %}

****

{% asset_img m_19.png  %}

****

{% asset_img m_20.png  %}

****

{% asset_img m_21.png  %}

在某些机器学习应用中，提前计算这些坐标是值得的，这样当需要应用转换时，你可以更快地完成。它们的特征在于，对于每对**特征向量**和**特征值**，方程{% mathjax %}A_v = \lambda_v{% endmathjax %}。沿着该向量，**矩阵乘法**就变成了**标量乘法**。从视觉上讲，你可以将**特征向量视为线性变换只是拉伸的方向，而特征值则是拉伸的程度**。你可以从**特征向量**创建一个称为**特征基**的基。你经常会看到**特征基**表示为一个矩阵，每列都有一个**特征向量**。从这个特征基的角度来看，线性变换{% mathjax %}A{% endmathjax %}只是拉伸的集合。**特征向量可以节省很多工作量并且帮助表征线性变换**。

##### 计算特征值和特征向量

首先，看一下矩阵{% mathjax %}\begin{bmatrix} 2 & 1\\ 0 & 3 \end{bmatrix}{% endmathjax %}，看看它对正方形周围这些点的作用。整个变换清晰可见。如前所述，两个水平向量被拉伸了`2`倍，对角线被拉伸了`3`倍。对于其他点，情况也是如此。与另一个矩阵进行比较，该矩阵只是将整个平面在任何方向上拉伸`3`倍。矩阵{% mathjax %}\begin{bmatrix} 3 & 0\\ 0 & 3 \end{bmatrix}{% endmathjax %}，它实际上是单位矩阵的`3`倍。这两个变换不是同一个变换，但它们在许多点上确实重合。换句话说，它们对无限多个点的作用完全相同。更具体地说，在这个对角线上，这两个变换做完全相同的事情。
{% asset_img m_22.png  %}

它们在无限多个点上匹配。这很奇怪，变换应该只在一个点匹配，即点`(0,0)`。当它们在无限多个点匹配时，就会发生一些非奇异的事情。发生了什么？让我们看看区别。如果这两个变换在无穷多个点上匹配，意味着在无穷多个点上的差异为`0`。如果将这两个矩阵的差异应用于这些对角线上的任意向量，则会得到向量`(0,0)`。换句话说，这个矩阵乘以向量{% mathjax %}(x,y){% endmathjax %}的**无穷度量向量**是`(0,0)`。这就是**奇异变换**的特征。回想一下，非奇异变换对方程矩阵乘以向量等于`(0,0)`有一个唯一解，那就是向量`(0,0)`。如果你有无穷多个解，意味着你的矩阵是**奇异**的，你可以验证这确实是一个**奇异矩阵**，因为行列式为`0`。
{% asset_img m_23.png  %}

但对于右边的另一个变换。我们的变换与之前完全一样。将它与在每个方向上将平面拉伸`2`倍的变换进行比较。这两个并不相同，但它们在整条线上匹配。
{% asset_img m_24.png  %}

左边的矩阵乘以{% mathjax %}xy{% endmathjax %}等于右边的矩阵乘以{% mathjax %}xy{% endmathjax %}。对于这条线上的任何向量{% mathjax %}xy{% endmathjax %}，都有无穷多个点。我们可以执行相同的过程，取差值，并将该矩阵乘以一个向量，对于无穷多个向量，该向量等于`(0,0)`。这意味着矩阵{% mathjax %}\begin{bmatrix} 0 & 1\\ 0 & 1 \end{bmatrix}{% endmathjax %}，或者矩阵与`2`倍**恒等矩阵**之间的差值，是一个**奇异矩阵**。您可以检查它确实是奇异矩阵，因为它的行列式为`0`。
{% asset_img m_25.png  %}

特征值有什么特别之处？如果{% mathjax %}\lambda{% endmathjax %}是特征值，那么对于无穷多个向量{% mathjax %}xy{% endmathjax %}，由矩阵给出的变换和通过将平面完全缩放{% mathjax %}\lambda{% endmathjax %}的因子给出的变换相等。意味着它们的差乘以一个向量，对于无穷多个向量，等于`(0,0)`，这是一个创建许多解的方程。因此，这个矩阵{% mathjax %}\begin{bmatrix} 2-\lambda & 1\\ 0 & 3-\lambda \end{bmatrix}{% endmathjax %}必须是**奇异矩阵**，所以我们的行列式是`0`，当我们展开它时，它的行列式由这个方程给出，这被称为**特征多项式**。基本上，要找到特征值{% mathjax %}\lambda{% endmathjax %}，我们需要做的就是查看特征多项式并找到根。特征多项式为`0`的地方就是特征值。在这种情况下，它们将是{% mathjax %}\lambda = 2{% endmathjax %}和{% mathjax %}\lambda = 3{% endmathjax %}。
{% asset_img m_26.png  %}

现在你有了特征值，让我们找到特征向量。回想一下，**特征向量**是满足方程矩阵乘以向量等于特征值乘以向量的向量。如果我们展开它，我们会得到这些方程，它们的解是{% mathjax %}x=1、y=0{% endmathjax %}或任何多重向量。这是特征值向量之一，对应于特征值`2`。我们对`3`个特征值做同样的事情，求解这些方程并得到{% mathjax %}x=1、y=0{% endmathjax %}。特征向量`(1,1)`对应于特征值`3`。
{% asset_img m_27.png  %}

##### 特征向量的数量

你有一个{% mathjax %}3\times 3{% endmathjax %}的矩阵，它有`3`个不同的特征值和`3`个不同的特征向量。难道所有{% mathjax %}3\times 3{% endmathjax %}的矩阵总是有`3`个特征向量吗？事实证明情况并非总是如此。让我们看一些例子。考虑这个{% mathjax %}3\times 3{% endmathjax %}矩阵{% mathjax %}A{% endmathjax %}，特征多项式作为行列式：{% mathjax %}A - \lambda I{% endmathjax %}。使用{% mathjax %}3\times 3{% endmathjax %}矩阵的行列式公式，我们得到{% mathjax %}(2 - \lambda)^2(4 - \lambda){% endmathjax %}。由于原始矩阵中的零点，剩余项都为零。现在要找到特征值，你需要找到这个多项式的零点，这给出了特征值`(4,2,2)`。
{% asset_img m_28.png  %}

`2`重复了两次。现在看看当我们找到与它们相关的特征向量时会发生什么？让我们从特征值`4`开始。特征向量需要满足{% mathjax %}A(x_1,x_2,x_3) = (4x_1,4x_2,4x_3){% endmathjax %}。矩阵与向量之间的乘积可以展开为向量{% mathjax %}(2x_1,-x_1 + 4x_2 - 0.5x_3,2x_3){% endmathjax %}。
{% asset_img m_29.png  %}

{% mathjax %}2x_3{% endmathjax %}，希望它等于向量{% mathjax %}(4x_1,4x_2,4x_3){% endmathjax %}，从而得到这个方程组：{% mathjax %}-2x_1 = 0,\;-x_1 - 0.5x_3 = 0,\;-2x_3 = 0{% endmathjax %}。请注意，这里所做的只是将所有内容移到等号左侧。可以稍微简化一下，得到第一个等式{% mathjax %}x_1 = 0{% endmathjax %}。第三个等式{% mathjax %}x_3 = 0{% endmathjax %}。现在，由于{% mathjax %}x_2{% endmathjax %}根本没有出现在这些方程中，因此第二个方程是(0,4,3)，并且{% mathjax %}x_2{% endmathjax %}可以是任何数字。为了简单起见，我们可以设置为{% mathjax %}x_2 = 1{% endmathjax %}，然后将得到与特征值`4`相关的特征向量`(0,1,0)`。现在让我们对特征值`2`重复上述步骤。向量{% mathjax %}(x_1,x_2,x_3){% endmathjax %}的点积应该等于向量{% mathjax %}(2x_1,2x_2,2x_3){% endmathjax %}。点积可以与之前的表达式展开。我们再次得到一个方程组：{% mathjax %}2x_1 = 2x_1,\;-x_1 + 4x_2-0.5x_3 = 2x_2,\;2x_3 = 2x_3{% endmathjax %}。通过将所有内容移到等号左边来处理这个问题时，我们会发现第一个方程非常简单，它是{% mathjax %}0 = 0{% endmathjax %}。第二个方程是{% mathjax %}-1 + 2x_2 - 0.5x_3 = 0{% endmathjax %}。第三个方程同样很简单，它是{% mathjax %}0 = 0{% endmathjax %}。你可以将第二个方程重写为 {% mathjax %}x_1 = 2x_2 - 0.5x_3{% endmathjax %}。这个方程有无数个解，具体取决于你选择的{% mathjax %}x_2{% endmathjax %}和{% mathjax %}x_3{% endmathjax %}的值。假设你选择{% mathjax %}x_2 = 1{% endmathjax %}和{% mathjax %}x_3 = 0{% endmathjax %}。这意味着 {% mathjax %}x_1{% endmathjax %}必须为`2`才能成为特征向量，从而给出向量`(2,1,0)`。但你也可以选择{% mathjax %}x_2 = 1{% endmathjax %}而{% mathjax %}x_3 = 2{% endmathjax %}，从而给出特征向量`(1,1,2)`。这两个向量指向不同的方向。所以实际上有两个不同的特征向量。请记住，您只关心特征向量的方向，因为它的任何缩放版本仍将是同一特征值的特征向量。值得一提的是，您可以根据{% mathjax %}x_2{% endmathjax %}和{% mathjax %}x_3{% endmathjax %}的值找到不同的**特征向量对**。但重要的是，您始终可以为特征值`2`找到两个不同的方向。
{% asset_img m_30.png  %}

****

{% asset_img m_31.png  %}

总而言之，此矩阵{% mathjax %}A{% endmathjax %}具有以下**特征值**和**特征向量对**。第一个特征值{% mathjax %}\lambda_1 = 4{% endmathjax %}，其特征向量为`(0,1,0)`。第二个特征值{% mathjax %}\lambda_2 = 2{% endmathjax %}，其特征向量为向量`(2,1,0)`。最后，第三个特征值{% mathjax %}\lambda_3 = 2{% endmathjax %}，其特征向量为`(1,1,2)`。在这种情况下，即使特征值重复，您也可以找到三个不同的特征向量。
{% asset_img m_32.png  %}

让我们再看一个例子。只改变矩阵中的一个值，看看会发生什么。特征多项式与之前相同。再一次，我们将值`(4、2、2)`作为特征值。让我们重复之前的过程来找到特征向量并从特征值`4`开始。这里唯一改变的方程是最后一个方程，现在有{% mathjax %}4x_2 + 2x_3{% endmathjax %}。其中前两个保持不变，即{% mathjax %}-2x_1 = 0{% endmathjax %}和{% mathjax %}-1-0.5x_3 = 0{% endmathjax %}。再次，我们将它们分别命名为方程`1、2、3`。从方程`1`中，您可以得出{% mathjax %}x_1 = 0{% endmathjax %}。结合方程`3`和`1`，您可以得出{% mathjax %}x_3 = 0{% endmathjax %}。{% mathjax %}x_2{% endmathjax %}可以取任意值。和上一个矩阵一样，您可以选择向量`(0,1,0)`作为特征向量。它和上一​​个矩阵的特征向量相同。
{% asset_img m_33.png  %}

****

{% asset_img m_34.png  %}

对于特征值`2`，也会发生同样的事情。当您解这个方程时，您会得到{% mathjax %}2x_1 = 2x_1{% endmathjax %}。{% mathjax %}-x_1 + 4x_2  - 0.5x_3 = 2x_2{% endmathjax %}，而{% mathjax %}4x_1 + 2x_3 = 2x_3{% endmathjax %}。这会产生以下方程组，可以简化为{% mathjax %}0 = 0{% endmathjax %}，因为{% mathjax %}-x_1 + 2x_2 -0.5x_3 = 0{% endmathjax %}。最后，{% mathjax %}4x_1 = 0{% endmathjax %}。这施加了{% mathjax %}x_1 = 0{% endmathjax %}只能为`0`的限制。现在，从方程`2`和方程`3`可以得出{% mathjax %}x_3{% endmathjax %}必须等于{% mathjax %}4x_2{% endmathjax %}。现在您只有一个自由度，因为{% mathjax %}x_1{% endmathjax %}始终为`0`。一旦固定了{% mathjax %}x_2,x_3{% endmathjax %}的值也会固定。例如，您可以考虑{% mathjax %}x_1 = 0{% endmathjax %}和{% mathjax %}x_2 = 1{% endmathjax %}，这使得{% mathjax %}x_3 = 4{% endmathjax %}，给定特征向量`(0,1,4)`。例如，如果您选择{% mathjax %}x_2 = \frac{1}{2}{% endmathjax %}，会发生什么情况？给定特征向量`(0,1/2,2)`，`x_3`必须为`2`。但是，这两个向量位于同一条线上。一个只是另一个的缩放，意味着它们实际上是相同的特征向量。意味着`2`是特征值的`2`倍，但您只能找到一个与之相关的特征向量。然后，特征向量的形式为`(0,k,4k)`，但只有一个方向可以与特征值`2`配对，即使`2`作为特征值出现了`2`次。对于矩阵项`(2,0、0,-1,4,-0.5,4,0,2)`，特征值`4`与特征向量`(0、1、0)`相关联。然后特征值`2`与特征向量`(0、1、4)`相关联。同样，特征值`2`没有相关联的特征向量。意味着您无法创建`n`个特征基的三维空间，因为您缺少一个向量来跨越整个空间。
{% asset_img m_35.png  %}

****

{% asset_img m_36.png  %}

****

{% asset_img m_37.png  %}

总之，如果您有一个{% mathjax %}2\times 2{% endmathjax %}的矩阵，其特征值为{% mathjax %}\lambda_1{% endmathjax %}和{% mathjax %}\lambda_2{% endmathjax %}。如果两个特征值不同，那么您总是会得到两个不同的特征向量。但是，如果特征值相同，那么您可以有一个或两个特征向量。如果您有一个{% mathjax %}3\times 3{% endmathjax %}矩阵，特征值为{% mathjax %}\lambda_1{% endmathjax %}、{% mathjax %}\lambda_2{% endmathjax %} 和{% mathjax %}\lambda_3{% endmathjax %}，那么您将获得更多选项来探索。如果`3`个特征值都不同，那么您总是可以找到`3`个不同的特征向量。如果一个特征值重复`2`次，而另一个不同，那么您可以有两个或三个特征向量。但是，如果相同的特征值重复三次，则可以有`1~3`之间的任意数量的特征向量。
