---
title: 数据科学 — 数学(四)（机器学习）
date: 2024-08-15 11:26:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 主成分分析(PCA)

现在您已经了解了**投影**的概念，让我们看看`PCA`如何使用它来降低数据集的维度。如下图所示，每个点代表一个不同的观测值，由两个以{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}位置为图形的特征​​组成。降低此数据的维度意味着将图形为平面点的二维数据转变为图形为一条线的一维数据。该集合不是以原点`(0,0)`为中心，现在让我们看看如果投影到{% mathjax %}x{% endmathjax %}轴上会发生什么？
<!-- more -->
{% asset_img m_1.png  %}

您已经可以看到，此投影中的点分布较少，因为这些点彼此更接近。您可以将它们投影到任何一条线上，例如这条线，可以用方程：{% mathjax %}y +  x = 0{% endmathjax %}来表示，这相当于投影到向量`(1,-1)`上，因为这些点跨越了这条线。
{% asset_img m_2.png  %}

****

{% asset_img m_3.png  %}

或者你也可以考虑另一条线，它求解方程：{% mathjax %}2x -  y = 0{% endmathjax %}，它投影到向量`(1,2)`上。这条线与数据非常吻合，并且得到的投影点仍然相对分散。经过这些投影后，数据点可能会或多或少地分散，而这最终会变得非常重要。**原因是分散程度更高的数据点会保留原始数据集中的更多信息。换句话说，保留更多的分散度意味着保留更多信息**。现在，我将按点分散程度从大到小对投影进行排序。**顶部的投影点分散程度最高，因此它保留了原始数据集中最多的信息，底部的投影点分散程度最低，因此保留的信息最少**。因此，`PCA`的目标是**找到即使在降低数据集维度的情况下也能保留数据中最大可能分散度的投影**。
{% asset_img m_4.png  %}

****

{% asset_img m_5.png  %}

****

{% asset_img m_6.png  %}

再次强调，**降维**和`PCA`中的好处如下。降维使数据集更容易管理，因为它们更小。`PCA`允许您在减少维度的同时最大限度地减少信息丢失。由于维度的减少，实现了不可能的方式分析可视化数据。

##### 方差(Variance)和协方差(Covariance)

