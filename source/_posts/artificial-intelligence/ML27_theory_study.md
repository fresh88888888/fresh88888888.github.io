---
title: 机器学习(ML)(二十七) — 强化学习探析
date: 2025-03-15 16:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

##### AGI系统：实现AGI机制

**超越**`Transformers`：尽管`Transformers`**架构**取得了巨大的成功，许多研究尝试寻找其他设计来克服其一些缺点。**混合专家**(`MoEs`)使用多个“**专家**”子网络组成的条件模块替换了`Transformer`**模型**中的稠密层。使用**路由机制**在**词元级**或**任务级**动态决定使用哪个**专家**。尽管拥有多个**专家**，稀疏的`MoEs`通常可以在相同模型大小下更快地训练和解码，并且预计能够在不同的抽象任务中实现。然而，`MoEs`在推理过程中也带来了其他挑战，例如将所有专家加载到`VRAM`中的要求以及在多个节点上分发**专家**。
<!-- more -->

**状态空间模型**(`SSMs`)最近被应用于建模**序列到序列**的转换，可以在各种模型架构拓扑中替代**二次自注意力机制**。（**离散化**）`SSM`通过一组可学习参数(`∆, A, ¯ B, C ¯`)在每个时间步（标记）上定义**递归关系**，大多数研究试图解决的主要挑战是如何以**并行化**的方式计算这种递归，以便有效利用**现代硬件加速器**（例如`FFTConv`）。在这一类别中，最简单的形式是**线性注意力**，它可以被视为一种退化的`SSM`。在其核心，**线性注意力**将**自注意力表示**为**核特征映射**的**线性点积**，并利用**矩阵乘积**的结合性质将复杂度降低到线性。`S4`通过**低秩校正**对`SSM`进行了表达性和高效的参数化，使其能够稳定**对角化**，并将`SSM`简化为**柯西核**的计算。在`S4`之后，还有许多后续研究尝试对**转移矩阵**`A¯`（和其他矩阵）进行不同的参数化，以提高计算效率和建模能力。`H3`提出了一个由两个堆叠的独立`SSM`组成的`SSM`块，专门设计用于应对回忆早期标记的挑战，并支持跨序列的标记比较。`Hyena`通过用交错和隐式参数化的**长卷积**以及**数据控制门**替换`S4`层，推广了`H3`，从而将参数大小与滤波器大小解耦，增加了表达能力。后来提出了一种名为`Retentive Network`的基础架构，其中包含额外的**门**和**多头注意力**的变体，实现了**恒定推理成本**和**线性长序列内存消耗**。`RWKV`是一种新架构，结合了`Transformer`的高效并行训练和`RNN`的高效推理。本质上，主要的“`WKV`”操作涉及线性时不变(`LTI`)递归，可以解释为两个`SSM`的比率。为了克服以前`SSM`模型的主要弱点，即它们无法进行基于内容的推理，`Mamba`提出了**选择性状态空间**，可以使`SSM`参数成为输入的函数，从而将`SSM`从`LTI`转变为时变。尽管不再能够应用高效卷积，但他们设计了面向硬件的并行算法，用于递归计算，称为**并行关联扫描**，使其能够实现比`Transformer`高`5`倍的吞吐量，在多种模态上达到最先进的性能，并在实际数据上不断提高，直至百万长度的序列。重新审视**循环神经网络**（`RNN`）的兴趣火花也随着其在长上下文处理中的主要优势（即**隐藏状态的线性时间**和**恒定内存**）而出现。`RNN`的一个挑战是如何高效地扩展训练和推理。一种基于`RNN`的模型，具有**门控线性递归**，以及`Griffin`，它将**门控线性递归**与**局部注意力**相结合。他们展示了`Hawk`在下游任务上的优越性能，`Griffin`的性能与`Llama-2`相当，但训练标记数量减少了六倍。他们不仅展示了**长上下文能力**的潜力，还解释了如何在**分布式训练**和推理期间通过将 `Griffin`扩展到`14B`参数来有效利用硬件加速器。紧接着，一系列名为`RecurrentGemma`的模型以各种尺寸发布，包括预训练和指令调优版本。这些进展展示了在不依赖`Transformer`架构的情况下，训练一个数据高效、固定状态大小、长上下文和表达能力强的模型的可能性。最近的研究还探索了高层次架构混合策略，旨在结合不同变体的优势。并提出将`Transformer`与`Mamba`结合，通过**交错层**实现，在标准和长上下文任务上取得了令人印象深刻的结果，同时资源需求可控。除了手动设计，`MAD`将这一过程整合到一个端到端的流水线中，包含能够预测扩展规律的小规模能力单元测试。`MAD`成功找到了一种高效的架构，名为`Striped Hyena`，基于**混合**和**稀疏**，在计算最优预算和过度训练条件下，其扩展性能优于最先进的`Transformer`、`卷积`和`循环架构`（`Transformer++、Hyena、Mamba`）。这些研究很可能将继续激发对既高效又具扩展性的架构设计的进一步探索，打破当前的`Transformer`范式。
