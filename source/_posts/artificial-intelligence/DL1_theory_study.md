---
title: 深度学习(DL)(一) — 探析
date: 2024-09-02 17:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

<span style="color:#295F98;font-weight:900;">在语音识别中</span>，您将获得一个输入音频片段{% mathjax %}x{% endmathjax %}，并被要求将其映射到文本转录{% mathjax %}y{% endmathjax %}。这里的输入和输出都是**序列数据**，因为{% mathjax %}x{% endmathjax %}是一个音频片段，因此它会随时间播放而输出{% mathjax %}y{% endmathjax %}，{% mathjax %}y{% endmathjax %}是一个单词序列。音乐生成是序列数据问题的另一个例子。在这种情况下，只有输出{% mathjax %}y{% endmathjax %}是一个序列，输入可以是空集，也可以是一个整数，可能指的是您想要生成的音乐类型，也可能是您想要的音乐的前几个音符。但这里的{% mathjax %}x{% endmathjax %}可以是零，也可以只是一个整数，而输出{% mathjax %}y{% endmathjax %}是一个序列。
<!-- more -->

<span style="color:#295F98;font-weight:900;">在情绪分类中</span>，输入{% mathjax %}x{% endmathjax %}是一个序列，因此，给定输入短语，如“这部电影没有什么可喜欢的”，您认为这篇评论会得到多少颗星？**序列模型**对于`DNA`序列分析也非常有用。`DNA`通过四个字母`A、C、G`和`T`表示。因此，给定一个`DNA`序列，您能否标记该`DNA`序列的哪一部分对应于蛋白质。<span style="color:#295F98;font-weight:900;">在机器翻译中</span>，您会得到一个输入句子，`voulez-vou chante avec moi？`然后要求以不同的语言输出翻译。<span style="color:#295F98;font-weight:900;">在视频活动识别中</span>，您可能会得到一系列视频帧并被要求识别活动。在名称实体识别中，您可能会得到一个句子并被要求识别该句子中的人。因此，所有这些问题都可以作为**监督学习**来解决，标签数据{% mathjax %}x,y{% endmathjax %}作为训练集。但是，从这个示例列表中可以看出，存在许多不同类型的序列问题。在某些情况下，输入{% mathjax %}x{% endmathjax %}和输出{% mathjax %}y{% endmathjax %}都是序列，有时{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}可以具有不同的长度，或者{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}具有相同的长度。或者只有{% mathjax %}x{% endmathjax %}或相反的{% mathjax %}y{% endmathjax %}是序列。

#### 循环神经网络(RNN)

现在，让我们讨论如何构建模型，构建**神经网络**来学习从{% mathjax %}x{% endmathjax %}到{% mathjax %}y{% endmathjax %}的映射。现在，您可以做的一件事是使用标准神经网络来完成此任务。有九个输入词。可以尝试将这九个输入词（可能是九个向量）输入到标准神经网络（可能是几个隐藏层）中，然后最终输出九个值`0`或`1`，这些值会告诉您每个单词是否是人名的一部分。但这种方法效果不佳，并且实际上存在两个主要问题：第一个问题是输入和输出的长度可能不同，并且示例也不同。因此，并不是每个示例都具有相同的输入长度{% mathjax %}T_x{% endmathjax %} 或相同的上限长度{% mathjax %}T_y{% endmathjax %}，也许每个句子都有最大长度。也许可以填充每个输入，直到达到最大长度，但这似乎仍然不是一个很好的表示。第二个更严重的问题是，像这样的简单神经网络架构，它不会**共享**文本的不同位置上学习到的特征。特别是，如果神经网络已经学会了单词`Harry`出现在位置`1`上，那是人名的一部分，如果它能自动找出`Harry`出现在其他位置{% mathjax %}x^{<T>}{% endmathjax %}上也意味着那可能是人名，那不是很好吗？这可能类似于你在卷积神经网络中看到的情况，你希望从图像的一部分学到的东西能够快速推广到图像的其他部分，我们也希望序列数据也能有类似的效果。与你看到的情况类似，使用更好的表示也会让你减少模型中的参数数量。

什么是循环神经网络？如果你从左到右阅读句子，你读到的第一个词，比如{% mathjax %}x_1{% endmathjax %}，把第一个词输入到神经网络层中。第一个神经网络有一个隐藏层，我们可以让神经网络尝试预测输出。**循环神经网络**的作用是，当它继续阅读句子中的第二个词，比如{% mathjax %}x_2{% endmathjax %}时，它不仅会使用{% mathjax %}x_2{% endmathjax %}预测{% mathjax %}y_2{% endmathjax %}，还会输入一些信息，比如计算机是否在时间步骤`1`中激活值会传递到时间步骤`2`。然后在下一个时间步骤，**循环神经网络**输入第三个单词{% mathjax %}x_3{% endmathjax %}，并尝试输出一些预测{% mathjax %}y_3{% endmathjax %}等等，直到最后一个时间步骤，它输入{% mathjax %}x^{<T_x>}{% endmathjax %}，然后输出{% mathjax %}\hat{y}^{<T_y>}{% endmathjax %}。至少在这个例子中，{% mathjax %}T_x = T_y{% endmathjax %}，如果{% mathjax %}t_x{% endmathjax %}和{% mathjax %}t_y{% endmathjax %}不相同，架构将发生一些变化。在每个时间步骤，**循环神经网络**都将激活传递到下一个时间步骤以供使用。为了启动整个过程，我们还会在开始时有一些虚构的激活，这通常是零向量。一些研究人员会随机初始化{% mathjax %}a^{<0>}{% endmathjax %}。还有其他方法可以初始化{% mathjax %}a^{<0>}{% endmathjax %}，但实际上使用零向量乘以零激活是最常见的选择。这样就可以输入到神经网络中。在一些研究论文或一些书籍中，你会看到这种类型的神经网络，如下图所示，在每个时间步骤中，你输入{% mathjax %}x{% endmathjax %}并输出{% mathjax %}\hat{y}{% endmathjax %}。也许有时会有一个{% mathjax %}T{% endmathjax %}索引，然后表示**循环连接**，有时人们会画一个这样的循环，**层反馈**给细胞。
{% asset_img dl_1.png %}

循环神经网络从左到右扫描数据。它在每个时间步骤使用的参数是**共享**的。在这个**循环神经网络**中，当对{% mathjax %}y_3{% endmathjax %}进行预测时，它不仅从{% mathjax %}x_3{% endmathjax %}获取信息，还从 {% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}获取信息，因为{% mathjax %}x_1{% endmathjax %}上的信息可以传递，以帮助预测{% mathjax %}y_3{% endmathjax %}。`RNN`的一个弱点是它只使用序列中较早的信息进行预测。特别是，当预测{% mathjax %}Y_3{% endmathjax %}时，不使用有关单词{% mathjax %}x_4,x_5,x_6{% endmathjax %}等的信息。所以这是一个问题，如果你得到一个句子，“他说泰迪·罗斯福是一位伟大的总统。”为了确定`Teddy`这个词是否是某人名字的一部分，不仅要知道前两个单词的信息，还要知道句子中后面的单词的信息，因为这个句子也可能是，“他说泰迪熊正在打折。”所以只给出前三个单词是不可能确定`Teddy`这个词是否是某人名字的一部分。在第一个例子中，它是。在第二个例子中，它不是。但是如果你只看前三个词，你就无法分辨出区别。所以这个特定的神经网络结构的一个限制是，在某个时间的预测使用输入或使用序列中较早的输入信息，但不使用序列中较晚的信息。现在需要明确说明这个神经网络所做的计算。正如之前提到的，从输入{% mathjax %}a_{\text{zero}}{% endmathjax %}开始，该输入等于全零向量。接下来，这是正向传播。要计算{% mathjax %}a_1{% endmathjax %}，需要将其计算为激活函数{% mathjax %}a^{<0>} = \vec{0}\;,\;\; a^{<1>} = g(W_{aa} a^{<0>} + W_{ax} x^{<1>} + b_a){% endmathjax %}。然后计算{% mathjax %}\hat{y}^{<1>} = g(W_{ya}a^{<1>} + b_y){% endmathjax %}。预测在时间`1`，这将是某个**激活函数**，与上面的激活函数不同。使用这些矩阵的底数的符号约定，例如{% mathjax %}W_{ax}{% endmathjax %}。第二个索引表示{% mathjax %} W_{ax}x{% endmathjax %}，而这个{% mathjax %}a{% endmathjax %}表示用于计算某个类似{% mathjax %}a{% endmathjax %}的量。在`RNN`中，用于计算激活函数是{% mathjax %}\tan h{% endmathjax %}，有时也会使用非常宽松的函数，尽管{% mathjax %}\tan h{% endmathjax %}实际上是一种常见的选择，还有其他方法可以防止**梯度消失问题**，输出{% mathjax %}y{% endmathjax %}是什么？如果它是一个二元分类问题，那么我猜您会使用`sigmoid`激活函数，或者它可能是`softmax`，如果是一个`k`路分类问题，激活函数的选择将取决于输出{% mathjax %}y{% endmathjax %}的类型。因此，对于名称实体识别任务，其中y为`01`，第二个{% mathjax %}g{% endmathjax %}可以是`S`型激活函数。如果您想区分不同的激活函数，可以写{% mathjax %}g^{2}{% endmathjax %}，但通常不会这样做。然后更一般地，在时间`t`，{% mathjax %}a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a){% endmathjax %}，并且{% mathjax %}\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y){% endmathjax %}。同样，它可以是不同的激活函数。这个方程定义为神经网络中的**前向传播**，其中您将以{% mathjax %}a^{<0>}{% endmathjax %}为全零向量开始，然后使用{% mathjax %}a^{<0>}{% endmathjax %}和{% mathjax %}x^{<1>}{% endmathjax %}，您将计算{% mathjax %}a^{<1>}{% endmathjax %}和{% mathjax %}\hat{y}^{<1>}{% endmathjax %}，然后取{% mathjax %}x^{<2>}{% endmathjax %}并使用{% mathjax %}x^{<2>}{% endmathjax %}和{% mathjax %}a^{<1>}{% endmathjax %}计算{% mathjax %}a^{<2>}{% endmathjax %}和{% mathjax %}\hat{y}^{<2>}{% endmathjax %}，依此类推，您将从左侧到右侧进行**前向传播**。

现在，为了开发更复杂的神经网络，要采用这种符号并对其进行一些简化。为了简化符号，要采用一种稍微简单的方式书写。因此，把它写成{% mathjax %}a^{<t>} = g(w_a[a^{<t-1>},x^{<t>}] + b_a){% endmathjax %}。定义{% mathjax %}W_a = W_{aa} W_ax{% endmathjax %}，这就是矩阵{% mathjax %}w_a{% endmathjax %}。例如，如果{% mathjax %}a{% endmathjax %}是`100`维数，而示例中{% mathjax %}x{% endmathjax %}是`10,000`维数，那么{% mathjax %}w_{aa}{% endmathjax %}就是一个{% mathjax %}100\times 100{% endmathjax %}维的矩阵，而{% mathjax %}w_{ax}{% endmathjax %}就是一个{% mathjax %}100\times 10{% endmathjax %}维的矩阵。将这两个矩阵叠加在一起时，它将是`100`维的。Wa 将是一个{% mathjax %}100\times 10100{% endmathjax %}维的矩阵。将两个向量叠加在一起。使用这个符号来表示时，将取`-1`处的向量，也就是`100`维，并将其叠加在上面，最终是一个`10100`维向量。则{% mathjax %}\begin{bmatrix} W_{aa} & W_{ax}\end{bmatrix}\begin{bmatrix} a^{<t-1>} \\x^{<t>} \end{bmatrix} = W_{aa}a^{<t-1>}+ W_{ax}x^{<t>}{% endmathjax %}。不必携带两个参数矩阵{% mathjax %}W_{aa}{% endmathjax %}和{% mathjax %}W_{ax}{% endmathjax %}，而是可以将它们压缩为一个参数矩阵{% mathjax %}W_a{% endmathjax %}，这只是为了简化更复杂模型的符号。把它写成{% mathjax %}\hat{y}^{<t>} = g(W_y a^{<t>}+ b_y){% endmathjax %}。{% mathjax %}W_y{% endmathjax %}表示**权重矩阵**，这里的{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}表示参数，用于计算激活输出量。

一个`RNN`架构，其中输入的数量等于输出数量{% mathjax %}T_x = T_y{% endmathjax %}。并且{% mathjax %}T_x{% endmathjax %}并不总是等于{% mathjax %}T_y{% endmathjax %}。看到的示例使用{% mathjax %}T_x = T_y{% endmathjax %}，其中我们有一个输入序列{% mathjax %}x^{<1>},x^{<2>},\ldots,x^{<T_x>}{% endmathjax %}，有一个循环神经网络，当输入{% mathjax %}x^{<1>}{% endmathjax %}来计算{% mathjax %}\hat{y}^{<1>}、hat{y}^{<2>}{% endmathjax %}等等直到{% mathjax %}\hat{y}^{<T_y>}{% endmathjax %}时，其工作方式如下。有许多输入作为一个序列，输出序列也有许多输出。让我们看一个不同的例子。假设你想解决情绪分类问题。{% mathjax %}x{% endmathjax %}可能是一段文本，比如可能是一条电影评论，上面写着“这部电影没什么好看的”。{% mathjax %}x{% endmathjax %}将被排序，{% mathjax %}y{% endmathjax %}可能是`1~5`之间的数值，也可能是`0`或`1`。是正面评论或负面评论，也可能是是一星、两星、三星、四星还是五星评论？在这种情况下，我们可以按如下方式简化神经网络架构。将输入{% mathjax %}x^{<1>},x^{<2>}{% endmathjax %}。如果输入文本是“这部电影没什么好看的”。然后，让`RNN`读入整个句子，并在最后一个时间步输入整个句子后输出{% mathjax %}y{% endmathjax %}。这个神经网络将是一个多对一架构。因为它输入了许多单词，然后只输出一个数值。现在，除了多对一之外，您还可以拥有一对多架构。一对多神经网络架构的一个例子就是音乐生成。事实上，您可以在本课程的主要练习之一中自己实现这一点，您将拥有一个神经网络，输出一组与一段音乐相对应的音符。输入{% mathjax %}x{% endmathjax %}可能只是一个整数，告诉它您想要什么类型的音乐或您想要的音乐的第一个音符是什么，如果您不想输入任何东西，{% mathjax %}x{% endmathjax %}可以是空输入，也可以始终是向量{% mathjax %}\varnothing{% endmathjax %}。`RNN`输出第一个值，然后，第二个值，然后继续输出。第三个值，依此类推，直到您合成乐曲的最后一个音符。如果您愿意，您也可以输入{% mathjax %}a^{<0>}{% endmathjax %}。当生成序列时，通常会将这些第一个合成的输出也提供给下一层。还有一个更有趣的多对多示例值得描述。即当输入和输出长度不同时。因此，在您刚才看到的多对多示例中，输入长度和输出长度必须完全相同。对于机器翻译这样的应用来说，单词的数量在输入句子中，比如说一个法语句子，以及输出句子中的单词数量，比如说翻译成英文，这些句子的长度可能不同。因此，这里有一个替代的新网络架构，首先，读取输入，比如说您想要翻译成英文的法语句子。然后，让神经网络输出翻译。{% mathjax %}T_x{% endmathjax %}和{% mathjax %}T_y{% endmathjax %}的长度可以不同。多对多网络架构有两个不同的部分。一个是**编码器**，它将一个法语句子作为输入，另一个是**解码器**，它读取句子后，将翻译输出到另一种语言。
{% asset_img dl_2.png %}

****

{% asset_img dl_3.png %}

##### 反向传播

**循环神经网络**中的**反向传播**是如何工作的？通常，当您在某个编程框架中实现此功能时，编程框架通常会自动处理**反向传播**。您已经了解了前向传播，在神经网络中按如下方式从左到右计算这些激活，因此您输出了所有预测。在反向传播中，正如您可能已经猜到的那样，最终会与前向传播箭头相反的方向进行反向传播计算。所以，让我们来看看前向传播计算。这个输入序列{% mathjax %}x^{<1>},x^{<2>},x^{<3>},\ldots,x^{<T_x>}{% endmathjax %}。使用{% mathjax %}x^{<1>}{% endmathjax %}和{% mathjax %}a^{<0>}{% endmathjax %}一起用于计算{% mathjax %}a^{<1>}{% endmathjax %}，{% mathjax %}x^{<2>}{% endmathjax %}和{% mathjax %}a^{<1>}{% endmathjax %}一起用于计算{% mathjax %}a^{<2>}{% endmathjax %}，然后是{% mathjax %}a^{<3>}{% endmathjax %}，依此类推，直到{% mathjax %}a^{<T_x>}{% endmathjax %}。计算{% mathjax %}a^{<1>}{% endmathjax %}还需要参数。{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}是用于计算{% mathjax %}a^{<1>}{% endmathjax %}的参数。这些参数用于每个时间步，直到最后一个时间步的所有激活都取决于参数{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}。给定{% mathjax %}a^{<1>}{% endmathjax %}，神经网络就可以计算第一个预测{% mathjax %}\hat{y}^{<1>}{% endmathjax %}，然后计算第二个时间步{% mathjax %}\hat{y}^{<2>},\hat{y}^{<2>}{% endmathjax %}等，以及{% mathjax %}\hat{y}^{<T_y>}{% endmathjax %}。要计算{% mathjax %}\hat{y}{% endmathjax %}，您需要参数{% mathjax %}W_y{% endmathjax %}以及{% mathjax %}b_y{% endmathjax %}，这些参数将进入此节点以及所有其他节点。为了**计算反向传播**，您需要一个**损失函数**。神经网络输出某个特定单词是人名的概率，可能是`0.1`。把它定义为**标准逻辑回归损失**，也称为**交叉熵损失**。这是与单个单词在单个位置或单个时间集{% mathjax %}t{% endmathjax %}的单个预测相关的损失。现在定义整个序列的总体损失，因此{% mathjax %}\mathcal{L}(\hat{y}^{<t>},y^{<t>}) = -y^{<t>}\log\hat{y}^{<t>} - (1 - y^{<t>})\log(1-\hat{y}^{<t>})\;,\mathcal{L}(\hat{y},y) = \sum_{t=1}^{T_y}\mathcal{L}^{<t>}(\hat{y}^{<t>},y^{<t>}){% endmathjax %}。这是整个序列的损失。在计算图中，要计算给定{% mathjax %}\hat{y}^{<t>}{% endmathjax %}的损失，计算第一个时间步的损失，前提是计算第二个时间步的损失，第三个时间步的损失，依此类推，最后一步的损失。最后，为了计算总体损失，将这些损失全部相加，使用该方程计算最终的{% mathjax %}\mathcal{L}{% endmathjax %}，即每个时间步的单个损失之和。所以，这是计算问题，从之前看到的反向传播示例中，**反向传播**只需要在相反的方向上进行计算。然后使用**梯度下降法**获取相关参数并更新参数。在这个**反向传播**过程中，最重要的信息或最重要的递归计算是从右到左的，称为“**时间反向传播**”。对于**前向传播**，从左到右扫描，增加时间{% mathjax %}t{% endmathjax %}的索引，而**反向传播**是从右到左，有点像在时间上倒退。

##### 语言模型和序列生成

**语言建模**是自然语言处理中最基本任务之一。这也是`RNN`非常擅长的任务之一。什么是**语言模型**？假设您正在构建一个语音识别系统，并且您听到这句话，“苹果梨沙拉很好吃”。你刚才听到我说了什么？我说的是苹果配对沙拉吗？还是我说的是苹果梨沙拉？您可能认为第二句话更有可能。事实上，这是一个好的语音识别系统会输出的结果，即使这两个句子听起来完全一样。语音识别系统选择第二句话的方式是使用了**语言模型**，该模型告诉它这两个句子中任何一个的概率是多少。例如，**语言模型**可能会说，第一句出现的概率是{% mathjax %}3.2\times 10^{-13}{% endmathjax %}，而第二句出现的概率是{% mathjax %}5.7\times 10^{-10}{% endmathjax %}，根据这些概率，第二句出现的概率比第一句高出了{% mathjax %}10^3{% endmathjax %}倍以上，这就是语音识别系统会选择第二个句子的原因。**语言模型**的作用是告诉你该句子出现的概率是多少，这是语音识别系统和机器翻译系统的基本组成部分，机器翻译系统只希望输出可能的句子。因此，**语言模型**的作用是预测特定单词序列的**概率**。

如何**构建语言模型**？要使用`RNN`构建这样的模型，您首先需要一个训练集，其中包含大量您想要构建**语言模型**的任何语言文本。**语料库**一词是`NLP`术语，表示大量的任何语言句子。假设您在训练集中得到一个句子，如下所示，“猫平均每天睡`15`个小时”。您要做的第一件事是**标记**该句子，意味着将形成一个**词汇表**，然后将每个单词映射到`one-hot`向量或词汇表中的索引。您可能还想做的一件事是模拟句子的结束时间。另一个常见的做法是添加一个额外的**标记**，称为`EOS`，代表句子的结束，这可以帮助您确定句子的结束时间。如果您希望模型捕获句子的结束时间，则可以将`EOS`标记附加到训练集中每个句子的末尾。在这个例子中，我们有{% mathjax %}y^{<1>},y^{<2>},y^{<3>},y^{<4>},y^{<5>},y^{<6>},y^{<7>},y^{<8>},y^{<9>}{% endmathjax %}。此示例有`9`个输入。执行标记化步骤时，您可以决定句号是否也应该是标记。在这个例子中，我忽略了标点符号，因此我只是使用日作为另一个标记并省略了句号。如果您想将句号或其他标点符号视为显式标记，那么您也可以将句号添加到您的词汇表中。现在，另一个细节是，如果训练集中的某些单词不在您的词汇表中怎么办？如果您的词汇表使用了`10,000`个单词，可能是英语中最常见的`10,000`个单词，那么`Mau`一词作为决策，`Mau`的猫品种，可能不在您的前`10,000`个标记中。在这种情况下，您可以将单词`Mau`替换为一个称为`UNK`的独特标记，它代表未知单词，我们只模拟未知单词而不是特定单词`Mau`的概率。执行标记化步骤后，意味着将输入句子映射到词汇表中的各个标记或各个单词，接下来，让我们构建一个`RNN`来模拟这些不同序列的概率。这将是`RNN`架构。在时间步为`0`的位置计算激活{% mathjax %}a^{<1>}{% endmathjax %}作为某个输入{% mathjax %}x^{<1>}{% endmathjax %}的函数，而{% mathjax %}x^{<1>}{% endmathjax %}将被设置为{% mathjax %}\varnothing{% endmathjax %}。按照惯例，之前的{% mathjax %}a^{<0>}{% endmathjax %}也将其设置为{% mathjax %}\varnothing{% endmathjax %}。但是 {% mathjax %}a^{<1>}{% endmathjax %}会进行`softmax`预测，试图找出第一个单词{% mathjax %}y{% endmathjax %}的概率，所以这将是{% mathjax %}y^{<1>}{% endmathjax %}。这一步有一个`softmax`，因此它会预测字典中任何单词的概率，第一个单词{% mathjax %}a{% endmathjax %}的概率是多少，第一个单词是{% mathjax %}Aaron{% endmathjax %}的概率是多少，然后第一个单词是`cats`的概率是多少，一直到第一个单词是`Zulu`的概率是多少，或者第一个单词出现在句子中的概率是多少。{% mathjax %}\hat{y}^{<1>}{% endmathjax %}是根据`softmax`输出的，它只是预测第一个单词最终的概率。在例子中，一个较大的单词{% mathjax %}cats{% endmathjax %}是`10,000`路`softmax`输出。如果你有`10,000`个单词的词汇表，这个句子有两个额外的标记。`RNN`前进到下一步，并在下一步中激活{% mathjax %}a^{<2>}{% endmathjax %}。在这一步，它的工作是尝试找出第二个单词。在第二步，输出再次由`softmax`预测，`RNN`的工作是预测单词的概率，是`A`还是`Aaron`，是`cats`还是`Zulu`，是未知单词还是`EOS`，然后进入`RNN`的下一步，现在计算{% mathjax %}a^{<3>}{% endmathjax %}。但要预测第三个单词，这里的下一个输入{% mathjax %}x^{<3>} = y^{<2>}{% endmathjax %}，它的工作是找出序列中的下一个单词是什么。另一个是试图找出字典中所有单词的概率，依此类推。直到最后，你最终输入{% mathjax %}x^{<9>} = y^{<8>}{% endmathjax %}。然后计算出{% mathjax %}a^{<9>}{% endmathjax %}。`RNN`中的每一步都会查看一些前面的单词集，例如，给定前三个单词，下一个单词的分布是什么？`RNN`学习从左到右一次预测一个单词。为了通过网络训练它，我们将定义**成本函数**。在某个时间`t`，如果真实单词是{% mathjax %}y^{<t>}{% endmathjax %}，而网络`softmax`预测某个{% mathjax %}\hat{y}^{<t>}{% endmathjax %}，整体损失就是与各个预测相关的损失在所有时间步上的总和。
{% asset_img dl_4.png %}

如果你在一个大型训练集上训练这个`RNN`，给定任何初始单词集，它都可以预测下一个单词的概率。给定一个新句子，比如{% mathjax %}y^{<1>},y^{<2>},y^{<3>}{% endmathjax %}只有三个单词，可以计算出整个句子的概率，第一个`softmax`告诉你{% mathjax %}y^{<1>}{% endmathjax %}的概率是{% mathjax %}\mathbf{P}(y^{<1>}){% endmathjax %}，这将是第一个输出。然后给定{% mathjax %}y^{<1>}{% endmathjax %},{% mathjax %}y^{<2>}{% endmathjax %}的概率是{% mathjax %}\mathbf{P}(y^{<2>}|y^{<1>}){% endmathjax %}。然后，给定{% mathjax %}y^{<1>}{% endmathjax %}和{% mathjax %}y^{<2>}{% endmathjax %}，{% mathjax %}y^{<3>}{% endmathjax %}的概率是{% mathjax %}\mathbf{P}(y^{<3>}|y^{<1>},y^{<2>}){% endmathjax %}。通过将这三个概率相乘，你最终会得到三个单词句子的概率。这就是使用`RNN`训练**语言模型**的基本结构。

##### 序列采样

首先对模型生成的第一个单词进行**抽样**。输入通常是{% mathjax %}x^{<0>} = 0,\;a^{<0>} = 0{% endmathjax %}。第一个时间步将有输出的最大概率。根据分布随机抽样。{% mathjax %}a{% endmathjax %}的概率是多少，`Aaron`的概率是多少？`Zulu`的概率是多少，第一个单词是未知单词标记的概率是多少。然后使用`numpy`命令`np.random.choice`根据这个向量概率定义的分布进行采样，这样你就可以对第一个单词进行**采样**。接下来，继续第二个时间步，第二个时间步需要{% mathjax %}y^{<1>}{% endmathjax %}作为输入。将刚刚采样的{% mathjax %}\hat{y}^{<1>}{% endmathjax %}作为下一个时间步的输入传递到这里。所以无论怎样，只需选择第一个时间步在第二个位置传递这个输入，然后对{% mathjax %}\hat{y}^{<2>}{% endmathjax %}进行预测。例如，假设你对第一个单词进行采样后，第一个单词恰好是“`the`”。然后将“`the`”作为{% mathjax %}x^{<2>}{% endmathjax %}传递，现在等于{% mathjax %}\hat{y}^{<1>}{% endmathjax %}。找出在第一个单词是“the”的情况下第二个单词的概率。这将是{% mathjax %}\hat{y}^{<2>}{% endmathjax %}。然后再次使用这种类型的采样函数对{% mathjax %}\hat{y}^{<2>}{% endmathjax %}进行采样。并将其传递到下一个时间步，然后对第三个单词进行采样，然后继续进行，直到到达最后一个时间步。那么如何知道序列何时结束？如果句子结尾标记是您词汇表的一部分，您可以继续采样，直到`EOS`标记。或者，如果词汇表中没有包含这个，那么也可以决定采样`20`个单词或`100`个单词，然后继续进行，直到达到该时间步数。这个过程有时会生成一个未知的单词标记。如果你想确保算法永远不会生成这个标记，你可以做拒绝任何作为未知单词标记出现的样本，并继续从词汇表中重新采样，直到你得到一个不是未知单词。这就是从`RNN`语言模型中生成随机句子的方法。到目前为止，一直在构建一个单词级`RNN`。根据你的应用程序，你可以构建一个字符级`RNN`。所以在这种情况下，你的词汇表将只是字母。最多是`a~z`，以及空格、标点符号，数字`0~9`。如果你想区分大写和小写，你也可以包括大写字母。如果你构建的是字符级语言模型而不是单词级语言模型，那么你的序列{% mathjax %}y^{<1>},y^{<2>},y^{<3>}{% endmathjax %}是训练数据中的单个字符，而不是训练数据中的单词。因此，对于之前的示例，句子`cats`平均每天睡眠`15`小时。在此示例中，{% mathjax %}c{% endmathjax %}将是{% mathjax %}y^{<1>}{% endmathjax %}，{% mathjax %}a{% endmathjax %}将是{% mathjax %}y^{<2>}{% endmathjax %}，{% mathjax %}t{% endmathjax %}将是{% mathjax %}y^{<3>}{% endmathjax %}，空格将是{% mathjax %}y^{<4>}{% endmathjax %}，依此类推。使用字符级语言模型有一些优点和缺点。一个优点是你永远不必担心未知的单词标记。特别是，字符级语言模型能够为`mau`这样的序列分配非零概率。而如果`mau`不在你的单词级语言模型词汇表中，你只需为其分配未知的单词标记。但字符级语言模型的主要缺点是会得到更长的序列。许多英文句子会有`10~20`个单词，但可能会有几十个字符。因此在捕捉句子前半部分如何影响后半部分的长距离依赖关系方面，**字符语言模型**不如**词级语言模型**。而且字符级模型的训练成本也更高。看到的自然语言处理趋势是，在大多数情况下，**词级语言模型**仍在使用，但随着计算机速度的提高，越来越多的应用程序，至少在某些特殊情况下，人们开始关注**字符级模型**。但它们往往需要更多的硬件，训练成本更高，因此目前尚未得到广泛使用。除非是需要大量处理未知单词或其他词汇的专业应用程序。这就是`RNN`，以及如何使用它构建**语言模型**，以及从训练过的语言模型中采样。

##### RNN梯度消失

`RNN`的问题之一是遇到**梯度消失问题**。假设您看到这句话。“`The cat, which already ate and maybe already ate a bunch of food that was delicious, dot dot, dot, dot, dot was full`”。其中较早的措辞会影响句子中较晚的内容。迄今为止看到的`RNN`并不擅长捕捉**长期依赖关系**。之前关于训练非常深的神经网络的讨论，谈到了**梯度消失的问题**。这是一个非常非常深的神经网络，比如说100层甚至更深。然后你将从左到右进行**前向传播**，然后进行**反向传播**。如果这是一个非常深的神经网络，那么来自这个输出{% mathjax %}y{% endmathjax %}的**梯度**将很难传播回来影响这些较早层的**权重**和计算。对于具有类似问题的`RNN`，从左到右的**前向传播**，然后从右到左的**反向传播**。由于**梯度消失问题**，与后续时间步相关的误差输出很难影响较早的计算。让神经网络意识到它需要记忆可能很困难。你看到了单数名词还是复数名词，因此在后面的序列中它可以生成`was`或 `were`，具体取决于它是单数还是复数。请注意，在英语中，中间的内容可能很长。你可能需要花很长时间记住单数/复数，然后才能使用这些信息。由于这个问题，`RNN`模型有许多局部影响，意味着输出{% mathjax %}\hat{y}^{<3>}{% endmathjax %}主要受接近{% mathjax %}\hat{y}^{<3>}{% endmathjax %}的值的影响，而这里的值主要受接近输入的影响。这里的输出很难受到序列中很早的输入的影响。这是因为无论输出是什么，不管这个输出正确还是错误，误差都很难**反向传播**到序列的开头，因此很难修改神经网络在序列早期的计算方式。这是`RNN`的一个弱点，`RNN`往往不太擅长捕捉长距离依赖关系。在进行**反向传播**时，不应该只是呈指数下降，随着经过的层数呈指数增加。事实证明，梯度消失往往是训练`RNN`的最大问题。当发生梯度爆炸时，它可能是灾难性的，因为指数级大的梯度会导致参数变得很大，以至于神经网络参数变得非常混乱。**梯度爆炸**更容易被发现，因为参数会爆炸。你可能经常看到`NaN`，而不是数值，这是因为神经网络计算中**数值溢出**的结果。如果看到了**梯度爆炸**，一种解决方案是使用**梯度裁剪**。查看梯度向量，如果它大于某个阈值，则重新缩放某些梯度向量，使其不会太大，以便根据某个最大值进行裁剪。这是一个强大的解决方案，可以解决**梯度爆炸**问题。但**梯度消失**更难解决。

总结一下，训练一个非常深的神经网络。你可能会遇到**梯度消失**或**梯度爆炸**问题，其中导数要么呈指数下降，要么呈指数增长。`RNN`，比如说处理超过`1,000`次集合或超过`10,000`次集合的数据的`RNN`，基本上就是`1,000`层或`10,000`层的神经网络。可以使用**梯度裁剪**来解决**梯度爆炸**问题。

##### 门控循环单元（GRU）

您已经了解了`RNN`的工作原理。**门控循环单元**(`GRU`)对`RNN`隐藏层进行了修改，使其能够更好地捕获长距离连接，并对**梯度消失问题**有很大帮助。您已经看到了计算`RNN`在时间{% mathjax %}t{% endmathjax %}的激活公式。{% mathjax %}a^{<t>} = g(W_a[a^{<t-1>},x^{<t>}] + b_a){% endmathjax %}。如果{% mathjax %}g{% endmathjax %}是{% mathjax %}\tan h{% endmathjax %}激活函数，那么在{% mathjax %}\tan h{% endmathjax %}之后，计算激活的输出{% mathjax %}a^{<t>}{% endmathjax %}。`GRU`的许多想法分别来自`Junyoung Chung、Caglar、Gulcehre、KyungHyun Cho`和 `Yoshua Bengio`的这两篇论文。`GRU`有一个名为{% mathjax %}c{% endmathjax %}的新变量，它代表细胞，即记忆细胞。记忆单元的作用是提供一些记忆。例如，记住`cat`是单数还是复数，这样当它深入句子时，它仍然可以考虑句子的主语是单数还是复数。在时间{% mathjax %}t{% endmathjax %}，记忆单元将具有某个{% mathjax %}t{% endmathjax %}的值{% mathjax %}c{% endmathjax %}。将看到`GRU`单元将输出{% mathjax %}c^{<t>} = a^c{% endmathjax %}。使用不同的符号{% mathjax %}c{% endmathjax %}和{% mathjax %}a{% endmathjax %}来表示**记忆单元值**和**输出激活值**，即使它们是相同的，对于`GRU`来说，{% mathjax %}c^{<t>} = a^c{% endmathjax %}。这些是控制`GRU`单元计算的方程。在每个时间步中，考虑用{% mathjax %}t{% endmathjax %}的值{% mathjax %}\tilde{c}{% endmathjax %}覆盖记忆单元。使用激活函数{% mathjax %}\tilde{c}^{<t>} = \tan h(W_c[c^{<t-1>,x^{<t>}}] + b_c){% endmathjax %}。有一个门。我将把这个门称为{% mathjax %}\Gamma_u = \sigma (W_u[c^{<t-1>},x^{<t>}] + b_u){% endmathjax %}。{% mathjax %}u{% endmathjax %}代表**更新门**。这将是一个介于`0`和`1`之间的值。`S`型函数看起来像这样，值始终在`0`和`1`之间。`S`型函数要么非常接近`0`，要么非常接近`1`。为了直观起见，可以认为{% mathjax %}\Gamma{% endmathjax %}是`0`或`1`。

`GRU`的关键部分是这个方程{% mathjax %}c^{<t>} = \Gamma_u\ast\tilde{c}^{<t>} + (1-\Gamma_u) + c^{<t-1>}{% endmathjax %}，门将决定我们是否真的更新它。可以这样想，也许这个记忆单元{% mathjax %}c{% endmathjax %}将被设置为`0`或`1`，这取决于你保存的单词，实际上是句子的主语是单数还是复数。因为它是单数，所以我们假设将其设置为`1`，如果是复数，我们将其设置为`0`，然后`GRU`单元将一直记住{% mathjax %}c^{<t>}{% endmathjax %}的值，直到这里，它仍然等于`1`，这表明它是单数，因此选择`was`。门（{% mathjax %}\Gamma_u{% endmathjax %}）的工作是决定何时更新此值。当看到短语`the cat`时，句子`cat`的主语。这将是更新此位的好时机，然后`the cat was full`，那么知道不再需要记忆了，可以忘记它。如果{% mathjax %}z{% endmathjax %}的更新值等于`1`，那么就是将{% mathjax %}c^{<t>}{% endmathjax %}的新值设置为这个候选值，就像在这里，将门设置为`1`，然后继续更新该位。对于中间的所有这些值，你应该让门等于`0`，不要更新它，只需保留旧值，如果{% mathjax %}\Gamma_u = 0{% endmathjax %}，它只是将{% mathjax %}c^{<t>}{% endmathjax %}设置为等于旧值，即使你从左到右扫描句子。当门等于`0`时，不要更新它，只需保留该值，不要忘记它的值是什么，一直将{% mathjax %}c^{<t>} = c^{<t-1>}{% endmathjax %}，并且仍然记住猫是单数。`GRU`单元输入{% mathjax %}c^{<t-1>}{% endmathjax %}作为前一个时间步长。然后它还接受这个输入{% mathjax %}x^{<t>}{% endmathjax %}。然后这两个东西结合在一起。这给了{% mathjax %}\tilde{c}^t{% endmathjax %}，它是替换{% mathjax %}c^{<t>}{% endmathjax %}的候选，然后有一组不同的参数，通过一个`sigmoid`激活函数，这给了{% mathjax %}\Gamma_u{% endmathjax %}，它是**更新门**，最后，所有这些东西通过另一个操作结合在一起。它以门值、候选新值和{% mathjax %}c^{<t>}{% endmathjax %} 的旧值作为输入，它们一起为记忆单元生成新值。这就是{% mathjax %}c^{<t>} = a{% endmathjax %}。如果你愿意，你也可以通过`softmax`或其他东西来对{% mathjax %}y^{<t>}{% endmathjax %}进行一些预测，这就是`GRU`单元。它非常擅长的是通过**门**来决定，当从左到右扫描句子时，假设这是**更新记忆单元**的好时机，然后输入，而不是更改它，直到你到达你真正需要使用你在句子中更早设置的这个记忆单元的点。因为只要这个量是一个很大的负值，**门**就很容易设置为`0`，然后直到数值舍入，**更新门**为`0`，非常接近零。如果是这种情况，那么这个更新方程{% mathjax %}c^{<t>} = c^{<t-1>}{% endmathjax %}，因此这非常适合保持单元的值，因为{% mathjax %}\Gamma{% endmathjax %}非常接近`0`，可以是`0.000001`甚至更小。它不会受到**梯度消失**问题的影响，因为假设{% mathjax %}\Gamma{% endmathjax %}非常接近`0`，这基本上变成{% mathjax %}c^{<t>} = c^{<t-1>}{% endmathjax %}，并且{% mathjax %}c^{<t>}{% endmathjax %}的值几乎保持精确，即使经过多次迭代也是如此。这可以解决**梯度消失**问题，因此允许网络学习有非常长距离的依赖关系，例如`cat`和`was`是相关的，即使它们中间有很多词分开。在方程式中，{% mathjax %}c^{<t>}{% endmathjax %}可以是一个向量。如果你有`100`维的隐藏激活值，那么{% mathjax %}c^{<t>}{% endmathjax %}可以是`100`维。在这种情况下，这里，如果门是`100`维向量，它实际上是`100`维的位向量，值主要是`0`和`1`，这是你想要更新的位。当然在实践中，{% mathjax %}\Gamma{% endmathjax %}不会正好是`0`和`1`。这些元素乘法的作用只是告诉`GRU`，在每个时间步中要更新的**记忆单元向量**的维度是什么。您可以选择在更新其他位时保持某些位不变。例如，您会使用一位来记住单数或复数猫，也许您会使用其他一些位来意识到您正在谈论食物。因为我们谈到了吃东西和食物，所以您会期望稍后谈论猫是否吃饱。您可以使用不同的位，并且在每个时间点只更改位的子集。现在您了解了`GRU`最重要的思想。
{% mathjax '{"conversion":{"em":14}}' %}
\begin{align}
\tilde{c}^{<t>} & = \tan h(W_c[\Gamma_r\ast c^{<t-1>,x^{<t>}}] + b_c) \\
\Gamma_u & = \sigma (W_u[c^{<t-1>},x^{<t>}] + b_u) \\
\Gamma_r & = \sigma (W_r[c^{<t-1>},x^{<t>}] + b_r) \\
c^{<t>} & = \Gamma_u\ast\tilde{c}^{<t>} + (1-\Gamma_u)\ast c^{<t-1>} \\
a^{<t>} & = c^{<t>} \\
\end{align}
{% endmathjax %}

你可以想象，有多种方法来设计这些类型的神经网络，事实证明，研究人员已经尝试了许多不同的版本来设计这些单元，拥有更长距离的连接。尝试对长距离效应进行建模，同时解决**梯度消失**问题。`GRU`是最常用的版本之一，研究人员已经将其融合在一起，然后发现它对于许多不同的问题来说强大且有用的。`GRU`是一个标准的，只是常用的单元。这就是`GRU`，即**门控循环单元**。它是为了更好地捕捉非常长的依赖关系，从而使`RNN`更加有效。

##### 长短期记忆（LSTM）

<span style="color:#295F98;font-weight:900;">长短期记忆单元(`LSTM`)</span>。比`GRU`更强大，对于`GRU`，有{% mathjax %}t=\tilde{c}^{<t>}{% endmathjax %}和两个门（**更新门**和**重置门**）。{% mathjax %}\tilde{c}^{<t>}{% endmathjax %}是替换**记忆单元**的候选。然后使用**更新门**{% mathjax %}\Gamma_u{% endmathjax %}来决定是否使用{% mathjax %}\tilde{c}^{<t>}{% endmathjax %}更新{% mathjax %}c^{<t>}{% endmathjax %}。`LSTM`是`GRU`的一个更强大、更通用的版本。对于`LSTM`，我们将不再有{% mathjax %}a^{<t>} = c^{<t>}{% endmathjax %}的情况。现在使用{% mathjax %}a^{<t>},a^{<t-1>},c^{<t-1>}{% endmathjax %}，不使用{% mathjax %}\Gamma_r{% endmathjax %}。这里有一个与`GRU`相同的更新门。{% mathjax %}\tilde{c}^{<t>} = \tan h(W_c[a^{<t-1>},x^{<t>}] + b_c){% endmathjax %}。**更新门**{% mathjax %}\Gamma_u = \sigma (W_u[a^{<t-1>},x^{<t>}] + b_u){% endmathjax %}，**遗忘门**{% mathjax %}\Gamma_f{% endmathjax %}是`S`型函数{% mathjax %}\Gamma_f = \sigma (W_f[a^{<t-1>},x^{<t>}] + b_f){% endmathjax %}。然后将得到一个新的**输出门**，**输出门**{% mathjax %}\Gamma_o = \sigma (W_o[a^{<t-1>},x^{<t>}] + b_o){% endmathjax %}，存储单元的更新值{% mathjax %}c^{<t>} = \Gamma_u \ast \tilde{c}^{<t>} + \Gamma_f\ast c^{<t-1>}\;,\; a^{<t>} = \Gamma_o \ast \tan h c^{<t>}{% endmathjax %}。这为**存储单元**提供了保留旧值{% mathjax %}c^{<t-1>}{% endmathjax %}的选项，然后将这个新值{% mathjax %}\tilde{c}^{<t-1>}{% endmathjax %}更新到它。这些控制`LSTM`的方程式。可以看出它有三个门(更新门，遗忘门，输出门)。因此，它有点复杂，并且位置略有不同。
{% asset_img dl_5.png %}

