---
title: 深度学习(DL)(一) — 探析
date: 2024-09-02 17:15:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

<span style="color:#295F98;font-weight:900;">在语音识别中</span>，您将获得一个输入音频片段{% mathjax %}x{% endmathjax %}，并被要求将其映射到文本转录{% mathjax %}y{% endmathjax %}。这里的输入和输出都是**序列数据**，因为{% mathjax %}x{% endmathjax %}是一个音频片段，因此它会随时间播放而输出{% mathjax %}y{% endmathjax %}，{% mathjax %}y{% endmathjax %}是一个单词序列。音乐生成是序列数据问题的另一个例子。在这种情况下，只有输出{% mathjax %}y{% endmathjax %}是一个序列，输入可以是空集，也可以是一个整数，可能指的是您想要生成的音乐类型，也可能是您想要的音乐的前几个音符。但这里的{% mathjax %}x{% endmathjax %}可以是零，也可以只是一个整数，而输出{% mathjax %}y{% endmathjax %}是一个序列。
<!-- more -->

<span style="color:#295F98;font-weight:900;">在情绪分类中</span>，输入{% mathjax %}x{% endmathjax %}是一个序列，因此，给定输入短语，如“这部电影没有什么可喜欢的”，您认为这篇评论会得到多少颗星？**序列模型**对于`DNA`序列分析也非常有用。`DNA`通过四个字母`A、C、G`和`T`表示。因此，给定一个`DNA`序列，您能否标记该`DNA`序列的哪一部分对应于蛋白质。<span style="color:#295F98;font-weight:900;">在机器翻译中</span>，您会得到一个输入句子，`voulez-vou chante avec moi？`然后要求以不同的语言输出翻译。<span style="color:#295F98;font-weight:900;">在视频活动识别中</span>，您可能会得到一系列视频帧并被要求识别活动。在名称实体识别中，您可能会得到一个句子并被要求识别该句子中的人。因此，所有这些问题都可以作为**监督学习**来解决，标签数据{% mathjax %}x,y{% endmathjax %}作为训练集。但是，从这个示例列表中可以看出，存在许多不同类型的序列问题。在某些情况下，输入{% mathjax %}x{% endmathjax %}和输出{% mathjax %}y{% endmathjax %}都是序列，有时{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}可以具有不同的长度，或者{% mathjax %}x{% endmathjax %}和{% mathjax %}y{% endmathjax %}具有相同的长度。或者只有{% mathjax %}x{% endmathjax %}或相反的{% mathjax %}y{% endmathjax %}是序列。

#### 循环神经网络(RNN)

现在，让我们讨论如何构建模型，构建**神经网络**来学习从{% mathjax %}x{% endmathjax %}到{% mathjax %}y{% endmathjax %}的映射。现在，您可以做的一件事是使用标准神经网络来完成此任务。有九个输入词。可以尝试将这九个输入词（可能是九个向量）输入到标准神经网络（可能是几个隐藏层）中，然后最终输出九个值`0`或`1`，这些值会告诉您每个单词是否是人名的一部分。但这种方法效果不佳，并且实际上存在两个主要问题：第一个问题是输入和输出的长度可能不同，并且示例也不同。因此，并不是每个示例都具有相同的输入长度{% mathjax %}T_x{% endmathjax %} 或相同的上限长度{% mathjax %}T_y{% endmathjax %}，也许每个句子都有最大长度。也许可以填充每个输入，直到达到最大长度，但这似乎仍然不是一个很好的表示。第二个更严重的问题是，像这样的简单神经网络架构，它不会**共享**文本的不同位置上学习到的特征。特别是，如果神经网络已经学会了单词`Harry`出现在位置`1`上，那是人名的一部分，如果它能自动找出`Harry`出现在其他位置{% mathjax %}x^{<T>}{% endmathjax %}上也意味着那可能是人名，那不是很好吗？这可能类似于你在卷积神经网络中看到的情况，你希望从图像的一部分学到的东西能够快速推广到图像的其他部分，我们也希望序列数据也能有类似的效果。与你看到的情况类似，使用更好的表示也会让你减少模型中的参数数量。

什么是循环神经网络？如果你从左到右阅读句子，你读到的第一个词，比如{% mathjax %}x_1{% endmathjax %}，把第一个词输入到神经网络层中。第一个神经网络有一个隐藏层，我们可以让神经网络尝试预测输出。**循环神经网络**的作用是，当它继续阅读句子中的第二个词，比如{% mathjax %}x_2{% endmathjax %}时，它不仅会使用{% mathjax %}x_2{% endmathjax %}预测{% mathjax %}y_2{% endmathjax %}，还会输入一些信息，比如计算机是否在时间步骤`1`中激活值会传递到时间步骤`2`。然后在下一个时间步骤，**循环神经网络**输入第三个单词{% mathjax %}x_3{% endmathjax %}，并尝试输出一些预测{% mathjax %}y_3{% endmathjax %}等等，直到最后一个时间步骤，它输入{% mathjax %}x^{<T_x>}{% endmathjax %}，然后输出{% mathjax %}\hat{y}^{<T_y>}{% endmathjax %}。至少在这个例子中，{% mathjax %}T_x = T_y{% endmathjax %}，如果{% mathjax %}t_x{% endmathjax %}和{% mathjax %}t_y{% endmathjax %}不相同，架构将发生一些变化。在每个时间步骤，**循环神经网络**都将激活传递到下一个时间步骤以供使用。为了启动整个过程，我们还会在开始时有一些虚构的激活，这通常是零向量。一些研究人员会随机初始化{% mathjax %}a^{<0>}{% endmathjax %}。还有其他方法可以初始化{% mathjax %}a^{<0>}{% endmathjax %}，但实际上使用零向量乘以零激活是最常见的选择。这样就可以输入到神经网络中。在一些研究论文或一些书籍中，你会看到这种类型的神经网络，如下图所示，在每个时间步骤中，你输入{% mathjax %}x{% endmathjax %}并输出{% mathjax %}\hat{y}{% endmathjax %}。也许有时会有一个{% mathjax %}T{% endmathjax %}索引，然后表示**循环连接**，有时人们会画一个这样的循环，**层反馈**给细胞。
{% asset_img dl_1.png %}

循环神经网络从左到右扫描数据。它在每个时间步骤使用的参数是**共享**的。在这个**循环神经网络**中，当对{% mathjax %}y_3{% endmathjax %}进行预测时，它不仅从{% mathjax %}x_3{% endmathjax %}获取信息，还从 {% mathjax %}x_1{% endmathjax %}和{% mathjax %}x_2{% endmathjax %}获取信息，因为{% mathjax %}x_1{% endmathjax %}上的信息可以传递，以帮助预测{% mathjax %}y_3{% endmathjax %}。`RNN`的一个弱点是它只使用序列中较早的信息进行预测。特别是，当预测{% mathjax %}Y_3{% endmathjax %}时，不使用有关单词{% mathjax %}x_4,x_5,x_6{% endmathjax %}等的信息。所以这是一个问题，如果你得到一个句子，“他说泰迪·罗斯福是一位伟大的总统。”为了确定`Teddy`这个词是否是某人名字的一部分，不仅要知道前两个单词的信息，还要知道句子中后面的单词的信息，因为这个句子也可能是，“他说泰迪熊正在打折。”所以只给出前三个单词是不可能确定`Teddy`这个词是否是某人名字的一部分。在第一个例子中，它是。在第二个例子中，它不是。但是如果你只看前三个词，你就无法分辨出区别。所以这个特定的神经网络结构的一个限制是，在某个时间的预测使用输入或使用序列中较早的输入信息，但不使用序列中较晚的信息。现在需要明确说明这个神经网络所做的计算。正如之前提到的，从输入{% mathjax %}a_{\text{zero}}{% endmathjax %}开始，该输入等于全零向量。接下来，这是正向传播。要计算{% mathjax %}a_1{% endmathjax %}，需要将其计算为激活函数{% mathjax %}a^{<0>} = \vec{0}\;,\;\; a^{<1>} = g(W_{aa} a^{<0>} + W_{ax} x^{<1>} + b_a){% endmathjax %}。然后计算{% mathjax %}\hat{y}^{<1>} = g(W_{ya}a^{<1>} + b_y){% endmathjax %}。预测在时间`1`，这将是某个**激活函数**，与上面的激活函数不同。使用这些矩阵的底数的符号约定，例如{% mathjax %}W_{ax}{% endmathjax %}。第二个索引表示{% mathjax %} W_{ax}x{% endmathjax %}，而这个{% mathjax %}a{% endmathjax %}表示用于计算某个类似{% mathjax %}a{% endmathjax %}的量。在`RNN`中，用于计算激活函数是{% mathjax %}\tan h{% endmathjax %}，有时也会使用非常宽松的函数，尽管{% mathjax %}\tan h{% endmathjax %}实际上是一种常见的选择，还有其他方法可以防止**梯度消失问题**，输出{% mathjax %}y{% endmathjax %}是什么？如果它是一个二元分类问题，那么我猜您会使用`sigmoid`激活函数，或者它可能是`softmax`，如果是一个`k`路分类问题，激活函数的选择将取决于输出{% mathjax %}y{% endmathjax %}的类型。因此，对于名称实体识别任务，其中y为`01`，第二个{% mathjax %}g{% endmathjax %}可以是`S`型激活函数。如果您想区分不同的激活函数，可以写{% mathjax %}g^{2}{% endmathjax %}，但通常不会这样做。然后更一般地，在时间`t`，{% mathjax %}a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a){% endmathjax %}，并且{% mathjax %}\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y){% endmathjax %}。同样，它可以是不同的激活函数。这个方程定义为神经网络中的**前向传播**，其中您将以{% mathjax %}a^{<0>}{% endmathjax %}为全零向量开始，然后使用{% mathjax %}a^{<0>}{% endmathjax %}和{% mathjax %}x^{<1>}{% endmathjax %}，您将计算{% mathjax %}a^{<1>}{% endmathjax %}和{% mathjax %}\hat{y}^{<1>}{% endmathjax %}，然后取{% mathjax %}x^{<2>}{% endmathjax %}并使用{% mathjax %}x^{<2>}{% endmathjax %}和{% mathjax %}a^{<1>}{% endmathjax %}计算{% mathjax %}a^{<2>}{% endmathjax %}和{% mathjax %}\hat{y}^{<2>}{% endmathjax %}，依此类推，您将从左侧到右侧进行**前向传播**。

现在，为了开发更复杂的神经网络，要采用这种符号并对其进行一些简化。为了简化符号，要采用一种稍微简单的方式书写。因此，把它写成{% mathjax %}a^{<t>} = g(w_a[a^{<t-1>},x^{<t>}] + b_a){% endmathjax %}。定义{% mathjax %}W_a = W_{aa} W_ax{% endmathjax %}，这就是矩阵{% mathjax %}w_a{% endmathjax %}。例如，如果{% mathjax %}a{% endmathjax %}是`100`维数，而示例中{% mathjax %}x{% endmathjax %}是`10,000`维数，那么{% mathjax %}w_{aa}{% endmathjax %}就是一个{% mathjax %}100\times 100{% endmathjax %}维的矩阵，而{% mathjax %}w_{ax}{% endmathjax %}就是一个{% mathjax %}100\times 10{% endmathjax %}维的矩阵。将这两个矩阵叠加在一起时，它将是`100`维的。Wa 将是一个{% mathjax %}100\times 10100{% endmathjax %}维的矩阵。将两个向量叠加在一起。使用这个符号来表示时，将取`-1`处的向量，也就是`100`维，并将其叠加在上面，最终是一个`10100`维向量。则{% mathjax %}\begin{bmatrix} W_{aa} & W_{ax}\end{bmatrix}\begin{bmatrix} a^{<t-1>} \\x^{<t>} \end{bmatrix} = W_{aa}a^{<t-1>}+ W_{ax}x^{<t>}{% endmathjax %}。不必携带两个参数矩阵{% mathjax %}W_{aa}{% endmathjax %}和{% mathjax %}W_{ax}{% endmathjax %}，而是可以将它们压缩为一个参数矩阵{% mathjax %}W_a{% endmathjax %}，这只是为了简化更复杂模型的符号。把它写成{% mathjax %}\hat{y}^{<t>} = g(W_y a^{<t>}+ b_y){% endmathjax %}。{% mathjax %}W_y{% endmathjax %}表示**权重矩阵**，这里的{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}表示参数，用于计算激活输出量。

##### 反向传播

**循环神经网络**中的**反向传播**是如何工作的？通常，当您在某个编程框架中实现此功能时，编程框架通常会自动处理**反向传播**。您已经了解了前向传播，在神经网络中按如下方式从左到右计算这些激活，因此您输出了所有预测。在反向传播中，正如您可能已经猜到的那样，最终会与前向传播箭头相反的方向进行反向传播计算。所以，让我们来看看前向传播计算。这个输入序列{% mathjax %}x^{<1>},x^{<2>},x^{<3>},\ldots,x^{<T_x>}{% endmathjax %}。使用{% mathjax %}x^{<1>}{% endmathjax %}和{% mathjax %}a^{<0>}{% endmathjax %}一起用于计算{% mathjax %}a^{<1>}{% endmathjax %}，{% mathjax %}x^{<2>}{% endmathjax %}和{% mathjax %}a^{<1>}{% endmathjax %}一起用于计算{% mathjax %}a^{<2>}{% endmathjax %}，然后是{% mathjax %}a^{<3>}{% endmathjax %}，依此类推，直到{% mathjax %}a^{<T_x>}{% endmathjax %}。计算{% mathjax %}a^{<1>}{% endmathjax %}还需要参数。{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}是用于计算{% mathjax %}a^{<1>}{% endmathjax %}的参数。这些参数用于每个时间步，直到最后一个时间步的所有激活都取决于参数{% mathjax %}W_a{% endmathjax %}和{% mathjax %}b_a{% endmathjax %}。给定{% mathjax %}a^{<1>}{% endmathjax %}，神经网络就可以计算第一个预测{% mathjax %}\hat{y}^{<1>}{% endmathjax %}，然后计算第二个时间步{% mathjax %}\hat{y}^{<2>},\hat{y}^{<2>}{% endmathjax %}等，以及{% mathjax %}\hat{y}^{<T_y>}{% endmathjax %}。要计算{% mathjax %}\hat{y}{% endmathjax %}，您需要参数{% mathjax %}W_y{% endmathjax %}以及{% mathjax %}b_y{% endmathjax %}，这些参数将进入此节点以及所有其他节点。为了**计算反向传播**，您需要一个**损失函数**。神经网络输出某个特定单词是人名的概率，可能是`0.1`。把它定义为**标准逻辑回归损失**，也称为**交叉熵损失**。这是与单个单词在单个位置或单个时间集{% mathjax %}t{% endmathjax %}的单个预测相关的损失。现在定义整个序列的总体损失，因此{% mathjax %}\mathcal{L}(\hat{y}^{<t>},y^{<t>}) = -y^{<t>}\log\hat{y}^{<t>} - (1 - y^{<t>})\log(1-\hat{y}^{<t>})\;,\mathcal{L}(\hat{y},y) = \sum_{t=1}^{T_y}\mathcal{L}^{<t>}(\hat{y}^{<t>},y^{<t>}){% endmathjax %}。这是整个序列的损失。在计算图中，要计算给定{% mathjax %}\hat{y}^{<t>}{% endmathjax %}的损失，计算第一个时间步的损失，前提是计算第二个时间步的损失，第三个时间步的损失，依此类推，最后一步的损失。最后，为了计算总体损失，将这些损失全部相加，使用该方程计算最终的{% mathjax %}\mathcal{L}{% endmathjax %}，即每个时间步的单个损失之和。所以，这是计算问题，从之前看到的反向传播示例中，**反向传播**只需要在相反的方向上进行计算。然后使用**梯度下降法**获取相关参数并更新参数。在这个**反向传播**过程中，最重要的信息或最重要的递归计算是从右到左的，称为“**时间反向传播**”。对于**前向传播**，从左到右扫描，增加时间{% mathjax %}t{% endmathjax %}的索引，而**反向传播**是从右到左，有点像在时间上倒退。

