---
title: SpatialVLM-探析(视觉语言模型 & 空间推理)
date: 2024-08-05 18:44:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

[`SpatialVLM`](https://arxiv.org/pdf/2401.12168)是由`Google DeepMind`开发的一种**视觉语言模型**(`Vision-Language Model, VLM`)，其主要目标是增强`VLMs`的**空间推理能力**(`Spatial Reasoning Capabilities`)，使其能够理解和推理三维空间中的物体关系。
<!-- more -->

主要特点：
- **增强的空间推理能力**：`SpatialVLM`通过训练在大规模的空间推理数据集上，显著提升了模型在定性和定量空间推理任务中的表现。这些任务包括回答关于物体位置、距离和尺寸的问题。
- **数据生成和训练**：为了训练`SpatialVLM`，研究团队开发了一种自动生成`3D`空间问答(`VQA`)数据的框架。该框架能够将二维图像提升为三维点云，并生成包含空间注释的大规模数据集。这些数据集包含了多达 `20`亿个`VQA`示例，覆盖了`1000`万张真实世界的图像。
- **多模态学习**：`SpatialVLM`结合了视觉和语言数据，通过多模态学习方法，将图像和文本数据映射到一个联合的嵌入空间中。这种方法使模型能够同时处理和理解图像和文本信息，从而在生成和理解内容时利用这两种信息源。

关键技术：
- **对比学习**：`SpatialVLM`使用**对比学习**的方法，通过最大化匹配图像-文本对的相似性和最小化不匹配对的相似性来训练模型。这种方法提高了模型在多模态任务中的表现。
- **嵌入投影**：`SpatialVLM`模型使用**嵌入投影**模块对齐图像和文本的嵌入表示，确保两者在同一个**语义空间**中具有可比性，从而能够进行有效的**空间推理**。
- **链式推理**：`SpatialVLM`能够执行复杂的**链式推理**任务，意味着它可以通过与强大的大语言模型(`LLM`)结合，解决**多步骤的空间推理**问题。这种能力使其在机器人学和其他需要复杂空间分析的领域中具有广泛的应用前景。

`SpatialVLM`通过结合**视觉**和**语言**处理能力，显著提升了模型的**空间推理能力**。它利用大规模的**空间推理数据集**和**多模态学习**方法，能够在多种任务中表现出色，如视觉问答和机器人学。
{% asset_img v_1.png "空间AI大脑：设想一般空间AI系统的表示和处理图形结构如何映射到图形处理器。我们确定的关键元素是实时处理循环、基于图形的地图存储和与传感器和输出执行器接口的块。" %}

传统`AI`通常擅长图像识别或文本分析等任务，专注于图像或文档中的“内容”，而`AI`中的**空间智能**则不仅限于**识别对象**。它使`AI`能够了解对象的“位置”和“方式”——它们的位置、大小、距离以及它们在`3D`空间中如何相互作用。

#### SpatialVLM

`SpatialVLM`是采用数据合与预训练机制实现的模型，目的是为了增强`VLM`的**空间推理能力**。事实证明在合成数据上训练的`VLM`表现出强大的**空间推理能力**，并且可以从`2D`输入图像生成度量距离估值，从而解决当前`VLM`（如`GPT-4V`）遗留的缺陷。理解和推理**空间关系**是**视觉问答**(`VQA`)和**机器人技术**的基本能力。虽然**视觉语言模型**(`VLM`)在某些`VQA`基准测试中表现出色，但它们仍然缺乏`3D`空间的推理能力，例如**识别物理对象的定量关系**，如距离或大小差异。我们假设`VLM`有限的空间推理能力是由于训练数据中缺乏`3D`空间的知识，目标是通过使用互联网规模的空间推理数据训练`VLM`解决此问题。为此，这里提出了一个解决方法。首先开发了一个自动`3D`空间`VQA`数据生成框架，该框架可在`1000`万张真实世界图像上扩`展20`亿个`VQA`示例。并研究了训练方法中的各种参数，包括数据质量、训练管道和`VLM`架构。通过在互联网规模`3D`空间推理数据集上训练`VLM`，从而显著增强了其在**定性**和**定量**空间`VQA`方面的能力。最后，由于其**定量**估计的能力，从而使其解锁了思维链空间推理和机器人技术中的下游应用。

近年来，**视觉语言模型**(`VLM`)在各种任务中取得了重大进展，包括**图像字幕**、**视觉问答**(`VQA`)、**具身规划**、**动作识别**等。虽然`VLM`是适用于各种任务的强大通用模型，但大多数最先进的`VLM`仍然难以进行空间推理，即需要理解物体在`3D`空间中的位置或空间关系的任务。**空间推理能力**本身很有用，但也适用于机器人或`AR`等下游应用。例如，具有**空间推理能力**的`VLM`可以用作更好的通用**奖励注释器**和**成功检测器**。对`VLM`基础模型的探索往往受到人类能力的启发。人类通过具体体验和进化发展，拥有天生的**空间推理能力**。我们毫不费力地确定**空间关系**，例如物体相对于彼此的定位或估算距离和大小，而无需复杂的思路或心理计算。这种直接空间推理任务的自然能力与`VLM`当前的局限性形成了鲜明对比，从而阻止了它们完成需要**多步空间推理**的现实世界任务。因此，我们假设，当前`VLM`的空间推理能力有限并不是由于其架构的根本限制，而是由于训练此类模型的规模上可用的通用数据集的限制。例如，许多`VLM`是在互联网规模的数据集上进行训练的，这些数据集以"图像-标题对" 为特征，其中只包含有限的空间信息（难以获得丰富的空间信息数据或用于`3D`感知查询的高质量人工标注）。

**自动数据生成**和**增强技术**是解决数据限制问题的方法之一。然而，大多数以前的数据生成工作都侧重于渲染具有地面真实语义注释的照片级图像，但忽略了对象和`3D`关系的丰富性。相比之下，直接从现实世界数据中提取空间信息，以捕捉真实`3D`世界的多样性和复杂性。现成的视觉模型的最新进展可以从`2D`图像自动生成丰富的`3D`空间注释。为此，这里提出了一个名为`SpatialVLM`的系统，该系统能够生成和训练`VLM`，以增强其**空间推理能力**。具体来说，通过结合1.开放词汇检测、2.度量深度估计、3.语义分割、4.以对象为中心图像描述模型，我们可以大规模地对现实世界数据进行密集注释。`SpatialVLM`将视觉模型生成的数据转换为一种格式，可用于在图像描述、`VQA`和空间推理数据混合训练`VLM`。通过实验，发现经过训练的`VLM`展现出理想的能力。首先，它回答**定性空间**问题的能力大大增强。其次，尽管训练的数据嘈杂，它仍能可靠地进行**定量估算**。这种能力不仅让它掌握了关于物体大小的常识性知识，而且使它成为重新排列任务的开放**词汇奖励注释器**。最后，发现这种**空间视觉语言模型**得益于其自然语言方面的能力，当与强大的大语言模型相结合时，可以执行**空间思维链**来解决复杂的**空间推理任务**。

- **学习空间推理**(`Learning Spatial Reasoning`)：空间距离估计是比较常见的任务之一，例如`SLAM`或**深度估计**。在将这些空间概念应用于推理时，先前的研究通常侧重于**显式空间场景记忆**或**空间场景图**。场景图允许基于编码的空间结构进行**可解释、结构化、统计关系学习**。要回答`VQA`格式的空间问题，它们必须将其作为场景图上的**寻路问题**来处理。另一方面，`VLM`是在来自视觉语言数据集的大量松散结构信息上进行预训练的。与场景图不同，空间理解是**隐式编码**的。我们可以通过辅助任务将深度和`3D`结构注入进权重中，从而捕获关系信息。
- **基于视觉语言模型**：大型语言模型(`LLM`)在互联网规模的数据上进行训练，使其成为有效的**常识推理器**。然而，`LLM`（以及扩展的`VLM`）可能缺乏在**社交推理**、**物理推理**、**具身任务**和**空间推理任务**中表现良好的必要基础。虽然具有交互式的语言模型基础，但引入大型视觉模型（如`Flamingo`、`PaLI`或`PaLM-E`）已使性能飞跃。这些基于视觉的模型已用于多个下游任务，例如机器人成功检测、动作预测和奖励预测。在这项工作中，通过在生成的`VQA`数据集上微调`VLM`来解决**空间推理**问题。
- **视觉语言数据集中的空间信息**：许多先前的研究都集中在对`VLM`进行基准测试。其他人则专注于细粒度场景理解，例如语义分割、对象检测或对象识别。该领域的真实数据可能受到人类标记者生成的数量限制，而合成数据的表达能力本质上是有边界的。在这项工作中，我们考虑如何自动生成真实数据，并关注**空间关系**、**度量空间距离**的问题。
{% asset_img v_2.png "数据合成管道：(a)使用CLIP过滤嘈杂的互联网图像并仅保留场景级照片。(b)在互联网规模的图像上应用预先训练的专家模型，以便获得以对象为中心的分割、深度和标题。(c)将2D 图像提升为3D点云，可以通过形状分析规则对其进行解析以提取有用的属性。(d)通过使用CLIP相似度得分对对象标题进行聚类来避免提出模棱两可的问题。(e)从对象标题和提取的属性中合成了数百万个空间问题和答案" %}

**语义过滤**：虽然互联网规模的图像描述数据集已广泛应用于`VLM`训练，但这些数据集中的许多图像不适合合成空间推理，因为它们要么由单个对象组成，要么没有场景背景（例如购物网站上的产品图片或计算机屏幕的屏幕截图）。因此，作为数据合成流程的第一步，基于`CLIP`的**开放词汇分类模型**对所有图像进行分类，并排除那些不适合的图像。

**从2D图像中提取以对象为中心的上下文**：为了从`2D`图像中提取以对象为中心的空间上下文，这里使用现成的专家模型，包括**区域提议**、**区域描述**和**语义分割**模块来提取以对象为中心的信息。通过这一步，我们获得了由**像素簇**组成的以对象为中心的实体以及**开放词汇图像描述**。

**将2D上下文提升到3D上下文**：使用对象检测和边界框定位生成的传统空间`VQA`数据集仅限于`2D`图像平面（缺乏深度或高度上下文）和像素级推理（缺乏公制尺度大小和距离上下文）。执行深度估计以将`2D`像素提升到公制尺度`3D`点云。进一步将点云的相机坐标转化为地理坐标系，这是通过水平面（例如“地板”、“桌面”）分割和帧传输完成的。

**歧义解决**：有时，一幅图像中会出现多个属于相似类别的物体，从而导致其标题标签出现歧义。例如，同一个标题标签“蛋糕”可能指代同一幅图像中的多个不同蛋糕。因此，在询问这些物体之前，我们需要确保参考表达没有歧义。这里做出了两个关键的设计选择，这些选择已通过实证验证，可有效应对这一挑战：
- 我们特意选择避免使用常见的物体检测器，因为它们往往会产生固定且粗略的类别，例如“蛋糕”，并采用`FlexCap`，这是一种用户可配置的以对象为中心的字幕方法。实际上，对于每个对象，我们可以抽取一个长度在`1~6`个字之间的可变随机字幕。因此，我们的对象注释是细粒度的，例如“形状像房子的蛋糕”和“塑料容器中的纸杯蛋糕”。
- 我们设计了一种面向语义的后处理算法，通过增强或拒绝对象字幕来进一步消除歧义。

大规模空间推理`VQA`数据集：研究的重点是通过使用合成数据进行预训练，将“直接”的**空间推理能力**注入`VLM`。因此，合成了图像中最多涉及两个对象（表示为“`A`”和“`B`”）的空间推理`QA`对，并考虑以下两类问题。
- **定性问题**：要求判断某些空间关系的问题。例如“给定两个物体A和B，哪个更靠左？”，“物体A比物体B更高吗？”和“A和B中，哪个宽度更大？”。
- **定量问题**：要求更细粒度的答案，包括数字和单位。例如“物体A比物体B靠左多少？”，“物体A距离B有多远？”，“找出A相对于相机位于B后面的距离”。与上述示例类似，可以使用主问题模板合成此类问题，并且在消除歧义后可以使用对象标题填充对象名称条目。此类属性允许我们进行基于模板的生成，这是指令调整工作中常用的方法。

{% asset_img v_3.png "来自合成数据集的示例数据条目" %}

#### 结论

通过构建基于互联网规模真实世界图像的`3D`空间推理`VQA`数据自动生成框架来解决这一问题。在训练`VLM`的方法中消除了不同的设计选择，例如使用大量噪声数据进行训练和解冻`ViT`。`SpatialVLM`可以处理需要空间推理组件的更复杂的**思维链推理**。`SpatialVLM`也被证明可用于机器人任务，`3D`空间感知`VLM`可以用作机器人任务的**奖励注释器**。