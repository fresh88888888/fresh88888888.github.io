---
title: SpatialVLM-探析(视觉语言模型 & 空间推理)
date: 2024-08-05 18:44:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

[`SpatialVLM`](https://arxiv.org/pdf/2401.12168)是由`Google DeepMind`开发的一种**视觉语言模型**(`Vision-Language Model, VLM`)，其主要目标是增强`VLMs`的**空间推理能力**(`Spatial Reasoning Capabilities`)，使其能够理解和推理三维空间中的物体关系。
<!-- more -->

主要特点：
- **增强的空间推理能力**：`SpatialVLM`通过训练在大规模的空间推理数据集上，显著提升了模型在定性和定量空间推理任务中的表现。这些任务包括回答关于物体位置、距离和尺寸的问题。
- **数据生成和训练**：为了训练`SpatialVLM`，研究团队开发了一种自动生成`3D`空间问答(`VQA`)数据的框架。该框架能够将二维图像提升为三维点云，并生成包含空间注释的大规模数据集。这些数据集包含了多达 `20`亿个`VQA`示例，覆盖了`1000`万张真实世界的图像。
- **多模态学习**：`SpatialVLM`结合了视觉和语言数据，通过多模态学习方法，将图像和文本数据映射到一个联合的嵌入空间中。这种方法使模型能够同时处理和理解图像和文本信息，从而在生成和理解内容时利用这两种信息源。

关键技术：
- **对比学习**：`SpatialVLM`使用**对比学习**的方法，通过最大化匹配图像-文本对的相似性和最小化不匹配对的相似性来训练模型。这种方法提高了模型在多模态任务中的表现。
- **嵌入投影**：`SpatialVLM`模型使用**嵌入投影**模块对齐图像和文本的嵌入表示，确保两者在同一个**语义空间**中具有可比性，从而能够进行有效的**空间推理**。
- **链式推理**：`SpatialVLM`能够执行复杂的**链式推理**任务，意味着它可以通过与强大的大语言模型(`LLM`)结合，解决**多步骤的空间推理**问题。这种能力使其在机器人学和其他需要复杂空间分析的领域中具有广泛的应用前景。

`SpatialVLM`通过结合**视觉**和**语言**处理能力，显著提升了模型的**空间推理能力**。它利用大规模的**空间推理数据集**和**多模态学习**方法，能够在多种任务中表现出色，如视觉问答和机器人学。
{% asset_img v_1.png "空间AI大脑：设想一般空间AI系统的表示和处理图形结构如何映射到图形处理器。我们确定的关键元素是实时处理循环、基于图形的地图存储和与传感器和输出执行器接口的块。" %}

传统`AI`通常擅长图像识别或文本分析等任务，专注于图像或文档中的“内容”，而`AI`中的**空间智能**则不仅限于**识别对象**。它使`AI`能够了解对象的“位置”和“方式”——它们的位置、大小、距离以及它们在`3D`空间中如何相互作用。

#### SpatialVLM

`SpatialVLM`是采用数据合与预训练机制实现的模型，目的是为了增强`VLM`的**空间推理能力**。事实证明在合成数据上训练的`VLM`表现出强大的**空间推理能力**，并且可以从`2D`输入图像生成度量距离估值，从而解决当前`VLM`（如`GPT-4V`）遗留的缺陷。理解和推理**空间关系**是**视觉问答**(`VQA`)和**机器人技术**的基本能力。虽然**视觉语言模型**(`VLM`)在某些`VQA`基准测试中表现出色，但它们仍然缺乏`3D`空间的推理能力，例如**识别物理对象的定量关系**，如距离或大小差异。我们假设`VLM`有限的空间推理能力是由于训练数据中缺乏`3D`空间的知识，目标是通过使用互联网规模的空间推理数据训练`VLM`解决此问题。为此，这里提出了一个解决方法。首先开发了一个自动`3D`空间`VQA`数据生成框架，该框架可在`1000`万张真实世界图像上扩`展20`亿个`VQA`示例。并研究了训练方法中的各种参数，包括数据质量、训练管道和`VLM`架构。通过在互联网规模`3D`空间推理数据集上训练`VLM`，从而显著增强了其在**定性**和**定量**空间`VQA`方面的能力。最后，由于其**定量**估计的能力，从而使其解锁了思维链空间推理和机器人技术中的下游应用。

近年来，**视觉语言模型**(`VLM`)在各种任务中取得了重大进展，包括**图像字幕**、**视觉问答**(`VQA`)、**具身规划**、**动作识别**等。虽然`VLM`是适用于各种任务的强大通用模型，但大多数最先进的`VLM`仍然难以进行空间推理，即需要理解物体在`3D`空间中的位置或空间关系的任务。**空间推理能力**本身很有用，但也适用于机器人或`AR`等下游应用。例如，具有**空间推理能力**的`VLM`可以用作更好的通用**奖励注释器**和**成功检测器**。对`VLM`基础模型的探索往往受到人类能力的启发。人类通过具体体验和进化发展，拥有天生的**空间推理能力**。我们毫不费力地确定**空间关系**，例如物体相对于彼此的定位或估算距离和大小，而无需复杂的思路或心理计算。这种直接空间推理任务的自然能力与`VLM`当前的局限性形成了鲜明对比，从而阻止了它们完成需要**多步空间推理**的现实世界任务。因此，我们假设，当前`VLM`的空间推理能力有限并不是由于其架构的根本限制，而是由于训练此类模型的规模上可用的通用数据集的限制。例如，许多`VLM`是在互联网规模的数据集上进行训练的，这些数据集以"图像-标题对" 为特征，其中只包含有限的空间信息（难以获得丰富的空间信息数据或用于`3D`感知查询的高质量人工标注）。

**自动数据生成**和**增强技术**是解决数据限制问题的方法之一。然而，大多数以前的数据生成工作都侧重于渲染具有地面真实语义注释的照片级图像，但忽略了对象和`3D`关系的丰富性。相比之下，直接从现实世界数据中提取空间信息，以捕捉真实`3D`世界的多样性和复杂性。现成的视觉模型的最新进展可以从`2D`图像自动生成丰富的`3D`空间注释。为此，这里提出了一个名为`SpatialVLM`的系统，该系统能够生成和训练`VLM`，以增强其**空间推理能力**。具体来说，通过结合1.开放词汇检测、2.度量深度估计、3.语义分割、4.以对象为中心图像描述模型，我们可以大规模地对现实世界数据进行密集注释。`SpatialVLM`将视觉模型生成的数据转换为一种格式，可用于在图像描述、`VQA`和空间推理数据混合训练`VLM`。通过实验，发现经过训练的`VLM`展现出理想的能力。首先，它回答**定性空间**问题的能力大大增强。其次，尽管训练的数据嘈杂，它仍能可靠地进行**定量估算**。这种能力不仅让它掌握了关于物体大小的常识性知识，而且使它成为重新排列任务的开放**词汇奖励注释器**。最后，发现这种**空间视觉语言模型**得益于其自然语言方面的能力，当与强大的大语言模型相结合时，可以执行**空间思维链**来解决复杂的**空间推理任务**。

- **学习空间推理**(`Learning Spatial Reasoning`)：空间距离估计是比较常见的任务之一，例如`SLAM`或**深度估计**。在将这些空间概念应用于推理时，先前的研究通常侧重于**显式空间场景记忆**或**空间场景图**。场景图允许基于编码的空间结构进行**可解释、结构化、统计关系学习**。要回答`VQA`格式的空间问题，它们必须将其作为场景图上的**寻路问题**来处理。另一方面，`VLM`是在来自视觉语言数据集的大量松散结构信息上进行预训练的。与场景图不同，空间理解是**隐式编码**的。我们可以通过辅助任务将深度和3D结构注入进权重中，从而捕获关系信息。
- **基于视觉语言模型**：大型语言模型(`LLM`)在互联网规模的数据上进行训练，使其成为有效的**常识推理器**。然而，`LLM`（以及扩展的`VLM`）可能缺乏在**社交推理**、**物理推理**、**具身任务**和**空间推理任务**中表现良好的必要基础。虽然具有交互式的语言模型基础，但引入大型视觉模型（如`Flamingo`、`PaLI`或`PaLM-E`）已使性能飞跃。这些基于视觉的模型已用于多个下游任务，例如机器人成功检测、动作预测和奖励预测。在这项工作中，通过在生成的`VQA`数据集上微调`VLM`来解决**空间推理**问题。
- **视觉语言数据集中的空间信息**：许多先前的研究都集中在对`VLM`进行基准测试。其他人则专注于细粒度场景理解，例如语义分割、对象检测或对象识别。该领域的真实数据可能受到人类标记者生成的数量限制，而合成数据的表达能力本质上是有边界的。在这项工作中，我们考虑如何自动生成真实数据，并关注**空间关系**、**度量空间距离**的问题。
{% asset_img v_2.png "数据合成管道：(a)使用CLIP过滤嘈杂的互联网图像并仅保留场景级照片。(b)在互联网规模的图像上应用预先训练的专家模型，以便获得以对象为中心的分割、深度和标题。(c)将2D 图像提升为3D点云，可以通过形状分析规则对其进行解析以提取有用的属性。(d)通过使用CLIP相似度得分对对象标题进行聚类来避免提出模棱两可的问题。(e)从对象标题和提取的属性中合成了数百万个空间问题和答案" %}
