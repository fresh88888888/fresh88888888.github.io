---
title: 直接偏好优化(DPO)：Bradley-Terry模型 & 对数概率（深度学习）
date: 2024-06-26 18:20:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

直接偏好优化(`Direct Preference Optimization`,`DPO`)是一种用于微调大型语言模型(`LLMs`)以符合人类偏好的新方法。`DPO`旨在通过人类偏好数据来优化语言模型的输出,使其更符合人类期望,而无需使用强化学习或显式的奖励模型。`DPO`利用了奖励函数和最优策略之间的映射关系；它直接在策略(语言模型)上优化,而不是先学习奖励模型再优化策略；`DPO`将问题转化为一个简单的分类任务,在人类偏好数据上进行训练。
<!-- more -->

在实践中为单词序列分配概率，例如：`“Shanghai is a city in”`，如下图所示：
{% asset_img d_1.png %}

这里简化为：一个`token`是一个单词，在大多数语言模型中实际上并不是这样的，但用于解释它很有帮助。给定特定提示的情况下，下一个`token`是`china`、`Beijing`、`Cat`、`Pizza`的概率是`85%、10%、2.5%、...`，语言模型给了我们这些概率。我们如何使用语言模型来生成文本呢？