---
title: 直接偏好优化(DPO)：Bradley-Terry模型 & 对数概率（深度学习）
date: 2024-06-26 18:20:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

直接偏好优化(`Direct Preference Optimization`,`DPO`)是一种用于微调大型语言模型(`LLMs`)以符合人类偏好的新方法。`DPO`旨在通过人类偏好数据来优化语言模型的输出,使其更符合人类期望,而无需使用强化学习或显式的奖励模型。`DPO`利用了奖励函数和最优策略之间的映射关系；它直接在策略(语言模型)上优化,而不是先学习奖励模型再优化策略；`DPO`将问题转化为一个简单的分类任务,在人类偏好数据上进行训练。
<!-- more -->

在实践中为单词序列分配概率，例如：`“Shanghai is a city in”`，如下图所示：
{% asset_img d_1.png %}

这里简化为：一个`token`是一个单词，在大多数语言模型中实际上并不是这样的，但用于解释它很有帮助。给定特定提示的情况下，下一个`token`是`china`、`Beijing`、`Cat`、`Pizza`的概率是`85%、10%、2.5%、...`，语言模型给了我们这些概率。我们如何使用语言模型来生成文本呢？首先我们会给出一个提示，如：`“Where is Shanghai?”`，我们把它提交给语言模型，语言模型会给出下一个单词或`token`的概率列表，假设我们选择概率分数最高的`token`，假设它是`“Shanghai”`，然后我们将其(`token`)放回到提示中：`“Where is Shanghai? Shanghai”`，再次交给语言模型，然后语言模型再次给出一个单词或`token`的概率列表，我们选择相关性最重要的一个。然后将其放回到提示后边，然后再次提交给语言模型等，直到完成了句子标记的结尾。这种情况下，这是一个特殊的`token`。
{% asset_img d_2.png %}

#### AI对齐

预训练模型不会教授语言模型：回复时要有礼貌、不使用任何攻击性语言、不使用任何种族主义式的表达等。因为语言模型只会根据其拥有的数据去执行，如果你提供了互联网上的数据，实际上语言模型会表现得非常糟糕。所以我们需要将语言模型与期望的动作保持一致。不希望语言模型使用任何攻击性的语言。这也是`AI`对齐的目标。

#### 强化学习

强化学习(`Reinforcement Learning, RL`)是机器学习的一个重要分支,主要关注如何让智能体(`agent`)在与环境的交互中学习最优策略。智能体通过试错的方式,在环境中采取行动,获得奖励或惩罚,从而学习如何最大化长期累积奖励。举个简单的例子：
|example_1|example_2|
|:---|:---|
|Agent：猫|Agent：语言模型|
|状态：猫在网格中的位置`(x,y)`|状态：提示（输入的`tokens`）|
|行为：在每个位置，猫可以移动到4个方向连接的单元格之一，如果移动无效，则单元格将不会移动并保持在同一位置。每次猫移动时，都会产生新的状态和奖励。|行为：哪个 token 被选为下一个 token|
|奖励模式：<br> 1.移至另一个空单元格将导致奖励`0`。<br> 2.移向扫帚将导致奖励`-1`。<br> 3.移向浴缸将导致奖励`-10`，猫会晕倒（剧集结束）。猫会再次重生在初始位置。<br> 4.移向肉将导致奖励`+100`|奖励模式：语言模型应该因产生“好的反应”而获得奖励，而不应该因产生“坏的反应”而获得任何奖励。|
|策略：策略规定代理如何在其所处的状态下选择要执行的操作：{% mathjax %}a_t\sim \pi(\cdot | s_t){% endmathjax %}|策略：对于语言模型来说，策略就是语言模型本身！因为它根据代理的当前状态模拟动作空间的概率：{% mathjax %}a_t\sim \pi(\cdot | s_t){% endmathjax %}|
|{% asset_img d_3.png %}|{% asset_img d_4.png %}|

`RL`中的目标是选择一种策略，当代理按照该策略采取行动时，该策略可以最大化预期回报。