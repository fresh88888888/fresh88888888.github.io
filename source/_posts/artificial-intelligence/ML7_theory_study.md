---
title: 机器学习(ML)(七) — 探析
date: 2024-09-28 15:24:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

什么是**决策树**？学习算法输出的模型看起来像一棵树。这里有一个新的测试示例，有一只猫，耳朵形状尖尖的，脸形圆润，有胡须。该模型学习此示例并做出**分类决策**，从树的最顶端节点开始，这称为树的**根节点**，然后查看写在里面的特征，即耳朵形状。根据此示例的耳朵形状的值向左或向右走。耳朵形状的值是尖的，所以将沿着树的左边分支向下走，最后到达椭圆形节点。然后查看脸部形状，脸部是圆形的，所以将沿着这里的箭头向下走。算法会推断这是一只猫。树中最上面的节点称为**根节点**。所有这些节点都称为**决策节点**。它们之所以是**决策节点**，是因为它们会查看特定特征，然后根据特征的值，来决策是沿着树向左走还是向右走。最后，这些底部的节点称为**叶节点**。它们会做出**预测**。
<!-- more -->
{% asset_img ml_1.png %}

给定训练集构建**决策树**有几个步骤。给定一个包含 10 个猫和狗示例的训练集，就像您在上一个视频中看到的那样。**决策树**学习的第一步是决定在**根节点**使用什么特征。这是**决策树**最顶端的第一个节点。假设选择耳朵形状作为根节点的特征。并查看所有训练示例，根据耳朵形状特征的值对它们进行拆分。具体来说，选出五个尖耳朵的例子并将它们移到左边。然后选出五个松软耳朵的例子并将它们移到右边。第二步是只关注**决策树**的左侧部分，根据指定（根据一种算法）的特征进行拆分。假设使用脸型特征。接下来要做的是根据这五个示例的脸型值，将它们拆分成两个子集。然后从这五个示例中取出四个脸型为圆脸的示例并将它们移到左侧。将一个脸型不圆的示例移到右侧。最后，观察到到这四个示例都是猫。这时不再进一步拆分，而是创建一个叶节点，该节点预测猫的分布。在决策树左侧分支的左侧部分完成此操作后，在**决策树**的右侧部分重复类似的过程。关注这五个示例，其中包含一只猫和四只狗。选择某个特征来进一步拆分这五个示例，如果选择了胡须特征，那么根据胡须存在或不存在拆分这五个示例，如下图所示：
{% asset_img ml_2.png %}

左侧的示例只有一个是猫。创建叶节点，在左侧进行猫预测，在右侧进行狗预测。这是一个构建决策树的过程。在此过程中，需要在算法的各个步骤中做出几个关键决策。第一个关键决策是**选择在每个节点上使用哪些特征进行拆分**？在根节点以及决策树的左分支和右分支上需要确定该节点上是否有一些由猫和狗混合而成的示例。目的是尽可能接近所有猫或所有狗的子集。例如，如果我们有一个特征表示这种动物是否有猫。根节点可以根据这个特征进行拆分，这样左分支中会有五只猫，而右分支中会有五只猫。这些数据的左子集和右子集都是完全纯净的，意味着在左子分支和右子分支中只有一个类别，要么只有猫，要么不是只有猫，那么这个特征将是一个很好的**使用特征**。但是对于实际拥有的特征，如果根据年份形状进行划分，这会导致左侧五个例子中有四个是猫，而右侧五个例子中有一个是猫，或者根据脸部形状进行划分，这会导致左侧七个例子中有四个是猫，而右侧三个例子中有一个是猫，这会导致左侧四个例子中有三个是猫，而右侧六个例子中有两个不是猫。**决策树**学习算法必须在耳朵形状、脸部形状和胡须之间选择。这些特征中的哪一个会导致左侧和右侧子分支上的标签纯度最高？如果可以得到高度纯净的样本子集，那么就可以预测猫或预测不是猫，并且大部分都是正确的。学习决策树时，我们必须做出的第一个决定是如何选择在每个节点上分割哪个特征。如下图所示：
{% asset_img ml_3.png %}

构建**决策树**时需要做出的第二个关键决策是**何时停止分割**。在**决策树**中，节点的深度从最顶端的根节点到该特定节点所需的跳数。因此，根节点经过零跳即可到达自身，深度为`0`。它下面的注释深度为`1`，再往下注释深度为`2`。如果决定了**决策树**的最大深度为`2`，那么将不拆分此级别以下的任何节点，以便树永远不会达到深度`3`。其次，通过保持树较小，它不太容易**过度拟合**。决定停止拆分的另一个指标是纯度值是否低于某个阈值。同样，这既是为了保持树较小，也是为了降低过度拟合的风险。最后，如果节点的示例数量低于某个阈值，则会停止拆分。例如，如果我们在根节点上根据脸部形状特征进行拆分，那么右分支将只有三个训练示例，其中一只猫和两只狗，如果不再拆分仅包含三个示例的示例集，而不是将其拆分成更小的子集，并创建一个决策节点，则会预测不是猫。

#### 决策树

如何测量一组示例的**纯度**(`quantify`)？如果这些示例都是同一类的猫，那么它们就非常纯净；如果不是猫，那它们也很纯净；但如果介于两者之间，那么如何量化示例集的**纯度**？让我们来看看**熵**的定义，它是衡量一组数据不**纯度**的指标。给定一组六个示例，例如三只猫和三只狗，我们将{% mathjax %}p_1{% endmathjax %}定义为猫的示例比例，即标签为`1`的示例比例。此示例中的{% mathjax %}p_1{% endmathjax %}等于`3/6`。这里使用**熵**函数来测量一组示例的不纯度。**熵函数**通常用大写字母{% mathjax %}H{% endmathjax %}表示，其中{% mathjax %}p_1{% endmathjax %}是数值，函数看起来就像条曲线，横轴是{% mathjax %}p_1{% endmathjax %}，即样本中猫的比例，纵轴是**熵值**。在这个例子中，{% mathjax %}p_1 = 0.5{% endmathjax %}，{% mathjax %}p_1{% endmathjax %}的熵值{% mathjax %}H(p_1) = 1{% endmathjax %}。当样本集是 `50:50`时，这条曲线最高，它的熵为{% mathjax %}H(p_1) = 1{% endmathjax %}，如果样本集要么全是猫，要么全不是猫，那么熵{% mathjax %}H(p_1) = 0{% endmathjax %}。这是一组不同的示例，其中有五只猫和一只狗，因此{% mathjax %}p_1{% endmathjax %}是正样本的比例，标记为`1`的比例是`5/6`，{% mathjax %}p_1 \approx 0.83{% endmathjax %}。{% mathjax %}p_1{% endmathjax %}的熵约为{% mathjax %}H(p_1) \approx 0.65{% endmathjax %}。这里保留两位有效数字。另一个示例。这个包含六张图片的样本全是猫，因此{% mathjax %}p_1 = \frac{6}{6} = 1{% endmathjax %}，所有六只都是猫，而{% mathjax %}p_1{% endmathjax %}的熵值{% mathjax %}H(p_1) = 0{% endmathjax %}。随着猫和狗的比例从`50:50`变为全是猫，纯度会增加。再看几个例子，这是另一个包含两只猫和四只狗的样本，因此这里的{% mathjax %}p_1 = \frac{2}{6}{% endmathjax %}，则{% mathjax %}p_1{% endmathjax %}的熵值为{% mathjax %}H(p_1) \approx 0.92{% endmathjax %}。这实际上相当不纯，因为它更接近`50:50`的混合。最后一个例子，如果我们有一组六只狗，那么{% mathjax %}p_1 = 0{% endmathjax %}，{% mathjax %}p_1{% endmathjax %}的熵值为{% mathjax %}H(p_1) = 0{% endmathjax %}，所以杂质为零。
{% asset_img ml_4.png %}

现在，让我们看看熵函数{% mathjax %}H(p_1){% endmathjax %}的方程。回想一下，{% mathjax %}p_1{% endmathjax %}是猫的样本比例，如果有一个`2/3`是猫的样本，那么该样本必须有`1/3`不是猫的。这里将{% mathjax %}p_0{% endmathjax %}定义为非猫的样本比例，则{% mathjax %} p_0 = 1 - p_1{% endmathjax %}。**熵函数**被定义为{% mathjax %}H(p_1) = -\log_2 (p_1) - p_0\log_2 (p_0) = -\log_2 (p_1) - (1 - p_1)\log_2 (1 - p_1){% endmathjax %}，按照惯例，在计算熵值时，取以`2`为底而不是以`e`为底的对数。如果要在计算机中绘制此函数，会发现它恰好就是左侧的这个函数。我们取{% mathjax %}\log_2{% endmathjax %}只是为了使该曲线的峰值等于`1`，如果我们取{% mathjax %}\log_e{% endmathjax %}，那么这只是垂直缩放该函数，它仍然有效，但数字变得有点难以解释，因为函数的峰值不再是整数。计算此函数时需要注意一点，如果{% mathjax %}p_1{% endmathjax %}或{% mathjax %}p_0{% endmathjax %}等于`0`，表达式将看起来像{% mathjax %}0\log (0){% endmathjax %}，而 {% mathjax %}\log (0){% endmathjax %}在技术上是未定义的，它实际上是负无穷大。但按照计算熵的惯例，我们将视为{% mathjax %}0\log (0) = 0{% endmathjax %}，这样就可以正确地计算出熵为`0`或`1`。**熵函数**是一组数据的不纯度的度量。它从`0`开始，上升到`1`，然后又下降到`0`。

在构建**决策树**时，在节点上决策哪个特征进行分割基于最小化熵值来选择。最小化熵或减少杂质，或最大化纯度。在**决策树**学习中，**熵的减少称为信息增益**。如果在根节点使用耳朵形状特征进行分割，则左边有五个示例，右边有五个示例。在左边，有五只猫中的四只，因此{% mathjax %}p_1 = \frac{4}{5} = 0.8{% endmathjax %}。在右边，五只猫中有一只，因此{% mathjax %}p_1 = \frac{1}{5} = 0.2{% endmathjax %}。如果将熵公式应用于左侧数据子集和右侧数据子集，发现左侧的不纯度为{% mathjax %}H(p_1) \approx 0.72{% endmathjax %}；而右侧的熵也为{% mathjax %}H(p_1) \approx 0.72{% endmathjax %}。另一个选择是按脸部形状特征进行拆分。如在左侧七个示例中的四个将是猫，因此{% mathjax %}p_1 = \frac{4}{7}{% endmathjax %}；在右侧，有`1/3`是猫，因此右侧的{% mathjax %}p_1 = \frac{1}{3}{% endmathjax %}。`4/7`的熵和`1/3`的熵分别为{% mathjax %}0.99{% endmathjax %}和{% mathjax %}0.92{% endmathjax %}。因此，左节点和右节点的杂质程度要高得多，左节点杂质程度分别为`0.99`和`0.92`，而右节点的杂质程度为`0.72`和`0.72`。
{% asset_img ml_5.png %}

事实证明，与其查看这些熵值并进行比较，不如对它们进行**加权平均值**。熵作为杂质的度量，如果数据集非常大且不纯，相比于只有几个示例和一个非常不纯的树分支，熵会更差。关键的决策是在根节点使用的这三种特征选择中，我们要使用哪一个？与每个分割相关的是两个数字，即左子分支的熵和右子分支的熵。为了从中挑选将这两个数字合并为一个数字。将这两个数字合并的方法是取**加权平均值**。因为在左子分支或右子分支中拥有低熵的重要性还取决于有多少示例进入左子分支或右子分支。如果左子分支中有很多示例，那么确保左子分支的熵值较低似乎更为重要。在这个例子中，`10`个例子中有`5`个进入了左子分支，计算**加权平均值**，即{% mathjax %}\frac{5}{10}\times H(0.8) + \frac{5}{10}\times H(0.2){% endmathjax %}。现在，左子分支已经收到了`10`个例子中的`7`个。即{% mathjax %}\frac{7}{10}\times H(0.57) + \frac{3}{10}\times H(0.33){% endmathjax %}。最后，在右侧，即{% mathjax %}\frac{4}{10}\times H(0.75) + \frac{6}{10}\times H(0.33){% endmathjax %}。选择分割的方式是计算这三个数字，然后取最小的一个，因为这样得到的左、右子分支的**平均加权熵**最小。

我们从根节点`10`个示例开始，根节点中有五只猫和五只狗，因此在根节点，{% mathjax %}p_1 = \frac{5}{10} = 0.5{% endmathjax %}。根节点的熵{% mathjax %}H(0.5) = 1{% endmathjax %}。这是最大杂质，因为它有五只猫和五只狗。现在选择分割公式不是左右子分支的加权熵，而是根节点的熵，也就是`0.5`的熵，然后减去这个公式：{% mathjax %}H(0.5) - (\frac{5}{10}\times H(0.8) + \frac{5}{10}\times H(0.2)) = 0.28{% endmathjax %}，对于胡须，减去这个公式：{% mathjax %}H(0.5) - (\frac{4}{10}\times H(0.75) + \frac{6}{10}\times H(0.33)) = 0.12{% endmathjax %}。刚刚计算的这些值是`0.28、0.03`和`0.12`，被称为**信息增益**(`information gain`)，它衡量的是分裂后树中熵的减少。因为熵最初在根节点是`1`，通过分裂，最终会得到一个较低的熵值，这两个值之间的差就是熵的减少，在耳朵形状分裂的情况下，这个差值为`0.28`。为什么要费心计算熵的减少，而不是只计算左、右子分支的熵？事实证明，决定何时不再进一步分裂的停止标准之一是熵的减少是否太小。计算**信息增益**公式。使用根据耳朵形状特征进行拆分的示例，将{% mathjax %}p_1^{\text{left}}{% endmathjax %}定义为左子树中具有正标签（即猫）的示例的比例。{% mathjax %}p_1^{\text{left}} = \frac{4}{5}{% endmathjax %}。另外，{% mathjax %}w^{\text{left}}{% endmathjax %}为进入左子分支的根节点的所有示例的比例，{% mathjax %}w^{\text{left}} = \frac{5}{10}{% endmathjax %}。{% mathjax %}p_1^{\text{right}} =\frac{1}{5}{% endmathjax %}为右分支中的所有示例的比例。{% mathjax %}w^{\text{right}} = \frac{5}{10}{% endmathjax %}是进入右分支的样本比例。{% mathjax %}p_1^{\text{root}}{% endmathjax %}定义为根节点中正样本的比例。在本例中，该比例为{% mathjax %}p_1^{\text{root}} = \frac{5}{10} = 0.5{% endmathjax %}。**信息增益**为{% mathjax %}H(0.5) - (w^{\text{left}}H(p_1^{\text{left}}) + w^{\text{right}}H(p_1^{\text{right}})){% endmathjax %}的熵，那么根节点的熵是：{% mathjax %}H(p_1^{\text{root}}) - (w^{\text{left}}H(p_1^{\text{left}}) + w^{\text{right}}H(p_1^{\text{right}})){% endmathjax %}计算值？。根据这个熵的定义，您可以计算与选择节点中特定特征进行分割相关的**信息增益**。选择分割，然后选择一个**最高信息增益**的结果。从而提高了**决策树**的左侧和右侧子分支上获得的数据子集的纯度。