---
title: 机器学习(ML)(七) — 探析
date: 2024-09-28 15:24:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

什么是**决策树**？学习算法输出的模型看起来像一棵树。这里有一个新的测试示例，有一只猫，耳朵形状尖尖的，脸形圆润，有胡须。该模型学习此示例并做出**分类决策**，从树的最顶端节点开始，这称为树的**根节点**，然后查看写在里面的特征，即耳朵形状。根据此示例的耳朵形状的值向左或向右走。耳朵形状的值是尖的，所以将沿着树的左边分支向下走，最后到达椭圆形节点。然后查看脸部形状，脸部是圆形的，所以将沿着这里的箭头向下走。算法会推断这是一只猫。树中最上面的节点称为**根节点**。所有这些节点都称为**决策节点**。它们之所以是**决策节点**，是因为它们会查看特定特征，然后根据特征的值，来决策是沿着树向左走还是向右走。最后，这些底部的节点称为**叶节点**。它们会做出**预测**。
<!-- more -->
{% asset_img ml_1.png %}

给定训练集构建**决策树**有几个步骤。给定一个包含 10 个猫和狗示例的训练集，就像您在上一个视频中看到的那样。**决策树**学习的第一步是决定在**根节点**使用什么特征。这是**决策树**最顶端的第一个节点。假设选择耳朵形状作为根节点的特征。并查看所有训练示例，根据耳朵形状特征的值对它们进行拆分。具体来说，选出五个尖耳朵的例子并将它们移到左边。然后选出五个松软耳朵的例子并将它们移到右边。第二步是只关注**决策树**的左侧部分，根据指定（根据一种算法）的特征进行拆分。假设使用脸型特征。接下来要做的是根据这五个示例的脸型值，将它们拆分成两个子集。然后从这五个示例中取出四个脸型为圆脸的示例并将它们移到左侧。将一个脸型不圆的示例移到右侧。最后，观察到到这四个示例都是猫。这时不再进一步拆分，而是创建一个叶节点，该节点预测猫的分布。在决策树左侧分支的左侧部分完成此操作后，在**决策树**的右侧部分重复类似的过程。关注这五个示例，其中包含一只猫和四只狗。选择某个特征来进一步拆分这五个示例，如果选择了胡须特征，那么根据胡须存在或不存在拆分这五个示例，如下图所示：
{% asset_img ml_2.png %}

左侧的示例只有一个是猫。创建叶节点，在左侧进行猫预测，在右侧进行狗预测。这是一个构建决策树的过程。在此过程中，需要在算法的各个步骤中做出几个关键决策。第一个关键决策是**选择在每个节点上使用哪些特征进行拆分**？在根节点以及决策树的左分支和右分支上需要确定该节点上是否有一些由猫和狗混合而成的示例。目的是尽可能接近所有猫或所有狗的子集。例如，如果我们有一个特征表示这种动物是否有猫。根节点可以根据这个特征进行拆分，这样左分支中会有五只猫，而右分支中会有五只猫。这些数据的左子集和右子集都是完全纯净的，意味着在左子分支和右子分支中只有一个类别，要么只有猫，要么不是只有猫，那么这个特征将是一个很好的**使用特征**。但是对于实际拥有的特征，如果根据年份形状进行划分，这会导致左侧五个例子中有四个是猫，而右侧五个例子中有一个是猫，或者根据脸部形状进行划分，这会导致左侧七个例子中有四个是猫，而右侧三个例子中有一个是猫，这会导致左侧四个例子中有三个是猫，而右侧六个例子中有两个不是猫。**决策树**学习算法必须在耳朵形状、脸部形状和胡须之间选择。这些特征中的哪一个会导致左侧和右侧子分支上的标签纯度最高？如果可以得到高度纯净的样本子集，那么就可以预测猫或预测不是猫，并且大部分都是正确的。学习决策树时，我们必须做出的第一个决定是如何选择在每个节点上分割哪个特征。如下图所示：
{% asset_img ml_3.png %}

构建**决策树**时需要做出的第二个关键决策是**何时停止分割**。在**决策树**中，节点的深度从最顶端的根节点到该特定节点所需的跳数。因此，根节点经过零跳即可到达自身，深度为`0`。它下面的注释深度为`1`，再往下注释深度为`2`。如果决定了**决策树**的最大深度为`2`，那么将不拆分此级别以下的任何节点，以便树永远不会达到深度`3`。其次，通过保持树较小，它不太容易**过度拟合**。决定停止拆分的另一个指标是纯度值是否低于某个阈值。同样，这既是为了保持树较小，也是为了降低过度拟合的风险。最后，如果节点的示例数量低于某个阈值，则会停止拆分。例如，如果我们在根节点上根据脸部形状特征进行拆分，那么右分支将只有三个训练示例，其中一只猫和两只狗，如果不再拆分仅包含三个示例的示例集，而不是将其拆分成更小的子集，并创建一个决策节点，则会预测不是猫。

#### 决策树

如何测量一组示例的**纯度**(`quantify`)？如果这些示例都是同一类的猫，那么它们就非常纯净；如果不是猫，那它们也很纯净；但如果介于两者之间，那么如何量化示例集的**纯度**？让我们来看看**熵**的定义，它是衡量一组数据不**纯度**的指标。给定一组六个示例，例如三只猫和三只狗，我们将{% mathjax %}p_1{% endmathjax %}定义为猫的示例比例，即标签为`1`的示例比例。此示例中的{% mathjax %}p_1{% endmathjax %}等于`3/6`。这里使用**熵**函数来测量一组示例的不纯度。**熵函数**通常用大写字母{% mathjax %}H{% endmathjax %}表示，其中{% mathjax %}p_1{% endmathjax %}是数值，函数看起来就像条曲线，横轴是{% mathjax %}p_1{% endmathjax %}，即样本中猫的比例，纵轴是**熵值**。在这个例子中，{% mathjax %}p_1 = 0.5{% endmathjax %}，{% mathjax %}p_1{% endmathjax %}的熵值{% mathjax %}H(p_1) = 1{% endmathjax %}。当样本集是 `50:50`时，这条曲线最高，它的熵为{% mathjax %}H(p_1) = 1{% endmathjax %}，如果样本集要么全是猫，要么全不是猫，那么熵{% mathjax %}H(p_1) = 0{% endmathjax %}。这是一组不同的示例，其中有五只猫和一只狗，因此{% mathjax %}p_1{% endmathjax %}是正样本的比例，标记为`1`的比例是`5/6`，{% mathjax %}p_1 \approx 0.83{% endmathjax %}。{% mathjax %}p_1{% endmathjax %}的熵约为{% mathjax %}H(p_1) \approx 0.65{% endmathjax %}。这里保留两位有效数字。另一个示例。这个包含六张图片的样本全是猫，因此{% mathjax %}p_1 = \frac{6}{6} = 1{% endmathjax %}，所有六只都是猫，而{% mathjax %}p_1{% endmathjax %}的熵值{% mathjax %}H(p_1) = 0{% endmathjax %}。随着猫和狗的比例从`50:50`变为全是猫，纯度会增加。再看几个例子，这是另一个包含两只猫和四只狗的样本，因此这里的{% mathjax %}p_1 = \frac{2}{6}{% endmathjax %}，则{% mathjax %}p_1{% endmathjax %}的熵值为{% mathjax %}H(p_1) \approx 0.92{% endmathjax %}。这实际上相当不纯，因为它更接近`50:50`的混合。最后一个例子，如果我们有一组六只狗，那么{% mathjax %}p_1 = 0{% endmathjax %}，{% mathjax %}p_1{% endmathjax %}的熵值为{% mathjax %}H(p_1) = 0{% endmathjax %}，所以杂质为零。
{% asset_img ml_4.png %}

现在，让我们看看熵函数{% mathjax %}H(p_1){% endmathjax %}的方程。回想一下，{% mathjax %}p_1{% endmathjax %}是猫的样本比例，如果有一个`2/3`是猫的样本，那么该样本必须有`1/3`不是猫的。这里将{% mathjax %}p_0{% endmathjax %}定义为非猫的样本比例，则{% mathjax %} p_0 = 1 - p_1{% endmathjax %}。**熵函数**被定义为{% mathjax %}H(p_1) = -\log_2 (p_1) - p_0\log_2 (p_0) = -\log_2 (p_1) - (1 - p_1)\log_2 (1 - p_1){% endmathjax %}，按照惯例，在计算熵值时，取以`2`为底而不是以`e`为底的对数。如果要在计算机中绘制此函数，会发现它恰好就是左侧的这个函数。我们取{% mathjax %}\log_2{% endmathjax %}只是为了使该曲线的峰值等于`1`，如果我们取{% mathjax %}\log_e{% endmathjax %}，那么这只是垂直缩放该函数，它仍然有效，但数字变得有点难以解释，因为函数的峰值不再是整数。计算此函数时需要注意一点，如果{% mathjax %}p_1{% endmathjax %}或{% mathjax %}p_0{% endmathjax %}等于`0`，表达式将看起来像{% mathjax %}0\log (0){% endmathjax %}，而 {% mathjax %}\log (0){% endmathjax %}在技术上是未定义的，它实际上是负无穷大。但按照计算熵的惯例，我们将视为{% mathjax %}0\log (0) = 0{% endmathjax %}，这样就可以正确地计算出熵为`0`或`1`。**熵函数**是一组数据的不纯度的度量。它从`0`开始，上升到`1`，然后又下降到`0`。

在构建**决策树**时，在节点上决策哪个特征进行分割基于最小化熵值来选择。最小化熵或减少杂质，或最大化纯度。在**决策树**学习中，**熵的减少称为信息增益**。如果在根节点使用耳朵形状特征进行分割，则左边有五个示例，右边有五个示例。在左边，有五只猫中的四只，因此{% mathjax %}p_1 = \frac{4}{5} = 0.8{% endmathjax %}。在右边，五只猫中有一只，因此{% mathjax %}p_1 = \frac{1}{5} = 0.2{% endmathjax %}。如果将熵公式应用于左侧数据子集和右侧数据子集，发现左侧的不纯度为{% mathjax %}H(p_1) \approx 0.72{% endmathjax %}；而右侧的熵也为{% mathjax %}H(p_1) \approx 0.72{% endmathjax %}。另一个选择是按脸部形状特征进行拆分。如在左侧七个示例中的四个将是猫，因此{% mathjax %}p_1 = \frac{4}{7}{% endmathjax %}；在右侧，有`1/3`是猫，因此右侧的{% mathjax %}p_1 = \frac{1}{3}{% endmathjax %}。`4/7`的熵和`1/3`的熵分别为{% mathjax %}0.99{% endmathjax %}和{% mathjax %}0.92{% endmathjax %}。因此，左节点和右节点的杂质程度要高得多，左节点杂质程度分别为`0.99`和`0.92`，而右节点的杂质程度为`0.72`和`0.72`。
{% asset_img ml_5.png %}

事实证明，与其查看这些熵值并进行比较，不如对它们进行**加权平均值**。熵作为杂质的度量，如果数据集非常大且不纯，相比于只有几个示例和一个非常不纯的树分支，熵会更差。关键的决策是在根节点使用的这三种特征选择中，我们要使用哪一个？与每个分割相关的是两个数字，即左子分支的熵和右子分支的熵。为了从中挑选将这两个数字合并为一个数字。将这两个数字合并的方法是取**加权平均值**。因为在左子分支或右子分支中拥有低熵的重要性还取决于有多少示例进入左子分支或右子分支。如果左子分支中有很多示例，那么确保左子分支的熵值较低似乎更为重要。在这个例子中，`10`个例子中有`5`个进入了左子分支，计算**加权平均值**，即{% mathjax %}\frac{5}{10}\times H(0.8) + \frac{5}{10}\times H(0.2){% endmathjax %}。现在，左子分支已经收到了`10`个例子中的`7`个。即{% mathjax %}\frac{7}{10}\times H(0.57) + \frac{3}{10}\times H(0.33){% endmathjax %}。最后，在右侧，即{% mathjax %}\frac{4}{10}\times H(0.75) + \frac{6}{10}\times H(0.33){% endmathjax %}。选择分割的方式是计算这三个数字，然后取最小的一个，因为这样得到的左、右子分支的**平均加权熵**最小。

我们从根节点`10`个示例开始，根节点中有五只猫和五只狗，因此在根节点，{% mathjax %}p_1 = \frac{5}{10} = 0.5{% endmathjax %}。根节点的熵{% mathjax %}H(0.5) = 1{% endmathjax %}。这是最大杂质，因为它有五只猫和五只狗。现在选择分割公式不是左右子分支的加权熵，而是根节点的熵，也就是`0.5`的熵，然后减去这个公式：{% mathjax %}H(0.5) - (\frac{5}{10}\times H(0.8) + \frac{5}{10}\times H(0.2)) = 0.28{% endmathjax %}，对于胡须，减去这个公式：{% mathjax %}H(0.5) - (\frac{4}{10}\times H(0.75) + \frac{6}{10}\times H(0.33)) = 0.12{% endmathjax %}。刚刚计算的这些值是`0.28、0.03`和`0.12`，被称为**信息增益**(`information gain`)，它衡量的是分裂后树中熵的减少。因为熵最初在根节点是`1`，通过分裂，最终会得到一个较低的熵值，这两个值之间的差就是熵的减少，在耳朵形状分裂的情况下，这个差值为`0.28`。为什么要费心计算熵的减少，而不是只计算左、右子分支的熵？事实证明，决定何时不再进一步分裂的停止标准之一是熵的减少是否太小。计算**信息增益**公式。使用根据耳朵形状特征进行拆分的示例，将{% mathjax %}p_1^{\text{left}}{% endmathjax %}定义为左子树中具有正标签（即猫）的示例的比例。{% mathjax %}p_1^{\text{left}} = \frac{4}{5}{% endmathjax %}。另外，{% mathjax %}w^{\text{left}}{% endmathjax %}为进入左子分支的根节点的所有示例的比例，{% mathjax %}w^{\text{left}} = \frac{5}{10}{% endmathjax %}。{% mathjax %}p_1^{\text{right}} =\frac{1}{5}{% endmathjax %}为右分支中的所有示例的比例。{% mathjax %}w^{\text{right}} = \frac{5}{10}{% endmathjax %}是进入右分支的样本比例。{% mathjax %}p_1^{\text{root}}{% endmathjax %}定义为根节点中正样本的比例。在本例中，该比例为{% mathjax %}p_1^{\text{root}} = \frac{5}{10} = 0.5{% endmathjax %}。**信息增益**为{% mathjax %}H(0.5) - (w^{\text{left}}H(p_1^{\text{left}}) + w^{\text{right}}H(p_1^{\text{right}})){% endmathjax %}的熵，那么根节点的熵是：{% mathjax %}H(p_1^{\text{root}}) - (w^{\text{left}}H(p_1^{\text{left}}) + w^{\text{right}}H(p_1^{\text{right}})){% endmathjax %}计算值？。根据这个熵的定义，您可以计算与选择节点中特定特征进行分割相关的**信息增益**。选择分割，然后选择一个**最高信息增益**的结果。从而提高了**决策树**的左侧和右侧子分支上获得的数据子集的纯度。

**信息增益**可以决定如何选择一个特征来分割一个节点。从树根节点的所有训练示例开始，计算所有特征的**信息增益**，并选择提供最高**信息增益**的特征进行分割。选择此特征后，根据所选特征将数据集拆分为两个子集，并创建树的左分支和右分支，并将训练示例移动到左分支或右分支。在根节点进行分割，之后，在树的左分支、树的右分支等上重复分割过程。直到满足停止标准。停止标准是：当节点`100%`为单个子句时，某个节点的熵值为`0`，或者当进一步拆分节点超出设置的最大深度时，或者额外拆分的**信息增益**小于阈值时，或者当节点中的示例数量低于阈值时。不断重复拆分过程，直到满足选择的停止标准（可能是这些标准中的一个或多个）。如下图所示：

从根节点开始所有示例，并根据计算所有三个特征的**信息增益**，决定耳朵形状是最佳的拆分特征。基于此，创建左子分支和右子分支，并将具有尖耳朵和松软耳朵的数据子集移动到左子分支和右子分支。当前只关注左子分支，这里有五个示例。继续拆分，直到节点中的所有内容都属于一个类。查看此节点并查看它是否满足拆分标准。下一步是选择一个特征进行拆分。然后，计算每个特征的**信息增益**，计算胡须特征上的拆分**信息增益**，以及脸部形状特征上的拆分**信息增益**。耳朵形状上的拆分**信息增益**将为`0`。在胡须和脸部形状之间，脸部形状的**信息增益**最高。根据脸部形状进行拆分，按如下方式构建左子分支和右子分支。对于左子分支，我们检查是否停止拆分的标准。停止标准满足，创建一个预测为猫的叶节点。对于右子分支，我们发现所有都是狗。由于已经满足了分裂标准，因此也将停止分裂，并放置一个预测不是猫的叶节点。构建完这棵左子树后，可以将注意力转向构建右子树。要构建右子树，同样有这五个示例。先检查是否满足停止分裂的标准是否满足。所有示例都是一个类，没有满足该标准。继续在这个右子分支中分裂。构建右子分支的过程将很像训练**决策树**学习算法，数据集仅包含这五个训练示例。计算所有可能分裂的特征的**信息增益**，你会发现胡须特征使用的信息增益最高。检查此处的左子分支和右子分支是否满足停止分割的标准，最终将得到预测猫和狗猫的叶节点。这是构建**决策树的**总体过程。在根部构建**决策树**的方式是通过在左分支和右分支中构建较小的**决策树**。在构建**决策树**时，这种方式是通过构建较小的子决策树然后将它们放在一起来构建整体**决策树**。如何选择最大深度参数。有很多不同的选择，但一些开源库会有很好的默认选择供你使用。一个直觉是，最大深度越大，你构建的**决策树**就越大。它让**决策树**学习更复杂的模型，但如果将非常复杂的函数拟合到数据中，它也会增加**过度拟合**的风险。理论上，使用**交叉验证**来选择参数，比如最大深度，你可以尝试不同的最大深度值，然后选择在交叉验证集上效果最好的值。虽然在实践中，开源库甚至有更好的方法来为你选择这个参数。或者，你可以使用另一个标准来决定何时停止拆分，即从额外拆分中获得的信息是否小于某个阈值。如果任何特征被拆分，只实现了熵的轻微减少或非常小的信息增益。最后，当节点中的示例数量低于某个阈值时，你可以决定停止拆分。这就是构**建决策树**的过程。

目前看到的例子中，每个特征只能采用两个可能值中的一个。耳朵形状要么尖要么软，脸形要么圆要么不圆，胡须要么有要么没有。但是，如果您的特征采用两个以上的离散值，耳朵形状不仅尖而软，现在还可以采用椭圆形。因此，初始特征仍然是分类值特征，但它可以采用三个可能值，而不仅仅是两个可能值。当拆分此特征时，最终会创建三个数据子集，并最终为这棵树构建三个子分支。使用`one-hot`编码。它们不是使用耳朵形状特征，而是可以采用三个可能值中的任意一个。将创建三个新特征，其中一个特征动物是否有尖耳朵，第二个特征是它们的耳朵是否有松软，第三个特征是它是否有椭圆形耳朵。因此，对于第一个例子，我们之前将耳朵形状定为尖形，现在我们改为说这种动物的尖耳朵特征值为`1`，松软和椭圆形的值为`0`。对于第二个例子，椭圆形的耳朵，现在会说它的尖耳朵特征值为`0`，因为它没有尖耳朵。它也没有松软的耳朵，但它有椭圆形的耳朵。这里不再让一个特征具有三个可能的值，而是构建了三个新特征，每个特征只能采用两个可能值中的一个，即`0`或`1`。更详细地说，如果分类特征可以采用{% mathjax %}k{% endmathjax %}个可能的值，在这三个特征中，如果您查看这里的任何角色，其中恰好有`1`个值等于`1`。这就是`one-hot`编码的原因。由于这些特征之一将始终采用`1`，因此它是**热特征**。通过这种特征选择，回到了原始设置，即每个特征只能采用两个可能值中的一个，因此之前看到的**决策树学习**算法将适用于此数据，而无需进一步修改。三个来自耳朵形状的`one-hot`编码，一个来自脸部形状，一个来自胡须，现在这五个特征的列表也可以输入到新网络中，训练猫分类器。`one-hot`编码是一种适用于决策树学习的技术，还可以使用`1`和`0`对分类特征进行编码输入到神经网络中，该**神经网络**也需要数字作为输入。
{% asset_img ml_6.png %}

如何修改**决策树**以处理连续值的特征。这些特征可以是任何数值。不是仅根据耳朵形状、脸型和胡须进行约束分割。必须根据耳朵形状、脸型胡须或体重进行分割。如果根据体重特征进行分割比其他选项提供更好的**信息增益**。则根据体重特征进行分割。但是如何根据体重特征进行分割呢？如下图所示：
{% asset_img ml_7.png %}

动物的位置和纵轴是猫在上面而不是猫在下面。所以纵轴表示标签，{% mathjax %}y{% endmathjax %}为`1`或`0`。根据权重特征进行分割。根据权重特征进行约束分割时应该考虑这个阈值的不同值，然后选择最好的一个。最好是产生最佳**信息增益**的那个。具体来说，如果考虑权重是{% mathjax %}\text{Weight} \leq 8{% endmathjax %}来分割示例，那么将这个数据集分成两个子集。左边的子集有两只猫，右边的子集有三只猫和五只狗。如果要计算**信息增益**，计算根节点 N C p f 0.5 处的熵，现在左侧分割{% mathjax %}\frac{2}{10}{% endmathjax %}熵。这是右边`8`个例子中的`3`只猫。根据权重是否小于等于`8`进行分割，这将是信息增益。根据权重是否小于等于`9`进行分割。计算**信息增益**为{% mathjax %}H(0.5) - (\frac{2}{10}H(\frac{2}{2}) + \frac{8}{10}H(\frac{3}{8})) = 0.24{% endmathjax %}。所以现在有四个例子，左边分割所有的猫。计算**信息增益**为{% mathjax %}H(0.5) - (\frac{4}{10}H(\frac{4}{4}) + \frac{6}{10}H(\frac{1}{6})) = 0.61{% endmathjax %}。**信息增益**远高于`0.24`。或者我们可以尝试另一个值，计算结果如下{% mathjax %}H(0.5)- (\frac{7}{10}H(\frac{5}{7}) + \frac{3}{10}H(\frac{0}{3})) = 0.4{% endmathjax %}。一般的情况下，实际上会尝试不只三个值，而是沿`X`轴的多个值。按照权重或根据此特征的值对所有示例进行排序，并取排序后的训练示例列表中间点的所有值作为此处阈值的考虑值。如果有`10`个训练示例，测试此阈值的九个不同可能值，然后选择最高**信息增益**值。最后，根据此阈值的给定值进行拆分所获得的信息优于根据其他特征进行拆分所获得的**信息增益**。在这个例子中，`0.61`的**信息增益**比任何其他特征都高。假设算法选择这个特征进行分割，最终会根据动物的重量是{% mathjax %}\text{Weight} \leq 9{% endmathjax %}英镑来分割数据集。最终会得到像这样的两个数据子集，然后可以使用这两个数据子集递归地构建其它的**决策树**。
{% asset_img ml_8.png %}

总结一下，让**决策树**在每个步骤上都处理连续值特征。在进行分割时，只需考虑不同的值进行分割，计算**信息增益**，并决定在该连续值特征上进行分割（如果它能提供最高的**信息增益**）。这就是**决策树**使用连续值特征的方法。尝试不同的阈值，计算**信息增益**，如果它能从所有特征中选择最佳的**信息增益**，则根据连续值特征进行分割（使用所选阈值）。

在这里，为回归问题构建了一棵树，其中根节点根据耳朵形状分裂，然后左右子树根据脸部形状分裂。如下图所示，下面的节点将有这四只动物，体重分别为`7.2、7.6`,`9.2`和`10.2`。决策树将根据下面训练示例中的**权重平均值**进行预测。通过对这四个数字取平均值，结果是`8.35`。如果一只动物有尖耳朵，脸型不圆，那么它的体重被预测为`9.2`磅。因此，这个模型将给出一个新的测试示例，像往常一样沿着决策节点向下，直到到达叶节点，然后预测叶节点的值，这个值是通过对训练期间到达同一叶节点的动物的体重取平均值而计算出来的。因此，如果使用这个数据集从头开始构建决策树以预测体重。关键决策是如何选择要拆分的特征？用一个例子来说明如何做出决策。在根节点可以做的一件事是按耳朵形状进行拆分，最终会得到树的左右分支，左右各有五只动物。接下来如果选择按脸部形状进行拆分，如下图所示，最终会得到左右两边的这些动物：
{% asset_img ml_9.png %}

给定这三个特征在根节点上进行分割，想选择哪一个来对动物的体重做出最佳预测？在构建**回归树**时，不是为了降低熵（即分类问题的杂质度量），而是为了降低每个数据子集{% mathjax %}y{% endmathjax %}权重的**方差**。对于体重为{% mathjax %}7.2,9.2,8.4,7.6,10.2{% endmathjax %}的样本子集，由方差计算公式算出{% mathjax %}\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (x_i - \mu)^2 = 1.47{% endmathjax %}；对于体重为{% mathjax %}8.8,15,11,18,20{% endmathjax %}的样本子集，由方差计算公式算出{% mathjax %} \sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (x_i - \mu)^2 = 21.87{% endmathjax %}。评估分割左右两个子集的质量，分别为{% mathjax %}w^{\text{left}} = \frac{5}{10}{% endmathjax %}，{% mathjax %}w^{\text{right}} = \frac{5}{10}{% endmathjax %}。这个**加权平均方差**与决定使用哪种分割方法处理分类问题时使用的**加权平均熵**的作用非常相似。然后，对其它特征（脸部，胡须）选择重复此计算。选择分割的最佳方式是选择最低的**加权方差值**。与计算**信息增益**类似。就像分类问题一样，不仅测量**平均加权熵**，还要测量熵的**信息增益**。对于**回归树**，需要测量方差的减少。选择耳朵特征分割的根节点的加权平均方差计算为{% mathjax %}\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (x_i - \mu)^2 = 20.51{% endmathjax %}。对于所有这些示例，根节点加权平均方差都是相同的，因为根节点处有相同的十个示例。选择耳朵特征分割的样本子集的方差差值计算为{% mathjax %}20.51 - (\frac{5}{10}\ast 1.47 + \frac{5}{10}\ast 21.87) = 8.84{% endmathjax %}。方差的减少量为`8.84`。同样，其他的方差减少量分别为`0.64，6.22`。在这三个分割示例中，`8.84`是方差减少最多的。因此，选择方差减少最多的特征，这就是选择耳朵形状作为特征分割的原因。选择耳朵形状特征进行分割后，在左侧和右侧分支中有两个子集，每个子​​集各有五个示例，然后，再次使用递归方法，将这五个示例作为分割依据，并创建一个新的**决策树**，仅关注这五个示例，再次评估要分割的不同特征选项，然后选择使方差减少最多的特征。右侧也类似。继续分割，直到满足不再进一步分割的标准。使用这种技术，决策树不仅用于执行**分类问题**，还可以用于执行**回归问题**。