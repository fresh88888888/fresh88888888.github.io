---
title: 优化算法 (机器学习)(TensorFlow)
date: 2024-05-24 10:24:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。
<!-- more -->
#### 优化

对于深度学习问题，我们通常会先定义损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。在优化中，损失函数通常被称为优化问题的目标函数。按照传统惯例，大多数优化算法都关注的是最小化。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可。
##### 优化目标

尽管优化提供了一种最大限度地减少深度学习损失函数的方法，但本质上，优化和深度学习的目标是根本不同的。前者主要关注的是最小化目标，后者则关注在给定有限数据量的情况下寻找合适的模型。由于优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习（或更广义地说，统计推断）的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。
##### 优化面临的挑战

深度学习优化存在许多挑战。其中最令人烦恼的是局部最小值、鞍点和梯度消失。
###### 局部最小值

对于任何目标函数{% mathjax %}f(x){% endmathjax %}，如果在{% mathjax %}x{% endmathjax %}处对应的{% mathjax %}f(x){% endmathjax %}值小于在{% mathjax %}x{% endmathjax %}附近任意其他点的{% mathjax %}f(x){% endmathjax %}值，那么{% mathjax %}f(x){% endmathjax %}可能是局部最小值。如果{% mathjax %}f(x){% endmathjax %}在{% mathjax %}x{% endmathjax %}处的值是整个域中目标函数的最小值，那么{% mathjax %}f(x){% endmathjax %}是全局最小值。例如，给定函数：
{% mathjax '{"conversion":{"em":14}}' %}
f(x) = x\cdot\cos(\pi x)\;\text{for}\; -1.0\leq x\leq 2.0
{% endmathjax %}
我们可以近似该函数的局部最小值和全局最小值。
{% asset_img oa_1.png %}

深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数局部最优，而不是全局最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。
###### 鞍点

除了局部最小值外，鞍点是 梯度消失的另一个原因**鞍点**(`saddle point`)是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数{% mathjax %}f(x) = x^3{% endmathjax %}。它的一阶和二阶导数在{% mathjax %}x = 0{% endmathjax %}时消失。这时优化可能会停止，尽管它不是最小值。我们假设函数的输入是{% mathjax %}k{% endmathjax %}维向量，其输出是标量，因此其`Hessian`矩阵（也称黑塞矩阵）将有{% mathjax %}k{% endmathjax %}个特征值。函数的解可能是局部最小值、局部最大值或函数梯度为零位置处的鞍点：
- 当函数在零梯度位置处的`Hessian`矩阵的特征值全部为正值时，我们有该函数的局部最小值；
- 当函数在零梯度位置处的`Hessian`矩阵的特征值全部为负值时，我们有该函数的局部最大值；
- 当函数在零梯度位置处的`Hessian`矩阵的特征值为负值和正值时，我们有该函数的一个鞍点。

对于高维度问题，至少部分特征值为负的可能性相当高。这使得鞍点比局部最小值更有可能出现。简而言之，**凸函数**是`Hessian`函数的特征值永远不为负值的函数。不幸的是，大多数深度学习问题并不属于这一类。尽管如此，它还是研究优化算法的一个很好的工具。
###### 梯度消失

可能遇到的最隐蔽问题是梯度消失。回想一下常用的激活函数及其衍生函数。例如，假设我们想最小化函数{% mathjax %}f(x) = \tanh(x){% endmathjax %}，然后我们恰好从{% mathjax %}x=4{% endmathjax %}开始。正如我们所看到的那样，{% mathjax %}f{% endmathjax %}的梯度接近零。更具体地说，{% mathjax %}f'(x) = 1 - \tanh_2 (x){% endmathjax %}，因此是{% mathjax %}f'(4) = 0.0013{% endmathjax %}。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入`ReLU`激活函数之前训练深度学习模型相当棘手的原因之一。
{% asset_img oa_2.png %}

正如我们所看到的那样，深度学习的优化充满挑战。幸运的是，有一系列强大的算法表现良好，即使对于初学者也很容易使用。此外，没有必要找到最优解。局部最优解或其近似解仍然非常有用。
##### 总结

最小化训练误差并不能保证我们找到最佳的参数集来最小化泛化误差。优化问题可能有许多局部最小值。一个问题可能有很多的鞍点，因为问题通常不是凸的。梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。

#### 凸性

**凸性**(`convexity`)在优化算法的设计中起到至关重要的作用，这主要是由于在这种情况下对算法进行分析和测试要容易。换言之，如果算法在凸性条件设定下的效果很差，那通常我们很难在其他条件下看到好的结果。此外，即使深度学习中的优化问题通常是非凸的，它们也经常在局部极小值附近表现出一些凸性。这可能会产生一些像这样比较有意思的新优化变体。
##### 定义

在进行凸分析之前，我们需要定义**凸集**(`convex sets`)和**凸函数**(`convex functions`)。

###### 凸集

