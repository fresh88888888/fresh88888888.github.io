---
title: 机器学习(ML)(十八) — 强化学习探析
date: 2024-11-25 10:10:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

**强化学习**(`Reinforcement Learning, RL`)是一种**机器学习**的范式，主要关注**智能体**(`agent`)如何通过与环境的互动来学习最优策略，以最大化累积奖励。与**监督学习**和**无监督学习**不同，**强化学习**并不依赖于标注数据，而是通过**试错**(`trial and error`)的方法来**优化决策**。在**强化学习**中，主要涉及以下几个核心要素：**智能体**(`Agent`)，执行动作以影响环境的实体；**环境**(`Environment`)，**智能体**所处的外部系统，它对**智能体**的动作做出反应并提供反馈；**状态**(`State`)，描述环境在某一时刻的情况，**智能体**根据当前状态做出决策；**动作**(`Action`)，**智能体**在特定状态下可以选择的行为；**奖励**(`Reward`)，环境对**智能体**行为的反馈信号，通常是一个标量值，用于评估该行为的好坏；**策略**(`Policy`)，定义了**智能体**在特定状态下选择动作的规则，可以是确定性的也可以是随机性的；**价值函数**(`Value Function`),用于评估在某一状态下，**智能体**能够获得的长期回报期望。
<!-- more -->

**强化学习**(`RL`)的工作原理：**强化学习**(`RL`)的核心在于通过与环境的互动来学习。**智能体**在每个时间步选择一个动作，然后环境根据这个动作返回新的**状态**和**奖励**。**智能体**根据这些反馈调整其策略，以期在未来获得更高的累积奖励。这一过程通常涉及到以下几个步骤：1、**观察当前状态**；2、**选择一个动作**，依据当前策略；3、**执行该动作**，并接收新的状态和奖励；4、**更新策略**，以优化未来的决策。这种循环过程使得**智能体**能够逐渐改善其决策能力，从而达到最大化长期收益的目标。

**强化学习**(`RL`)已经在多个领域取得了显著成就，包括但不限于：
- **游戏**：如`AlphaGo、AlphaStar和OpenAI Five`等，这些系统通过**强化学习技术**在复杂游戏中击败了人类顶级选手。
- **机器人控制**：利用**强化学习**使机器人能够自主学习复杂任务，如抓取物体、行走等。
- **推荐系统**：通过用户反馈优化推荐算法，提高用户满意度。
- **金融交易**：在股票市场中应用**强化学习**进行自动化交易策略优化。

**强化学习**(`RL`)算法大致可以分为两类：
- **有模型学习**(`Model-Based Learning`)：**智能体**尝试构建**环境模型**，并利用该模型进行**规划**和**决策**。
- **无模型学习**(`Model-Free Learning`)：直接从环境交互中学习，不构建**环境模型**，常见方法包括`Q-learning`和**策略梯度**方法等。

如果假设脚下有一张五美元的钞票，这时可以弯腰捡起，或穿过街区，步行半小时捡起一张十美元的钞票。你更愿意选择哪一个？十美元比五美元多，但相比于步行半小时拿这张十美元，也许直接捡起五美元更方便。**回报**(`return captures`)的概念表明，更快获得奖励比需要花很长时间获得奖励更有价值。来看看它究竟是如何运作的？这里有一个火星探测器的例子。如果从状态`4`开始向左走，我们看到从状态`4`开始的第一步获得的奖励为`0`，从状态`3`开始的奖励为`0`，从状态`2`开始的奖励为`0`，状态`1`（最终状态）获得奖励为`100`。**回报**(`return captures`)定义为这些奖励的总和，但需要加权一个因子，称为**折扣因子**。**折扣因子**是一个略小于`1`的实数。这里选择`0.9`作为**折扣因子**。第一步的奖励加权为0，第二步的奖励是{% mathjax %}0.9 \times 0{% endmathjax %}，第三步的奖励是{% mathjax %}0.9^2 \times 0{% endmathjax %}，第三步的奖励是{% mathjax %}0.9^3 \times 100{% endmathjax %}。最终奖励的加权和为`72.9`。假设第一步获得奖励为{% mathjax %}R_1{% endmathjax %}，在第二步获得奖励为{% mathjax %}R_2{% endmathjax %}，第三步获得奖励为{% mathjax %}R_3{% endmathjax %}，那么**回报**(`return captures`)为{% mathjax %}R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots{% endmathjax %}。**折扣因子**({% mathjax %}\gamma{% endmathjax %})的作用是让强化学习算法能够在做出决策时平衡当前和未来的收益。**回报**(`return captures`)将主要取决于第一个奖励，即{% mathjax %}R_1{% endmathjax %}，少一点的**回报**(`return captures`)归于第二步的奖励，即{% mathjax %}\gamma R_2{% endmathjax %}，奖励更少来自于第三步，即{% mathjax %}\gamma^2 R_3 {% endmathjax %}，由此越早获得奖励，**总回报**就越高。在许多**强化学习**算法中，**折扣因子**通常设为接近1的实数，例如`0.9、0.99、0.999`。在使用的示例中，将折扣因子设置为`0.5`。这会大大降低奖励，每经过一个额外的时间戳，获得的奖励只有前一步奖励的一半。如果{% mathjax %}\gamma = 0.5{% endmathjax %}，则上述示例中的**回报**(`return captures`)将是{% mathjax %}0\times + 0.5\times 0 + 0.5^2\times + 0.5^3\times 100 = 12.5{% endmathjax %}。在金融应用中，**折扣因子**还有一个非常自然的解释，即利率或货币的时间价值。如果你今天能得到一美元，那么它的价值可能比你未来只能得到一美元要高一点。因为即使是今天的一美元，你也可以存入银行，赚取一些利息，一年后最终会多一点钱。对于金融应用，**折扣因子**表示未来`1`美元与今天的`1`美元相比少了多少。**回报**(`return captures`)取决于采取的**动作**(`Action`)。如果机器人从状态`4`开始，**回报**(`return captures`)是`12.5`，如果它从状态`3`开始，回报将是`25`，因为它早一步到达`100`的奖励。如果从状态`2`开始，回报将是`50`。如果从状态`1`开始，那么它会获得`100`的奖励，因此没有折扣。如果从状态`1`开始，回报将是`100`，如果你从状态`6`开始，那么回报是`6.25`。现在，如果采取不同的动作，回报实际上会有所不同。总而言之，**强化学习**(`RL`)中的**回报**(`return captures`)是系统获得的奖励的总和，由**折扣因子加权**，其中远期的奖励由**折扣因子**的更高次方加权。在我们讨论的例子中，所有奖励都是`0`或正数。但如果奖励是负数，那么**折扣因子**实际上会激励系统将**负奖励**尽可能推迟到未来。以金融为例，如果你必须向某人支付`10`美元，那么这可能是`-10`的负奖励。但如果你可以将付款推迟几年，那么你实际上会更好，因为几年后的`10`美元，由于利率，实际上价值低于今天支付的`10`美元。对于具有**负奖励**的系统，它会导致算法将**奖励**尽可能推迟到未来。

**强化学习**算法如何选择动作呢？在**强化学习**中，目标是提出一个称为**策略**(`Policy`){% mathjax %}\pi{% endmathjax %}函数，它的工作原理是将状态{% mathjax %}s{% endmathjax %}作为输入并将其映射到它希望的某个动作{% mathjax %}a{% endmathjax %}。例如，如果处于状态`2`，那么它会将我们映射到左侧操作。如果处于状态`3`，策略会向左走。如果处于状态`4`，策略会向左走，如果你处于状态`5`，策略会向右走。{% mathjax %}a = \pi(s){% endmathjax %}，代表**策略函数**{% mathjax %}\pi{% endmathjax %}在状态{% mathjax %}s{% endmathjax %}下执行什么动作。**强化学习**的目标是找到一个策略{% mathjax %}\pi{% endmathjax %}，在每个状态下需要采取什么行动，能够最大化**回报**(`return captures`)。**强化学习**在应用中被称为**马尔可夫决策过程**(`MDP`)，**马尔可夫决策过程**(`MDP`)中**马尔可夫**指的是**未来只取决于当前状态，而不取决于当前状态之前的任何状态**。也就是说，在**马尔可夫决策过程**中，未来只取决于你现在的位置。
{% asset_img ml_1.png %}

#### 状态动作值函数

在**强化学习**中，**状态动作值函数**(`State Action Value Function`)是一个关键概念，用于评估在特定状态下采取某个动作的期望回报。它通常用符号{% mathjax %}Q(s,a){% endmathjax %}表示，其中{% mathjax %}s{% endmathjax %}代表当前状态，{% mathjax %}a{% endmathjax %}代表在该状态下采取的动作。**状态动作值函数**{% mathjax %}Q(s,a){% endmathjax %}定义为在状态{% mathjax %}s{% endmathjax %}下采取动作{% mathjax %}a{% endmathjax %}后，按照某一**策略**(`Policy`)所能获得的期望累积奖励。具体来说，它可以表示为：{% mathjax %}Q(s,a) = \mathbb{E}[R_t|S_t = s,A_t = a]{% endmathjax %}，其中，{% mathjax %}R_t{% endmathjax %}是从时间{% mathjax %}t{% endmathjax %}开始的未来奖励的总和，{% mathjax %}\mathbb{E}{% endmathjax %}表示期望值。**状态动作值函数**的作用：**决策支持**，通过评估不同动作在特定状态下的价值，**智能体**(`Agent`)可以选择最优动作，以最大化其长期回报；**策略改进**，在策略迭代中，**状态动作值函数**用于更新策略，使得**智能体**(`Agent`)能够逐步学习到更优的行为方式。

假设我们有一个简单的`K-`**臂赌博机**(`k-armed bandit`)，其中有三个不同的动作{% mathjax %}a_1,a_2,a_3{% endmathjax %}，每个动作都有一个未知的奖励分布，**智能体**(`Agent`)的目标是通过选择不同的动作来最大化其获得的累积奖励。在这个例子中，**状态动作值函数**{% mathjax %}Q(s,a){% endmathjax %}可以定义为选择动作{% mathjax %}a{% endmathjax %}时所期望的奖励，例如，假设我们在某个状态{% mathjax %}s{% endmathjax %}下选择动作{% mathjax %}a_1{% endmathjax %}，我们可以表示为：{% mathjax %}Q(s,a_1) = \mathbb{E}[R_t|S_t = s,A_t = a_1]{% endmathjax %}，其中{% mathjax %}R_t{% endmathjax %}是时间步{% mathjax %}t{% endmathjax %}获得的奖励，实际操作步骤：
- **初始化**：假设我们初始化每个动作的值函数为`0`：{% mathjax %}Q(s,a_1) = 0,Q(s,a_2) = 0,Q(s,a_3) = 0{% endmathjax %}。
- **选择动作**：**智能体**(`Agent`)根据当前的**状态动作值函数**选择一个动作。例如，可以使用**ε-贪婪策略**（以概率`ε`随机选择一个动作，以探索新的可能性）。
- **观察奖励**：执行选定的动作后，**智能体**(`Agent`)会收到一个即时奖励。例如，如果选择了{% mathjax %}a_1{% endmathjax %}，并获得了奖励{% mathjax %}r_1{% endmathjax %}。
- **更新值函数**：使用**样本平均法**更新该动作的值函数：如果选择了{% mathjax %}a_1{% endmathjax %}，则更新公式为{% mathjax %}Q(s,a_1) = Q(s,a_1) + \frac{1}{N(a_1)}(r_1 - Q(s,a_1)){% endmathjax %}，其中{% mathjax %}N(a_1){% endmathjax %}是已选择该动作的次数。
- **重复过程**：**智能体**(`Agent`)不断重复选择、执行、观察和更新的过程，逐渐收敛到每个动作的真实期望奖励。

通过这个**K-臂赌博机**的问题示例，我们可以看到，**状态动作值函数**不仅帮助**智能体**评估不同动作的价值，还能指导其在复杂环境中做出更好的**决策**。这种方法在许多强化学习算法中都得到了广泛应用，如`Q`学习和深度`Q`网络(`DQN`)等。**状态动作值函数**是**强化学习**中的核心组成部分，它帮助**智能体**(`Agent`)在复杂环境中做出有效决策。通过理解和应用这一概念，**智能体**(`Agent`)能够不断优化其行为，从而实现更高的累积奖励。

#### 贝尔曼方程

**贝尔曼方程**(`Bellman Equation`)是**强化学习**和**动态规划**中的一个核心概念，它描述了在给定状态下，如何通过选择最佳动作来最大化未来的期望回报。**贝尔曼方程**为决策过程提供了一种递归关系，使得我们能够从当前状态推导出未来状态的价值。在**强化学习**中，**贝尔曼方程**通常分为两种类型：**状态值函数**(`State Value Function`)和**动作值函数**(`Action Value Function`)。
- **状态值函数**(`State Value Function`)：**状态值函数**{% mathjax %}V(s){% endmathjax %}表示在状态{% mathjax %}s{% endmathjax %}下，遵循某一策略{% mathjax %}\pi{% endmathjax %}所能获得的期望回报，其贝尔曼方程可以表示为：{% mathjax %}V(s) = \sum\limits_{a}\pi(a|s)\sum\limits_{s',r} P(s',r|s,a)[r + \gamma V(s')]{% endmathjax %}，其中{% mathjax %}\pi(a|s){% endmathjax %}表示为在状态{% mathjax %}s{% endmathjax %}下选择动作{% mathjax %}a{% endmathjax %}的概率，{% mathjax %}P(s',r|s,a){% endmathjax %}表示为在状态{% mathjax %}s{% endmathjax %}下采取动作{% mathjax %}a{% endmathjax %}后转移到状态{% mathjax %}s'{% endmathjax %}并获得奖励{% mathjax %}r{% endmathjax %}的概率，{% mathjax %}\gamma{% endmathjax %}为折扣因子，介于`0~1`之间，用于权衡未来奖励的重要性。
- **动作值函数**(`Action Value Function`)：**动作值函数**{% mathjax %}Q(s,a){% endmathjax %}表示在状态{% mathjax %}s{% endmathjax %}下采取动作{% mathjax %}a{% endmathjax %}后，遵循某一策略所能获得的期望回报。为了描述**贝尔曼方程**，我将使用以下符号。使用{% mathjax %}s{% endmathjax %}来表示当前状态。使用{% mathjax %}R(s){% endmathjax %}表示当前状态的奖励。在之前的示例中，状态1的奖励{% mathjax %}R(1) = 100{% endmathjax %}、状态2的奖励为{% mathjax %}R(2) = 0{% endmathjax %}、状态6的奖励是{% mathjax %}R(6) = 40{% endmathjax %}。使用{% mathjax %}a{% endmathjax %}表示当前动作，即在状态{% mathjax %}s{% endmathjax %}中采取的动作。执行动作{% mathjax %}a{% endmathjax %}后进入某个新的状态。例如，状态4采取左侧的动作，那么进入状态3。用{% mathjax %}s'{% endmathjax %}表示当前状态{% mathjax %}s{% endmathjax %}执行动作{% mathjax %}a{% endmathjax %}后进入的状态。用{% mathjax %}a'{% endmathjax %}表示状态{% mathjax %}s'{% endmathjax %}中执行的动作。贝尔曼方程可以表示为：{% mathjax %}Q(s,a) = \sum\limits_{s',r} P(s',r|s,a)[r + \gamma V(s')]{% endmathjax %}，如果使用最优策略，可以写成{% mathjax %}Q^{*}(s,a) = \sum\limits_{s',r} P(s',r|s,a)[r + \gamma\underset{a'}{\max}Q^{*}(s',a')]{% endmathjax %}。

**贝尔曼方程**(`Bellman Equation`)的重要性体现在以下几个方面：**递归结构**，它将一个复杂问题分解为更简单的子问题，使得我们可以通过**动态规划**的方法来求解；**最优性原则**，**贝尔曼方程**体现了**最优策略**的特性，即在每个决策点上选择能最大化未来回报的动作；**强化学习算法基础**，许多强化学习算法（如`Q-learning`、`SARSA`等）都是基于**贝尔曼方程**进行更新和优化的。**贝尔曼方程**是**强化学习**和**动态规划**中的一个基本工具，它为**智能体**(`Agent`)提供了一种系统的方法来评估和优化决策过程。
