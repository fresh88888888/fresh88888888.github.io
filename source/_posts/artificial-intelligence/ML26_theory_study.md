---
title: 机器学习(ML)(二十六) — 强化学习探析
date: 2025-02-23 09:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 论文解读

这篇文章主要解读关于[SFT记忆，RL泛化：基础模型后训练的比较研究](https://arxiv.org/pdf/2501.17161)，从标题中也可以看出比较的的对象主要是**监督微调**(`SFT`)和**强化学习**(`RL`)，主要探讨了**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型**后训练**(`post training`)中的不同作用。特别是在模型的**泛化能力**和**记忆力**方面的比较。
<!-- more -->

**监督微调**(`Supervised Fine-Tuning，SFT`)是一种在**机器学习**领域广泛应用的技术，特别是在**迁移学习**的背景下。它主要用于将**预训练模型**调整到特定的下游任务上，以提高模型在该任务上的表现。**监督微调**(`SFT`)和**强化学习**(`RL`)常用于**基础模型**的两种**后训练**技术。研究表明，**监督微调**(`SFT`)倾向于记忆训练数据，而**强化学习**(`RL`)则更关注于适应新场景和任务。论文引入了两个评估任务：`GeneralPoints`：一个算术推理卡牌游戏，要求模型使用四个数字创建等于目标数字（默认是24）的方程。`V-IRL`：一个真实世界的导航环境，模型需要根据视觉标志导航到目标位置。这两个任务都包含规则变体和视觉变体，用于评估模型在未见数据上的泛化能力。

虽然**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型训练中被广泛使用，但它们对泛化的不同影响仍然不清楚，这使得构建可靠和稳健的人工智能系统变得具有挑战性。分析基础模型的**泛化能力**中的一个关键挑战是**区分数据记忆**与**可转移原则的获取**。因此，作者研究了一个关键问题：**监督微调**(`SFT`)或**强化学习**(`RL`)是否**主要记忆训练数据**，或者它们是否学习了可以适应新任务变体的**可泛化原则**。为了解决这个问题，需关注**泛化**的两个方面：基于**文本的规则泛化**和**视觉泛化**。对于文本规则，研究模型应用学习到的规则（给定文本指令）到这些规则变体的能力。对于**视觉-语言模型**(`VLMs`)，**视觉泛化**衡量在给定任务中对视觉输入变化（如颜色和空间布局）的表现一致性。为了研究基于**文本**和**视觉的泛化**，使用了两个不同的任务，这些任务体现了基于规则和视觉变体。第一个任务是`GeneralPoints`，这是一个算术推理卡牌游戏任务，旨在评估模型的**算术推理能力**。在`GeneralPoints`中，模型接收四张卡片，并需要使用每张卡片的数值计算目标数字（默认是`24`），每张卡片只能使用一次。第二个任务是`V-IRL`，这是一个关注**模型空间推理能力**的真实世界导航任务。采用了类似于多步`RL`框架，在对主干模型进行`SFT`后实例化`RL`，使用序列修订公式。在`GeneralPoints`和`V-IRL`中，观察到`RL`学习到了**可泛化的规则**（以文本形式表达），其分布内性能提升也能转移到未见规则上。相反，`SFT`似乎只是**记忆了训练规则**，并未能实现**泛化**。除了基于文本的规则泛化外，还进一步探索了**视觉领域的泛化**，观察到`RL`也能对视觉分布外任务进行**泛化**，而`SFT`仍然无法做到泛化。作为视觉分布外泛化能力的副产品，通过多轮`RL`方法在`V-IRL`小型基准测试中**泛化能力**提升了`33.8%`（`44.0% → 77.8%`），突显了`RL`的泛化能力。为了理解`RL`如何影响模型的**视觉能力**，对`GeneralPoints`进行了额外分析，揭示使用**基于结果**的**奖励函数**训练`RL`能够改善视觉识别能力。与`SFT`相比，`RL`表现出更好的**泛化能力**，但`SFT`仍然有助于稳定模型输出格式，从而使`RL`能够实现其**泛化能力**的提升。同样增加**最大步骤数**来扩大推理计算时间也可以提高**泛化能力**。
{% asset_img ml_1.png "RL与SFT在OOD泛化下的比较研究" %}


