---
title: 机器学习(ML)(二十六) — 强化学习探析
date: 2025-02-23 09:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 论文解读

这篇文章[SFT记忆，RL泛化：基础模型后训练的比较研究](https://arxiv.org/pdf/2501.17161)，从标题中也可以看出比较的的对象主要是**监督微调**(`SFT`)和**强化学习**(`RL`)，主要探讨了**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型**后训练**(`post training`)中的不同作用。特别是在模型的**泛化能力**和**记忆力**方面的比较。
<!-- more -->

**监督微调**(`Supervised Fine-Tuning，SFT`)是一种在**机器学习**领域广泛应用的技术，特别是在**迁移学习**的背景下。它主要用于将**预训练模型**调整到特定的下游任务上，以提高模型在该任务上的表现。`SFT`通常分为以下几个步骤：
- **预训练**：首先，**基础模型**在大规模数据集上进行预训练，学习语言模式、语法和上下文。这一阶段使模型具备广泛的语言理解能力。
- **数据标注**：为**微调**准备一个特定任务的数据集，每个数据点都带有正确的输出或答案。这些标注数据对于监督学习至关重要，因为它们指导模型在微调过程中的参数调整。
- **微调**：将**预训练模型**在标注数据集上进一步训练，调整其参数以提高在特定任务上的性能。例如，如果模型需要处理法律文件，则可以使用标注的法律文本进行微调，使其更好地理解法律术语和结构。

**监督微调**(`SFT`)和**强化学习**(`RL`)常用于**基础模型**的两种**后训练**技术。研究表明，**监督微调**(`SFT`)倾向于记忆训练数据，而**强化学习**(`RL`)则更关注于适应新场景和任务。论文引入了两个评估任务：`GeneralPoints`：一个算术推理卡牌游戏，要求模型使用四个数字创建等于目标数字（默认是24）的方程。`V-IRL`：一个真实世界的导航环境，模型需要根据视觉标志导航到目标位置。这两个任务都包含规则变体和视觉变体，用于评估模型在未见数据上的泛化能力。

虽然**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型训练中被广泛使用，但它们对泛化的不同影响仍然不清楚，这使得构建可靠和稳健的人工智能系统变得具有挑战性。分析基础模型的**泛化能力**中的一个关键挑战是**区分数据记忆**与**可转移原则的获取**。因此，作者研究了一个关键问题：**监督微调**(`SFT`)或**强化学习**(`RL`)是否**主要记忆训练数据**，或者它们是否学习了可以适应新任务变体的**可泛化原则**。为了解决这个问题，需关注**泛化**的两个方面：基于**文本的规则泛化**和**视觉泛化**。对于文本规则，研究模型应用学习到的规则（给定文本指令）到这些规则变体的能力。对于**视觉-语言模型**(`VLMs`)，**视觉泛化**衡量在给定任务中对视觉输入变化（如颜色和空间布局）的表现一致性。为了研究基于**文本**和**视觉的泛化**，使用了两个不同的任务，这些任务体现了基于规则和视觉变体。第一个任务是`GeneralPoints`，这是一个算术推理卡牌游戏任务，旨在评估模型的**算术推理能力**。在`GeneralPoints`中，模型接收四张卡片，并需要使用每张卡片的数值计算目标数字（默认是`24`），每张卡片只能使用一次。第二个任务是`V-IRL`，这是一个关注**模型空间推理能力**的真实世界导航任务。采用了类似于多步`RL`框架，在对主干模型进行`SFT`后实例化`RL`，使用序列修订公式。在`GeneralPoints`和`V-IRL`中，观察到`RL`学习到了**可泛化的规则**（以文本形式表达），其分布内性能提升也能转移到未见规则上。相反，`SFT`似乎只是**记忆了训练规则**，并未能实现**泛化**。除了基于文本的规则泛化外，还进一步探索了**视觉领域的泛化**，观察到`RL`也能对视觉分布外任务进行**泛化**，而`SFT`仍然无法做到泛化。作为视觉分布外泛化能力的副产品，通过多轮`RL`方法在`V-IRL`小型基准测试中**泛化能力**提升了`33.8%`（`44.0% → 77.8%`），突显了`RL`的泛化能力。为了理解`RL`如何影响模型的**视觉能力**，对`GeneralPoints`进行了额外分析，揭示使用**基于结果**的**奖励函数**训练`RL`能够改善视觉识别能力。与`SFT`相比，`RL`表现出更好的**泛化能力**，但`SFT`仍然有助于稳定模型输出格式，从而使`RL`能够实现其**泛化能力**的提升。同样增加**最大步骤数**来扩大推理计算时间也可以提高**泛化能力**。
{% asset_img ml_1.png "RL与SFT在OOD泛化下的比较研究" %}

实验结果表明：
- `RL`**的泛化能力**：研究发现，经过`RL`训练的模型，特别是在使用**基于结果**的奖励进行训练时，能够在文本和视觉变体之间有效泛化。与此相比，`SFT`则更倾向于**记忆训练数据**，并且在面对分布外场景时表现不佳。
- `SFT`**的记忆偏向**：`SFT`训练的模型往往会对特定的输入模式进行匹配，而不是理解其背后的逻辑。例如，在`GeneralPoints`任务中，如果模型仅仅记住了特定卡片颜色与数字的关联，当规则变化时，其表现会显著下降。
- `RL`**对视觉识别能力的提升**：研究还发现，`RL`不仅提高了模型在文本任务中的表现，也增强了其在视觉任务中的基础视觉识别能力。
- `SFT`**与**`RL`**的互补性**：尽管`RL`在**泛化能力**上表现更好，但研究表明，`SFT`仍然对有效的`RL`训练至关重要。`SFT`能够稳定模型输出格式，从而为后续的`RL`提供良好的基础。

