---
title: 机器学习(ML)(二十六) — 强化学习探析
date: 2025-02-23 09:00:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 论文解读

这篇文章[SFT记忆，RL泛化：基础模型后训练的比较研究](https://arxiv.org/pdf/2501.17161)，从标题中也可以看出比较的的对象主要是**监督微调**(`SFT`)和**强化学习**(`RL`)，主要探讨了**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型**后训练**(`post training`)中的不同作用。特别是在模型的**泛化能力**和**记忆力**方面的比较。
<!-- more -->

**监督微调**(`Supervised Fine-Tuning，SFT`)是一种在**机器学习**领域广泛应用的技术，特别是在**迁移学习**的背景下。它主要用于将**预训练模型**调整到特定的下游任务上，以提高模型在该任务上的表现。`SFT`通常分为以下几个步骤：
- **预训练**：首先，**基础模型**在大规模数据集上进行预训练，学习语言模式、语法和上下文。这一阶段使模型具备广泛的语言理解能力。
- **数据标注**：为**微调**准备一个特定任务的数据集，每个数据点都带有正确的输出或答案。这些标注数据对于监督学习至关重要，因为它们指导模型在微调过程中的参数调整。
- **微调**：将**预训练模型**在标注数据集上进一步训练，调整其参数以提高在特定任务上的性能。例如，如果模型需要处理法律文件，则可以使用标注的法律文本进行微调，使其更好地理解法律术语和结构。

**监督微调**(`SFT`)和**强化学习**(`RL`)常用于**基础模型**的两种**后训练**技术。研究表明，**监督微调**(`SFT`)倾向于记忆训练数据，而**强化学习**(`RL`)则更关注于适应新场景和任务。论文引入了两个评估任务：`GeneralPoints`：一个算术推理卡牌游戏，要求模型使用四个数字创建等于目标数字（默认是24）的方程。`V-IRL`：一个真实世界的导航环境，模型需要根据视觉标志导航到目标位置。这两个任务都包含规则变体和视觉变体，用于评估模型在未见数据上的泛化能力。

虽然**监督微调**(`SFT`)和**强化学习**(`RL`)在基础模型训练中被广泛使用，但它们对泛化的不同影响仍然不清楚，这使得构建可靠和稳健的人工智能系统变得具有挑战性。分析基础模型的**泛化能力**中的一个关键挑战是**区分数据记忆**与**可转移原则的获取**。因此，作者研究了一个关键问题：**监督微调**(`SFT`)或**强化学习**(`RL`)是否**主要记忆训练数据**，或者它们是否学习了可以适应新任务变体的**可泛化原则**。为了解决这个问题，需关注**泛化**的两个方面：基于**文本的规则泛化**和**视觉泛化**。对于文本规则，研究模型应用学习到的规则（给定文本指令）到这些规则变体的能力。对于**视觉-语言模型**(`VLMs`)，**视觉泛化**衡量在给定任务中对视觉输入变化（如颜色和空间布局）的表现一致性。为了研究基于**文本**和**视觉的泛化**，使用了两个不同的任务，这些任务体现了基于规则和视觉变体。第一个任务是`GeneralPoints`，这是一个算术推理卡牌游戏任务，旨在评估模型的**算术推理能力**。在`GeneralPoints`中，模型接收四张卡片，并需要使用每张卡片的数值计算目标数字（默认是`24`），每张卡片只能使用一次。第二个任务是`V-IRL`，这是一个关注**模型空间推理能力**的真实世界导航任务。采用了类似于多步`RL`框架，在对主干模型进行`SFT`后实例化`RL`，使用序列修订公式。在`GeneralPoints`和`V-IRL`中，观察到`RL`学习到了**可泛化的规则**（以文本形式表达），其分布内性能提升也能转移到未见规则上。相反，`SFT`似乎只是**记忆了训练规则**，并未能实现**泛化**。除了基于文本的规则泛化外，还进一步探索了**视觉领域的泛化**，观察到`RL`也能对视觉分布外任务进行**泛化**，而`SFT`仍然无法做到泛化。作为视觉分布外泛化能力的副产品，通过多轮`RL`方法在`V-IRL`小型基准测试中**泛化能力**提升了`33.8%`（`44.0% → 77.8%`），突显了`RL`的泛化能力。为了理解`RL`如何影响模型的**视觉能力**，对`GeneralPoints`进行了额外分析，揭示使用**基于结果**的**奖励函数**训练`RL`能够改善视觉识别能力。与`SFT`相比，`RL`表现出更好的**泛化能力**，但`SFT`仍然有助于稳定模型输出格式，从而使`RL`能够实现其**泛化能力**的提升。同样增加**最大步骤数**来扩大推理计算时间也可以提高**泛化能力**。
{% asset_img ml_1.png "RL与SFT在OOD泛化下的比较研究" %}

实验结果表明：
- `RL`**的泛化能力**：研究发现，经过`RL`训练的模型，特别是在使用**基于结果**的奖励进行训练时，能够在文本和视觉变体之间有效泛化。与此相比，`SFT`则更倾向于**记忆训练数据**，并且在面对分布外场景时表现不佳。
- `SFT`**的记忆偏向**：`SFT`训练的模型往往会对特定的输入模式进行匹配，而不是理解其背后的逻辑。例如，在`GeneralPoints`任务中，如果模型仅仅记住了特定卡片颜色与数字的关联，当规则变化时，其表现会显著下降。
- `RL`**对视觉识别能力的提升**：研究还发现，`RL`不仅提高了模型在文本任务中的表现，也增强了其在视觉任务中的基础视觉识别能力。
- `SFT`**与**`RL`**的互补性**：尽管`RL`在**泛化能力**上表现更好，但研究表明，`SFT`仍然对有效的`RL`训练至关重要。`SFT`能够稳定模型输出格式，从而为后续的`RL`提供良好的基础。

#### AGI 探析

通往**人工通用智能**的道路不仅是一场技术征程，更是一次哲学层面的探索——它要求我们重新诠释数字时代**智能**与**伦理**的深层内涵。———— `Alex Kim`，未来洞察研究院人工智能伦理部主任。

要开始探讨我们距离`AGI`（**人工通用智能**）还有多远这一命题，首先需要以人工智能发展史为锚点，理解人类对更先进系统的深层诉求。通过本文，我们希望以大型语言模型(`LLMs`)等现代`AI`系统为观测视角，为当前`AGI`发展进程提供证据与洞见。核心目标在于审慎叩问：`LLMs`是否就是终极答案？只有秉持这种持续探索的科研自觉，我们才有可能真正触碰`AGI`的疆界。

**人工智能简史**，**人工智能**(`AI`)的发展通过其在**视觉感知**、**语言理解**、**推理优化**等领域的强大能力深刻改变了人类社会。典型案例是`DeepMind`于`2021`年推出的`AlphaFold`，彻底革新了蛋白质结构预测领域，推动了生物科学研究的前沿突破。值得注意的是，`AI`发展历程并非一帆风顺：
- **奠基阶段**(`1950s-1970s`)：早期研究聚焦**符号主义**与**连接主义**，为智能计算的**范式**奠定理论基础。受限于算力与数据规模，研究多停留在概念验证层面。
- **寒冬与复苏**(`1980s-1990s`)：因技术预期过高与现实落差，`AI`经历发展低谷。**机器学习**与**神经网络理论**突破为技术复苏注入动力。
- **深度学习革命**(`2010s`至今)：**图像识别**与**语音识别**取得跨越式发展，`ChatGPT`的横空出世标志着大语言模型(`LLMs`)开启`AI`研究的新纪元。**统一知识表征体系，多任务协同求解能力突破**。

尽管**人工智能**(`AI`)为人类社会带来了巨大的改善，但社会的物质和精神需求日益增长，使得人们对`AI`仅提供的便利性感到不满足。因此，实现能够高效、有效地执行更广泛任务的**人工通用智能**(`AGI`)已成为一个迫切关注的问题。`AGI`被描述为一种至少在大多数任务上与人类一样能力的`AI`系统(`Wang et al., 2018; Voss and Jovanovic, 2023`)。我们到底距离`AGI`还有多远，以及如何实现`AGI`？为了探讨这些问题，现有研究主要分为三个类别：**定义与概念**、**技术方法与应用**、以及**伦理与社会影响**。
- **定义与概念**：`Wang`等人(`2018`)从与人类的比较角度定义了`AGI`的概念，并提出了不同层次的`AGI`。`Voss`和`Jovanovic`(`2023`)为实现`AGI`提供了方向，设定了与`AGI`相关的人类化要求。
- **技术方法与应用**：`Yan`(`2022`)和`Wang`等人(`2019`)提出，`AGI`可以通过将**逻辑**与**深度学习**相结合来实现。`Das`等人(`2023`)认为，`AGI`技术的发展存在许多风险，如安全和隐私问题。
- **伦理与社会影响**：`Rayhan`(`2023`)认为，人们应该考虑创建`AGI`的**伦理影响**，包括对**人类社会、隐私和权力动态**的影响。`Bugaj`和`Goertzel`(`2007`)提出了**五项伦理原则**及其对`AGI`交互的影响。这些研究从不同角度刻画了`AGI`，但仍缺乏对`AGI`发展过程的系统性评估和对`AGI`目标的明确定义，这使得衡量当前`AI`发展与`AGI`未来的差距变得困难，并且难以提出实现`AGI`的可能路径。
{% asset_img ml_2.png  %}

由上图所示，从`AGI`(**人工通用智能**)所需的主要能力概述开始，分为**内部能力**、与**外部世界的接口连接**以及支持这些功能的**基础设施系统**。在部署方面，需要更为复杂的对齐程序，以在约束和人类期望下释放`AGI`系统的潜力。此外，我们描绘了一个路线图。`AGI`的三个层次：**胚胎**`AGI`、**超人类**`AGI`和**终极**`AGI`，帮助我们定位当前状态、相关评估框架以及对一些可能阻碍我们向`AGI`前进的关键问题的见解。
{% asset_img ml_3.png  %}

由上图所示，`AGI`内部，即`AGI`的“**大脑**”，由四个主要组成部分：**感知**、**推理**、**记忆**和**元认知**。人类大脑的复杂性，以及其特定功能区域分别负责**认知**和**行为**的不同方面，为`AGI`系统的架构提供了一个引人入胜的类比。类似于人类大脑分为感官处理、情感、认知和执行功能等区域，`AGI`系统的“**大脑**”也可以基本上分为四个主要组成部分：**感知**、**记忆**、**推理能力**和**元认知**。这些组成部分反映了人类**认知**的基本方面，并在创建一个真正智能的系统中扮演着不同的关键角色。**感知**是指在`AGI`与其环境交互过程中对感官信息的组织和解释，被视为`AGI`的基本能力，包括**视觉、听觉、触觉、嗅觉**等。`AGI`的**推理**是基于对环境的感知，并对环境执行行动。`AGI`与环境的互动，包括**感知**的获取和行动的执行，将被保存为`AGI`的**记忆**。这些**记忆**将被用于`AGI`的**元认知**。

##### AI感知

**感知**是指系统解释和理解周围世界的能力。这涉及对感官数据的处理和分析，以构建对环境的动态和上下文理解。**自然语言**作为人类交流的主要方式，已经从早期人类互动的起源发展到复杂的系统，如**大语言模型**(`LLMs`)。这些模型扩展了理解和参与对话以及执行创意任务的能力。然而，文本本身可能无法完全捕捉到现实世界经验的深度，这凸显了**多模态智能**的重要性，即结合图像、视频和音频以实现更丰富的**人机交互**。从传统`LLMs`到**多模态模型**的转变代表了一次重大的技术飞跃，促进了跨多种输入的更加逼真的互动。这一转变由近期**多模态**`LLMs`的发展所突显，解决了仅依赖语言理解的局限性，并为涉及多种数据形式的复杂挑战打开了大门。整合各种模型应遵循两个原则：1）理解“如何”将外部模态信息纳入，并确保不同模块的无缝整合；2）确定“使用什么”信息以保持原始模型的完整性并增强整体能力。利用现成的`LLMs`和**多模态编码器**的主要目标是在它们之间建立无缝连接。这种连接可以是外部的，即在不改变现有模型结构的情况下对齐多模态知识，也可以是内部的，允许`LLMs`与其他模态编码器进行更为复杂的交互。这些方法通常需要大量训练，例如创建一个可学习的接口，将`LLM`与非语言模态（特别是视觉）联系起来。类似于`LLM`的**预训练**和**微调**，**多模态**`LLMs`(`MLLMs`)遵循一个基于**预训练**`LLM`的两阶段训练范式，并将其适应**多模态领域**。第一阶段，称为**视觉-语言对齐阶段**，旨在使**语言模型**能够理解视觉标记。第二阶段涉及**多模态指令调整**，以使模型与人类感知相一致。这些阶段根据`LLM`与**多模态编码器**之间的组合架构有明确的分类。

**模态的外部连接**，外部方法基于通过额外结构和现有模型将视觉分支与`LLMs`（**大语言模型**）连接起来的理念。
- **投影式**：**模态连接器**存在于`LLMs`和多**模态编码器**之外，可以通过简单的**线性投影**或相对复杂的选择方法来实现。这种类型的`MLLM`（**多模态大语言模型**）通常在两个对齐训练阶段激活投影层或`LLMs`。
- **查询式**：这些`MLLMs`使用设计更为复杂的连接器，但仍然独立于`LLMs`和**多模态编码器**之外。这种模型本质上利用了类似**注意力**的交互，即在可学习变量与视觉标记之间进行交互。由于其连接器能够学习到比简单投影式更复杂的数据模式，因此仅**激活连接器**也能获得更优的多模态性能。
- **语言式**：语言作为接口是将所有现成模型整合为一个整体的流行方向。这些方法利用各种预构建模块进行生成和其他任务，`LLMs`主要负责模块的协调。利用工具的一个主要优势是这些系统可以更灵活地进行规划，以便做出决策或创作多媒体内容，语言作为桥梁。一个突出的最新方法是`GPT-4V`**模型**，它可以通过连接最先进的生成器生成生动的图像。虽然这些方法为各种任务提供了更广泛的技术解决方案，但它们在实现与接口式方法相当的性能深度方面通常不如后者。

**模态的内部连接**，将**多模态编码器**与`LLM`（**大语言模型**）连接的另一种方法是调整`LLM`的内部模块。
- **基于交叉注意力**：`Flamingo(Alayrac et al., 2022)`提出了一种**感知器**，在`LLMs`的**注意力模块**内增加了额外的**交叉注意力机制**。`Flamingo`的几种变体也使用相同或类似的框架来调整`MLLMs`（`多模态大语言模型`）。
- **自回归式**：像`Fuyu(Bavishi et al., 2023)`及其变体这样的`MLLMs`将视觉标记视为**语言标记**，并从预训练阶段开始使用相同的自回归训练损失来更新整个模型参数。

**多模态大语言模型**(`MLLMs`)的额外模态：尽管早期模型主要集中在视觉输入和文本输出上，但近期的发展已扩展到包括多种模态的输入和输出形式。在输入方面，通过适当的**模态编码器**和训练数据，`LLMs`（**大语言模型**）现在可以理解视频、音频以及多种非语言模态，使这种方法具有**可扩展性**和**可访问性**。在输出方面，最近的研究转向了创建超越单纯文本生成的混合内容。`LLMs`已经从最初的检索图像和生成文本发展到同时生成视觉和文本内容。生成图像和文本的详细技术路径包括对具有统一表示的图像-文本数据进行**自回归调整**，以及将文本特征转换为**图像生成模型**(`Stable Diffusion`)的符号调整。此外，视觉领域的最新进展为在没有文本的情况下生成内容提供了可扩展的方法，增强了将视觉模型扩展到生成任务的潜力。这为在语言之外的其他模态中扩展和发现类似的“`AGI`**现象**”提供了可能性。

当前`AGI`级别的**感知模型**仍然受到模态有限和**鲁棒性**不足的限制。为解决这些问题，所以提出了几个潜在的未来研究方向：
- **模态多样化**：整合多种数据类型以提升模型能力是至关重要的。需要探索不常见的模态（如图形），并同时整合多种模态（如图像、音频和视频）。这需要精心设计的模块、高质量的数据以及平衡管理不同模态之间的相互作用及其与语言的关系。例如，虽然`GPT-4V`只能处理语言和视觉信息，但最新的`Gemini`**模型**已将其能力扩展到更广泛的音频和视频范围。潜在的方法包括使用统一模态表示工具（如`ImageBind`和`LanguageBind`）来弥合模态差距，减轻从其他模态学习的负担。
- **提高多模态系统的鲁棒性和可靠性**：随着越来越多的综合基准测试出现，这些测试不仅涵盖一般情况，还包括挑战性输入（如数学问题、反事实指令和攻击字符串），显然**多模态系统**（特别是较小的系统）在面对对抗性示例时表现不佳，并且严重依赖语言，缺乏在分布异常情况下的**推理能力**（如多面板图像、草图和长序列图像）。这些观察结果在实际应用中可能带来潜在风险。为应对这些挑战并构建更具**鲁棒性**的多模态`AGI`模型，可以考虑将对抗性示例纳入训练或增加训练数据指令格式的多样性。
- **可解释的多模态模型**：与传统模型不同，**多模态模型**涉及不同模态之间的复杂交互，因此揭示其内部工作机制以理解和创建更强大的**多模态模型**至关重要。为此，研究努力提供了训练或生成过程中的解释，揭示了模型性能和推理的洞见。方法如用多样化的训练数据探测模型性能已被探索。此外，`Gemini`团队通过提供生成解释来增强用户对`AI`推理过程的信任和理解。提高**多模态模型**的另一个方面是增加透明度，这包括识别特定的模型组件或配置，这些组件或配置有助于系统的能力（如**视觉编码器**、**连接器**或**训练范式**）。研究还特别调查了不同模态处理器对整体模型性能的影响。随着**多模态模型**的发展，未来的研究必须优先考虑**可解释性**和**透明度**，以便充分发挥这些强大`AI`系统的潜力，同时确保其负责任和道德。例如，未来的研究方向可以探索严格控制的实验来训练`AI`模型，以分解每个部分，或探测模型组件以找到最有效的模块。

##### AI推理

**推理**是基于**可用信息**、**逻辑**和**先验知识**得出**结论**或做出决策的**认知过程**。它包括**评估证据**、**识别关系**以及**应用规则**或**原则**来解决问题。`AI`**推理**指的是`AI`**系统**模拟这一过程的能力，使机器能够**理解情境**、**推断结论**并以类似人类推理的方式**做出决策**。

当前`AI`**推理**的状况，大量研究表明，**推理能力**已经在**大型机器学习模型**中显现。**大语言模型**(`LLMs`)，包括`GPT-3`、`LLaMA 2`和`PALM 2`，已经在各种`NLP`任务中实现了灵活的零样本和少样本推理能力。**大视觉语言模型**(`LVLMs`)，如`GPT-4`**视觉版**和`Gemini`，通过有效地整合视觉和语言推理，进一步推动了这一进展。已经开发出多种策略，以在不更新模型的情况下激发有效和高效的推理。这些方法在包括算术、常识、符号推理以及模拟和现实世界挑战在内的广泛任务中显著提高了模型性能。

思维导航：
- **思维链**(`CoT`)：**思维链**生成一系列中间推理步骤，称为“**思维**”，以使模型能够分解多步骤问题，并为更复杂的任务分配额外的计算。这为模型的推理过程提供了**可解释**的洞见，帮助理解答案的推导过程，并识别推理中可能出现的错误。
- **思维树**(`ToT`)：**思维树**使用基于树的**搜索算法**来导航“**思维**”，以进行深思熟虑的问题解决。这使得**大语言模型**(`LLMs`)能够探索多种推理路径，并在必要时进行**前瞻**或**回溯**。
- **思维图**(`GoT`)：**思维图**将信息组织成图结构，其中“**思维**”是顶点，边对应于这些顶点之间的依赖关系。这种基于图的组织方式促进了更复杂的思维整合和操作，允许创建更复杂的**推理路径**和**反馈机制**。
- **程序思维**(`PoT`)：**程序思维**利用语言模型将推理过程表达为程序，将计算委托给外部计算机执行生成的程序以获得答案。这种计算与推理的分离提高了对高度符号化推理问题的性能。
- **自洽推理**：自洽性通过采样多样化的推理路径并选择最一致的答案，克服了**贪婪解码**的**局限性**，实现了更可靠的结果。
- **其他提示策略**：许多其他提示方法已被开发出来，以提高`LLMs`的推理能力。例如，复杂性提示、自动思维链、最简至最复杂提示、分解提示、工具`LLM`和`ToRA`等方法，通过不同的方式增强了模型的推理能力。
- **动态推理与规划**：`ReAct`通过交替生成`推理轨迹`和`行动计划`，实现`动态推理`。`DEPS`通过`动态反馈循环`提高计划的可靠性。`Inner Monologue`和`ProgPrompt`等方法通过实时反馈调整计划，提高任务完成率和适应性。
- **反思与改进**：自我改进和反思等方法通过迭代生成和反馈，使模型能够根据反馈进行调整。`CRITIC`利用外部工具验证`LLMs`的行动，并进行自我校正。
- **整合语言模型、世界模型和代理模型**：`LAW`框架结合`语言模型`、`世界模型`和`代理模型`，促进更强大的推理能力。`RAP`和`BIP-ALM`等方法通过语言模型实现更复杂的`推理`和`规划`。
- **具身智能体的推理与规划**：`Voyager`和`Generative Agents`等方法通过`动态推理`和`反馈循环`，提高了**具身智能体**在执行任务和与环境互动方面的能力。

尽管当前系统在各种任务中展示了令人印象深刻的推理技能，但它们仍存在一些重大缺陷和挑战：
- **因果学习**：基础模型需要学习**因果关系**，以实现更好的**理解**和**泛化**。这些模型主要依赖训练数据中的模式，但这些模式并不总是能捕捉到人类知识和经验的深度和广度。此外，这些模型通常基于数据中提取的模式运行，而不是真正理解底层的**因果关系**。`Zečević`等人（`2023`）描述了`LLMs`如何表面上复制**因果关系**，但缺乏底层的**因果机制**，使它们更像是“因果鹦鹉”而非真正的因果模型。`Jin`等人（`2023`）提出了一个具有挑战性的因果推理数据集，并建议LLMs在可靠推理因果关系方面仍有很长的路要走。未来`AGI`的进步应专注于**学习因果关系**而非**相关性**，从而实现更好的**泛化**和更深入的理解。
- **复杂和长上下文推理**：`AGI`必须解决复杂的**多步推理任务**的挑战。尽管已经开发了许多策略来缓解这一问题，但这些策略通常需要明确的指导或问题的仔细构建，这在未来可能变得不必要。即使采用这些方法，模型在处理长上下文信息并在整个推理任务中保持连贯和逻辑推理方面仍然面临挑战。
- **幻觉、不确定性评估和模糊处理**：`AGI`应解决**幻觉问题**，即生成与提供的来源内容不符或无意义的内容。这种倾向影响性能，并在实际应用中引发重大安全问题。此外，这些模型通常难以准确评估其不确定性，并在输出中有效传达这种不确定性，这可能导致误导性的结果。它们还难以处理**模糊性**，这可能使其在复杂情境中的**可用性**复杂化。
- **社交推理**：`AGI`应提高**社交推理能力**，以增强与人类和其他**智能体**的互动。当前的`AI`模型缺乏健全的**心智理论**，即理解他人心理状态的能力。提高这一能力对于`AGI`系统在开放环境中与人类和其他**智能体**安全有效地互动至关重要。理解社交线索和规范是这一发展的核心，因为它使`AGI`能够在不同情境中解释和回应隐含的沟通和行为期望。
- **可解释性和透明度**：`AGI`应解决可**解释性**和**透明度**的挑战，从而提高其决策的**可靠性**。大多数`AI`系统缺乏这些特性，使得难以理解它们如何得出特定结论或答案。旨在用**自然语言**引出推理的技术并不总是与模型实际使用的推理过程一致，生成的解释可能具有系统性误导。这一局限性阻碍了它们的推理能力，并在需要**决策审计**或**证明**的领域（如医疗和法律）带来重大挑战。
- **动态推理和跨领域规划**：未来的`AGI`系统旨在实现跨领域的**动态推理**、**道德**和**高效的规划**，以及前所未有的规模和速度的**类人智能**。我们仍然远未实现`AGI`级别的能力，即在没有重新训练或人类监督的情况下，在各种领域进行**推理**和**规划**。这一旅程包括增强`AI`系统在不同领域之间转移知识和技能的能力，使它们能够高效地应对未知情况。关键的发展重点是创建能够在广泛的战略目标和详细行动之间进行规划的算法。此外，`AI`系统在规划阶段需要更有效地管理资源（如时间、能源和成本），并确保这些规划过程符合道德标准和安全法规，特别是在敏感领域，以避免滥用或意外后果。
- **创新解决方案**：未来的`AGI`系统将能够**理解上下文**、**推断因果关系**，并在各种领域中**动态应用高级逻辑规划**。通过综合大量信息并进行深思熟虑的规划，它们可以为**创造性假设的构建**、**复杂道德判断的做出**、**新场景结果的预测**以及**持续学习**和完善对世界的理解提供创新解决方案。这些未来的`AGI`系统不仅将在处理和生成信息方面表现出色，还将能够以深度类似于**人类智能**的方式理解和与世界互动，但规模和速度将远超人类能力。

##### AI记忆

**语言和视觉模型的无状态特性**，语言和视觉模型本质上是无状态的，它们在交互之间不保留信息。然而，**高级智能体**不同，它们能够管理内部或外部记忆，从而能够进行复杂的多步交互。这种记忆存储了中间信息、领域特定或广泛的知识以及**智能体**之前**观察、思考和行动的序列**等。它帮助**智能体**利用以前的知识或经验进行**推理、规划**和**自我提升**。

当前`AI`**记忆**的状态从三个关键方面审视当前`AI`**记忆**的现状：
- **记忆管理**：决定存储什么信息以及何时存储。
- **记忆表示**：定义信息的结构形式。
- **记忆利用**：解决如何高效有效地应用和使用**记忆**。

**记忆管理**，记忆根据持续时间分为**短期记忆**和**长期记忆**。**短期记忆**：短期记忆在维持当前决策过程所需的信息方面起着至关重要的作用。一个显著的例子是**上下文提示**，它利用基础模型自身的上下文作为一种**短期记忆**。这种方法可以提供额外的信息或示例，或用于生成**中间推理**。更广泛地说，**短期记忆**包括所有决策所需的即时数据，包括：1、感知模块收集或处理的实时数据；2、推理、规划和自我进化模块的即时输出；3、从长期记忆中主动检索的信息。这些元素共同合成，指导并告知后续行动。**长期记忆**：长期记忆可以广泛分为两类：**经验**和**知识**。经验包括过去的**观察、思考、行动**等。这些丰富的经验在决策过程中起着关键作用。通过检索相关经验，**智能体**可以获得必要的信息，理解过去行动的反馈，并在理解和推理中实现一定程度的**泛化**。例如，`Reflexion`反思任务反馈信号，并将其作为文本摘要保存，直接纳入后续情境的上下文中，以提高性能。`Generative Agents`以自然语言记录经验，并使用**相关性**（基于**嵌入**）、**最近性**（基于**规则**）和**重要性**（基于**推理**）标准检索记忆。**知识**代表了**智能体**对世界和自身的理解，增强了其推理和决策能力。**知识**可以来自两个来源：**从经验中收集**和**同化的知识**，或者利用**外部知识库**。例如，`Voyager`维护一个不断扩展的可执行代码技能库，用于初步行动以完成任务。`ReAct`使用`Wikipedia API`在**智能体**缺乏信息时获取外部知识。

**记忆表示**，记忆表示分为**文本记忆**和**参数记忆**。**文本记忆**是当前表示记忆内容的主要方法，包括**自然语言**的原始形式和结构化形式（如元组、数据库等）。**参数记忆**可以通过**监督微调**、**知识编辑**和**模型合并**等技术将领域特定知识整合到模型参数中。**文本记忆**在每次推理时都需要将记忆纳入**上下文提示**，导致成本更高且处理时间更长。相比之下，**参数记忆**在写入阶段成本更高，因为微调模型比简单的文本存储更具挑战性。在**可解释性**方面，**文本记忆**通常比**参数记忆**更透明，因为**自然语言**是人类理解的最直接方式。

**记忆利用**，利用记忆的两种常见技术是**记忆检索**和**长上下文**`LLMs`。**记忆检索**：**记忆检索**涉及从**长期记忆**中读取信息到**短期记忆**中以供即时使用。这可以通过**基于规则的检索**或**检索增强方法**实现。**基于规则的检索**可以使用**关键词**、**时间步长**或**特定模式搜索记忆**。**检索增强方法**如`Dense Passage Retriever`(`DPR`)创建文档的**稠密表示**，并基于其**先验概率检索**最相关的文档。**长上下文**`LLMs`：**长上下文**`LLMs`通过扩展**上下文窗口**，为模型访问**长期记忆**开辟了新的途径。`Ring Attention`和`LongRoPE`等技术通过改进**注意机制**和**存储方法**，大大降低了**长上下文推理**的时间和成本。随着`GPU`性能的提升和注意机制的突破，`LLMs`的**上下文窗口**已从`GPT-2`的`1024`个`token`扩展到`GPT-4`的`8192`个`token`，甚至超过`16K`个`token`。这些扩展的**上下文窗口**使得`AI`系统能够更有效地存储和回忆上下文中的**知识**和**经验**，从而实现更快速和全面的**基于上下文的推理**。

