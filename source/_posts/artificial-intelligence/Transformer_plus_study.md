---
title: Transformer模型—探析（深度学习）
date: 2024-07-23 11:40:11
tags:
  - AI
categories:
  - 人工智能
mathjax:
  tex:
    tags: 'ams'
  svg:
    exFactor: 0.03
---

#### 介绍

`Transformer`是一种由`Google`团队在`2017`年提出的**深度学习模型**，专门用于**自然语言处理**(`NLP`)任务。它的核心机制是**自注意力**(`Self-Attention`)或**缩放点积注意力**(`Scaled Dot-Product Attention`)，能够处理输入序列中的每个元素，并计算其与序列中其他元素的交互关系。这使得模型能够更好地理解序列中的上下文关系。
<!-- more -->

**主要特点**：
- **并行计算**：`Transformer`模型可以同时处理整个序列，而不像循环神经网络(`RNN`)那样需要逐个处理序列中的元素。这使得模型能够更好地利用现代硬件的**并行计算**能力。
- **捕捉长距离依赖关系**：`Transformer`模型能够直接处理序列中**远距离的依赖关系**。例如，在处理语言时，一个词的含义可能会受到很远处的其他词的影响。

`Transformer`模型主要由两个模块构成：
- **编码器**(`Encoder`)：负责处理输入文本，为每个输入构造对应的**语义表示**。
- **解码器**(`Decoder`)：负责生成输出，使用编码器输出的语义表示结合其他输入来生成**目标序列**。

这两个模块可以根据任务的需求单独使用：
- **纯编码器模型**：适用于只需要理解输入语义的任务，例如句子分类、命名实体识别。
- **纯解码器模型**：适用于生成式任务，例如文本生成。
- **编码器-解码器模型**（或Seq2Seq 模型）：适用于需要基于输入的生成式任务，例如翻译、摘要。

`Transformer`模型本质上都是预训练语言模型，大都采用自监督学习(`Self-supervised learning`)的方式在大量生语料上进行训练，也就是说，训练这些 Transformer 模型完全不需要人工标注数据。
{% asset_img tp_1.png %}

#### 编码器(Encoder)

这里的**编码器**(`encoder`)，从**输入嵌入**(`input embeding`)开始，什么是输入嵌入？首先让我们从”`You CAT is a Lovely CAT.`“句子开始。我们有`6`个单词组成的句子，目标就是将其转化为`token`。在这里我们将句子分割成单个单词（为了简单起见），每个单词表示为一个`token`，下一步我们将要使这些单词映射为数字。这些数字代表的含义是在词汇表的位置。假设有一个包含了所有单词的词汇表在训练集中，每个单词在词汇表中拥有一个位置，例如，”`You`“单词占据词汇表的位置是”`105`“，”`CAT`“单词占据词汇表的位置是”`6587`“等等，我们将这些数字（称为输入`ID`）映射到大小为512的向量中。
{% asset_img tp_2.png %}